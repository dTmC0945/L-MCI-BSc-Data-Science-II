{"cells":[{"cell_type":"markdown","id":"56139299-d64c-4c5d-8642-9eb4513dd3d4","metadata":{},"source":"Ensemble Learning and Random Forests\n====================================\n\n"},{"cell_type":"markdown","id":"8c9e8637-7834-4946-80ec-4e1c06aa0591","metadata":{},"source":[":PROPERTIES:\n:ID:       CCE460A7-E7A1-43FF-96E6-84F11D414780\n:CREATED:  16-09-2025\n\nThese are the code snippets used in Ensemble Learning and Random Forests\npart of Data Science II.\n\n"]},{"cell_type":"markdown","id":"bf6d7095-3106-46bd-bf0a-64c08cc8ea33","metadata":{},"source":["\n### Table of Contents\n\n"]},{"cell_type":"markdown","id":"772f31aa-f65f-4e2b-b92c-bb62c8eb13a5","metadata":{},"source":["-   [BROKEN LINK: introduction]\n    -   [BROKEN LINK: voting-classifiers]\n-   [BROKEN LINK: bagging-and-pasting]\n    -   [BROKEN LINK: using-scikit-learn]\n    -   [BROKEN LINK: out-of-bag-evaluation]\n-   [BROKEN LINK: random-forests]\n    -   [BROKEN LINK: feature-importance]\n-   [BROKEN LINK: boosting]\n    -   [BROKEN LINK: adaboost]\n    -   [BROKEN LINK: gradient-boosting]\n\n"]},{"cell_type":"markdown","id":"93ffe761-dddd-42c3-ae7a-b818e47ed0cc","metadata":{},"source":["\n### Introduction\n\n"]},{"cell_type":"markdown","id":"773ff185-4904-44ae-90ae-c1ba8ba177a9","metadata":{},"source":["We start with making sure the python version is at least `3.7` and that the\n[scikit-learn](https://scikit-learn.org/stable/) (an excellent AI/ML module for python)  module is at least 1.0.1:\n\n"]},{"cell_type":"code","execution_count":1,"id":"c809dc29-8922-4ce7-bc5f-cd9265a6a1d4","metadata":{},"outputs":[],"source":["import sys\n\nassert sys.version_info >= (3, 7)\n\nfrom packaging import version\nimport sklearn\n\nassert version.parse(sklearn.__version__) >= version.parse(\"1.0.1\")"]},{"cell_type":"markdown","id":"a4d1914a-152a-4e53-b800-a57e409e10fd","metadata":{},"source":["Here we now define of matplotlib configurations and define some customisations\nto the plot:\n\n"]},{"cell_type":"code","execution_count":1,"id":"06674f39-67fe-4b88-be91-4d9083204d9f","metadata":{},"outputs":[],"source":["import matplotlib.pyplot as plt\n\nplt.rc('font', size=14)\nplt.rc('axes', labelsize=14, titlesize=14)\nplt.rc('legend', fontsize=14)\nplt.rc('xtick', labelsize=10)\nplt.rc('ytick', labelsize=10)"]},{"cell_type":"markdown","id":"ca95ca56-2d92-43cf-a259-1fa06781c253","metadata":{},"source":["Now we create the `images/` folder (if it doesn't already exist which `mkdir` comes\ninto play), and define a `save_fig()` function which is used through this script\nto save the figures:\n\n"]},{"cell_type":"code","execution_count":1,"id":"49a4c303-8576-440d-abfd-cbdc18a3844c","metadata":{},"outputs":[],"source":["import matplotlib.pyplot as plt\nfrom pathlib import Path\n\nIMAGES_PATH = Path() / \"images\" / \"Ensemble-Learning-and-Random-Forests\"\nIMAGES_PATH.mkdir(parents=True, exist_ok=True)\n\ndef save_fig(fig_id, tight_layout=True, fig_extension=\"pdf\", resolution=300):\n    \"\"\"Saves the figures with proper extention\n\n    Function checks the given PATH and saves the figures based on predefined\n    conditions.\n\n    Parameters\n    ----------\n    fig_id : string\n        FILENAME of the plot\n    tight_layout : boolean\n        Whether the figure should be tight or not\n    fig_extension : string\n        type the extention of the figure (i.e., pdf, svg)\n    resolution : integer\n        Resolution of the saved image\n\n    Examples\n    --------\n    save_fig(\"large-margin-classification-plot\")\n\n    \"\"\"\n    \n    path = IMAGES_PATH / f\"{fig_id}.{fig_extension}\"\n    if tight_layout:\n        plt.tight_layout()\n    plt.savefig(path, format=fig_extension, dpi=resolution)"]},{"cell_type":"markdown","id":"1e5f375a-e77b-4a36-a1a5-f3b5acbf398e","metadata":{},"source":["\n#### Voting Classifiers\n\n"]},{"cell_type":"code","execution_count":1,"id":"c9ee3bd2-a460-42a7-bd05-54007e070848","metadata":{},"outputs":[],"source":["import matplotlib.pyplot as plt\nimport numpy as np\n\nheads_proba = 0.51\nnp.random.seed(42)\ncoin_tosses = (np.random.rand(10000, 10) < heads_proba).astype(np.int32)\ncumulative_heads = coin_tosses.cumsum(axis=0)\ncumulative_heads_ratio = cumulative_heads / np.arange(1, 10001).reshape(-1, 1)\n\nplt.figure(figsize=(8, 3.5))\nplt.plot(cumulative_heads_ratio)\nplt.plot([0, 10000], [0.51, 0.51], \"k--\", linewidth=2, label=\"51%\")\nplt.plot([0, 10000], [0.5, 0.5], \"k-\", label=\"50%\")\nplt.xlabel(\"Number of coin tosses\")\nplt.ylabel(\"Heads ratio\")\nplt.legend(loc=\"lower right\")\nplt.axis([0, 10000, 0.42, 0.58])\n\nsave_fig(\"law-of-large-numbers\")\nplt.show()"]},{"cell_type":"markdown","id":"e39ebc62-0659-43d8-984f-858648cf8eb0","metadata":{},"source":["We now build a voting classifier:\n\n"]},{"cell_type":"code","execution_count":1,"id":"62efbe59-9d89-4e7a-b3e6-435813f4b257","metadata":{},"outputs":[],"source":["from sklearn.datasets import make_moons\nfrom sklearn.ensemble import RandomForestClassifier, VotingClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.svm import SVC\nX, y = make_moons(n_samples=500, noise=0.30, random_state=42)\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\nvoting_clf = VotingClassifier(\n    estimators=[\n        ('lr', LogisticRegression(random_state=42)),\n        ('rf', RandomForestClassifier(random_state=42)),\n        ('svc', SVC(random_state=42))\n    ]\n)\nvoting_clf.fit(X_train, y_train)"]},{"cell_type":"code","execution_count":1,"id":"a811b707-f1ed-42d1-bb59-4b56b06c6bbf","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"RF-3\n#+begin_example\nlr = 0.864\nrf = 0.896\nsvc = 0.896\n#+end_example"}],"source":["for name, clf in voting_clf.named_estimators_.items():\n    print(name, \"=\", clf.score(X_test, y_test))"]},{"cell_type":"code","execution_count":1,"id":"4cbb8114-1d77-49a9-9361-bd4d91f6c36d","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"RF-4\n#+begin_example\n[1]\n[array([1]), array([1]), array([0])]\n0.912\n#+end_example"}],"source":["print(voting_clf.predict(X_test[:1]))\nprint([clf.predict(X_test[:1]) for clf in voting_clf.estimators_])\nprint(voting_clf.score(X_test, y_test))"]},{"cell_type":"code","execution_count":1,"id":"4364d5d7-dc31-48b6-8d8b-a47e88145aa5","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"RF-5\n#+begin_example\n0.92\n#+end_example"}],"source":["voting_clf.voting = \"soft\"\nvoting_clf.named_estimators[\"svc\"].probability = True\nvoting_clf.fit(X_train, y_train)\nprint(voting_clf.score(X_test, y_test))"]},{"cell_type":"markdown","id":"3439e4be-7871-4f83-b47e-8685d44d9a42","metadata":{},"source":["\n### Bagging and Pasting\n\n"]},{"cell_type":"markdown","id":"5fc03520-e66f-4a9b-8aa6-e75dc8b6db51","metadata":{},"source":["\n#### Using Scikit-Learn\n\n"]},{"cell_type":"code","execution_count":1,"id":"0968bb9d-4545-4ab6-9d00-a2a1dce3ff5d","metadata":{},"outputs":[],"source":["from sklearn.ensemble import BaggingClassifier\nfrom sklearn.tree import DecisionTreeClassifier\n\nbag_clf = BaggingClassifier(DecisionTreeClassifier(), n_estimators=500,\n                            max_samples=100, n_jobs=-1, random_state=42)\nbag_clf.fit(X_train, y_train)"]},{"cell_type":"code","execution_count":1,"id":"d647a97f-5139-4bab-9e07-40a20dd28a7a","metadata":{},"outputs":[],"source":["def plot_decision_boundary(clf, X, y, alpha=1.0):\n    axes=[-1.5, 2.4, -1, 1.5]\n    x1, x2 = np.meshgrid(np.linspace(axes[0], axes[1], 100),\n                         np.linspace(axes[2], axes[3], 100))\n    X_new = np.c_[x1.ravel(), x2.ravel()]\n    y_pred = clf.predict(X_new).reshape(x1.shape)\n    \n    plt.contourf(x1, x2, y_pred, alpha=0.3 * alpha, cmap='Wistia')\n    plt.contour(x1, x2, y_pred, cmap=\"Greys\", alpha=0.8 * alpha)\n    colors = [\"#78785c\", \"#c47b27\"]\n    markers = (\"o\", \"^\")\n    for idx in (0, 1):\n        plt.plot(X[:, 0][y == idx], X[:, 1][y == idx],\n                 color=colors[idx], marker=markers[idx], linestyle=\"none\")\n    plt.axis(axes)\n    plt.xlabel(r\"$x_1$\")\n    plt.ylabel(r\"$x_2$\", rotation=0)\n\ntree_clf = DecisionTreeClassifier(random_state=42)\ntree_clf.fit(X_train, y_train)\n\nfig, axes = plt.subplots(ncols=2, figsize=(10, 4), sharey=True)\nplt.sca(axes[0])\nplot_decision_boundary(tree_clf, X_train, y_train)\nplt.title(\"Decision Tree\")\nplt.sca(axes[1])\nplot_decision_boundary(bag_clf, X_train, y_train)\nplt.title(\"Decision Trees with Bagging\")\nplt.ylabel(\"\")\n\nsave_fig(\"decision-tree-without-and-with-bagging-plot\")\nplt.show()"]},{"cell_type":"markdown","id":"91f4877e-215f-4d17-a643-c33f2b5c7022","metadata":{},"source":["\n#### Out-of-Bag Evaluation\n\n"]},{"cell_type":"code","execution_count":1,"id":"2f46b5f6-3bdf-463e-8d3c-fa970b0e90d1","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"RF-8\n#+begin_example\n0.896\n#+end_example"}],"source":["bag_clf = BaggingClassifier(DecisionTreeClassifier(), n_estimators=500,\n                            oob_score=True, n_jobs=-1, random_state=42)\nbag_clf.fit(X_train, y_train)\nprint(bag_clf.oob_score_)"]},{"cell_type":"code","execution_count":1,"id":"d15b6350-6991-4253-a453-ec73037ea76f","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"RF-9\n#+begin_example\n0.912\n#+end_example"}],"source":["from sklearn.metrics import accuracy_score\n\ny_pred = bag_clf.predict(X_test)\nprint(accuracy_score(y_test, y_pred))"]},{"cell_type":"code","execution_count":1,"id":"6c21c91f-49f3-4666-abd2-3c2c04f03c9d","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"RF-A1\n#+begin_example\n[[0.32352941 0.67647059]\r\n [0.3375     0.6625    ]\r\n [1.         0.        ]]\n#+end_example"}],"source":["print(bag_clf.oob_decision_function_[:3])  # probas for the first 3 instances"]},{"cell_type":"markdown","id":"fdf55fdc-e6bd-4e93-9c3a-3c11e7348b37","metadata":{},"source":["\n### Random Forests\n\n"]},{"cell_type":"code","execution_count":1,"id":"1a9feab6-0793-492d-9fb4-49eb402aebe5","metadata":{},"outputs":[],"source":["from sklearn.ensemble import RandomForestClassifier\n\nrnd_clf = RandomForestClassifier(n_estimators=500, max_leaf_nodes=16,\n                                 n_jobs=-1, random_state=42)\nrnd_clf.fit(X_train, y_train)\ny_pred_rf = rnd_clf.predict(X_test)"]},{"cell_type":"code","execution_count":1,"id":"e45a9bcf-4d99-4fdd-a811-585a14c5d79e","metadata":{},"outputs":[],"source":["bag_clf = BaggingClassifier(\n    DecisionTreeClassifier(max_features=\"sqrt\", max_leaf_nodes=16),\n    n_estimators=500, n_jobs=-1, random_state=42)"]},{"cell_type":"code","execution_count":1,"id":"c88aaac9-f251-4213-8e3a-15f9134db72d","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"RF-A4\n#+begin_example\n0.11 sepal length (cm)\r\n0.02 sepal width (cm)\r\n0.44 petal length (cm)\r\n0.42 petal width (cm)\n#+end_example"}],"source":["from sklearn.datasets import load_iris\n\niris = load_iris(as_frame=True)\nrnd_clf = RandomForestClassifier(n_estimators=500, random_state=42)\nrnd_clf.fit(iris.data, iris.target)\nfor score, name in zip(rnd_clf.feature_importances_, iris.data.columns):\n    print(round(score, 2), name)"]},{"cell_type":"code","execution_count":1,"id":"1c715c14-ade1-4c6c-9320-624de3934fff","metadata":{},"outputs":[],"source":["from sklearn.datasets import fetch_openml\n\nX_mnist, y_mnist = fetch_openml('mnist_784', return_X_y=True, as_frame=False,\n                                parser='auto')\n\nrnd_clf = RandomForestClassifier(n_estimators=100, random_state=42)\nrnd_clf.fit(X_mnist, y_mnist)\n\nheatmap_image = rnd_clf.feature_importances_.reshape(28, 28)\nplt.imshow(heatmap_image, cmap=\"hot\")\ncbar = plt.colorbar(ticks=[rnd_clf.feature_importances_.min(),\n                           rnd_clf.feature_importances_.max()])\ncbar.ax.set_yticklabels(['Not important', 'Very important'], fontsize=14)\n\nsave_fig(\"mnist-feature-importance-plot\")\nplt.show()"]},{"cell_type":"markdown","id":"81651f2e-5309-4b32-ac89-af55f794834a","metadata":{},"source":["\n### Boosting\n\n"]},{"cell_type":"markdown","id":"e949e129-23d1-4e88-9517-ab96d76b5d93","metadata":{},"source":["\n#### AdaBoost\n\n"]},{"cell_type":"code","execution_count":1,"id":"51c4fd8f-9862-4ccb-8463-38604001f0c4","metadata":{},"outputs":[],"source":["m = len(X_train)\n\nfig, axes = plt.subplots(ncols=2, figsize=(10, 4), sharey=True)\nfor subplot, learning_rate in ((0, 1), (1, 0.5)):\n    sample_weights = np.ones(m) / m\n    plt.sca(axes[subplot])\n    for i in range(5):\n        svm_clf = SVC(C=0.2, gamma=0.6, random_state=42)\n        svm_clf.fit(X_train, y_train, sample_weight=sample_weights * m)\n        y_pred = svm_clf.predict(X_train)\n\n        error_weights = sample_weights[y_pred != y_train].sum()\n        r = error_weights / sample_weights.sum()  # equation 7-1\n        alpha = learning_rate * np.log((1 - r) / r)  # equation 7-2\n        sample_weights[y_pred != y_train] *= np.exp(alpha)  # equation 7-3\n        sample_weights /= sample_weights.sum()  # normalization step\n\n        plot_decision_boundary(svm_clf, X_train, y_train, alpha=0.4)\n        plt.title(f\"learning_rate = {learning_rate}\")\n    if subplot == 0:\n        plt.text(-0.75, -0.95, \"1\", fontsize=16)\n        plt.text(-1.05, -0.95, \"2\", fontsize=16)\n        plt.text(1.0, -0.95, \"3\", fontsize=16)\n        plt.text(-1.45, -0.5, \"4\", fontsize=16)\n        plt.text(1.36,  -0.95, \"5\", fontsize=16)\n    else:\n        plt.ylabel(\"\")\n\nsave_fig(\"boosting-plot\")\nplt.show()"]},{"cell_type":"code","execution_count":1,"id":"1a2da73f-4aec-497b-aaf8-0c880dd82d0f","metadata":{},"outputs":[],"source":["from sklearn.ensemble import AdaBoostClassifier\n\nada_clf = AdaBoostClassifier(\n    DecisionTreeClassifier(max_depth=1), n_estimators=30,\n    learning_rate=0.5, random_state=42)\nada_clf.fit(X_train, y_train)"]},{"cell_type":"markdown","id":"8bcb90a6-8dc2-4e7a-b5b7-0d038a12ce0c","metadata":{},"source":["\n#### Gradient Boosting\n\n"]},{"cell_type":"code","execution_count":1,"id":"59eb9d69-bc9e-49dc-b7f3-37c62204b2be","metadata":{},"outputs":[],"source":["import numpy as np\nfrom sklearn.tree import DecisionTreeRegressor\n\nnp.random.seed(42)\nX = np.random.rand(100, 1) - 0.5\ny = 3 * X[:, 0] ** 2 + 0.05 * np.random.randn(100)  # y = 3x^{2} + Gaussian noise\n\ntree_reg1 = DecisionTreeRegressor(max_depth=2, random_state=42)\ntree_reg1.fit(X, y)"]},{"cell_type":"code","execution_count":1,"id":"89d91b5c-4b9a-4309-9d11-e10d69fdb1b3","metadata":{},"outputs":[],"source":["y2 = y - tree_reg1.predict(X)\ntree_reg2 = DecisionTreeRegressor(max_depth=2, random_state=43)\ntree_reg2.fit(X, y2)"]},{"cell_type":"code","execution_count":1,"id":"54a0d723-8c85-48f2-b565-fb28b86844a6","metadata":{},"outputs":[],"source":["y3 = y2 - tree_reg2.predict(X)\ntree_reg3 = DecisionTreeRegressor(max_depth=2, random_state=44)\ntree_reg3.fit(X, y3)"]},{"cell_type":"code","execution_count":1,"id":"9e534a95-b071-431c-925b-d9ef2a9bd771","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"GP-4\n#+begin_example\n[0.49484029 0.04021166 0.75026781]\n#+end_example"}],"source":["X_new = np.array([[-0.4], [0.], [0.5]])\nprint(sum(tree.predict(X_new) for tree in (tree_reg1, tree_reg2, tree_reg3)))"]},{"cell_type":"code","execution_count":1,"id":"08fbe916-2224-48ad-b9a8-be884dc44c81","metadata":{},"outputs":[],"source":["def plot_predictions(regressors, X, y, axes, style,\n                     label=None, data_style=\"b.\", data_label=None):\n    x1 = np.linspace(axes[0], axes[1], 500)\n    y_pred = sum(regressor.predict(x1.reshape(-1, 1))\n                 for regressor in regressors)\n    plt.plot(X[:, 0], y, data_style, label=data_label)\n    plt.plot(x1, y_pred, style, linewidth=2, label=label)\n    if label or data_label:\n        plt.legend(loc=\"upper center\")\n    plt.axis(axes)\n\nplt.figure(figsize=(11, 11))\n\nplt.subplot(3, 2, 1)\nplot_predictions([tree_reg1], X, y, axes=[-0.5, 0.5, -0.2, 0.8], style=\"g-\",\n                 label=\"$h_1(x_1)$\", data_label=\"Training set\")\nplt.ylabel(\"$y$  \", rotation=0)\nplt.title(\"Residuals and tree predictions\")\n\nplt.subplot(3, 2, 2)\nplot_predictions([tree_reg1], X, y, axes=[-0.5, 0.5, -0.2, 0.8], style=\"r-\",\n                 label=\"$h(x_1) = h_1(x_1)$\", data_label=\"Training set\")\nplt.title(\"Ensemble predictions\")\n\nplt.subplot(3, 2, 3)\nplot_predictions([tree_reg2], X, y2, axes=[-0.5, 0.5, -0.4, 0.6], style=\"g-\",\n                 label=\"$h_2(x_1)$\", data_style=\"k+\",\n                 data_label=\"Residuals: $y - h_1(x_1)$\")\nplt.ylabel(\"$y$  \", rotation=0)\n\nplt.subplot(3, 2, 4)\nplot_predictions([tree_reg1, tree_reg2], X, y, axes=[-0.5, 0.5, -0.2, 0.8],\n                  style=\"r-\", label=\"$h(x_1) = h_1(x_1) + h_2(x_1)$\")\n\nplt.subplot(3, 2, 5)\nplot_predictions([tree_reg3], X, y3, axes=[-0.5, 0.5, -0.4, 0.6], style=\"g-\",\n                 label=\"$h_3(x_1)$\", data_style=\"k+\",\n                 data_label=\"Residuals: $y - h_1(x_1) - h_2(x_1)$\")\nplt.xlabel(\"$x_1$\")\nplt.ylabel(\"$y$  \", rotation=0)\n\nplt.subplot(3, 2, 6)\nplot_predictions([tree_reg1, tree_reg2, tree_reg3], X, y,\n                 axes=[-0.5, 0.5, -0.2, 0.8], style=\"r-\",\n                 label=\"$h(x_1) = h_1(x_1) + h_2(x_1) + h_3(x_1)$\")\nplt.xlabel(\"$x_1$\")\n\nsave_fig(\"gradient_boosting_plot\")\nplt.show()"]},{"cell_type":"code","execution_count":1,"id":"70dd4430-28ef-4247-90ec-fa0bcd11497d","metadata":{},"outputs":[],"source":["from sklearn.ensemble import GradientBoostingRegressor\n\ngbrt = GradientBoostingRegressor(max_depth=2, n_estimators=3,\n                                 learning_rate=1.0, random_state=42)\ngbrt.fit(X, y)"]},{"cell_type":"code","execution_count":1,"id":"b5518acc-087d-4740-bb07-3ddd10883ad5","metadata":{},"outputs":[],"source":["gbrt_best = GradientBoostingRegressor(\n    max_depth=2, learning_rate=0.05, n_estimators=500,\n    n_iter_no_change=10, random_state=42)\ngbrt_best.fit(X, y)"]},{"cell_type":"code","execution_count":1,"id":"da2fee12-2552-4ae5-9e46-7d5df6ce6722","metadata":{},"outputs":[],"source":["fig, axes = plt.subplots(ncols=2, figsize=(10, 4), sharey=True)\n\nplt.sca(axes[0])\nplot_predictions([gbrt], X, y, axes=[-0.5, 0.5, -0.1, 0.8], style=\"r-\",\n                 label=\"Ensemble predictions\")\nplt.title(f\"learning_rate={gbrt.learning_rate}, \"\n          f\"n_estimators={gbrt.n_estimators_}\")\nplt.xlabel(\"$x_1$\")\nplt.ylabel(\"$y$\", rotation=0)\n\nplt.sca(axes[1])\nplot_predictions([gbrt_best], X, y, axes=[-0.5, 0.5, -0.1, 0.8], style=\"r-\")\nplt.title(f\"learning_rate={gbrt_best.learning_rate}, \"\n          f\"n_estimators={gbrt_best.n_estimators_}\")\nplt.xlabel(\"$x_1$\")\n\nsave_fig(\"gbrt-learning-rate-plot\")\nplt.show()"]},{"cell_type":"code","execution_count":1,"id":"dfbf9c0c-b73c-4599-8d6f-632fed14fd31","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"GP-9\n#+begin_example\n92\n#+end_example"}],"source":["print(gbrt_best.n_estimators_)"]},{"cell_type":"code","execution_count":1,"id":"3612d636-dc7a-44ad-8d0f-20eb4a20c628","metadata":{},"outputs":[],"source":["from sklearn.pipeline import make_pipeline\nfrom sklearn.compose import make_column_transformer\nfrom sklearn.ensemble import HistGradientBoostingRegressor\nfrom sklearn.preprocessing import OrdinalEncoder\nhgb_reg = make_pipeline(\nmake_column_transformer((OrdinalEncoder(), [\"ocean_proximity\"]),\nremainder=\"passthrough\"),\nHistGradientBoostingRegressor(categorical_features=[0], random_state=42)\n)\nhgb_reg.fit(housing, housing_labels)"]},{"cell_type":"code","execution_count":1,"id":"61238f93-659c-45b6-b496-e7c222d1148d","metadata":{},"outputs":[],"source":["from sklearn.ensemble import StackingClassifier\nstacking_clf = StackingClassifier(\n    estimators=[\n        ('lr', LogisticRegression(random_state=42)),\n        ('rf', RandomForestClassifier(random_state=42)),\n        ('svc', SVC(probability=True, random_state=42))\n    ],\n    final_estimator=RandomForestClassifier(random_state=43),\n\ncv=5 # number of cross-validation folds\n)\nstacking_clf.fit(X_train, y_train)"]}],"metadata":{"org":null,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.2"}},"nbformat":4,"nbformat_minor":5}