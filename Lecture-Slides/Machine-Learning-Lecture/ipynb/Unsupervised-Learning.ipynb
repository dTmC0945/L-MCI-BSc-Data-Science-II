{"cells":[{"cell_type":"markdown","id":"01d40121-3581-4070-a583-7c84ab4109ba","metadata":{},"source":"Code for Machine Learning and Data Science II\n=============================================\n\n"},{"cell_type":"markdown","id":"fbe96012-5895-45b7-acb5-e27592263c4a","metadata":{},"source":["These are the code snippets used in Unsupervised Learning\npart of `Machine Learning and Data Science II`.\n\n"]},{"cell_type":"markdown","id":"e2bdfecb-081f-45f8-865c-391f0e2a68a2","metadata":{},"source":["### Introduction\n\n"]},{"cell_type":"markdown","id":"e66b8271-05ca-4cd7-aba4-8e7a2bf196ab","metadata":{},"source":["The following is a custom package written to handle plotting\nand other functions required by the lecture.\n\n"]},{"cell_type":"code","execution_count":1,"id":"af4d2d27-d1d8-478c-9e21-27cb2e5f2345","metadata":{},"outputs":[],"source":["import ChalcedonPy as cp # custom-pakcage for lecture materials and publications\nSAVE_PATH = \"Unsupervised-Learning\" # sets the default save path\nstyle=\"web\" # sets the default rcParams stylce sheet"]},{"cell_type":"markdown","id":"3e31188b-4aff-4e5e-bf38-9a38b3b283cc","metadata":{},"source":["### Clustering\n\n"]},{"cell_type":"markdown","id":"9cba95a4-c1a2-4782-8240-c21350bfd961","metadata":{},"source":["Clustering is the task of identifying similar instances and assigning them to\nclusters, or groups of similar instances. However, unlike classification,\nclustering is an unsupervised task.\n\nLet's make a plot comparing labelled and unlabelled plot of data.\n\n"]},{"cell_type":"code","execution_count":1,"id":"d13b1e70-56a4-41ba-8712-8e6fe8c16640","metadata":{},"outputs":[],"source":["import matplotlib.pyplot as plt\nfrom sklearn.datasets import load_iris\n\ndata = load_iris()\nX = data.data\ny = data.target\ndata.target_names"]},{"cell_type":"markdown","id":"77719a06-7fe9-459f-852d-8a3e32b21f4c","metadata":{},"source":["\n| FUNCTION|DEFINITION|MORE INFO|\n| load<sub>iris</sub>()|Load and return the iris dataset (classification).|[a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_iris.html\">Link</a](a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_iris.html\">Link</a)|\n\nThe following code plots the aforementioned data.\n\n"]},{"cell_type":"code","execution_count":1,"id":"726a140a-42e0-4b87-81b3-442484daf1cc","metadata":{},"outputs":[],"source":["plt.figure(figsize=(9, 3.5))\n\nplt.subplot(121)\nplt.plot(X[y==0, 2], X[y==0, 3], \"o\", label=\"Iris setosa\")\nplt.plot(X[y==1, 2], X[y==1, 3], \"s\", label=\"Iris versicolor\")\nplt.plot(X[y==2, 2], X[y==2, 3], \"^\", label=\"Iris virginica\")\nplt.xlabel(\"Petal length\")\nplt.ylabel(\"Petal width\")\nplt.legend()\n\nplt.subplot(122)\nplt.scatter(X[:, 2], X[:, 3], marker=\".\")\nplt.xlabel(\"Petal length\")\nplt.tick_params(labelleft=False)\nplt.gca().set_axisbelow(True)\n\nplt.show()\n             filepath = SAVE_PATH,\n             style = style,\n             close = True)"]},{"cell_type":"markdown","id":"0874dc1c-a84e-44ab-8f69-7bdaea5bfb81","metadata":{},"source":["While on the right it is easy to separate, on the right you it is not possible, this is\nwhere clustering algorithms step in as many of them can easily detect the lower-left\ncluster.\n\nNOTE the following code shows how a Gaussian mixture model\n(explained at the end of this chapter) can actually separate these clusters\npretty well using all 4 features:\n\n-   petal length & width,\n-   sepal length & width.\n\nThis code maps each cluster to a class. Instead of hard coding the mapping,\nthe code picks the most common class for each cluster\nusing the scipy.stats.mode() function:\n\n"]},{"cell_type":"code","execution_count":1,"id":"1987cb7b-457b-4a49-9afe-794b04e458e2","metadata":{},"outputs":[],"source":["import numpy as np\nfrom scipy import stats\nfrom sklearn.mixture import GaussianMixture\n\ny_pred = GaussianMixture(n_components=3, random_state=42).fit(X).predict(X)\n\nmapping = {}\nfor class_id in np.unique(y):\n    mode, _ = stats.mode(y_pred[y==class_id])\n    mapping[mode] = class_id\n\ny_pred = np.array([mapping[cluster_id] for cluster_id in y_pred])"]},{"cell_type":"markdown","id":"c85566f3-a1db-464c-9d18-d35b47c35f49","metadata":{},"source":["\n| FUNCTION|DEFINITION|MORE INFO|\n| GaussianMixture()|Representation of a Gaussian mixture model probability distribution. This class allows to estimate the parameters of a Gaussian mixture distribution.|[a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.mixture.GaussianMixture.html#gaussianmixture\">Link</a](a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.mixture.GaussianMixture.html#gaussianmixture\">Link</a)|\n\n"]},{"cell_type":"code","execution_count":1,"id":"fc61250f-8faf-45db-ae85-997a8d300c71","metadata":{},"outputs":[],"source":["plt.plot(X[y_pred==0, 2], X[y_pred==0, 3], \"o\", label=\"Cluster 1\")\nplt.plot(X[y_pred==1, 2], X[y_pred==1, 3], \"s\", label=\"Cluster 2\")\nplt.plot(X[y_pred==2, 2], X[y_pred==2, 3], \"^\", label=\"Cluster 3\")\n\nplt.xlabel(\"Petal length\")\nplt.ylabel(\"Petal width\")\nplt.legend(loc=\"upper left\")\n\nplt.show()\n             filepath = SAVE_PATH,\n             style = style,\n             close = True)"]},{"cell_type":"markdown","id":"5fc9bc91-ccf2-4170-865b-dfb65d61f150","metadata":{},"source":["And the ratio of iris plants assigned to right cluster ?\n\n"]},{"cell_type":"code","execution_count":1,"id":"2feb46a5-5478-418a-9aa1-a3f0bfe6757f","metadata":{},"outputs":[],"source":["(y_pred==y).sum() / len(y_pred)"]},{"cell_type":"markdown","id":"2cecbd1c-ce79-4a06-a35d-485c97b0f15f","metadata":{},"source":["As can be seen for this plot the Gaussian mixture works really well. But don't worry about\nit now as we will have a look at it later.\n\n"]},{"cell_type":"markdown","id":"fdc72140-eda1-4717-8b52-7d82410f4924","metadata":{},"source":["### K-Means\n\n"]},{"cell_type":"markdown","id":"f74d1506-046d-423d-a3a7-f9b7639f1b32","metadata":{},"source":["K means clustering, assigns data points to one of the K clusters depending\non their distance from the centre of the clusters. It starts by randomly\nassigning the clusters centred in the space. Then each data point assign\nto one of the cluster based on its distance from centroid of the cluster.\nAfter assigning each point to one of the cluster, new cluster centroids are assigned.\n\nThis process iterative until it finds good cluster. In the analysis we assume\nthat number of cluster is given in advanced and we have to put points in one of\nthe group.\n\n"]},{"cell_type":"markdown","id":"3ec82864-d8dd-451a-b161-dc0e94046cc1","metadata":{},"source":["#### Fit & Predict\n\n"]},{"cell_type":"markdown","id":"112e00e9-d485-48c0-9478-22d06e1389fd","metadata":{},"source":["Let's create some unlabelled dataset.\n\n"]},{"cell_type":"code","execution_count":1,"id":"04acd845-e8d6-4db1-9a82-947416da312c","metadata":{},"outputs":[],"source":["from sklearn.cluster import KMeans\nfrom sklearn.datasets import make_blobs\n\nblob_centers = np.array([[ 0.2,  2.3], [-1.5 ,  2.3], [-2.8,  1.8],\n                         [-2.8,  2.8], [-2.8,  1.3]])\nblob_std = np.array([0.4, 0.3, 0.1, 0.1, 0.1])\nX, y = make_blobs(n_samples=2000, centers=blob_centers, cluster_std=blob_std,\n                  random_state=7)\n\nk = 5\nkmeans = KMeans(n_clusters=k, n_init=10, random_state=42)\ny_pred = kmeans.fit_predict(X)"]},{"cell_type":"markdown","id":"382036f5-8b87-455e-b03d-afabb9306d77","metadata":{},"source":["\n| FUNCTION|DEFINITION|MORE INFO|\n| KMeans()|K-Means clustering.|[a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html\">Link</a](a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html\">Link</a)|\n\nand plot it.\n\n"]},{"cell_type":"code","execution_count":1,"id":"8d21fc0f-fe29-4069-accc-f0fd9c59d9d4","metadata":{},"outputs":[],"source":["def plot_clusters(X, y=None):\n    plt.scatter(X[:, 0], X[:, 1], c=y, s=1)\n    plt.xlabel(\"$x_1$\")\n    plt.ylabel(\"$x_2$\", rotation=0)\n\nplt.figure(figsize=(8, 4))\nplot_clusters(X)\nplt.gca().set_axisbelow(True)\n\nplt.show()\n             filepath = SAVE_PATH,\n             style = style,\n             close = True)"]},{"cell_type":"markdown","id":"5aed25f9-c0c4-4026-ad68-49ffa490bc44","metadata":{},"source":["Each instance of the data is assigned to one of 5 k value.\n\n"]},{"cell_type":"code","execution_count":1,"id":"5e6e9592-d39b-4a4e-ae8d-5971e8e464f6","metadata":{},"outputs":[],"source":["print(y_pred)"]},{"cell_type":"code","execution_count":1,"id":"c910f239-92ac-4836-820d-8dc7af93d4cf","metadata":{},"outputs":[],"source":["y_pred is kmeans.labels_"]},{"cell_type":"markdown","id":"408281bc-19fe-4a19-9e9f-b39cb64932b1","metadata":{},"source":["Let's look at the coordinates of the 5 centroids (i.e., cluster centres).\n\n"]},{"cell_type":"code","execution_count":1,"id":"c9e33d04-dd25-454a-950d-111679182a1f","metadata":{},"outputs":[],"source":["print(kmeans.cluster_centers_)"]},{"cell_type":"code","execution_count":1,"id":"a75b365c-5466-4349-80bc-bbe380c22ba2","metadata":{},"outputs":[],"source":["print(kmeans.labels_)"]},{"cell_type":"markdown","id":"afbb8096-a3b1-49d8-b9de-f273fa5fea6f","metadata":{},"source":["Of course, we can predict the labels of new instances:\n\n"]},{"cell_type":"code","execution_count":1,"id":"ed82a59a-109a-4252-ac59-e8140b7afc90","metadata":{},"outputs":[],"source":["import numpy as np\n\nX_new = np.array([[0, 2], [3, 2], [-3, 3], [-3, 2.5]])\nprint(kmeans.predict(X_new))"]},{"cell_type":"markdown","id":"a9e6ef2d-ef6a-42d9-8038-9fda7e57867e","metadata":{},"source":["#### Decision Boundaries\n\n"]},{"cell_type":"markdown","id":"2663228f-08e9-4b83-8a0f-e6e22c0d617a","metadata":{},"source":["Let's plot the model's decision boundaries. This gives us a [Voronoi diagram](https://en.wikipedia.org/wiki/Voronoi_diagram).\n\n"]},{"cell_type":"code","execution_count":1,"id":"59434515-cfff-4296-b566-d9b462387bca","metadata":{},"outputs":[],"source":["def plot_data(X):\n    plt.plot(X[:, 0], X[:, 1], 'k.', markersize=2)\n\ndef plot_centroids(centroids, weights=None, circle_color='w', cross_color='k'):\n    if weights is not None:\n        centroids = centroids[weights > weights.max() / 10]\n    plt.scatter(centroids[:, 0], centroids[:, 1],\n                marker='o', s=35, linewidths=8,\n                color=circle_color, zorder=10, alpha=0.9)\n    plt.scatter(centroids[:, 0], centroids[:, 1],\n                marker='x', s=2, linewidths=12,\n                color=cross_color, zorder=11, alpha=1)\n\ndef plot_decision_boundaries(clusterer, X, resolution=1000, show_centroids=True,\n                             show_xlabels=True, show_ylabels=True):\n    mins = X.min(axis=0) - 0.1\n    maxs = X.max(axis=0) + 0.1\n    xx, yy = np.meshgrid(np.linspace(mins[0], maxs[0], resolution),\n                         np.linspace(mins[1], maxs[1], resolution))\n    Z = clusterer.predict(np.c_[xx.ravel(), yy.ravel()])\n    Z = Z.reshape(xx.shape)\n\n    plt.contourf(Z, extent=(mins[0], maxs[0], mins[1], maxs[1]),\n                cmap=\"Pastel2\")\n    plt.contour(Z, extent=(mins[0], maxs[0], mins[1], maxs[1]),\n                linewidths=1, colors='k')\n    plot_data(X)\n    if show_centroids:\n        plot_centroids(clusterer.cluster_centers_)\n\n    if show_xlabels:\n        plt.xlabel(\"$x_1$\")\n    else:\n        plt.tick_params(labelbottom=False)\n    if show_ylabels:\n        plt.ylabel(\"$x_2$\", rotation=0)\n    else:\n        plt.tick_params(labelleft=False)\n\nplt.figure(figsize=(8, 4))\nplot_decision_boundaries(kmeans, X)\nplt.show()\n             filepath = SAVE_PATH,\n             style = style,\n             close = True)"]},{"cell_type":"markdown","id":"f41c55ad-2680-4ded-9ab4-b1908ef136e3","metadata":{},"source":["It seems some edge cases are assigned the wrong value (look at pink-yellow) but the\nalgorithm seems to have clustered correctly.\n\n"]},{"cell_type":"markdown","id":"b8ac7339-ba4b-48ef-ad9e-3bfac693c572","metadata":{},"source":["#### Hard vs Soft Clustering\n\n"]},{"cell_type":"markdown","id":"75fa1043-341f-4fda-9745-314d80d949d2","metadata":{},"source":["Instead of assigning each instance to a single cluster, which is called hard\nclustering, it can be useful to give each instance a score per cluster,\nwhich is called soft clustering.\n\nThe transform() allows the distance measure from each instance to every centroid.\n\n"]},{"cell_type":"code","execution_count":1,"id":"95b2a415-ba30-42c0-9a86-dd66b27c5930","metadata":{},"outputs":[],"source":["print(kmeans.transform(X_new).round(2))"]},{"cell_type":"code","execution_count":1,"id":"d7432834-ff42-418f-ad71-c418cdc634d0","metadata":{},"outputs":[],"source":["print(np.linalg.norm(np.tile(X_new, (1, k)).reshape(-1, k, 2)\n               - kmeans.cluster_centers_, axis=2).round(2))"]},{"cell_type":"markdown","id":"93db5db5-156b-4075-a6ae-75068f8d40b3","metadata":{},"source":["\n| FUNCTION|DEFINITION|MORE INFO|\n| numpy.linalg.norm()|This function is able to return one of eight different matrix norms, or one of an infinite number of vector norms (described below), depending on the value of the ord parameter.|[a href=\"https://numpy.org/doc/stable/reference/generated/numpy.linalg.norm.html\">Link</a](a href=\"https://numpy.org/doc/stable/reference/generated/numpy.linalg.norm.html\">Link</a)|\n\n"]},{"cell_type":"markdown","id":"933f4ee8-1eb2-4777-96b7-6b2908b2959f","metadata":{},"source":["#### K-Means Algorithm\n\n"]},{"cell_type":"markdown","id":"084c251e-df22-4f0f-860c-01ae518ac512","metadata":{},"source":["$k$ means is one of the fastest clustering algorithms.\n\nThe algorithms steps are as follows:\n\n-   Initialise $k$ centroids randomly (i.e., $k$ distinct instances are randomly chosen\n    from the dataset and the centroids are placed at their locations)\n-   Repeat until convergence, meaning until centroid location are established.\n    -   Assign each instance to closest centroid.\n    -   Update centroids to be the $\\mu$ of instances assigned.\n\nFor this use Kmeans() which uses an optimised initialisation\ntechnique by default.\n\n"]},{"cell_type":"code","execution_count":1,"id":"56da53cc-e326-4c0f-89ad-ba6e59f8c69c","metadata":{},"outputs":[],"source":["kmeans_iter1 = KMeans(n_clusters=5, init=\"random\", n_init=1, max_iter=1,\n                      random_state=5)\nkmeans_iter2 = KMeans(n_clusters=5, init=\"random\", n_init=1, max_iter=2,\n                      random_state=5)\nkmeans_iter3 = KMeans(n_clusters=5, init=\"random\", n_init=1, max_iter=6,\n                      random_state=5)\nkmeans_iter1.fit(X)\nkmeans_iter2.fit(X)\nkmeans_iter3.fit(X)"]},{"cell_type":"code","execution_count":1,"id":"716b6e02-1f38-4ad0-8ca1-d9b3f2c6db17","metadata":{},"outputs":[],"source":["plt.figure(figsize=(10, 8))\n\nplt.subplot(321)\nplot_data(X)\nplot_centroids(kmeans_iter1.cluster_centers_, circle_color='r', cross_color='w')\nplt.ylabel(\"$x_2$\", rotation=0)\nplt.tick_params(labelbottom=False)\nplt.title(\"Update the centroids (initially randomly)\")\n\nplt.subplot(322)\nplot_decision_boundaries(kmeans_iter1, X, show_xlabels=False,\n                         show_ylabels=False)\nplt.title(\"Label the instances\")\n\nplt.subplot(323)\nplot_decision_boundaries(kmeans_iter1, X, show_centroids=False,\n                         show_xlabels=False)\nplot_centroids(kmeans_iter2.cluster_centers_)\n\nplt.subplot(324)\nplot_decision_boundaries(kmeans_iter2, X, show_xlabels=False,\n                         show_ylabels=False)\n\nplt.subplot(325)\nplot_decision_boundaries(kmeans_iter2, X, show_centroids=False)\nplot_centroids(kmeans_iter3.cluster_centers_)\n\nplt.subplot(326)\nplot_decision_boundaries(kmeans_iter3, X, show_ylabels=False)\n\nplt.show()\n             filepath = SAVE_PATH,\n             style = style,\n             close = True)"]},{"cell_type":"markdown","id":"530603cc-9307-44de-9235-77816aeef331","metadata":{},"source":["#### K-Means Variability\n\n"]},{"cell_type":"code","execution_count":1,"id":"6dc079da-88e2-4eca-852f-3383c1688fe0","metadata":{},"outputs":[],"source":["def plot_clusterer_comparison(clusterer1, clusterer2, X, title1=None,\n                              title2=None):\n    clusterer1.fit(X)\n    clusterer2.fit(X)\n\n    plt.figure(figsize=(12, 5))\n\n    plt.subplot(121)\n    plot_decision_boundaries(clusterer1, X)\n    if title1:\n        plt.title(title1)\n\n    plt.subplot(122)\n    plot_decision_boundaries(clusterer2, X, show_ylabels=False)\n    if title2:\n        plt.title(title2)\n\nkmeans_rnd_init1 = KMeans(n_clusters=5, init=\"random\", n_init=1, random_state=2)\nkmeans_rnd_init2 = KMeans(n_clusters=5, init=\"random\", n_init=1, random_state=9)\n\nplot_clusterer_comparison(kmeans_rnd_init1, kmeans_rnd_init2, X,\n                          \"Solution 1\",\n                          \"Solution 2 (with a different random init)\")\n\nplt.show()\n             filepath = SAVE_PATH,\n             style = style,\n             close = True)"]},{"cell_type":"code","execution_count":1,"id":"70099651-d6f9-47de-a9db-5e0e0e6514f8","metadata":{},"outputs":[],"source":["good_init = np.array([[-3, 3], [-3, 2], [-3, 1], [-1, 2], [0, 2]])\nkmeans = KMeans(n_clusters=5,\n                init=good_init,\n                n_init=1,\n                random_state=42)\nkmeans.fit(X)"]},{"cell_type":"markdown","id":"b27c9753-261e-4f42-afae-cf465f10a38f","metadata":{},"source":["We use inertia for figuring out which iteration is the best.\n\n"]},{"cell_type":"code","execution_count":1,"id":"391fd214-790f-46b1-a8cb-37013dc8360f","metadata":{},"outputs":[],"source":["kmeans.inertia_"]},{"cell_type":"code","execution_count":1,"id":"b433758a-24f8-4ad4-8444-d7d2480ed7c8","metadata":{},"outputs":[],"source":["kmeans.score(X)"]},{"cell_type":"markdown","id":"f0a50d6c-4053-41e1-abf0-0afb7c084aed","metadata":{},"source":["#### Mini-Batch K-Means\n\n"]},{"cell_type":"markdown","id":"6cbae4aa-0b87-4c0a-96b0-515655b9537f","metadata":{},"source":["Scikit-Learn also implements a variant of the K-Means algorithm that supports\nmini-batches.\n\n"]},{"cell_type":"code","execution_count":1,"id":"61a5cfe3-7a56-4b74-a856-fda96b86c536","metadata":{},"outputs":[],"source":["from sklearn.cluster import MiniBatchKMeans\n\nminibatch_kmeans = MiniBatchKMeans(n_clusters=5, n_init=3, random_state=42)\nminibatch_kmeans.fit(X)"]},{"cell_type":"markdown","id":"d5de5159-8835-4a24-99bc-0a3e132297af","metadata":{},"source":["NOTE: when n<sub>init</sub> was not set when creating a MiniBatchKMeans estimator,\nI explicitly set it to n<sub>init</sub>=3 to avoid a warning about the fact that the default\nvalue for this hyperparameter will change from 3 to \"auto\" in Scikit-Learn 1.4.\n\n"]},{"cell_type":"code","execution_count":1,"id":"4d81921b-cb43-415d-91ef-71c322ba647e","metadata":{},"outputs":[],"source":["print(minibatch_kmeans.inertia_)"]},{"cell_type":"markdown","id":"f538e147-31ca-4483-aff3-57f4056bb195","metadata":{},"source":["If the dataset does not fit in memory, the simplest option is to use\nthe memmap class.\n\nFirst load MNIST:\n\n"]},{"cell_type":"code","execution_count":1,"id":"517515ed-6e8d-4e94-8e27-0ae384eef848","metadata":{},"outputs":[],"source":["from sklearn.datasets import fetch_openml\n\nmnist = fetch_openml('mnist_784', as_frame=False, parser=\"auto\")"]},{"cell_type":"markdown","id":"3675501c-10b3-4d96-8f18-35c6a6adff1a","metadata":{},"source":["Once loaded, split the dataset:\n\n"]},{"cell_type":"code","execution_count":1,"id":"8f1449ae-1140-42cb-9371-8c3112d50f99","metadata":{},"outputs":[],"source":["X_train, y_train = mnist.data[:60000], mnist.target[:60000]\nX_test, y_test = mnist.data[60000:], mnist.target[60000:]"]},{"cell_type":"markdown","id":"0a97be9c-a76b-4162-ad3e-2de54608aeb1","metadata":{},"source":["Next, let's write the training set to a memmap:\n\n"]},{"cell_type":"code","execution_count":1,"id":"43d33504-0149-4652-9cfb-6e7c6e18d23b","metadata":{},"outputs":[],"source":["filename = \"my_mnist.mmap\"\nX_memmap = np.memmap(filename, dtype='float32', mode='write',\n                     shape=X_train.shape)\nX_memmap[:] = X_train\nX_memmap.flush()"]},{"cell_type":"code","execution_count":1,"id":"0a6bbaeb-eee7-4b93-b1ea-c36da80fb45c","metadata":{},"outputs":[],"source":["from sklearn.cluster import MiniBatchKMeans\n\nminibatch_kmeans = MiniBatchKMeans(n_clusters=10, batch_size=10,\n                                   n_init=3, random_state=42)\nminibatch_kmeans.fit(X_memmap)"]},{"cell_type":"code","execution_count":1,"id":"6e849909-bf07-4417-b885-3ff5f0507612","metadata":{},"outputs":[],"source":["from timeit import timeit\n\nmax_k = 100\ntimes = np.empty((max_k, 2))\ninertias = np.empty((max_k, 2))\nfor k in range(1, max_k + 1):\n    kmeans_ = KMeans(n_clusters=k, algorithm=\"lloyd\", n_init=10, random_state=42)\n    minibatch_kmeans = MiniBatchKMeans(n_clusters=k, n_init=10, random_state=42)\n    print(f\"\\r{k}/{max_k}\", end=\"\")  # \\r returns to the start of line\n    times[k - 1, 0] = timeit(\"kmeans_.fit(X)\", number=10, globals=globals())\n    times[k - 1, 1] = timeit(\"minibatch_kmeans.fit(X)\", number=10,\n                             globals=globals())\n    inertias[k - 1, 0] = kmeans_.inertia_\n    inertias[k - 1, 1] = minibatch_kmeans.inertia_\n\nplt.figure(figsize=(10, 4))\n\nplt.subplot(121)\nplt.plot(range(1, max_k + 1), inertias[:, 0], \"--\", label=\"K-Means\")\nplt.plot(range(1, max_k + 1), inertias[:, 1], \".-\", label=\"Mini-batch K-Means\")\nplt.xlabel(\"$k$\")\nplt.title(\"Inertia\")\nplt.legend()\nplt.axis([1, max_k, 0, 100])\nplt.grid()\n\nplt.subplot(122)\nplt.plot(range(1, max_k + 1), times[:, 0], \"--\", label=\"K-Means\")\nplt.plot(range(1, max_k + 1), times[:, 1], \".-\", label=\"Mini-batch K-Means\")\nplt.xlabel(\"$k$\")\nplt.title(\"Training time (seconds)\")\nplt.axis([1, max_k, 0, 4])\n\nplt.show()\n             filepath = SAVE_PATH,\n             style = style,\n             close = True)"]},{"cell_type":"markdown","id":"c8e040b0-ed44-43c5-a9d0-1fb001d18aa7","metadata":{},"source":["#### Finding the Optimal Cluster Number\n\n"]},{"cell_type":"markdown","id":"eb39f8aa-8bf2-4442-889a-2b1f06ffc266","metadata":{},"source":["Let's see what would the behaviour be if the cluster number is not 5 with one being\nlower (3) and one being higher (8).\n\n"]},{"cell_type":"code","execution_count":1,"id":"37a6f319-2e71-47ac-bab6-98ebeb105150","metadata":{},"outputs":[],"source":["kmeans_k3 = KMeans(n_clusters=3, n_init=10, random_state=42)\nkmeans_k8 = KMeans(n_clusters=8, n_init=10, random_state=42)\n\nplt.figure(figsize=(10, 4))\nplot_clusterer_comparison(kmeans_k3, kmeans_k8, X, \"$k=3$\", \"$k=8$\")\nplt.show()\n             filepath = SAVE_PATH,\n             style = style,\n             close = True)"]},{"cell_type":"markdown","id":"f5898331-0405-42f1-8a64-8a11caa7fffc","metadata":{},"source":["It seems lower one (k=3) has underestimated and the higher one (k=8) is over\nestimated. But what about their inertia values. Let's have a look at them.\n\n"]},{"cell_type":"code","execution_count":1,"id":"fdd3d41f-8872-450e-8941-d4497121b05e","metadata":{},"outputs":[],"source":["kmeans_k3.inertia_"]},{"cell_type":"code","execution_count":1,"id":"82a6dda3-86a9-41c1-80f4-1f7c7d40602a","metadata":{},"outputs":[],"source":["kmeans_k8.inertia_"]},{"cell_type":"markdown","id":"7f53c33c-99fe-4cc9-9205-e05805cf97a5","metadata":{},"source":["It seems as we increase the $k$ value, the inertia value gets higher, but as we see\nit does not necessarily get better. This is because as there are more clusters declared,\non average, the instances will be closer to one, making the inertia lower.\n\nTo see this visually, let's plot it and see.\n\n"]},{"cell_type":"code","execution_count":1,"id":"9636fbb5-e6d8-4a9a-a942-7c2d4167e0d4","metadata":{},"outputs":[],"source":["kmeans_per_k = [KMeans(n_clusters=k, n_init=10, random_state=42).fit(X)\n                for k in range(1, 10)]\ninertias = [model.inertia_ for model in kmeans_per_k]"]},{"cell_type":"code","execution_count":1,"id":"03c49033-f8da-4ae6-b2ef-c3222e20ffca","metadata":{},"outputs":[],"source":["plt.figure(figsize=(8, 3.5))\n\nplt.plot(range(1, 10), inertias, \"o-\")\nplt.xlabel(\"$k$\")\nplt.ylabel(\"Inertia\")\n\nplt.annotate(\"\", xy=(4, inertias[3]), xytext=(4.45, 650),\n             arrowprops=dict(facecolor='black', shrink=0.1))\n\nplt.text(4.5, 650, \"Elbow\", horizontalalignment=\"center\")\nplt.axis([1, 8.5, 0, 1300])\n\nplt.show()\n             filepath = SAVE_PATH,\n             style = style,\n             close = True)"]},{"cell_type":"markdown","id":"a0b4bb57-45d6-482f-8140-9a25c883905b","metadata":{},"source":["As you can see, there is an elbow at $k=4$, meaning less clusters would be bad,\nand more clusters would not help much and might cut clusters in half.\n\nSo $k=4$ is a pretty good choice. Of course in this example it is not perfect since\nit means that the two blobs in the lower left will be considered as just a single\nluster, but it's a pretty good clustering nonetheless.\n\n"]},{"cell_type":"code","execution_count":1,"id":"5ec585c4-3d49-48f9-9229-0f197a456667","metadata":{},"outputs":[],"source":["plot_decision_boundaries(kmeans_per_k[4 - 1], X)\nplt.show()\n             filepath = SAVE_PATH,\n             style = style,\n             close = True)"]},{"cell_type":"markdown","id":"6310c29f-6001-4b19-885c-df4a686e2bd2","metadata":{},"source":["It would be better to see if there is an alternative method for estimating a good cluster number.\nAn alternative method is called silhouette score, the mean silhouette coefficient over all instances.\n\nAn instance's silhouette coefficient is calculated to be:\n\n\\begin{equation*}\ns(i) = \\frac{b(i) - a(i)}{\\text{max} \\{ a(i),\\,b(i) \\}}\n\\end{equation*}\n\nwhere $a(i)$ is the mean distance to the other instances in the same cluster, and $b(i)$ is the mean\nnearest-cluster distance, that is the mean distance to the instances of the next\nclosest cluster (defined as the one that minimizes b, excluding the instance's\nown cluster).\n\nThe silhouette coefficient can vary between -1 to +1. Coefficient value of\nclose to +1 means that the instance is well inside its own cluster and far from\nother clusters, while a coefficient close to 0 means that it is close to a\ncluster boundary, and finally a coefficient close to -1 means that the instance\nmay have been assigned to the wrong cluster.\n\nLet's plot the silhouette score as a function of $k$:\n\n"]},{"cell_type":"code","execution_count":1,"id":"0821a3b9-db9e-487a-9933-735667793509","metadata":{},"outputs":[],"source":["from sklearn.metrics import silhouette_score\n\nsilhouette_score(X, kmeans.labels_)"]},{"cell_type":"markdown","id":"d2b26147-13b5-4cf9-afe2-228cb4993b55","metadata":{},"source":["The cluster value of the current $k$ value is around 0.65, but\nwhat about the other values ?\n\n"]},{"cell_type":"code","execution_count":1,"id":"5cb4d73b-7277-4f89-8c67-f670f13028e3","metadata":{},"outputs":[],"source":["silhouette_scores = [silhouette_score(X, model.labels_)\n                     for model in kmeans_per_k[1:]]\n\nplt.figure(figsize=(8, 3))\nplt.plot(range(2, 10), silhouette_scores, \"o-\")\nplt.xlabel(\"$k$\")\nplt.ylabel(\"Silhouette score\")\nplt.axis([1.8, 8.5, 0.55, 0.7])\n\nplt.show()\n             filepath = SAVE_PATH,\n             style = style,\n             close = True)"]},{"cell_type":"markdown","id":"37259e02-a6fb-43ff-9ed7-076e1bb6f8ca","metadata":{},"source":["This visual shows a bit more information compared to inertia as it confirms $k$ = 4 is a good choice\nfor clustering. It also shows $k$ = 5 as a respectable choice as well. But let's look at another visual\nview and plot a silhouette plot. The silhouette plot displays a measure of how close each point in one cluster\nis to points in the neighboring clusters and thus provides a way to assess parameters like number of clusters visually.\n\n"]},{"cell_type":"code","execution_count":1,"id":"3ce7dc6b-ce4f-4ae2-9d0f-bd56dc31faa6","metadata":{},"outputs":[],"source":["from sklearn.metrics import silhouette_samples\nfrom matplotlib.ticker import FixedLocator, FixedFormatter\n\nplt.figure(figsize=(11, 9))\n\nfor k in (3, 4, 5, 6):\n    plt.subplot(2, 2, k - 2)\n    \n    y_pred = kmeans_per_k[k - 1].labels_\n    silhouette_coefficients = silhouette_samples(X, y_pred)\n\n    padding = len(X) // 30\n    pos = padding\n    ticks = []\n    for i in range(k):\n        coeffs = silhouette_coefficients[y_pred == i]\n        coeffs.sort()\n\n        color = plt.cm.Spectral(i / k)\n        plt.fill_betweenx(np.arange(pos, pos + len(coeffs)), 0, coeffs,\n                          facecolor=color, edgecolor=color, alpha=0.7)\n        ticks.append(pos + len(coeffs) // 2)\n        pos += len(coeffs) + padding\n\n    plt.gca().yaxis.set_major_locator(FixedLocator(ticks))\n    plt.gca().yaxis.set_major_formatter(FixedFormatter(range(k)))\n    if k in (3, 5):\n        plt.ylabel(\"Cluster\")\n    \n    if k in (5, 6):\n        plt.gca().set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n        plt.xlabel(\"Silhouette Coefficient\")\n    else:\n        plt.tick_params(labelbottom=False)\n\n    plt.axvline(x=silhouette_scores[k - 2], color=\"red\", linestyle=\"--\")\n    plt.title(f\"$k={k}$\")\n    \nplt.show()\n             filepath = SAVE_PATH,\n             style = style,\n             close = True)"]},{"cell_type":"markdown","id":"8d0a4759-d618-40fd-96a1-591b47821167","metadata":{},"source":["As can be seen, $k=5$ looks to be the best option here, as all clusters are roughly\nthe same size, and they all cross the dashed line, which represents the mean\nsilhouette score.\n\n"]},{"cell_type":"markdown","id":"75b92101-1a76-456e-a8c7-d2dbbc7e99a2","metadata":{},"source":["#### Limits of K-Means\n\n"]},{"cell_type":"markdown","id":"c0da509c-8ac2-49d5-9d28-1baada8231f4","metadata":{},"source":["We have seen how good K-means is but it is not perfect, if the data is too elongated or\nspread it may not detect them well.\n\nLet's load some data to show it.\n\n"]},{"cell_type":"code","execution_count":1,"id":"9dd52cb3-c6f7-46ee-b98c-8c7ff0d647aa","metadata":{},"outputs":[],"source":["X1, y1 = make_blobs(n_samples=1000, centers=((4, -4), (0, 0)), random_state=42)\nX1 = X1.dot(np.array([[0.374, 0.95], [0.732, 0.598]]))\nX2, y2 = make_blobs(n_samples=250, centers=1, random_state=42)\nX2 = X2 + [6, -8]\nX = np.r_[X1, X2]\ny = np.r_[y1, y2]\n\nkmeans_good = KMeans(n_clusters=3,\n                     init=np.array([[-1.5, 2.5], [0.5, 0], [4, 0]]),\n                     n_init=1, random_state=42)\nkmeans_bad = KMeans(n_clusters=3, n_init=10, random_state=42)\nkmeans_good.fit(X)\nkmeans_bad.fit(X)"]},{"cell_type":"markdown","id":"8b0bc29b-5bb3-4aae-8bcc-6786f6eae039","metadata":{},"source":["And plot this data.\n\n"]},{"cell_type":"code","execution_count":1,"id":"6853d2f4-6fb2-4f77-a730-12aefd35418f","metadata":{},"outputs":[],"source":["plt.figure(figsize=(10, 3.2))\n\nplt.subplot(121)\nplot_decision_boundaries(kmeans_good, X)\nplt.title(f\"Inertia = {kmeans_good.inertia_:.1f}\")\n\nplt.subplot(122)\nplot_decision_boundaries(kmeans_bad, X, show_ylabels=False)\nplt.title(f\"Inertia = {kmeans_bad.inertia_:.1f}\")\n\nplt.show()\n             filepath = SAVE_PATH,\n             style = style,\n             close = True)"]},{"cell_type":"markdown","id":"e0199ed4-740e-44d7-9656-ba56c203fd54","metadata":{},"source":["As can be seen, the k-means is not the choices for this kind of data.\n\n"]},{"cell_type":"markdown","id":"16fd64ec-e086-4df0-9c75-9da170f630f5","metadata":{},"source":["### Using Clustering in Image Segmentation\n\n"]},{"cell_type":"markdown","id":"692c1eb3-472f-4bb6-8722-945bf8172c57","metadata":{},"source":["First download the data from the internet.\n\n"]},{"cell_type":"code","execution_count":1,"id":"87b3a34d-3967-45f1-8df9-de2c45ae4b59","metadata":{},"outputs":[],"source":["import urllib.request\nfrom pathlib import Path # to work with paths\n\nhoml3_root = \"https://github.com/dTmC0945/L-MCI-BSc-Data-Science-II/raw/main\"\nfilename = \"Fruit.jpg\"\nfilepath = Path(\"datasets/Fruit.jpg\")\nif not filepath.is_file():\n    print(\"Downloading\", filename)\n    url = f\"{homl3_root}/data/{filename}\"\n    urllib.request.urlretrieve(url, filepath)"]},{"cell_type":"markdown","id":"a77a31c3-9621-4fee-a8a6-06fbdcac7792","metadata":{},"source":["Start by loading the PIL and the image.\n\n"]},{"cell_type":"code","execution_count":1,"id":"97d46581-a9d3-49be-84fd-49f1f0925284","metadata":{},"outputs":[],"source":["import PIL\n\nimage = np.asarray(PIL.Image.open(filepath))\nprint(image.shape)"]},{"cell_type":"markdown","id":"8325894b-6d2d-408b-bb2d-cd1e12f0421a","metadata":{},"source":["The image is a 3D array with each dimension representing the three primary colour (R, G, B).\n\nNow reshape the array and use k-means clustering for categorising the pixels.\n\n"]},{"cell_type":"code","execution_count":1,"id":"c74c9bdc-efdc-47d3-8827-0cfbf208fca8","metadata":{},"outputs":[],"source":["X = image.reshape(-1, 3)\nkmeans = KMeans(n_clusters=8, n_init=10, random_state=42).fit(X)\nsegmented_img = kmeans.cluster_centers_[kmeans.labels_]\nsegmented_img = segmented_img.reshape(image.shape)\n\nsegmented_imgs = []\nn_colors = (10, 8, 6, 4, 2)\n\nfor n_clusters in n_colors:\n    kmeans = KMeans(n_clusters=n_clusters, n_init=10, random_state=42).fit(X)\n    segmented_img = kmeans.cluster_centers_[kmeans.labels_]\n    segmented_imgs.append(segmented_img.reshape(image.shape))"]},{"cell_type":"markdown","id":"6c876073-3934-443c-a719-5539e9f33391","metadata":{},"source":["Let's see how the images look.\n\n"]},{"cell_type":"code","execution_count":1,"id":"f11f7143-ef67-465e-b1ae-fb57ca18bff1","metadata":{},"outputs":[],"source":["plt.figure(figsize=(10, 5))\nplt.subplots_adjust(wspace=0.05, hspace=0.1)\n\nplt.subplot(2, 3, 1)\nplt.imshow(image)\nplt.title(\"Original image\")\nplt.axis('off')\n\nfor idx, n_clusters in enumerate(n_colors):\n    plt.subplot(2, 3, 2 + idx)\n    plt.imshow(segmented_imgs[idx] / 255)\n    plt.title(f\"{n_clusters} colors\")\n    plt.axis('off')\n\n\nplt.show()\n             filepath = SAVE_PATH,\n             style = style,\n             close = True)"]},{"cell_type":"markdown","id":"b6deab50-a6f7-4a36-be56-538c4e3ae5b5","metadata":{},"source":["This is another application of clustering where we can turn an image to a selective set of colours.\n\n"]},{"cell_type":"markdown","id":"ff58c644-24d2-4607-be29-1e9768a912bb","metadata":{},"source":["#### Using Clustering for Semi-Supervised Learning\n\n"]},{"cell_type":"markdown","id":"c5201d8e-ce1a-4625-b50d-b83ee4a677be","metadata":{},"source":["Another use case for clustering is semi-supervised learning, when we have plenty\nof unlabeled instances and very few labeled instances.\n\nLet's tackle the digits dataset which is a simple MNIST-like dataset\ncontaining 1,797 grey-scale 8-by-8 images representing digits 0 to 9.\n\n"]},{"cell_type":"code","execution_count":1,"id":"c9551399-b202-486a-b6c9-3052858091b5","metadata":{},"outputs":[],"source":["from sklearn.datasets import load_digits\n\nX_digits, y_digits = load_digits(return_X_y=True)\nX_train, y_train = X_digits[:1400], y_digits[:1400]\nX_test, y_test = X_digits[1400:], y_digits[1400:]"]},{"cell_type":"markdown","id":"055463dc-2f6d-401d-a0d2-2756226af923","metadata":{},"source":["Let's look at the performance of a logistic regression model when we only have 50 labeled instances:\n\n"]},{"cell_type":"code","execution_count":1,"id":"dc39433b-554b-4d3f-bc50-74294ba1e7b3","metadata":{},"outputs":[],"source":["from sklearn.linear_model import LogisticRegression\n\nn_labeled = 50\nlog_reg = LogisticRegression(max_iter=10_000)\nlog_reg.fit(X_train[:n_labeled], y_train[:n_labeled])"]},{"cell_type":"markdown","id":"a71f4767-a4ef-4cc5-bd1e-a77bd279f1aa","metadata":{},"source":["We can see the performance (i.e., accuracy of the model on the test set)\n\nNOTE The test must be labelled.\n\n"]},{"cell_type":"code","execution_count":1,"id":"99639324-b4f2-4a86-b22f-06144be760d2","metadata":{},"outputs":[],"source":["log_reg.score(X_test, y_test)"]},{"cell_type":"markdown","id":"cdd9d046-11c0-4ff7-b54f-2b9294aa66e1","metadata":{},"source":["Not good. What is we had the model train on the full data set?\n\n"]},{"cell_type":"code","execution_count":1,"id":"4a6daf26-0b72-4926-be24-2a6921cfc430","metadata":{},"outputs":[],"source":["log_reg_full = LogisticRegression(max_iter=10_000)\nlog_reg_full.fit(X_train, y_train)\nlog_reg_full.score(X_test, y_test)"]},{"cell_type":"markdown","id":"af615405-bc6e-438e-98dc-94bb4ad3898d","metadata":{},"source":["Hmm. Still not the best. Let's see if we can do it better. Start by setting the training set into 50 clusters.\nThen for each cluster, find an image closest to the centroid. We’ll call these images the representative images:\n\n"]},{"cell_type":"code","execution_count":1,"id":"4048ad0a-d65d-4511-9598-6c8c8c893d60","metadata":{},"outputs":[],"source":["k = 50\nkmeans = KMeans(n_clusters=k, n_init=10, random_state=42)\nX_digits_dist = kmeans.fit_transform(X_train)\nrepresentative_digit_idx = X_digits_dist.argmin(axis=0)\nX_representative_digits = X_train[representative_digit_idx]"]},{"cell_type":"code","execution_count":1,"id":"c55781c8-0323-48d1-9ff2-03c3c50b4b89","metadata":{},"outputs":[],"source":["plt.figure(figsize=(8, 2))\nfor index, X_representative_digit in enumerate(X_representative_digits):\n    plt.subplot(k // 10, 10, index + 1)\n    plt.imshow(X_representative_digit.reshape(8, 8), cmap=\"binary\",\n               interpolation=\"bilinear\")\n    plt.axis('off')\n\nplt.show()\n             filepath = SAVE_PATH,\n             style = style,\n             close = True)"]},{"cell_type":"code","execution_count":1,"id":"b12447c5-b835-47b2-a236-adf9120ddabb","metadata":{},"outputs":[],"source":["y_representative_digits = np.array([\n    8, 4, 9, 6, 7, 5, 3, 0, 1, 2,\n    3, 3, 4, 7, 2, 1, 5, 1, 6, 4,\n    5, 6, 5, 7, 3, 1, 0, 8, 4, 7,\n    1, 1, 8, 2, 9, 9, 5, 9, 7, 4,\n    4, 9, 7, 8, 2, 6, 6, 3, 2, 8\n])"]},{"cell_type":"markdown","id":"546e767a-1868-4ee9-b058-bdbb823b63d3","metadata":{},"source":["We now have a dataset with just 50 labeled, but instead of being completely random instances,\neach of them is a representative image of its cluster. Let's see if the performance is any better:\n\n"]},{"cell_type":"code","execution_count":1,"id":"9a54bb76-0360-4852-b526-c8029b0ae5fe","metadata":{},"outputs":[],"source":["log_reg = LogisticRegression(max_iter=10_000)\nlog_reg.fit(X_representative_digits, y_representative_digits)\nlog_reg.score(X_test, y_test)"]},{"cell_type":"markdown","id":"8c2d25e3-65ec-4e3a-bd51-e232ccd34dcc","metadata":{},"source":["Our result jumped from 74 to 83 accuracy and this is only by training on 50 labelled instances.  Since\nit's often costly and painful to label instances, especially when it has to be done manually by experts,\nit's a good idea to make them label representative instances rather than just random instances.\n\nLet's see if we can improve this further. Let's try to propagate the labels to all the other instances in the same cluster\n\n"]},{"cell_type":"code","execution_count":1,"id":"3ff1ebfa-40fd-4afa-a3d4-d8237a7b80b7","metadata":{},"outputs":[],"source":["y_train_propagated = np.empty(len(X_train), dtype=np.int64)\nfor i in range(k):\n    y_train_propagated[kmeans.labels_ == i] = y_representative_digits[i]"]},{"cell_type":"code","execution_count":1,"id":"4f2292a2-8cb8-4236-a8b5-3fb4d3ed2243","metadata":{},"outputs":[],"source":["log_reg = LogisticRegression(max_iter=10_000)\nlog_reg.fit(X_train, y_train_propagated)"]},{"cell_type":"markdown","id":"88552f4a-54ee-490b-8554-be4e74372e48","metadata":{},"source":["Let's see our score.\n\n"]},{"cell_type":"code","execution_count":1,"id":"dd025913-487c-41c5-ba48-582ce3e0db12","metadata":{},"outputs":[],"source":["log_reg.score(X_test, y_test)"]},{"cell_type":"markdown","id":"4ac845bb-314a-42d1-bb7c-be526e7ce822","metadata":{},"source":["Our result is a bit better.\n\n"]},{"cell_type":"markdown","id":"c38d7063-e556-4974-b27c-ecc3c0439c51","metadata":{},"source":["### DBSCAN\n\n"]},{"cell_type":"markdown","id":"5f85b954-a561-4e89-b692-48677aa2797a","metadata":{},"source":["The density-based spatial clustering of applications with noise (DBSCAN) algorithm defines\nclusters as continuous regions of high density. The DBSCAN class in Scikit-Learn is as\nsimple to use as you might expect. Let’s test it on the moons dataset:\n\n"]},{"cell_type":"code","execution_count":1,"id":"f9e98395-59b5-44aa-80b0-90bfb2c3e920","metadata":{},"outputs":[],"source":["from sklearn.cluster import DBSCAN\nfrom sklearn.datasets import make_moons\n\nX, y = make_moons(n_samples=1000, noise=0.05, random_state=42)\ndbscan = DBSCAN(eps=0.05, min_samples=5)\ndbscan.fit(X)"]},{"cell_type":"markdown","id":"f938c8fc-c79d-417a-bb33-9635e494d1ca","metadata":{},"source":["The labels of all the instances are now available in the labels\\_ instance variable:\n\n"]},{"cell_type":"code","execution_count":1,"id":"f8351d72-48e5-4035-a96f-1ee778e0f550","metadata":{},"outputs":[],"source":["print(dbscan.labels_[:10])"]},{"cell_type":"markdown","id":"df9e0eef-1eb9-4a90-ad5c-f0c3b0e81f2b","metadata":{},"source":["Some instances have a cluster index equal to –1, which means that they are considered as anomalies by the algorithm.\nThe indices of the core instances are available in the core<sub>sample</sub><sub>indices</sub>\\_ instance variable,\nand the core instances themselves are available in the components\\_ instance variable:\n\n"]},{"cell_type":"code","execution_count":1,"id":"27ede524-ced5-4b59-adff-6c3f507150d4","metadata":{},"outputs":[],"source":["print(dbscan.core_sample_indices_[:10])"]},{"cell_type":"code","execution_count":1,"id":"5e4475f9-bc2c-47d6-8996-9be24a04dcd0","metadata":{},"outputs":[],"source":["print(dbscan.components_)"]},{"cell_type":"markdown","id":"8a377b8d-2717-4b25-a074-cce497ebcaad","metadata":{},"source":["Let's see how this clustering looks.\n\n"]},{"cell_type":"code","execution_count":1,"id":"83f6f7cc-a6de-4afd-a919-700bda4edefa","metadata":{},"outputs":[],"source":["def plot_dbscan(dbscan, X, size, show_xlabels=True, show_ylabels=True):\n    core_mask = np.zeros_like(dbscan.labels_, dtype=bool)\n    core_mask[dbscan.core_sample_indices_] = True\n    anomalies_mask = dbscan.labels_ == -1\n    non_core_mask = ~(core_mask | anomalies_mask)\n\n    cores = dbscan.components_\n    anomalies = X[anomalies_mask]\n    non_cores = X[non_core_mask]\n    \n    plt.scatter(cores[:, 0], cores[:, 1],\n                c=dbscan.labels_[core_mask], marker='o', s=size, cmap=\"Paired\")\n    plt.scatter(cores[:, 0], cores[:, 1], marker='*', s=20,\n                c=dbscan.labels_[core_mask])\n    plt.scatter(anomalies[:, 0], anomalies[:, 1],\n                c=\"r\", marker=\"x\", s=100)\n    plt.scatter(non_cores[:, 0], non_cores[:, 1],\n                c=dbscan.labels_[non_core_mask], marker=\".\")\n    if show_xlabels:\n        plt.xlabel(\"$x_1$\")\n    else:\n        plt.tick_params(labelbottom=False)\n    if show_ylabels:\n        plt.ylabel(\"$x_2$\", rotation=0)\n    else:\n        plt.tick_params(labelleft=False)\n    plt.title(f\"eps={dbscan.eps:.2f}, min_samples={dbscan.min_samples}\")\n    plt.grid()\n    plt.gca().set_axisbelow(True)\n\ndbscan2 = DBSCAN(eps=0.2)\ndbscan2.fit(X)\n\nplt.figure(figsize=(10, 4))\n\nplt.subplot(121)\nplot_dbscan(dbscan, X, size=100)\n\nplt.subplot(122)\nplot_dbscan(dbscan2, X, size=600, show_ylabels=False)\n\nplt.show()\n             filepath = SAVE_PATH,\n             style = style,\n             close = True)"]},{"cell_type":"markdown","id":"f6f1180d-f8af-4215-b835-c713630bc4e0","metadata":{},"source":["It identified quite a lot of anomalies, plus seven different clusters. If we widen each instance’s neighborhood\nby increasing eps to 0.2, we get the clustering on the right, which looks perfect.\n\nAs the right is better, lets use dbscan2 as default value.\n\n"]},{"cell_type":"code","execution_count":1,"id":"81de73d5-be0c-4ab9-91e0-8ee571d2d97b","metadata":{},"outputs":[],"source":["dbscan = dbscan2  # extra code – the text says we now use eps=0.2"]},{"cell_type":"markdown","id":"4532dccd-125c-4b99-85e2-be86bbef0ce3","metadata":{},"source":["As it currently stands DBSCAN class can't predict which cluster a new instance can belong to.\nTo allow this feature new insertion of instances lets train a KNeighborsClassifier.\n\n"]},{"cell_type":"code","execution_count":1,"id":"003fa2ae-3fb8-4c24-bb02-9728996004e3","metadata":{},"outputs":[],"source":["from sklearn.neighbors import KNeighborsClassifier\n\nknn = KNeighborsClassifier(n_neighbors=50)\nknn.fit(dbscan.components_, dbscan.labels_[dbscan.core_sample_indices_])"]},{"cell_type":"markdown","id":"cdc588d6-bdef-4d30-bb6b-b3840bab211a","metadata":{},"source":["Let's give it some instances and see which instances they belong to.\n\n"]},{"cell_type":"code","execution_count":1,"id":"bb723aaf-f300-4c72-89d3-aa8b6b4e9b23","metadata":{},"outputs":[],"source":["X_new = np.array([[-0.5, 0], [0, 0.5], [1, -0.1], [2, 1]])\nprint(knn.predict(X_new))"]},{"cell_type":"markdown","id":"753e557f-c069-4646-af6a-8421003aafb0","metadata":{},"source":["We can see the probability estimation for each cluster.\n\n"]},{"cell_type":"code","execution_count":1,"id":"85aaa831-0aa4-46e7-8953-0ba847bcfb7f","metadata":{},"outputs":[],"source":["print(knn.predict_proba(X_new))"]},{"cell_type":"code","execution_count":1,"id":"88fdae50-158a-4b17-b4fa-a84761d41aa3","metadata":{},"outputs":[],"source":["plt.figure(figsize=(10, 6))\nplot_decision_boundaries(knn, X, show_centroids=False)\nplt.scatter(X_new[:, 0], X_new[:, 1], c=\"b\", marker=\"+\", s=200, zorder=10)\nplt.show()\n             filepath = SAVE_PATH,\n             style = style,\n             close = True)"]},{"cell_type":"code","execution_count":1,"id":"807c7949-9e5c-4a6f-9916-9f5e8a2996bc","metadata":{},"outputs":[],"source":["y_dist, y_pred_idx = knn.kneighbors(X_new, n_neighbors=1)\ny_pred = dbscan.labels_[dbscan.core_sample_indices_][y_pred_idx]\ny_pred[y_dist > 0.2] = -1\nprint(y_pred.ravel())"]},{"cell_type":"markdown","id":"97b08686-a8e8-4195-b971-f7a72d8716e1","metadata":{},"source":["### Other Clustering Algorithms\n\n"]},{"cell_type":"markdown","id":"01a62c2a-88a0-439b-9cf2-8cf795ff77ce","metadata":{},"source":["These are additional clustering methods which you may encounter.\n\n"]},{"cell_type":"markdown","id":"0a8882b3-153e-4de3-80a8-4a2e96131bfd","metadata":{},"source":["#### Spectral Clustering\n\n"]},{"cell_type":"markdown","id":"46d26bb2-a3f9-4d25-b7a6-1e3aeaaff60c","metadata":{},"source":["Spectral Clustering is a variant of the clustering algorithm that uses the\nconnectivity between the data points to form the clustering. It uses eigenvalues\nand eigenvectors of the data matrix to forecast the data into lower dimensions\nspace to cluster the data points. It is based on the idea of a graph representation\nof data where the data point are represented as nodes and the similarity between\nthe data points are represented by an edge.\n\n"]},{"cell_type":"code","execution_count":1,"id":"ecedf439-9f15-4046-b77e-26a4dc9dcf41","metadata":{},"outputs":[],"source":["from sklearn.cluster import SpectralClustering"]},{"cell_type":"code","execution_count":1,"id":"d5dae322-b3a1-48bd-baba-27ea42547351","metadata":{},"outputs":[],"source":["sc1 = SpectralClustering(n_clusters=2, gamma=100, random_state=42)\nsc1.fit(X)"]},{"cell_type":"code","execution_count":1,"id":"744bcb49-4704-4d50-a19c-6a2423a61f4f","metadata":{},"outputs":[],"source":["print(sc1.affinity_matrix_.round(2))"]},{"cell_type":"code","execution_count":1,"id":"ce21e3e7-9aab-421f-b64d-d44ed07c6a0c","metadata":{},"outputs":[],"source":["sc2 = SpectralClustering(n_clusters=2, gamma=1, random_state=42)\nsc2.fit(X)"]},{"cell_type":"code","execution_count":1,"id":"cec0e4bf-4e4f-4015-887c-33784aa90e64","metadata":{},"outputs":[],"source":["def plot_spectral_clustering(sc, X, size, alpha, show_xlabels=True,\n                             show_ylabels=True):\n    plt.scatter(X[:, 0], X[:, 1], marker='o', s=size, c='gray', alpha=alpha)\n    plt.scatter(X[:, 0], X[:, 1], marker='o', s=30, c='w')\n    plt.scatter(X[:, 0], X[:, 1], marker='.', s=10, c=sc.labels_, cmap=\"Paired\")\n    \n    if show_xlabels:\n        plt.xlabel(\"$x_1$\")\n    else:\n        plt.tick_params(labelbottom=False)\n    if show_ylabels:\n        plt.ylabel(\"$x_2$\", rotation=0)\n    else:\n        plt.tick_params(labelleft=False)\n    plt.title(f\"RBF gamma={sc.gamma}\")"]},{"cell_type":"code","execution_count":1,"id":"2ae7b3b0-def3-42c0-af4e-1541ea92673e","metadata":{},"outputs":[],"source":["plt.figure(figsize=(9, 3.2))\n\nplt.subplot(121)\nplot_spectral_clustering(sc1, X, size=500, alpha=0.1)\n\nplt.subplot(122)\nplot_spectral_clustering(sc2, X, size=4000, alpha=0.01, show_ylabels=False)\n\nplt.show()\n             filepath = SAVE_PATH,\n             style = style,\n             close = True)"]},{"cell_type":"markdown","id":"0ff3acde-3485-4a63-8310-29bbaa1d0aa0","metadata":{},"source":["#### Agglomerate Clustering\n\n"]},{"cell_type":"code","execution_count":1,"id":"f525a988-a620-402c-b409-fafc4e07a9cd","metadata":{},"outputs":[],"source":["from sklearn.cluster import AgglomerativeClustering"]},{"cell_type":"code","execution_count":1,"id":"88b9fb2e-3f19-4597-b23c-ceb463d99dc7","metadata":{},"outputs":[],"source":["X = np.array([0, 2, 5, 8.5]).reshape(-1, 1)\nagg = AgglomerativeClustering(linkage=\"complete\").fit(X)"]},{"cell_type":"code","execution_count":1,"id":"0c106396-0759-41f0-a4c3-2ef40970c606","metadata":{},"outputs":[],"source":["def learned_parameters(estimator):\n    return [attrib for attrib in dir(estimator)\n            if attrib.endswith(\"_\") and not attrib.startswith(\"_\")]"]},{"cell_type":"code","execution_count":1,"id":"f04248bf-5cfb-4c0b-a24d-55cb9c579e9d","metadata":{},"outputs":[],"source":["print(learned_parameters(agg))"]},{"cell_type":"code","execution_count":1,"id":"b1faec8e-873d-4d28-b295-b6a0f8307c74","metadata":{},"outputs":[],"source":["print(agg.children_)"]},{"cell_type":"markdown","id":"e413356c-7b4e-4263-a45a-f31a436fc503","metadata":{},"source":["#### Gaussian Mixtures\n\n"]},{"cell_type":"markdown","id":"be691688-91dd-441f-a5b0-b05c09d39e0b","metadata":{},"source":["A Gaussian mixture model (GMM) is a probabilistic model\nthat assumes that the instances were generated from a mixture\nof several Gaussian distributions whose parameters are unknown.\n\nLet's generate the same dataset as earlier with three ellipsoids\n(the one K-Means had trouble with):\n\n"]},{"cell_type":"code","execution_count":1,"id":"254c1629-ea59-4cf0-9cd1-91c9fa165370","metadata":{},"outputs":[],"source":["X1, y1 = make_blobs(n_samples=1000, centers=((4, -4), (0, 0)), random_state=42)\nX1 = X1.dot(np.array([[0.374, 0.95], [0.732, 0.598]]))\nX2, y2 = make_blobs(n_samples=250, centers=1, random_state=42)\nX2 = X2 + [6, -8]\nX = np.r_[X1, X2]\ny = np.r_[y1, y2]"]},{"cell_type":"markdown","id":"44bead6e-1904-4325-9ffe-f71b43e08ef6","metadata":{},"source":["Let's train a Gaussian mixture model on the previous dataset:\n\n"]},{"cell_type":"code","execution_count":1,"id":"8adeff2c-ef35-4d48-90a7-53fd9437248b","metadata":{},"outputs":[],"source":["from sklearn.mixture import GaussianMixture"]},{"cell_type":"markdown","id":"d498b01b-49fe-4110-b313-3674fc3b8de1","metadata":{},"source":["given the dataset $\\vec{X}$, you typically want to start by estimating the weights $\\phi$ and all\nthe distribution parameters μ(1) to μ(k) and Σ(1) to Σ(k). For this we use GaussianMixture.\n\n"]},{"cell_type":"code","execution_count":1,"id":"acd1ea6c-329c-4130-8d7f-ba273492e12a","metadata":{},"outputs":[],"source":["gm = GaussianMixture(n_components=3, n_init=10, random_state=42)\ngm.fit(X)"]},{"cell_type":"markdown","id":"4f56cd98-e164-4682-a8ff-96d489482413","metadata":{},"source":["Let's look at the parameters the algorithm estimated:\n\n"]},{"cell_type":"code","execution_count":1,"id":"84c424bb-6945-4e21-9659-a59fb921e729","metadata":{},"outputs":[],"source":["print(gm.weights_)"]},{"cell_type":"code","execution_count":1,"id":"b4cfa52d-d831-4d24-ab26-271865b0c5b1","metadata":{},"outputs":[],"source":["print(gm.means_)"]},{"cell_type":"code","execution_count":1,"id":"eb6f72dd-0ab1-4fbf-8446-41f87ef4a6aa","metadata":{},"outputs":[],"source":["print(gm.covariances_)"]},{"cell_type":"markdown","id":"da9e9ab2-27d1-48e9-b964-0a58ca877ba5","metadata":{},"source":["Let's check if the algorithm has converged.\n\n"]},{"cell_type":"code","execution_count":1,"id":"34db9409-09b5-46c5-a398-118ba9bf0623","metadata":{},"outputs":[],"source":["print(gm.converged_)"]},{"cell_type":"markdown","id":"a8d2c52a-1b24-4b39-88e7-395bf9ff2cae","metadata":{},"source":["But how many iterations did it take ?\n\n"]},{"cell_type":"code","execution_count":1,"id":"3e649fae-909d-40b2-9ffe-eddc46da680b","metadata":{},"outputs":[],"source":["print(gm.n_iter_)"]},{"cell_type":"markdown","id":"639cd08e-7c70-4d31-9801-10660152fa3c","metadata":{},"source":["You can now use the model to predict which cluster each instance belongs to\n(hard clustering) or the probabilities that it came from each cluster.\n\nFor this, just use predict() method or the predict<sub>proba</sub>() method:\n\n"]},{"cell_type":"code","execution_count":1,"id":"37ad7bfa-1844-49a1-88b0-93136147dbc8","metadata":{},"outputs":[],"source":["print(gm.predict(X))"]},{"cell_type":"code","execution_count":1,"id":"b4523880-99c1-4212-8e3b-c727f78aa3b1","metadata":{},"outputs":[],"source":["print(gm.predict_proba(X).round(3))"]},{"cell_type":"markdown","id":"0c229519-210a-4ae5-a142-26b3fcd373d1","metadata":{},"source":["This is a generative model, so you can sample new instances from it (and get their labels):\n\n"]},{"cell_type":"code","execution_count":1,"id":"4f352df4-36a5-436f-98a3-80d06e5310ee","metadata":{},"outputs":[],"source":["X_new, y_new = gm.sample(6)\nprint(X_new)"]},{"cell_type":"code","execution_count":1,"id":"03471857-90a4-45b3-8c16-bb589dbb24e6","metadata":{},"outputs":[],"source":["print(y_new)"]},{"cell_type":"markdown","id":"0f701158-50f7-411c-86a6-11952bf9d8a5","metadata":{},"source":["Notice that they are sampled sequentially from each cluster.\n\nYou can also estimate the log of the probability density function (PDF) at any location using the score<sub>samples</sub>() method:\n\n"]},{"cell_type":"code","execution_count":1,"id":"e96949b9-fc1a-4f2b-a686-337e028678a0","metadata":{},"outputs":[],"source":["print(gm.score_samples(X).round(2))"]},{"cell_type":"markdown","id":"3128712a-a958-40ea-b686-d7d08c8a2aa3","metadata":{},"source":["Let's check that the PDF integrates to 1 over the whole space.\nWe just take a large square around the clusters, and chop it into\na grid of tiny squares, then we compute the approximate probability\nthat the instances will be generated in each tiny square\n(by multiplying the PDF at one corner of the tiny square\nby the area of the square), and finally summing all these probabilities).\n\nThe result is very close to 1:\n\n"]},{"cell_type":"code","execution_count":1,"id":"80dd144a-c1be-47d8-babf-6322d15470f2","metadata":{},"outputs":[],"source":["resolution = 100\ngrid = np.arange(-10, 10, 1 / resolution)\nxx, yy = np.meshgrid(grid, grid)\nX_full = np.vstack([xx.ravel(), yy.ravel()]).T\n\npdf = np.exp(gm.score_samples(X_full))\npdf_probas = pdf * (1 / resolution) ** 2\nprint(pdf_probas.sum())"]},{"cell_type":"markdown","id":"70447a4d-98e4-4fcc-95a6-b6f0cc830300","metadata":{},"source":["Now let's plot the resulting decision boundaries (dashed lines) and density contours:\n\n"]},{"cell_type":"code","execution_count":1,"id":"d239746e-dbca-4c76-9419-0782fb71b3a8","metadata":{},"outputs":[],"source":["from matplotlib.colors import LogNorm\n\ndef plot_gaussian_mixture(clusterer, X, resolution=1000, show_ylabels=True):\n    mins = X.min(axis=0) - 0.1\n    maxs = X.max(axis=0) + 0.1\n    xx, yy = np.meshgrid(np.linspace(mins[0], maxs[0], resolution),\n                         np.linspace(mins[1], maxs[1], resolution))\n    Z = -clusterer.score_samples(np.c_[xx.ravel(), yy.ravel()])\n    Z = Z.reshape(xx.shape)\n\n    plt.contourf(xx, yy, Z,\n                 norm=LogNorm(vmin=1.0, vmax=30.0),\n                 levels=np.logspace(0, 2, 12))\n    plt.contour(xx, yy, Z,\n                norm=LogNorm(vmin=1.0, vmax=30.0),\n                levels=np.logspace(0, 2, 12),\n                linewidths=1, colors='k')\n\n    Z = clusterer.predict(np.c_[xx.ravel(), yy.ravel()])\n    Z = Z.reshape(xx.shape)\n    plt.contour(xx, yy, Z,\n                linewidths=2, colors='r', linestyles='dashed')\n    \n    plt.plot(X[:, 0], X[:, 1], 'k.', markersize=2)\n    plot_centroids(clusterer.means_, clusterer.weights_)\n\n    plt.xlabel(\"$x_1$\")\n    if show_ylabels:\n        plt.ylabel(\"$x_2$\", rotation=0)\n    else:\n        plt.tick_params(labelleft=False)\n\nplt.figure(figsize=(8, 4))\n\nplot_gaussian_mixture(gm, X)\n\nplt.show()\n             filepath = SAVE_PATH,\n             style = style,\n             close = True)"]},{"cell_type":"markdown","id":"965f6dbd-aa43-4abc-9401-3f388f9b9c73","metadata":{},"source":["You can impose constraints on the covariance matrices that the algorithm looks for by setting the covariance<sub>type</sub> hyperparameter:\n\n\"spherical\": all clusters must be spherical, but they can have different diameters (i.e., different variances).\n\"diag\": clusters can take on any ellipsoidal shape of any size, but the ellipsoid's axes must be parallel to the axes (i.e., the covariance matrices must be diagonal).\n\"tied\": all clusters must have the same shape, which can be any ellipsoid (i.e., they all share the same covariance matrix).\n\"full\" (default): no constraint, all clusters can take on any ellipsoidal shape of any size.\n\n"]},{"cell_type":"code","execution_count":1,"id":"caca4a64-f022-46b0-a879-36e30754b788","metadata":{},"outputs":[],"source":["gm_full = GaussianMixture(n_components=3, n_init=10,\n                          covariance_type=\"full\", random_state=42)\ngm_tied = GaussianMixture(n_components=3, n_init=10,\n                          covariance_type=\"tied\", random_state=42)\ngm_spherical = GaussianMixture(n_components=3, n_init=10,\n                               covariance_type=\"spherical\", random_state=42)\ngm_diag = GaussianMixture(n_components=3, n_init=10,\n                          covariance_type=\"diag\", random_state=42)\ngm_full.fit(X)\ngm_tied.fit(X)\ngm_spherical.fit(X)\ngm_diag.fit(X)\n\ndef compare_gaussian_mixtures(gm1, gm2, X):\n    plt.figure(figsize=(9, 4))\n\n    plt.subplot(121)\n    plot_gaussian_mixture(gm1, X)\n    plt.title(f'covariance_type=\"{gm1.covariance_type}\"')\n\n    plt.subplot(122)\n    plot_gaussian_mixture(gm2, X, show_ylabels=False)\n    plt.title(f'covariance_type=\"{gm2.covariance_type}\"')\n\ncompare_gaussian_mixtures(gm_tied, gm_spherical, X)\n\nplt.show()\n             filepath = SAVE_PATH,\n             style = style,\n             close = True)"]},{"cell_type":"code","execution_count":1,"id":"59a5c144-dee7-4466-91d4-2055011b5e87","metadata":{},"outputs":[],"source":["compare_gaussian_mixtures(gm_full, gm_diag, X)\n\nplt.show()\n             filepath = SAVE_PATH,\n             style = style,\n             close = True)"]},{"cell_type":"markdown","id":"78cf384b-94a3-4ff4-9a7e-d905c4e36e25","metadata":{},"source":["### Anomaly Detection Using Gaussian Mixtures\n\n"]},{"cell_type":"markdown","id":"db0a6abc-61b0-4671-aad8-b145b3450908","metadata":{},"source":["Gaussian Mixtures can be used for anomaly detection: instances located\nin low-density regions can be considered anomalies. You must define what\ndensity threshold you want to use. For example, in a manufacturing company\nthat tries to detect defective products, the ratio of defective products is\nusually well-known. Say it is equal to 2%, then you can set the density\nthreshold to be the value that results in having 2% of the instances\nlocated in areas below that threshold density:\n\n"]},{"cell_type":"code","execution_count":1,"id":"45a96c14-7099-42d6-a2e3-9f332455614d","metadata":{},"outputs":[],"source":["densities = gm.score_samples(X)\ndensity_threshold = np.percentile(densities, 2)\nanomalies = X[densities < density_threshold]"]},{"cell_type":"code","execution_count":1,"id":"2cf0dd77-0b95-4dec-a353-14b8fbf7779f","metadata":{},"outputs":[],"source":["plt.figure(figsize=(8, 4))\n\nplot_gaussian_mixture(gm, X)\nplt.scatter(anomalies[:, 0], anomalies[:, 1], color='r', marker='*')\nplt.ylim(top=5.1)\n\nplt.show()\n             filepath = SAVE_PATH,\n             style = style,\n             close = True)"]},{"cell_type":"code","execution_count":1,"id":"6d79b8c2-258e-451a-b5f9-c7d181acd271","metadata":{},"outputs":[],"source":["from scipy.stats import norm\n\nx_val = 2.5\nstd_val = 1.3\nx_range = [-6, 4]\nx_proba_range = [-2, 2]\nstds_range = [1, 2]\n\nxs = np.linspace(x_range[0], x_range[1], 501)\nstds = np.linspace(stds_range[0], stds_range[1], 501)\nXs, Stds = np.meshgrid(xs, stds)\nZ = 2 * norm.pdf(Xs - 1.0, 0, Stds) + norm.pdf(Xs + 4.0, 0, Stds)\nZ = Z / Z.sum(axis=1)[:, np.newaxis] / (xs[1] - xs[0])\n\nx_example_idx = (xs >= x_val).argmax()  # index of the first value >= x_val\nmax_idx = Z[:, x_example_idx].argmax()\nmax_val = Z[:, x_example_idx].max()\ns_example_idx = (stds >= std_val).argmax()\nx_range_min_idx = (xs >= x_proba_range[0]).argmax()\nx_range_max_idx = (xs >= x_proba_range[1]).argmax()\nlog_max_idx = np.log(Z[:, x_example_idx]).argmax()\nlog_max_val = np.log(Z[:, x_example_idx]).max()\n\nplt.figure(figsize=(16, 8))\n\nplt.subplot(2, 2, 1)\nplt.contourf(Xs, Stds, Z, cmap=\"GnBu\")\nplt.plot([-6, 4], [std_val, std_val], \"k-\", linewidth=2)\nplt.plot([x_val, x_val], [1, 2], \"b-\", linewidth=2)\nplt.ylabel(r\"$\\theta$\", rotation=0, labelpad=10)\nplt.title(r\"Model $f(x; \\theta)$\")\n\nplt.subplot(2, 2, 2)\nplt.plot(stds, Z[:, x_example_idx], \"b-\")\nplt.plot(stds[max_idx], max_val, \"r.\")\nplt.plot([stds[max_idx], stds[max_idx]], [0, max_val], \"r:\")\nplt.plot([0, stds[max_idx]], [max_val, max_val], \"r:\")\nplt.text(stds[max_idx]+ 0.01, 0.081, r\"$\\hat{\\theta}$\")\nplt.text(stds[max_idx]+ 0.01, max_val - 0.006, r\"$Max$\")\nplt.text(1.01, max_val - 0.008, r\"$\\hat{\\mathcal{L}}$\")\nplt.ylabel(r\"$\\mathcal{L}$\", rotation=0, labelpad=10)\nplt.title(fr\"$\\mathcal{{L}}(\\theta|x={x_val}) = f(x={x_val}; \\theta)$\")\nplt.grid()\nplt.axis([1, 2, 0.08, 0.12])\n\nplt.subplot(2, 2, 3)\nplt.plot(xs, Z[s_example_idx], \"k-\")\nplt.fill_between(xs[x_range_min_idx:x_range_max_idx+1],\n                 Z[s_example_idx, x_range_min_idx:x_range_max_idx+1], alpha=0.2)\nplt.xlabel(r\"$x$\")\nplt.ylabel(\"PDF\")\nplt.title(fr\"PDF $f(x; \\theta={std_val})$\")\nplt.grid()\nplt.axis([-6, 4, 0, 0.25])\n\nplt.subplot(2, 2, 4)\nplt.plot(stds, np.log(Z[:, x_example_idx]), \"b-\")\nplt.plot(stds[log_max_idx], log_max_val, \"r.\")\nplt.plot([stds[log_max_idx], stds[log_max_idx]], [-5, log_max_val], \"r:\")\nplt.plot([0, stds[log_max_idx]], [log_max_val, log_max_val], \"r:\")\nplt.text(stds[log_max_idx]+ 0.01, log_max_val - 0.06, r\"$Max$\")\nplt.text(stds[log_max_idx]+ 0.01, -2.49, r\"$\\hat{\\theta}$\")\nplt.text(1.01, log_max_val - 0.08, r\"$\\log \\, \\hat{\\mathcal{L}}$\")\nplt.xlabel(r\"$\\theta$\")\nplt.ylabel(r\"$\\log\\mathcal{L}$\", rotation=0, labelpad=10)\nplt.title(fr\"$\\log \\, \\mathcal{{L}}(\\theta|x={x_val})$\")\nplt.grid()\nplt.axis([1, 2, -2.5, -2.1])\n\nplt.show()\n             filepath = SAVE_PATH,\n             style = style,\n             close = True)"]},{"cell_type":"code","execution_count":1,"id":"f84166b0-c267-41a3-824f-2e93e27b968a","metadata":{},"outputs":[],"source":["print(gm.bic(X))"]},{"cell_type":"code","execution_count":1,"id":"54084ef6-249a-40d9-8430-3a8fd889ae07","metadata":{},"outputs":[],"source":["print(gm.aic(X))"]},{"cell_type":"code","execution_count":1,"id":"190abd58-ce86-421c-b52d-36d6fb3dc302","metadata":{},"outputs":[],"source":["n_clusters = 3\nn_dims = 2\nn_params_for_weights = n_clusters - 1\nn_params_for_means = n_clusters * n_dims\nn_params_for_covariance = n_clusters * n_dims * (n_dims + 1) // 2\nn_params = n_params_for_weights + n_params_for_means + n_params_for_covariance\nmax_log_likelihood = gm.score(X) * len(X) # log(L^)\nbic = np.log(len(X)) * n_params - 2 * max_log_likelihood\naic = 2 * n_params - 2 * max_log_likelihood\nprint(f\"bic = {bic}\")\nprint(f\"aic = {aic}\")\nprint(f\"n_params = {n_params}\")"]},{"cell_type":"code","execution_count":1,"id":"4a2a3b6f-541f-47ec-ba9a-aa12f0c87d1e","metadata":{},"outputs":[],"source":["gms_per_k = [GaussianMixture(n_components=k, n_init=10, random_state=42).fit(X)\n             for k in range(1, 11)]\nbics = [model.bic(X) for model in gms_per_k]\naics = [model.aic(X) for model in gms_per_k]\n\nplt.figure(figsize=(8, 3))\nplt.plot(range(1, 11), bics, \"o-\", label=\"BIC\")\nplt.plot(range(1, 11), aics, \"o--\", label=\"AIC\")\nplt.xlabel(\"$k$\")\nplt.ylabel(\"Information Criterion\")\nplt.axis([1, 9.5, min(aics) - 50, max(aics) + 50])\nplt.annotate(\"\", xy=(3, bics[2]), xytext=(3.4, 8650),\n             arrowprops=dict(facecolor='black', shrink=0.1))\nplt.text(3.5, 8660, \"Minimum\", horizontalalignment=\"center\")\nplt.legend()\nplt.show()\n             filepath = SAVE_PATH,\n             style = style,\n             close = True)"]},{"cell_type":"markdown","id":"4db3d1fa-3369-43ed-966f-7323382b2e76","metadata":{},"source":["#### Bayesian Gaussian Mixture Models\n\n"]},{"cell_type":"markdown","id":"33b7a867-2549-483e-b237-458c0a57eb8c","metadata":{},"source":["Rather than manually searching for the optimal number of clusters, it is\npossible to use instead the BayesianGaussianMixture class which is capable\nof giving weights equal (or close) to zero to unnecessary clusters.\n\nJust set the number of components to a value that you believe is greater\nthan the optimal number of clusters, and the algorithm will eliminate the\nunnecessary clusters automatically.\n\n"]},{"cell_type":"code","execution_count":1,"id":"861049cc-1d04-424f-bb7b-86ed71491662","metadata":{},"outputs":[],"source":["from sklearn.mixture import BayesianGaussianMixture\n\nbgm = BayesianGaussianMixture(n_components=10, n_init=10, random_state=42)\nbgm.fit(X)\nbgm.weights_.round(2)"]},{"cell_type":"markdown","id":"fa3cfe36-1c0d-4dcb-aa87-fc6296b86d0f","metadata":{},"source":["The algorithm automatically detected that only 3 components are needed!\n\n"]},{"cell_type":"code","execution_count":1,"id":"3e9d4c68-71ed-4610-bbe7-255cc6051495","metadata":{},"outputs":[],"source":["plt.figure(figsize=(8, 5))\nplot_gaussian_mixture(bgm, X)\n\nplt.show()\n             filepath = SAVE_PATH,\n             style = style,\n             close = True)"]},{"cell_type":"code","execution_count":1,"id":"ee76d477-8f14-411f-9435-49cc470edbc6","metadata":{},"outputs":[],"source":["X_moons, y_moons = make_moons(n_samples=1000, noise=0.05, random_state=42)\n\nbgm = BayesianGaussianMixture(n_components=10, n_init=10, random_state=42)\nbgm.fit(X_moons)\n\nplt.figure(figsize=(9, 3.2))\n\nplt.subplot(121)\nplot_data(X_moons)\nplt.xlabel(\"$x_1$\")\nplt.ylabel(\"$x_2$\", rotation=0)\nplt.grid()\n\nplt.subplot(122)\nplot_gaussian_mixture(bgm, X_moons, show_ylabels=False)\n\nplt.show()\n             filepath = SAVE_PATH,\n             style = style,\n             close = True)"]},{"cell_type":"markdown","id":"33076ede-5734-48d7-b24a-465ae4771b4a","metadata":{},"source":["Oops, not great&#x2026; instead of detecting 2 moon-shaped clusters,\nthe algorithm detected 8 ellipsoidal clusters. However, the density plot\ndoes not look too bad, so it might be usable for anomaly detection.\n\n"]}],"metadata":{"org":null,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.2"}},"nbformat":4,"nbformat_minor":5}