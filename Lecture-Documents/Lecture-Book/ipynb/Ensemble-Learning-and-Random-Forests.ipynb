{"cells":[{"cell_type":"markdown","id":"7a47f828-5183-40a0-b500-b3c19c61cbef","metadata":{},"source":"Code for\n========\n\n"},{"cell_type":"markdown","id":"429c41c3-4c51-4c1a-9e0d-c69f365445c8","metadata":{},"source":["These are the code snippets used in Ensemble Learning and Random Forests\npart of .\n\n"]},{"cell_type":"markdown","id":"eb63dbd4-a2c1-432c-aba4-119f29d85883","metadata":{},"source":["### Introduction\n\n"]},{"cell_type":"code","execution_count":1,"id":"7ab3912b-425e-418e-962d-da1280bfe75d","metadata":{},"outputs":[],"source":["import matplotlib.pyplot as plt\nimport ChalcedonPy as cp\n\n# Initialise ChalcedonPy\ncp.init(save_path=\"Ensemble-Learning-and-Random-Forests\",\n        display_mode=\"slide\")"]},{"cell_type":"code","execution_count":1,"id":"ca8097fa-12f8-4ff7-8aa8-25150b04b838","metadata":{},"outputs":[],"source":["# extra code – this cell generates and saves Figure 7–3\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nheads_proba = 0.51\nnp.random.seed(42)\ncoin_tosses = (np.random.rand(10000, 10) < heads_proba).astype(np.int32)\ncumulative_heads = coin_tosses.cumsum(axis=0)\ncumulative_heads_ratio = cumulative_heads / np.arange(1, 10001).reshape(-1, 1)\n\nplt.figure(figsize=(8, 3.5))\nplt.plot(cumulative_heads_ratio)\nplt.plot([0, 10000], [0.51, 0.51], \"k--\", linewidth=2, label=\"51%\")\nplt.plot([0, 10000], [0.5, 0.5], \"k-\", label=\"50%\")\nplt.xlabel(\"Number of coin tosses\")\nplt.ylabel(\"Heads ratio\")\nplt.legend(loc=\"lower right\")\nplt.axis([0, 10000, 0.42, 0.58])\nplt.show()"]},{"cell_type":"code","execution_count":1,"id":"2894536c-a7f7-42c2-b9e1-904909a130a8","metadata":{},"outputs":[],"source":["from sklearn.datasets import make_moons\nfrom sklearn.ensemble import RandomForestClassifier, VotingClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.svm import SVC\nX, y = make_moons(n_samples=500, noise=0.30, random_state=42)\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\nvoting_clf = VotingClassifier(\n    estimators=[\n        ('lr', LogisticRegression(random_state=42)),\n        ('rf', RandomForestClassifier(random_state=42)),\n        ('svc', SVC(random_state=42))\n    ]\n)\nvoting_clf.fit(X_train, y_train)"]},{"cell_type":"code","execution_count":1,"id":"e7af18eb-99ff-494e-aea1-65d1cb357da4","metadata":{},"outputs":[],"source":["for name, clf in voting_clf.named_estimators_.items():\n    print(name, \"=\", clf.score(X_test, y_test))"]},{"cell_type":"code","execution_count":1,"id":"08fe1d3f-4ea1-48df-b9f4-4d5f7bba5376","metadata":{},"outputs":[],"source":["print(voting_clf.predict(X_test[:1]))\nprint([clf.predict(X_test[:1]) for clf in voting_clf.estimators_])\nprint(voting_clf.score(X_test, y_test))"]},{"cell_type":"code","execution_count":1,"id":"5c1f76e2-6244-4487-9cc7-a073e49e7d53","metadata":{},"outputs":[],"source":["voting_clf.voting = \"soft\"\nvoting_clf.named_estimators[\"svc\"].probability = True\nvoting_clf.fit(X_train, y_train)\nprint(voting_clf.score(X_test, y_test))"]},{"cell_type":"code","execution_count":1,"id":"58cbce93-c578-40e7-97a6-b5f577cbae14","metadata":{},"outputs":[],"source":["from sklearn.ensemble import BaggingClassifier\nfrom sklearn.tree import DecisionTreeClassifier\n\nbag_clf = BaggingClassifier(DecisionTreeClassifier(), n_estimators=500,\n                            max_samples=100, n_jobs=-1, random_state=42)\nbag_clf.fit(X_train, y_train)"]},{"cell_type":"code","execution_count":1,"id":"ee301708-43fe-444f-b12a-55b3d9fa3cd9","metadata":{},"outputs":[],"source":["def plot_decision_boundary(clf, X, y, alpha=1.0):\n    axes=[-1.5, 2.4, -1, 1.5]\n    x1, x2 = np.meshgrid(np.linspace(axes[0], axes[1], 100),\n                         np.linspace(axes[2], axes[3], 100))\n    X_new = np.c_[x1.ravel(), x2.ravel()]\n    y_pred = clf.predict(X_new).reshape(x1.shape)\n    \n    plt.contourf(x1, x2, y_pred, alpha=0.3 * alpha, cmap='Wistia')\n    plt.contour(x1, x2, y_pred, cmap=\"Greys\", alpha=0.8 * alpha)\n    colors = [\"#78785c\", \"#c47b27\"]\n    markers = (\"o\", \"^\")\n    for idx in (0, 1):\n        plt.plot(X[:, 0][y == idx], X[:, 1][y == idx],\n                 color=colors[idx], marker=markers[idx], linestyle=\"none\")\n    plt.axis(axes)\n    plt.xlabel(r\"$x_1$\")\n    plt.ylabel(r\"$x_2$\", rotation=0)\n\ntree_clf = DecisionTreeClassifier(random_state=42)\ntree_clf.fit(X_train, y_train)\n\nfig, axes = plt.subplots(ncols=2, figsize=(10, 4), sharey=True)\nplt.sca(axes[0])\nplot_decision_boundary(tree_clf, X_train, y_train)\nplt.title(\"Decision Tree\")\nplt.sca(axes[1])\nplot_decision_boundary(bag_clf, X_train, y_train)\nplt.title(\"Decision Trees with Bagging\")\nplt.ylabel(\"\")\nplt.show()"]},{"cell_type":"code","execution_count":1,"id":"9764c46f-e1ae-4760-b233-57495c159987","metadata":{},"outputs":[],"source":["bag_clf = BaggingClassifier(DecisionTreeClassifier(), n_estimators=500,\n                            oob_score=True, n_jobs=-1, random_state=42)\nbag_clf.fit(X_train, y_train)\nprint(bag_clf.oob_score_)"]},{"cell_type":"code","execution_count":1,"id":"fcd5c6cf-1165-42eb-9757-e2e8b41ed302","metadata":{},"outputs":[],"source":["from sklearn.metrics import accuracy_score\n\ny_pred = bag_clf.predict(X_test)\nprint(accuracy_score(y_test, y_pred))"]},{"cell_type":"code","execution_count":1,"id":"1be9b7d0-270a-41e5-b9e2-a8a3f0921c61","metadata":{},"outputs":[],"source":["print(bag_clf.oob_decision_function_[:3])  # probas for the first 3 instances"]},{"cell_type":"code","execution_count":1,"id":"30f33642-3168-4cab-a66b-5c91381f7a5b","metadata":{},"outputs":[],"source":["from sklearn.ensemble import RandomForestClassifier\n\nrnd_clf = RandomForestClassifier(n_estimators=500, max_leaf_nodes=16,\n                                 n_jobs=-1, random_state=42)\nrnd_clf.fit(X_train, y_train)\ny_pred_rf = rnd_clf.predict(X_test)"]},{"cell_type":"code","execution_count":1,"id":"e0016bf3-5aa7-4a77-84f9-46adc35ded69","metadata":{},"outputs":[],"source":["bag_clf = BaggingClassifier(\n    DecisionTreeClassifier(max_features=\"sqrt\", max_leaf_nodes=16),\n    n_estimators=500, n_jobs=-1, random_state=42)"]},{"cell_type":"code","execution_count":1,"id":"1b003c96-1e05-42e2-a97e-0a391b626d39","metadata":{},"outputs":[],"source":["from sklearn.datasets import load_iris\n\niris = load_iris(as_frame=True)\nrnd_clf = RandomForestClassifier(n_estimators=500, random_state=42)\nrnd_clf.fit(iris.data, iris.target)\nfor score, name in zip(rnd_clf.feature_importances_, iris.data.columns):\n    print(round(score, 2), name)"]},{"cell_type":"code","execution_count":1,"id":"ebbe3203-5d88-4072-924e-f2dbe87ba078","metadata":{},"outputs":[],"source":["from sklearn.datasets import fetch_openml\n\nX_mnist, y_mnist = fetch_openml('mnist_784', return_X_y=True, as_frame=False,\n                                parser='auto')\n\nrnd_clf = RandomForestClassifier(n_estimators=100, random_state=42)\nrnd_clf.fit(X_mnist, y_mnist)\n\nheatmap_image = rnd_clf.feature_importances_.reshape(28, 28)\nplt.imshow(heatmap_image, cmap=\"hot\")\ncbar = plt.colorbar(ticks=[rnd_clf.feature_importances_.min(),\n                           rnd_clf.feature_importances_.max()])\ncbar.ax.set_yticklabels(['Not important', 'Very important'], fontsize=14)\nplt.show()"]},{"cell_type":"code","execution_count":1,"id":"95f91bbb-9cf4-4e5c-9f54-68db0625ff1f","metadata":{},"outputs":[],"source":["m = len(X_train)\n\nfig, axes = plt.subplots(ncols=2, figsize=(10, 4), sharey=True)\nfor subplot, learning_rate in ((0, 1), (1, 0.5)):\n    sample_weights = np.ones(m) / m\n    plt.sca(axes[subplot])\n    for i in range(5):\n        svm_clf = SVC(C=0.2, gamma=0.6, random_state=42)\n        svm_clf.fit(X_train, y_train, sample_weight=sample_weights * m)\n        y_pred = svm_clf.predict(X_train)\n\n        error_weights = sample_weights[y_pred != y_train].sum()\n        r = error_weights / sample_weights.sum()  # equation 7-1\n        alpha = learning_rate * np.log((1 - r) / r)  # equation 7-2\n        sample_weights[y_pred != y_train] *= np.exp(alpha)  # equation 7-3\n        sample_weights /= sample_weights.sum()  # normalization step\n\n        plot_decision_boundary(svm_clf, X_train, y_train, alpha=0.4)\n        plt.title(f\"learning_rate = {learning_rate}\")\n    if subplot == 0:\n        plt.text(-0.75, -0.95, \"1\", fontsize=16)\n        plt.text(-1.05, -0.95, \"2\", fontsize=16)\n        plt.text(1.0, -0.95, \"3\", fontsize=16)\n        plt.text(-1.45, -0.5, \"4\", fontsize=16)\n        plt.text(1.36,  -0.95, \"5\", fontsize=16)\n    else:\n        plt.ylabel(\"\")\n\nplt.show()"]},{"cell_type":"code","execution_count":1,"id":"0e0e24b5-e778-4479-b6cc-f4f57e0b2de1","metadata":{},"outputs":[],"source":["from sklearn.ensemble import AdaBoostClassifier\n\nada_clf = AdaBoostClassifier(\n    DecisionTreeClassifier(max_depth=1), n_estimators=30,\n    learning_rate=0.5, random_state=42)\nada_clf.fit(X_train, y_train)"]},{"cell_type":"markdown","id":"90a93bd1-5ab8-4d9c-a2a0-e894e2082bbe","metadata":{},"source":["#### Gradient Boosting\n\n"]},{"cell_type":"code","execution_count":1,"id":"a5b04b0b-e068-401b-8b77-7e5702994312","metadata":{},"outputs":[],"source":["import numpy as np\nfrom sklearn.tree import DecisionTreeRegressor\n\nnp.random.seed(42)\nX = np.random.rand(100, 1) - 0.5\ny = 3 * X[:, 0] ** 2 + 0.05 * np.random.randn(100)  # y = 3x² + Gaussian noise\n\ntree_reg1 = DecisionTreeRegressor(max_depth=2, random_state=42)\ntree_reg1.fit(X, y)"]},{"cell_type":"code","execution_count":1,"id":"3a6f643d-1036-44d6-aba3-9493a501840a","metadata":{},"outputs":[],"source":["y2 = y - tree_reg1.predict(X)\ntree_reg2 = DecisionTreeRegressor(max_depth=2, random_state=43)\ntree_reg2.fit(X, y2)"]},{"cell_type":"code","execution_count":1,"id":"d0628757-f4b4-44a7-a903-e7a226aad606","metadata":{},"outputs":[],"source":["y3 = y2 - tree_reg2.predict(X)\ntree_reg3 = DecisionTreeRegressor(max_depth=2, random_state=44)\ntree_reg3.fit(X, y3)"]},{"cell_type":"code","execution_count":1,"id":"3acbeddf-9474-4d5c-84d4-3ac7c9e8443f","metadata":{},"outputs":[],"source":["X_new = np.array([[-0.4], [0.], [0.5]])\nprint(sum(tree.predict(X_new) for tree in (tree_reg1, tree_reg2, tree_reg3)))"]},{"cell_type":"code","execution_count":1,"id":"a81424c6-8a78-4e78-8a7d-4d59979455f1","metadata":{},"outputs":[],"source":["def plot_predictions(regressors, X, y, axes, style,\n                     label=None, data_style=\"b.\", data_label=None):\n    x1 = np.linspace(axes[0], axes[1], 500)\n    y_pred = sum(regressor.predict(x1.reshape(-1, 1))\n                 for regressor in regressors)\n    plt.plot(X[:, 0], y, data_style, label=data_label)\n    plt.plot(x1, y_pred, style, linewidth=2, label=label)\n    if label or data_label:\n        plt.legend(loc=\"upper center\")\n    plt.axis(axes)\n\nplt.figure(figsize=(11, 11))\n\nplt.subplot(3, 2, 1)\nplot_predictions([tree_reg1], X, y, axes=[-0.5, 0.5, -0.2, 0.8], style=\"g-\",\n                 label=\"$h_1(x_1)$\", data_label=\"Training set\")\nplt.ylabel(\"$y$  \", rotation=0)\nplt.title(\"Residuals and tree predictions\")\n\nplt.subplot(3, 2, 2)\nplot_predictions([tree_reg1], X, y, axes=[-0.5, 0.5, -0.2, 0.8], style=\"r-\",\n                 label=\"$h(x_1) = h_1(x_1)$\", data_label=\"Training set\")\nplt.title(\"Ensemble predictions\")\n\nplt.subplot(3, 2, 3)\nplot_predictions([tree_reg2], X, y2, axes=[-0.5, 0.5, -0.4, 0.6], style=\"g-\",\n                 label=\"$h_2(x_1)$\", data_style=\"k+\",\n                 data_label=\"Residuals: $y - h_1(x_1)$\")\nplt.ylabel(\"$y$  \", rotation=0)\n\nplt.subplot(3, 2, 4)\nplot_predictions([tree_reg1, tree_reg2], X, y, axes=[-0.5, 0.5, -0.2, 0.8],\n                  style=\"r-\", label=\"$h(x_1) = h_1(x_1) + h_2(x_1)$\")\n\nplt.subplot(3, 2, 5)\nplot_predictions([tree_reg3], X, y3, axes=[-0.5, 0.5, -0.4, 0.6], style=\"g-\",\n                 label=\"$h_3(x_1)$\", data_style=\"k+\",\n                 data_label=\"Residuals: $y - h_1(x_1) - h_2(x_1)$\")\nplt.xlabel(\"$x_1$\")\nplt.ylabel(\"$y$  \", rotation=0)\n\nplt.subplot(3, 2, 6)\nplot_predictions([tree_reg1, tree_reg2, tree_reg3], X, y,\n                 axes=[-0.5, 0.5, -0.2, 0.8], style=\"r-\",\n                 label=\"$h(x_1) = h_1(x_1) + h_2(x_1) + h_3(x_1)$\")\nplt.xlabel(\"$x_1$\")\n\nplt.show()"]},{"cell_type":"code","execution_count":1,"id":"f69db40b-c3ed-4b7f-a5af-765ed47e84e4","metadata":{},"outputs":[],"source":["from sklearn.ensemble import GradientBoostingRegressor\n\ngbrt = GradientBoostingRegressor(max_depth=2, n_estimators=3,\n                                 learning_rate=1.0, random_state=42)\ngbrt.fit(X, y)"]},{"cell_type":"code","execution_count":1,"id":"f2a41fab-ca70-422b-a965-debc75133cd9","metadata":{},"outputs":[],"source":["gbrt_best = GradientBoostingRegressor(\n    max_depth=2, learning_rate=0.05, n_estimators=500,\n    n_iter_no_change=10, random_state=42)\ngbrt_best.fit(X, y)"]},{"cell_type":"code","execution_count":1,"id":"0c3a9280-1f98-460f-8af3-e520ba76f39b","metadata":{},"outputs":[],"source":["fig, axes = plt.subplots(ncols=2, figsize=(10, 4), sharey=True)\n\nplt.sca(axes[0])\nplot_predictions([gbrt], X, y, axes=[-0.5, 0.5, -0.1, 0.8], style=\"r-\",\n                 label=\"Ensemble predictions\")\nplt.title(f\"learning_rate={gbrt.learning_rate}, \"\n          f\"n_estimators={gbrt.n_estimators_}\")\nplt.xlabel(\"$x_1$\")\nplt.ylabel(\"$y$\", rotation=0)\n\nplt.sca(axes[1])\nplot_predictions([gbrt_best], X, y, axes=[-0.5, 0.5, -0.1, 0.8], style=\"r-\")\nplt.title(f\"learning_rate={gbrt_best.learning_rate}, \"\n          f\"n_estimators={gbrt_best.n_estimators_}\")\nplt.xlabel(\"$x_1$\")\n\nplt.show()"]},{"cell_type":"code","execution_count":1,"id":"fad267a3-baaf-431d-97eb-c55d5309ba9d","metadata":{},"outputs":[],"source":["print(gbrt_best.n_estimators_)"]},{"cell_type":"code","execution_count":1,"id":"30bd7a8c-d9c3-41f3-9a97-80ea3a24ac64","metadata":{},"outputs":[],"source":["from sklearn.pipeline import make_pipeline\nfrom sklearn.compose import make_column_transformer\nfrom sklearn.ensemble import HistGradientBoostingRegressor\nfrom sklearn.preprocessing import OrdinalEncoder\nhgb_reg = make_pipeline(\nmake_column_transformer((OrdinalEncoder(), [\"ocean_proximity\"]),\nremainder=\"passthrough\"),\nHistGradientBoostingRegressor(categorical_features=[0], random_state=42)\n)\nhgb_reg.fit(housing, housing_labels)"]},{"cell_type":"code","execution_count":1,"id":"ce121fca-553f-45fc-bd23-19f81e17d4e9","metadata":{},"outputs":[],"source":["from sklearn.ensemble import StackingClassifier\nstacking_clf = StackingClassifier(\n    estimators=[\n        ('lr', LogisticRegression(random_state=42)),\n        ('rf', RandomForestClassifier(random_state=42)),\n        ('svc', SVC(probability=True, random_state=42))\n    ],\n    final_estimator=RandomForestClassifier(random_state=43),\n\ncv=5 # number of cross-validation folds\n)\nstacking_clf.fit(X_train, y_train)"]}],"metadata":{"org":null,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.2"}},"nbformat":4,"nbformat_minor":5}
