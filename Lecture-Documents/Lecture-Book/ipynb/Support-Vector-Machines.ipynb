{"cells":[{"cell_type":"markdown","id":"9c563d54-8a1a-4042-87e3-519ab777d9e4","metadata":{},"source":"Code for\n========\n\n"},{"cell_type":"markdown","id":"5d0d0147-63bc-46b3-8b43-1493109af2f8","metadata":{},"source":["These are the code snippets used in Support Vector Machines\npart of .\n\n"]},{"cell_type":"markdown","id":"266f9f4c-a14f-4b78-aa51-3766fcc34410","metadata":{},"source":["### Introduction\n\n"]},{"cell_type":"code","execution_count":1,"id":"66370baf-fa01-4b56-ae56-7c52927521ab","metadata":{},"outputs":[],"source":["import matplotlib.pyplot as plt\nimport ChalcedonPy as cp\n\n# Initialise ChalcedonPy\ncp.init(save_path=\"Support-Vector-Machines\",\n        display_mode=\"slide\")"]},{"cell_type":"code","execution_count":1,"id":"80fe1b7f-25db-4abf-ba19-07f5d650b823","metadata":{},"outputs":[],"source":["import matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn.svm import SVC\nfrom sklearn import datasets\n\niris = datasets.load_iris(as_frame=True)\nX = iris.data[[\"petal length (cm)\", \"petal width (cm)\"]].values\ny = iris.target\n\nsetosa_or_versicolor = (y == 0) | (y == 1)\nX = X[setosa_or_versicolor]\ny = y[setosa_or_versicolor]\n\n# SVM Classifier model\nsvm_clf = SVC(kernel=\"linear\", C=1e100)\nsvm_clf.fit(X, y)\n\n# Bad models\nx0 = np.linspace(0, 5.5, 200)\npred_1 = 5 * x0 - 20\npred_2 = x0 - 1.8\npred_3 = 0.1 * x0 + 0.5\n\ndef plot_svc_decision_boundary(svm_clf, xmin, xmax):\n    w = svm_clf.coef_[0]\n    b = svm_clf.intercept_[0]\n\n    # At the decision boundary, w0*x0 + w1*x1 + b = 0\n    # => x1 = -w0/w1 * x0 - b/w1\n    x0 = np.linspace(xmin, xmax, 200)\n    decision_boundary = -w[0] / w[1] * x0 - b / w[1]\n\n    margin = 1/w[1]\n    gutter_up = decision_boundary + margin\n    gutter_down = decision_boundary - margin\n    svs = svm_clf.support_vectors_\n\n    plt.plot(x0, decision_boundary, \"k-\", linewidth=2, zorder=-2)\n    plt.plot(x0, gutter_up, \"k--\", linewidth=2, zorder=-2)\n    plt.plot(x0, gutter_down, \"k--\", linewidth=2, zorder=-2)\n    plt.scatter(svs[:, 0], svs[:, 1], s=180, facecolors='#AAA',\n                zorder=-1)\n\nfig, axes = plt.subplots(ncols=2, figsize=(10, 2.7), sharey=True)\n\nplt.sca(axes[0])\nplt.plot(x0, pred_1, \"g--\", linewidth=2)\nplt.plot(x0, pred_2, \"m-\", linewidth=2)\nplt.plot(x0, pred_3, \"r-\", linewidth=2)\nplt.plot(X[:, 0][y==1], X[:, 1][y==1], \"bs\", label=\"Iris versicolor\")\nplt.plot(X[:, 0][y==0], X[:, 1][y==0], \"yo\", label=\"Iris setosa\")\nplt.xlabel(\"Petal length\")\nplt.ylabel(\"Petal width\")\nplt.legend(loc=\"upper left\")\nplt.axis([0, 5.5, 0, 2])\nplt.gca().set_aspect(\"equal\")\nplt.grid()\n\nplt.sca(axes[1])\nplot_svc_decision_boundary(svm_clf, 0, 5.5)\nplt.plot(X[:, 0][y==1], X[:, 1][y==1], \"bs\")\nplt.plot(X[:, 0][y==0], X[:, 1][y==0], \"yo\")\nplt.xlabel(\"Petal length\")\nplt.axis([0, 5.5, 0, 2])\nplt.gca().set_aspect(\"equal\")\nplt.grid()\n\nplt.show()"]},{"cell_type":"code","execution_count":1,"id":"648ff032-fbc1-4b87-8217-4b1244c2607e","metadata":{},"outputs":[],"source":["from sklearn.preprocessing import StandardScaler\n\nXs = np.array([[1, 50], [5, 20], [3, 80], [5, 60]]).astype(np.float64)\nys = np.array([0, 0, 1, 1])\nsvm_clf = SVC(kernel=\"linear\", C=100).fit(Xs, ys)\n\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(Xs)\nsvm_clf_scaled = SVC(kernel=\"linear\", C=100).fit(X_scaled, ys)\n\nplt.figure(figsize=(9, 2.7))\nplt.subplot(121)\nplt.plot(Xs[:, 0][ys==1], Xs[:, 1][ys==1], \"bo\")\nplt.plot(Xs[:, 0][ys==0], Xs[:, 1][ys==0], \"ms\")\nplot_svc_decision_boundary(svm_clf, 0, 6)\nplt.xlabel(\"$x_0$\")\nplt.ylabel(\"$x_1$    \", rotation=0)\nplt.title(\"Unscaled\")\nplt.axis([0, 6, 0, 90])\nplt.grid()\n\nplt.subplot(122)\nplt.plot(X_scaled[:, 0][ys==1], X_scaled[:, 1][ys==1], \"bo\")\nplt.plot(X_scaled[:, 0][ys==0], X_scaled[:, 1][ys==0], \"ms\")\nplot_svc_decision_boundary(svm_clf_scaled, -2, 2)\nplt.xlabel(\"$x'_0$\")\nplt.ylabel(\"$x'_1$  \", rotation=0)\nplt.title(\"Scaled\")\nplt.axis([-2, 2, -2, 2])\nplt.grid()\n\nplt.show()"]},{"cell_type":"code","execution_count":1,"id":"0d272ecc-01c8-463d-902e-ef5b9dfbd6a8","metadata":{},"outputs":[],"source":["# extra code – this cell generates and saves Figure 5–3\n\nX_outliers = np.array([[3.4, 1.3], [3.2, 0.8]])\ny_outliers = np.array([0, 0])\nXo1 = np.concatenate([X, X_outliers[:1]], axis=0)\nyo1 = np.concatenate([y, y_outliers[:1]], axis=0)\nXo2 = np.concatenate([X, X_outliers[1:]], axis=0)\nyo2 = np.concatenate([y, y_outliers[1:]], axis=0)\n\nsvm_clf2 = SVC(kernel=\"linear\", C=10**9)\nsvm_clf2.fit(Xo2, yo2)\n\nfig, axes = plt.subplots(ncols=2, figsize=(10, 2.7), sharey=True)\n\nplt.sca(axes[0])\nplt.plot(Xo1[:, 0][yo1==1], Xo1[:, 1][yo1==1], \"bs\")\nplt.plot(Xo1[:, 0][yo1==0], Xo1[:, 1][yo1==0], \"yo\")\nplt.text(0.3, 1.0, \"Impossible!\", color=\"red\", fontsize=18)\nplt.xlabel(\"Petal length\")\nplt.ylabel(\"Petal width\")\nplt.annotate(\n    \"Outlier\",\n    xy=(X_outliers[0][0], X_outliers[0][1]),\n    xytext=(2.5, 1.7),\n    ha=\"center\",\n    arrowprops=dict(facecolor='black', shrink=0.1),\n)\nplt.axis([0, 5.5, 0, 2])\nplt.grid()\n\nplt.sca(axes[1])\nplt.plot(Xo2[:, 0][yo2==1], Xo2[:, 1][yo2==1], \"bs\")\nplt.plot(Xo2[:, 0][yo2==0], Xo2[:, 1][yo2==0], \"yo\")\nplot_svc_decision_boundary(svm_clf2, 0, 5.5)\nplt.xlabel(\"Petal length\")\nplt.annotate(\n    \"Outlier\",\n    xy=(X_outliers[1][0], X_outliers[1][1]),\n    xytext=(3.2, 0.08),\n    ha=\"center\",\n    arrowprops=dict(facecolor='black', shrink=0.1),\n)\nplt.axis([0, 5.5, 0, 2])\nplt.grid()\n\nplt.show()"]},{"cell_type":"code","execution_count":1,"id":"84a279e9-f2d1-4a42-b04f-58d95289d06c","metadata":{},"outputs":[],"source":["from sklearn.datasets import load_iris\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.svm import LinearSVC\niris = load_iris(as_frame=True)\nX = iris.data[[\"petal length (cm)\", \"petal width (cm)\"]].values\ny = (iris.target == 2) # Iris virginica\nsvm_clf = make_pipeline(StandardScaler(),\nLinearSVC(C=1, random_state=42))\nsvm_clf.fit(X, y)"]},{"cell_type":"code","execution_count":1,"id":"a41972f6-ed32-41ac-9197-5119622048e7","metadata":{},"outputs":[],"source":["X_new = [[5.5, 1.7], [5.0, 1.5]]\nprint(svm_clf.predict(X_new))"]},{"cell_type":"code","execution_count":1,"id":"fcc96150-0536-4d9d-a0cc-9063b15e70e4","metadata":{},"outputs":[],"source":["print(svm_clf.decision_function(X_new))"]},{"cell_type":"code","execution_count":1,"id":"8e222402-96f4-4760-8866-e1a0488a357e","metadata":{},"outputs":[],"source":["scaler = StandardScaler()\nsvm_clf1 = LinearSVC(C=1, max_iter=10_000, dual=True, random_state=42)\nsvm_clf2 = LinearSVC(C=100, max_iter=10_000, dual=True, random_state=42)\n\nscaled_svm_clf1 = make_pipeline(scaler, svm_clf1)\nscaled_svm_clf2 = make_pipeline(scaler, svm_clf2)\n\nscaled_svm_clf1.fit(X, y)\nscaled_svm_clf2.fit(X, y)\n\n# Convert to unscaled parameters\nb1 = svm_clf1.decision_function([-scaler.mean_ / scaler.scale_])\nb2 = svm_clf2.decision_function([-scaler.mean_ / scaler.scale_])\nw1 = svm_clf1.coef_[0] / scaler.scale_\nw2 = svm_clf2.coef_[0] / scaler.scale_\nsvm_clf1.intercept_ = np.array([b1])\nsvm_clf2.intercept_ = np.array([b2])\nsvm_clf1.coef_ = np.array([w1])\nsvm_clf2.coef_ = np.array([w2])\n\n# Find support vectors (LinearSVC does not do this automatically)\nt = y * 2 - 1\nsupport_vectors_idx1 = (t * (X.dot(w1) + b1) < 1).ravel()\nsupport_vectors_idx2 = (t * (X.dot(w2) + b2) < 1).ravel()\nsvm_clf1.support_vectors_ = X[support_vectors_idx1]\nsvm_clf2.support_vectors_ = X[support_vectors_idx2]\n\nfig, axes = plt.subplots(ncols=2, figsize=(10, 2.7), sharey=True)\n\nplt.sca(axes[0])\nplt.plot(X[:, 0][y==1], X[:, 1][y==1], \"g^\", label=\"Iris virginica\")\nplt.plot(X[:, 0][y==0], X[:, 1][y==0], \"bs\", label=\"Iris versicolor\")\nplot_svc_decision_boundary(svm_clf1, 4, 5.9)\nplt.xlabel(\"Petal length\")\nplt.ylabel(\"Petal width\")\nplt.legend(loc=\"upper left\")\nplt.title(f\"$C = {svm_clf1.C}$\")\nplt.axis([4, 5.9, 0.8, 2.8])\nplt.grid()\n\nplt.sca(axes[1])\nplt.plot(X[:, 0][y==1], X[:, 1][y==1], \"g^\")\nplt.plot(X[:, 0][y==0], X[:, 1][y==0], \"bs\")\nplot_svc_decision_boundary(svm_clf2, 4, 5.99)\nplt.xlabel(\"Petal length\")\nplt.title(f\"$C = {svm_clf2.C}$\")\nplt.axis([4, 5.9, 0.8, 2.8])\nplt.grid()\n\nplt.show()"]},{"cell_type":"markdown","id":"30ff0b97-cdca-457d-b06b-affbd4477883","metadata":{},"source":["### NonLinear SVM Classification\n\n"]},{"cell_type":"code","execution_count":1,"id":"9f7f6f52-5b1b-4120-9bfd-c6323c43c307","metadata":{},"outputs":[],"source":["X1D = np.linspace(-4, 4, 9).reshape(-1, 1)\nX2D = np.c_[X1D, X1D**2]\ny = np.array([0, 0, 1, 1, 1, 1, 1, 0, 0])\n\nplt.figure(figsize=(10, 3))\n\nplt.subplot(121)\nplt.grid(True)\nplt.axhline(y=0, color='k')\nplt.plot(X1D[:, 0][y==0], np.zeros(4), \"bs\")\nplt.plot(X1D[:, 0][y==1], np.zeros(5), \"g^\")\nplt.gca().get_yaxis().set_ticks([])\nplt.xlabel(\"$x_1$\")\nplt.axis([-4.5, 4.5, -0.2, 0.2])\n\nplt.subplot(122)\nplt.grid(True)\nplt.axhline(y=0, color='k')\nplt.axvline(x=0, color='k')\nplt.plot(X2D[:, 0][y==0], X2D[:, 1][y==0], \"bs\")\nplt.plot(X2D[:, 0][y==1], X2D[:, 1][y==1], \"g^\")\nplt.xlabel(\"$x_1$\")\nplt.ylabel(\"$x_2$  \", rotation=0)\nplt.gca().get_yaxis().set_ticks([0, 4, 8, 12, 16])\nplt.plot([-4.5, 4.5], [6.5, 6.5], \"r--\", linewidth=3)\nplt.axis([-4.5, 4.5, -1, 17])\n\nplt.subplots_adjust(right=1)\n\nplt.show()"]},{"cell_type":"code","execution_count":1,"id":"10aa3e4e-0b9c-41c8-ac70-0ab093f00e38","metadata":{},"outputs":[],"source":["from sklearn.datasets import make_moons\nfrom sklearn.preprocessing import PolynomialFeatures\n\nX, y = make_moons(n_samples=100, noise=0.15, random_state=42)\n\npolynomial_svm_clf = make_pipeline(\n    PolynomialFeatures(degree=3),\n    StandardScaler(),\n    LinearSVC(C=10, max_iter=10_000, random_state=42)\n)\npolynomial_svm_clf.fit(X, y)"]},{"cell_type":"code","execution_count":1,"id":"31a1b7c4-7b60-4a60-985e-e1f34c3cf783","metadata":{},"outputs":[],"source":["def plot_dataset(X, y, axes):\n    plt.plot(X[:, 0][y==0], X[:, 1][y==0], \"bs\")\n    plt.plot(X[:, 0][y==1], X[:, 1][y==1], \"g^\")\n    plt.axis(axes)\n    plt.grid(True)\n    plt.xlabel(\"$x_1$\")\n    plt.ylabel(\"$x_2$\", rotation=0)\n\ndef plot_predictions(clf, axes):\n    x0s = np.linspace(axes[0], axes[1], 100)\n    x1s = np.linspace(axes[2], axes[3], 100)\n    x0, x1 = np.meshgrid(x0s, x1s)\n    X = np.c_[x0.ravel(), x1.ravel()]\n    y_pred = clf.predict(X).reshape(x0.shape)\n    y_decision = clf.decision_function(X).reshape(x0.shape)\n    plt.contourf(x0, x1, y_pred, cmap=plt.cm.brg, alpha=0.2)\n    plt.contourf(x0, x1, y_decision, cmap=plt.cm.brg, alpha=0.1)\n\nplot_predictions(polynomial_svm_clf, [-1.5, 2.5, -1, 1.5])\nplot_dataset(X, y, [-1.5, 2.5, -1, 1.5])\n\nplt.show()"]},{"cell_type":"code","execution_count":1,"id":"61c8d614-c132-40b9-8916-3a1dedfb0802","metadata":{},"outputs":[],"source":["from sklearn.svm import SVC\n\npoly_kernel_svm_clf = make_pipeline(\n    StandardScaler(),\n    SVC(kernel=\"poly\", degree=3, coef0=1, C=5)\n)\npoly_kernel_svm_clf.fit(X, y)"]},{"cell_type":"code","execution_count":1,"id":"f06a8ee5-68d9-4bd1-8443-e4cd9abb53af","metadata":{},"outputs":[],"source":["poly100_kernel_svm_clf = make_pipeline(\n    StandardScaler(),\n    SVC(kernel=\"poly\", degree=10, coef0=100, C=5)\n)\n\npoly100_kernel_svm_clf.fit(X, y)\n\nfig, axes = plt.subplots(ncols=2, figsize=(10.5, 4), sharey=True)\n\nplt.sca(axes[0])\nplot_predictions(poly_kernel_svm_clf, [-1.5, 2.45, -1, 1.5])\nplot_dataset(X, y, [-1.5, 2.4, -1, 1.5])\nplt.title(\"degree=3, coef0=1, C=5\")\n\nplt.sca(axes[1])\nplot_predictions(poly100_kernel_svm_clf, [-1.5, 2.45, -1, 1.5])\nplot_dataset(X, y, [-1.5, 2.4, -1, 1.5])\nplt.title(\"degree=10, coef0=100, C=5\")\nplt.ylabel(\"\")\n\nplt.show()"]},{"cell_type":"markdown","id":"00c8b2c9-1734-4c78-b881-0166f40976df","metadata":{},"source":["#### Similarity Features\n\n"]},{"cell_type":"code","execution_count":1,"id":"6a200ffb-7701-4ec7-bc45-4a27490ca576","metadata":{},"outputs":[],"source":["def gaussian_rbf(x, landmark, gamma):\n    return np.exp(-gamma * np.linalg.norm(x - landmark, axis=1)**2)\n\ngamma = 0.3\n\nx1s = np.linspace(-4.5, 4.5, 200).reshape(-1, 1)\nx2s = gaussian_rbf(x1s, -2, gamma)\nx3s = gaussian_rbf(x1s, 1, gamma)\n\nXK = np.c_[gaussian_rbf(X1D, -2, gamma), gaussian_rbf(X1D, 1, gamma)]\nyk = np.array([0, 0, 1, 1, 1, 1, 1, 0, 0])\n\nplt.figure(figsize=(10.5, 4))\n\nplt.subplot(121)\nplt.grid(True)\nplt.axhline(y=0, color='k')\nplt.scatter(x=[-2, 1], y=[0, 0], s=150, alpha=0.5, c=\"red\")\nplt.plot(X1D[:, 0][yk==0], np.zeros(4), \"bs\")\nplt.plot(X1D[:, 0][yk==1], np.zeros(5), \"g^\")\nplt.plot(x1s, x2s, \"g--\")\nplt.plot(x1s, x3s, \"b:\")\nplt.gca().get_yaxis().set_ticks([0, 0.25, 0.5, 0.75, 1])\nplt.xlabel(\"$x_1$\")\nplt.ylabel(\"Similarity\")\nplt.annotate(\n    r'$\\mathbf{x}$',\n    xy=(X1D[3, 0], 0),\n    xytext=(-0.5, 0.20),\n    ha=\"center\",\n    arrowprops=dict(facecolor='black', shrink=0.1),\n    fontsize=16,\n)\nplt.text(-2, 0.9, \"$x_2$\", ha=\"center\", fontsize=15)\nplt.text(1, 0.9, \"$x_3$\", ha=\"center\", fontsize=15)\nplt.axis([-4.5, 4.5, -0.1, 1.1])\n\nplt.subplot(122)\nplt.grid(True)\nplt.axhline(y=0, color='k')\nplt.axvline(x=0, color='k')\nplt.plot(XK[:, 0][yk==0], XK[:, 1][yk==0], \"bs\")\nplt.plot(XK[:, 0][yk==1], XK[:, 1][yk==1], \"g^\")\nplt.xlabel(\"$x_2$\")\nplt.ylabel(\"$x_3$  \", rotation=0)\nplt.annotate(\n    r'$\\phi\\left(\\mathbf{x}\\right)$',\n    xy=(XK[3, 0], XK[3, 1]),\n    xytext=(0.65, 0.50),\n    ha=\"center\",\n    arrowprops=dict(facecolor='black', shrink=0.1),\n    fontsize=16,\n)\nplt.plot([-0.1, 1.1], [0.57, -0.1], \"r--\", linewidth=3)\nplt.axis([-0.1, 1.1, -0.1, 1.1])\n    \nplt.subplots_adjust(right=1)\n\nplt.show()"]},{"cell_type":"markdown","id":"b55d82a2-d8ff-41c2-b78f-7aca31e9819c","metadata":{},"source":["#### Gaussian RBF Kernel\n\n"]},{"cell_type":"code","execution_count":1,"id":"f03607fe-a58f-499f-b6b2-f16302d2140c","metadata":{},"outputs":[],"source":["rbf_kernel_svm_clf = make_pipeline(\n    StandardScaler(),\n    SVC(kernel=\"rbf\", gamma=5, C=0.001)\n)\nrbf_kernel_svm_clf.fit(X, y)"]},{"cell_type":"code","execution_count":1,"id":"a0f2b311-31a3-489e-88a9-b96194c11fac","metadata":{},"outputs":[],"source":["from sklearn.svm import SVC\n\ngamma1, gamma2 = 0.1, 5\nC1, C2 = 0.001, 1000\nhyperparams = (gamma1, C1), (gamma1, C2), (gamma2, C1), (gamma2, C2)\n\nsvm_clfs = []\nfor gamma, C in hyperparams:\n    rbf_kernel_svm_clf = make_pipeline(\n        StandardScaler(),\n        SVC(kernel=\"rbf\", gamma=gamma, C=C)\n    )\n    rbf_kernel_svm_clf.fit(X, y)\n    svm_clfs.append(rbf_kernel_svm_clf)\n\nfig, axes = plt.subplots(nrows=2, ncols=2, figsize=(10.5, 7), sharex=True, sharey=True)\n\nfor i, svm_clf in enumerate(svm_clfs):\n    plt.sca(axes[i // 2, i % 2])\n    plot_predictions(svm_clf, [-1.5, 2.45, -1, 1.5])\n    plot_dataset(X, y, [-1.5, 2.45, -1, 1.5])\n    gamma, C = hyperparams[i]\n    plt.title(f\"gamma={gamma}, C={C}\")\n    if i in (0, 1):\n        plt.xlabel(\"\")\n    if i in (1, 3):\n        plt.ylabel(\"\")\n\nplt.show()"]},{"cell_type":"markdown","id":"cc4fafab-e845-4704-a91d-1bbab89cf040","metadata":{},"source":["### SVM Regression\n\n"]},{"cell_type":"code","execution_count":1,"id":"daedbf24-1539-4ecd-85df-41a4998803fe","metadata":{},"outputs":[],"source":["from sklearn.svm import LinearSVR\n\nnp.random.seed(42)\nX = 2 * np.random.rand(50, 1)\ny = 4 + 3 * X[:, 0] + np.random.randn(50)\n\nsvm_reg = make_pipeline(\n    StandardScaler(),\n    LinearSVR(epsilon=0.5, dual=True, random_state=42)\n)\nsvm_reg.fit(X, y)"]},{"cell_type":"code","execution_count":1,"id":"8de82934-8fdc-44a9-9fcb-f0ad28f92326","metadata":{},"outputs":[],"source":["def find_support_vectors(svm_reg, X, y):\n    y_pred = svm_reg.predict(X)\n    epsilon = svm_reg[-1].epsilon\n    off_margin = np.abs(y - y_pred) >= epsilon\n    return np.argwhere(off_margin)\n\ndef plot_svm_regression(svm_reg, X, y, axes):\n    x1s = np.linspace(axes[0], axes[1], 100).reshape(100, 1)\n    y_pred = svm_reg.predict(x1s)\n    epsilon = svm_reg[-1].epsilon\n    plt.plot(x1s, y_pred, \"k-\", linewidth=2, label=r\"$\\hat{y}$\", zorder=-2)\n    plt.plot(x1s, y_pred + epsilon, \"k--\", zorder=-2)\n    plt.plot(x1s, y_pred - epsilon, \"k--\", zorder=-2)\n    plt.scatter(X[svm_reg._support], y[svm_reg._support], s=180,\n                facecolors='#AAA', zorder=-1)\n    plt.plot(X, y, \"bo\")\n    plt.xlabel(\"$x_1$\")\n    plt.legend(loc=\"upper left\")\n    plt.axis(axes)\n\nsvm_reg2 = make_pipeline(StandardScaler(),\n                         LinearSVR(epsilon=1.2, dual=True, random_state=42))\nsvm_reg2.fit(X, y)\n\nsvm_reg._support = find_support_vectors(svm_reg, X, y)\nsvm_reg2._support = find_support_vectors(svm_reg2, X, y)\n\neps_x1 = 1\neps_y_pred = svm_reg2.predict([[eps_x1]])\n\nfig, axes = plt.subplots(ncols=2, figsize=(9, 4), sharey=True)\nplt.sca(axes[0])\nplot_svm_regression(svm_reg, X, y, [0, 2, 3, 11])\nplt.title(f\"epsilon={svm_reg[-1].epsilon}\")\nplt.ylabel(\"$y$\", rotation=0)\nplt.grid()\nplt.sca(axes[1])\nplot_svm_regression(svm_reg2, X, y, [0, 2, 3, 11])\nplt.title(f\"epsilon={svm_reg2[-1].epsilon}\")\nplt.annotate(\n        '', xy=(eps_x1, eps_y_pred), xycoords='data',\n        xytext=(eps_x1, eps_y_pred - svm_reg2[-1].epsilon),\n        textcoords='data', arrowprops={'arrowstyle': '<->', 'linewidth': 1.5}\n    )\nplt.text(0.90, 5.4, r\"$\\epsilon$\", fontsize=16)\nplt.grid()\n\nplt.show()"]},{"cell_type":"code","execution_count":1,"id":"23c7dfb2-facd-4fab-99ca-1fcde9aaa53e","metadata":{},"outputs":[],"source":["from sklearn.svm import SVR\n\n# extra code – these 3 lines generate a simple quadratic dataset\nnp.random.seed(42)\nX = 2 * np.random.rand(50, 1) - 1\ny = 0.2 + 0.1 * X[:, 0] + 0.5 * X[:, 0] ** 2 + np.random.randn(50) / 10\n\nsvm_poly_reg = make_pipeline(\n    StandardScaler(),\n    SVR(kernel=\"poly\", degree=2, C=0.01, epsilon=0.1)\n)\n\nsvm_poly_reg.fit(X, y)"]},{"cell_type":"code","execution_count":1,"id":"df522ee6-6478-4631-8210-152882329733","metadata":{},"outputs":[],"source":["svm_poly_reg2 = make_pipeline(StandardScaler(),\n                             SVR(kernel=\"poly\", degree=2, C=100))\nsvm_poly_reg2.fit(X, y)\n\nsvm_poly_reg._support = find_support_vectors(svm_poly_reg, X, y)\nsvm_poly_reg2._support = find_support_vectors(svm_poly_reg2, X, y)\n\nfig, axes = plt.subplots(ncols=2, figsize=(9, 4), sharey=True)\nplt.sca(axes[0])\nplot_svm_regression(svm_poly_reg, X, y, [-1, 1, 0, 1])\nplt.title(f\"degree={svm_poly_reg[-1].degree}, \"\n          f\"C={svm_poly_reg[-1].C}, \"\n          f\"epsilon={svm_poly_reg[-1].epsilon}\")\nplt.ylabel(\"$y$\", rotation=0)\nplt.grid()\n\nplt.sca(axes[1])\nplot_svm_regression(svm_poly_reg2, X, y, [-1, 1, 0, 1])\nplt.title(f\"degree={svm_poly_reg2[-1].degree}, \"\n          f\"C={svm_poly_reg2[-1].C}, \"\n          f\"epsilon={svm_poly_reg2[-1].epsilon}\")\nplt.grid()\nplt.show()"]},{"cell_type":"markdown","id":"f27e0fc1-87f0-48ee-bb65-81502e3cee0a","metadata":{},"source":["### Under the Hood\n\n"]},{"cell_type":"code","execution_count":1,"id":"6ec25a6a-af44-4dba-ae81-9041ed131cf8","metadata":{},"outputs":[],"source":["import matplotlib.patches as patches\n\ndef plot_2D_decision_function(w, b, ylabel=True, x1_lim=[-3, 3]):\n    x1 = np.linspace(x1_lim[0], x1_lim[1], 200)\n    y = w * x1 + b\n    half_margin = 1 / w\n\n    plt.plot(x1, y, \"b-\", linewidth=2, label=r\"$s = w_1 x_1$\")\n    plt.axhline(y=0, color='k', linewidth=1)\n    plt.axvline(x=0, color='k', linewidth=1)\n    rect = patches.Rectangle((-half_margin, -2), 2 * half_margin, 4,\n                             edgecolor='none', facecolor='gray', alpha=0.2)\n    plt.gca().add_patch(rect)\n    plt.plot([-3, 3], [1, 1], \"k--\", linewidth=1)\n    plt.plot([-3, 3], [-1, -1], \"k--\", linewidth=1)\n    plt.plot(half_margin, 1, \"k.\")\n    plt.plot(-half_margin, -1, \"k.\")\n    plt.axis(x1_lim + [-2, 2])\n    plt.xlabel(\"$x_1$\")\n    if ylabel:\n        plt.ylabel(\"$s$\", rotation=0, labelpad=5)\n        plt.legend()\n        plt.text(1.02, -1.6, \"Margin\", ha=\"left\", va=\"center\", color=\"k\")\n\n    plt.annotate(\n        '', xy=(-half_margin, -1.6), xytext=(half_margin, -1.6),\n        arrowprops={'ec': 'k', 'arrowstyle': '<->', 'linewidth': 1.5}\n    )\n    plt.title(f\"$w_1 = {w}$\")\n\nfig, axes = plt.subplots(ncols=2, figsize=(9, 3.2), sharey=True)\nplt.sca(axes[0])\nplot_2D_decision_function(1, 0)\nplt.grid()\nplt.sca(axes[1])\nplot_2D_decision_function(0.5, 0, ylabel=False)\nplt.grid()\nplt.show()"]},{"cell_type":"code","execution_count":1,"id":"859099c9-6239-475d-87a0-f2e44c9ebc31","metadata":{},"outputs":[],"source":["s = np.linspace(-2.5, 2.5, 200)\nhinge_pos = np.where(1 - s < 0, 0, 1 - s)  # max(0, 1 - s)\nhinge_neg = np.where(1 + s < 0, 0, 1 + s)  # max(0, 1 + s)\n\ntitles = (r\"Hinge loss = $max(0, 1 - s\\,t)$\", \"Squared Hinge loss\")\n\nfix, axs = plt.subplots(1, 2, sharey=True, figsize=(8.2, 3))\n\nfor ax, loss_pos, loss_neg, title in zip(\n        axs, (hinge_pos, hinge_pos ** 2), (hinge_neg, hinge_neg ** 2), titles):\n    ax.plot(s, loss_pos, \"g-\", linewidth=2, zorder=10, label=\"$t=1$\")\n    ax.plot(s, loss_neg, \"r--\", linewidth=2, zorder=10, label=\"$t=-1$\")\n    ax.grid(True)\n    ax.axhline(y=0, color='k')\n    ax.axvline(x=0, color='k')\n    ax.set_xlabel(r\"$s = \\mathbf{w}^\\intercal \\mathbf{x} + b$\")\n    ax.axis([-2.5, 2.5, -0.5, 2.5])\n    ax.legend(loc=\"center right\")\n    ax.set_title(title)\n    ax.set_yticks(np.arange(0, 2.5, 1))\n    ax.set_aspect(\"equal\")\n\nplt.show()"]}],"metadata":{"org":null,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.2"}},"nbformat":4,"nbformat_minor":5}
