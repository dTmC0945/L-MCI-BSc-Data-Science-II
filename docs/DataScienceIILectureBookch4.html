<!DOCTYPE html>
<html lang="en-US" xml:lang="en-US">
 <head>
  <title>
   4 Dimensionality Reduction
  </title>
    <link rel='icon' type='image/x-icon' href='figures/logo.svg'></link>
  <meta charset="utf-8"/>
  <meta content="ParSnip" name="generator"/>
  <meta content="width=device-width,initial-scale=1" name="viewport"/>
  <link href="DataScienceIILectureBook.css" rel="stylesheet" type="text/css"/>
  <meta content="DataScienceIILectureBook.tex" name="src"/>
  <script async="async" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/mml-chtml.js" type="text/javascript">
  </script>
  <link href="flatweb.css" rel="stylesheet" type="text/css"/>
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.7.1/jquery.min.js">
  </script>
  <script src="flatjs.js">
  </script>
  <link href="http://fonts.googleapis.com/css?family=Roboto" rel="stylesheet" type="text/css"/>
  <script type="module">
   import pdfjsDist from 'https://cdn.jsdelivr.net/npm/pdfjs-dist@5.3.93/+esm'
  </script>
  <script>
   function moveTOC() { 
var htmlShow = document.getElementsByClassName("wide"); 
if (htmlShow[0].style.display === "none") { 
htmlShow[0].style.display = "block"; 
} else { 
htmlShow[0].style.display = "none"; 
} 
}
  </script>
  <link href="https://unpkg.com/nord-highlightjs@0.1.0/dist/nord.css" rel="stylesheet" type="text/css"/>
  <script src="https://unpkg.com/@highlightjs/cdn-assets@11.3.1/highlight.min.js">
  </script>
  <script src="https://unpkg.com/highlightjs-copy/dist/highlightjs-copy.min.js">
  </script>
  <link href="https://unpkg.com/highlightjs-copy/dist/highlightjs-copy.min.css" rel="stylesheet"/>
  <script src="flatjs.js">
  </script>
  <header>
  </header>
 </head>
 <body id="top">
  <nav class="wide">
   <div class="contents">
    <h3 style="font-weight:lighter; padding: 20px 0 20px 0;">
     4
   
     
   

   Dimensionality Reduction
    </h3>
    <h4 style="font-weight:lighter">
     Chapter Table of Contents
    </h4>
    <ul>
     <div class="chapterTOCS">
      <li>
       <span class="sectionToc">
        <small>
         4.1
        </small>
        <a href="#x6-430004.1">
         Introduction
        </a>
       </span>
      </li>
      <li>
       <span class="subsectionToc" style="font-size:12px;margin:0px;">
        <small>
         4.1.1
        </small>
        <a href="#x6-440004.1.1">
         The Problems of Dimensions
        </a>
       </span>
      </li>
      <li>
       <span class="sectionToc">
        <small>
         4.2
        </small>
        <a href="#x6-450004.2">
         Main Approaches to Dimensionality Reduction
        </a>
       </span>
      </li>
      <li>
       <span class="subsectionToc" style="font-size:12px;margin:0px;">
        <small>
         4.2.1
        </small>
        <a href="#x6-460004.2.1">
         Projection
        </a>
       </span>
      </li>
      <li>
       <span class="subsectionToc" style="font-size:12px;margin:0px;">
        <small>
         4.2.2
        </small>
        <a href="#x6-470004.2.2">
         Manifold Learning
        </a>
       </span>
      </li>
      <li>
       <span class="sectionToc">
        <small>
         4.3
        </small>
        <a href="#x6-480004.3">
         Principal Component Analysis (PCA)
        </a>
       </span>
      </li>
      <li>
       <span class="subsectionToc" style="font-size:12px;margin:0px;">
        <small>
         4.3.1
        </small>
        <a href="#x6-490004.3.1">
         Preserving the Variance
        </a>
       </span>
      </li>
      <li>
       <span class="subsectionToc" style="font-size:12px;margin:0px;">
        <small>
         4.3.2
        </small>
        <a href="#x6-500004.3.2">
         Principal Components
        </a>
       </span>
      </li>
      <li>
       <span class="subsectionToc" style="font-size:12px;margin:0px;">
        <small>
         4.3.3
        </small>
        <a href="#x6-510004.3.3">
         Downgrading Dimensions
        </a>
       </span>
      </li>
      <li>
       <span class="subsectionToc" style="font-size:12px;margin:0px;">
        <small>
         4.3.4
        </small>
        <a href="#x6-540004.3.4">
         The Right Number of Dimensions
        </a>
       </span>
      </li>
      <li>
       <span class="subsectionToc" style="font-size:12px;margin:0px;">
        <small>
         4.3.5
        </small>
        <a href="#x6-550004.3.5">
         PCA for Compression
        </a>
       </span>
      </li>
      <li>
       <span class="subsectionToc" style="font-size:12px;margin:0px;">
        <small>
         4.3.6
        </small>
        <a href="#x6-560004.3.6">
         Randomized PCA
        </a>
       </span>
      </li>
      <li>
       <span class="subsectionToc" style="font-size:12px;margin:0px;">
        <small>
         4.3.7
        </small>
        <a href="#x6-570004.3.7">
         Incremental PCA
        </a>
       </span>
      </li>
      <li>
       <span class="sectionToc">
        <small>
         4.4
        </small>
        <a href="#x6-580004.4">
         Random Projection
        </a>
       </span>
      </li>
      <li>
       <span class="sectionToc">
        <small>
         4.5
        </small>
        <a href="#x6-590004.5">
         Locally Linear Embedding
        </a>
       </span>
      </li>
     </div>
    </ul>
    <div class="prev-next" style="text-align: center">
     <p class="noindent">
      <a href="DataScienceIILectureBookch5.html" style="float:right; font-size:10px">
       NEXT →
      </a>
      <a href="DataScienceIILectureBookch3.html" style="float:left; font-size:10px">
       ← PREV
      </a>
      <a href="index.html">
       TOP
      </a>
     </p>
    </div>
    <div id="author-info" style="bottom: 0; padding-bottom: 10px;">
     <div id="author-logo">
      <img src="figures/logo.svg" style="margin: 10px 0;"/>
      <div>
       <div>
        <h4 style="color:#7aa0b8; font-weight:lighter;">
         WebBook | D. T. McGuiness, P.hD
        </h4>
       </div>
      </div>
      <div id="author-text" style="bottom: 0; padding-top: 50px;border-top: solid 1px #3b4b5e;font-size: 12px;text-align: right;">
       <p>
        <b>
         Authors Note
        </b>
        The website you are viewing is auto-generated
    using ParSnip and therefore subject to slight errors in
    typography and formatting. When in doubt, please consult the
    LectureBook.
       </p>
      </div>
     </div>
    </div>
   </div>
  </nav>
  <div class="page">
   <article class="chapter">
    <h1 class="chapterHead">
     <div class="number">
      4
     </div>
     <a id="x6-420004">
     </a>
     Dimensionality Reduction
    </h1><button id='toc-button' onclick='moveTOC()'>TOC</button>
    <div class="section-control" style="text-align: center">
     <a id="prev-section" onclick="goPrev()" style="float:left;">
      ← Previous Section
     </a>
     <a id="next-section" onclick="goNext()" style="float:right;">
      Next Section →
     </a>
    </div>
    <h2 class="sectionHead">
     <a aria-label="Link to this section in PDF version" class="pdf-link" href="DataScienceIILectureBook.pdf#section.4.1" style="color: red;  position: absolute;  left: -30px;" target="_blank">
      <svg class="bi bi-file-pdf" fill="currentColor" height="20" viewbox="0 0 16 16" width="20" xmlns="http://www.w3.org/2000/svg">
       <path d="M4 0a2 2 0 0 0-2 2v12a2 2 0 0 0 2 2h8a2 2 0 0 0 2-2V2a2 2 0 0 0-2-2H4zm0 1h8a1 1 0 0 1 1 1v12a1 1 0 0 1-1 1H4a1 1 0 0 1-1-1V2a1 1 0 0 1 1-1z">
       </path>
       <path d="M4.603 12.087a.81.81 0 0 1-.438-.42c-.195-.388-.13-.776.08-1.102.198-.307.526-.568.897-.787a7.68 7.68 0 0 1 1.482-.645 19.701 19.701 0 0 0 1.062-2.227 7.269 7.269 0 0 1-.43-1.295c-.086-.4-.119-.796-.046-1.136.075-.354.274-.672.65-.823.192-.077.4-.12.602-.077a.7.7 0 0 1 .477.365c.088.164.12.356.127.538.007.187-.012.395-.047.614-.084.51-.27 1.134-.52 1.794a10.954 10.954 0 0 0 .98 1.686 5.753 5.753 0 0 1 1.334.05c.364.065.734.195.96.465.12.144.193.32.2.518.007.192-.047.382-.138.563a1.04 1.04 0 0 1-.354.416.856.856 0 0 1-.51.138c-.331-.014-.654-.196-.933-.417a5.716 5.716 0 0 1-.911-.95 11.642 11.642 0 0 0-1.997.406 11.311 11.311 0 0 1-1.021 1.51c-.29.35-.608.655-.926.787a.793.793 0 0 1-.58.029zm1.379-1.901c-.166.076-.32.156-.459.238-.328.194-.541.383-.647.547-.094.145-.096.25-.04.361.01.022.02.036.026.044a.27.27 0 0 0 .035-.012c.137-.056.355-.235.635-.572a8.18 8.18 0 0 0 .45-.606zm1.64-1.33a12.647 12.647 0 0 1 1.01-.193 11.666 11.666 0 0 1-.51-.858 20.741 20.741 0 0 1-.5 1.05zm2.446.45c.15.162.296.3.435.41.24.19.407.253.498.256a.107.107 0 0 0 .07-.015.307.307 0 0 0 .094-.125.436.436 0 0 0 .059-.2.095.095 0 0 0-.026-.063c-.052-.062-.2-.152-.518-.209a3.881 3.881 0 0 0-.612-.053zM8.078 5.8a6.7 6.7 0 0 0 .2-.828c.031-.188.043-.343.038-.465a.613.613 0 0 0-.032-.198.517.517 0 0 0-.145.04c-.087.035-.158.106-.196.283-.04.192-.03.469.046.822.024.111.054.227.09.346z">
       </path>
      </svg>
     </a>
     <small class="titlemark">
      4.1
     </small>
     <a id="x6-430004.1">
     </a>
     Introduction
    </h2>
    <p class="noindent">
     Many
     <a class="glossary" href="DataScienceIILectureBookli2.html#glo:ml">
      ML
     </a>
     problems
involve
thousands
or
even
millions
of
features
for
each
training
instance.
Not
only
do
all
these
features
make
training
extremely
slow,
but
                                                                                
                                                                                
they
can
also
make
it
much
harder
to
find
a
good
solution,
which
we
will
see
in
the
continuing
parts
of
this
chapter.
This
problem
is
often
referred
to
as
the
curse
of
dimensionality.
     <alert style="color: #821131;">
      <span id="bold" style="font-weight:bold;">
       <sup class="textsuperscript">
        1
       </sup>
      </span>
     </alert>
     <span class="marginnote">
      <alert style="color: #821131;">
       <span id="bold" style="font-weight:bold;">
        <sup class="textsuperscript">
         1
        </sup>
       </span>
      </alert>
      <span style="color:#0063B2;">
       Refers
to
various
phenomena
that
arise
when
analyzing
and
organizing
data
in
high-dimensional
spaces
that
do
not
occur
in
low-dimensional
settings
such
as
the
three-dimensional
physical
space
of
everyday
experience <a id="x6-43001"></a><a href="#X0-bellman1957dynamic">[7]</a>.
      </span>
     </span>
    </p>
    <p class="noindent">
     Fortunately,
in
                                                                                
                                                                                
real-world
problems,
it
is
often
possible
to
reduce
the
number
of
features
considerably,
turning
an
intractable
problem
into
a
tractable
one.
    </p>
    <p class="noindent">
     For
example,
consider
the
MNIST
images
we
worked
previously.
The
pixels
on
the
image
borders
are
almost
always
white,
so
we
could
completely
drop
these
pixels
from
the
training
set
without
losing
any
information.
    </p>
    <p class="noindent">
     As
we
saw
previously
in
Ensemble
Learning,
we
have
confirmed
these
pixels
                                                                                
                                                                                
are
unimportant
for
the
classification
task,
shown
in
     <span id="bold" style="font-weight:bold;">
      Fig.
     </span>
     <a href="DataScienceIILectureBookch3.html#x5-33023r3">
      3.3
     </a>.
Additionally,
two
<alert style="color: #821131;">(2)</alert> neighboring
pixels
are
often
     <span id="bold" style="font-weight:bold;">
      highly
correlated
     </span>
     :
    </p>
    <p class="noindent">
    </p>
    <div class="quoteblock">
     <p class="noindent">
      if
we
merge
them
into
a
single
pixel
(e.g.,
by
taking
the
mean
of
the
two
pixel
intensities),
we
will
      <alert style="color: #821131;">
       not
lose
much
information
      </alert>
      .
     </p>
    </div>
    <div class="informationblock" id="tcolobox-25">
     <div class="title">
      <p class="noindent">
       <span id="bold" style="font-weight:bold;">
        Information
       </span>
       : Limits of Reduction
      </p>
     </div>
     <div class="box-content">
      <p class="noindent">
       Reducing dimensionality does cause some
       <span id="bold" style="font-weight:bold;">
        information loss
       </span>
       , similar to compressing an image to JPEG can
degrade its quality, so even though it will speed up training, it may make your system perform slightly worse. It
also makes your pipelines a bit more complex and thus harder to maintain.
      </p>
      <p class="noindent">
       Therefore, it is in our best interest to first try to train your system with the
       <span id="bold" style="font-weight:bold;">
        original data
       </span>
       before considering
using dimensionality reduction.
      </p>
      <p class="noindent">
       In some cases, reducing the dimensionality of the training data may filter out some noise and unnecessary details
and thus result in higher performance, but in general it won’t. It will just speed up training.
      </p>
     </div>
    </div>
    <p class="noindent">
     Apart
from
speeding
up
training,
dimensionality
reduction
is
also
highly
useful
                                                                                
                                                                                
for
     <alert style="color: #821131;">
      data
visualisation
     </alert>
     .
Reducing
the
number
of
dimensions
down
to
two
<alert style="color: #821131;">(2)</alert>,
or
three
<alert style="color: #821131;">(3)</alert>,
makes
it
possible
to
plot
a
condensed
view
of
a
high-dimensional
training
set
on
a
graph
and
often
gain
some
important
insights
by
visually
detecting
patterns,
such
as
clusters.
     <alert style="color: #821131;">
      <span id="bold" style="font-weight:bold;">
       <sup class="textsuperscript">
        2
       </sup>
      </span>
     </alert>
     <span class="marginnote">
      <alert style="color: #821131;">
       <span id="bold" style="font-weight:bold;">
        <sup class="textsuperscript">
         2
        </sup>
       </span>
      </alert>
      <span style="color:#0063B2;">
       A
type
of
plot
or
mathematical
diagram
using
Cartesian
coordinates
to
display
values
for
typically
two
variables
for
a
set
of
data.
      </span>
     </span>
     Moreover,
data
visualisation
is
essential
to
communicate
our
conclusions
to
people
who
are
not
data
scientists-in
particular,
decision
makers
who
will
use
your
results.
    </p>
    <p class="noindent">
     In this chapter, we will first discuss the curse of dimensionality and get a sense of what goes on in high-dimensional space.
Following this, we will consider the two <alert style="color: #821131;">(2)</alert> main approaches to dimensionality reduction:
    </p>
    <ul class="itemize1">
     <li class="itemize">
      <p class="noindent">
       projection,
and
      </p>
     </li>
     <li class="itemize">
      <p class="noindent">
       manifold
learning,
      </p>
     </li>
    </ul>
    <p class="noindent">
     and we will go through three <alert style="color: #821131;">(3)</alert> of the most popular dimensionality reduction techniques:
    </p>
    <dl class="enumerate-enumitem">
     <dt class="enumerate-enumitem">
      1.
     </dt>
     <dd class="enumerate-enumitem">
      <a class="glossary" href="DataScienceIILectureBookli2.html#glo:pca">
       Principal
Component
Analysis
(PCA)
      </a>,
     </dd>
     <dt class="enumerate-enumitem">
      2.
     </dt>
     <dd class="enumerate-enumitem">
      Random
projection,
     </dd>
     <dt class="enumerate-enumitem">
      3.
     </dt>
     <dd class="enumerate-enumitem">
      <a class="glossary" href="DataScienceIILectureBookli2.html#glo:lle">
       Locally
Linear
Embedding
(LLE)
      </a>.
     </dd>
    </dl>
    <h3 class="subsectionHead">
     <a aria-label="Link to this section in PDF version" class="pdf-link" href="DataScienceIILectureBook.pdf#subsection.4.1.1" style="color: red;  position: absolute;  left: -30px;" target="_blank">
      <svg class="bi bi-file-pdf" fill="currentColor" height="20" viewbox="0 0 16 16" width="20" xmlns="http://www.w3.org/2000/svg">
       <path d="M4 0a2 2 0 0 0-2 2v12a2 2 0 0 0 2 2h8a2 2 0 0 0 2-2V2a2 2 0 0 0-2-2H4zm0 1h8a1 1 0 0 1 1 1v12a1 1 0 0 1-1 1H4a1 1 0 0 1-1-1V2a1 1 0 0 1 1-1z">
       </path>
       <path d="M4.603 12.087a.81.81 0 0 1-.438-.42c-.195-.388-.13-.776.08-1.102.198-.307.526-.568.897-.787a7.68 7.68 0 0 1 1.482-.645 19.701 19.701 0 0 0 1.062-2.227 7.269 7.269 0 0 1-.43-1.295c-.086-.4-.119-.796-.046-1.136.075-.354.274-.672.65-.823.192-.077.4-.12.602-.077a.7.7 0 0 1 .477.365c.088.164.12.356.127.538.007.187-.012.395-.047.614-.084.51-.27 1.134-.52 1.794a10.954 10.954 0 0 0 .98 1.686 5.753 5.753 0 0 1 1.334.05c.364.065.734.195.96.465.12.144.193.32.2.518.007.192-.047.382-.138.563a1.04 1.04 0 0 1-.354.416.856.856 0 0 1-.51.138c-.331-.014-.654-.196-.933-.417a5.716 5.716 0 0 1-.911-.95 11.642 11.642 0 0 0-1.997.406 11.311 11.311 0 0 1-1.021 1.51c-.29.35-.608.655-.926.787a.793.793 0 0 1-.58.029zm1.379-1.901c-.166.076-.32.156-.459.238-.328.194-.541.383-.647.547-.094.145-.096.25-.04.361.01.022.02.036.026.044a.27.27 0 0 0 .035-.012c.137-.056.355-.235.635-.572a8.18 8.18 0 0 0 .45-.606zm1.64-1.33a12.647 12.647 0 0 1 1.01-.193 11.666 11.666 0 0 1-.51-.858 20.741 20.741 0 0 1-.5 1.05zm2.446.45c.15.162.296.3.435.41.24.19.407.253.498.256a.107.107 0 0 0 .07-.015.307.307 0 0 0 .094-.125.436.436 0 0 0 .059-.2.095.095 0 0 0-.026-.063c-.052-.062-.2-.152-.518-.209a3.881 3.881 0 0 0-.612-.053zM8.078 5.8a6.7 6.7 0 0 0 .2-.828c.031-.188.043-.343.038-.465a.613.613 0 0 0-.032-.198.517.517 0 0 0-.145.04c-.087.035-.158.106-.196.283-.04.192-.03.469.046.822.024.111.054.227.09.346z">
       </path>
      </svg>
     </a>
     <small class="titlemark">
      4.1.1
     </small>
     <a id="x6-440004.1.1">
     </a>
     The Problems of Dimensions
    </h3>
    <p class="noindent">
     We are used to living in three dimensions that our intuition fails us when we try to imagine a high-dimensional space. Even a
basic 4D hypercube is incredibly hard to picture in our minds, let alone a 200-dimensional ellipsoid bent in a 1,000-dimensional
space.
    </p>
    <div class="knowledge">
     <p class="noindent">
      The more dimensions the data has, the less geometrically explainable it becomes.
     </p>
    </div>
    <p class="noindent">
     Turns out many things behave very differently in high-dimensional space. As an example, picking a random point in a unit square
(a 1-by-1 square), will have only about a 0.4% chance of being located less than 0.001 from a border (in other words, it is very
unlikely that a random point will be “extreme” along any dimension). However, if we do the same thin in a 10,000-dimensional
unit hypercube, this probability is greater than 99.999999%. Most points in a high-dimensional hypercube are very close to the
border which as can be seen is very counter-intuitive.
    </p>
    <div class="theoryblock" id="tcolobox-26">
     <div class="title">
      <a id="x6-44002r2">
      </a>
      <p class="noindent">
       <span id="bold" style="font-weight:bold;">
        Theory
       </span>
       4.2: Distance Between Points in n-Dimensions
      </p>
     </div>
     <div class="box-content">
      <p class="noindent">
       Let
       <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
        <mi>
         d
        </mi>
       </math>
       be the dimension of
a cube in
       <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
        <mi>
         d
        </mi>
       </math>
       -dimensions.
A point within the cube will have 2 boundaries. To be less than 0.001 from a boundary in a
       <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
        <mi>
         d
        </mi>
       </math>
       -dimensional unit cube, means it is
not inside the cube of side length
       <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
        <mn>
         1
        </mn>
        <mo class="MathClass-bin" stretchy="false">
         −
        </mo>
        <mn>
         2
        </mn>
        <mo class="MathClass-bin" stretchy="false">
         ×
        </mo>
        <mn>
         0
        </mn>
        <mo class="MathClass-punc" stretchy="false">
         .
        </mo>
        <mn>
         0
        </mn>
        <mn>
         0
        </mn>
        <mn>
         1
        </mn>
       </math>
       sharing the same origin point as the original cube.
      </p>
      <math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
       <mtable class="align-star" columnalign="left" displaystyle="true">
        <mtr>
         <mtd class="align-odd" columnalign="right">
          <mstyle class="text">
           <mtext>
            2D
           </mtext>
          </mstyle>
         </mtd>
         <mtd class="align-even">
          <mspace class="qquad" width="2em">
          </mspace>
          <mspace width="2em">
          </mspace>
         </mtd>
         <mtd class="align-odd" columnalign="right">
         </mtd>
         <mtd class="align-even">
          <msup>
           <mrow>
            <mrow>
             <mo fence="true" form="prefix">
              (
             </mo>
             <mrow>
              <mn>
               1
              </mn>
              <mo class="MathClass-bin" stretchy="false">
               −
              </mo>
              <msup>
               <mrow>
                <mrow>
                 <mo fence="true" form="prefix">
                  (
                 </mo>
                 <mrow>
                  <mn>
                   1
                  </mn>
                  <mo class="MathClass-bin" stretchy="false">
                   −
                  </mo>
                  <mn>
                   2
                  </mn>
                  <mo class="MathClass-bin" stretchy="false">
                   ×
                  </mo>
                  <mn>
                   0
                  </mn>
                  <mo class="MathClass-punc" stretchy="false">
                   .
                  </mo>
                  <mn>
                   0
                  </mn>
                  <mn>
                   0
                  </mn>
                  <mn>
                   1
                  </mn>
                 </mrow>
                 <mo fence="true" form="postfix">
                  )
                 </mo>
                </mrow>
               </mrow>
               <mrow>
                <mn>
                 2
                </mn>
               </mrow>
              </msup>
             </mrow>
             <mo fence="true" form="postfix">
              )
             </mo>
            </mrow>
           </mrow>
           <mrow>
           </mrow>
          </msup>
          <mo class="MathClass-bin" stretchy="false">
           ×
          </mo>
          <mn>
           1
          </mn>
          <mn>
           0
          </mn>
          <mn>
           0
          </mn>
          <mi>
           %
          </mi>
          <mo class="MathClass-rel" stretchy="false">
           ∼
          </mo>
          <mn>
           0
          </mn>
          <mo class="MathClass-punc" stretchy="false">
           .
          </mo>
          <mn>
           3
          </mn>
          <mn>
           9
          </mn>
          <mn>
           9
          </mn>
          <mn>
           6
          </mn>
          <mi>
           %
          </mi>
          <mspace width="2em">
          </mspace>
         </mtd>
         <mtd class="align-label" columnalign="right">
         </mtd>
         <mtd class="align-label">
          <mspace width="2em">
          </mspace>
         </mtd>
         <mtd class="align-label" columnalign="right">
         </mtd>
         <mtd class="align-label">
          <mspace width="2em">
          </mspace>
         </mtd>
        </mtr>
        <mtr>
         <mtd class="align-odd" columnalign="right">
          <mstyle class="text">
           <mtext>
            10,000 D
           </mtext>
          </mstyle>
         </mtd>
         <mtd class="align-even">
          <mspace class="qquad" width="2em">
          </mspace>
          <mspace width="2em">
          </mspace>
         </mtd>
         <mtd class="align-odd" columnalign="right">
         </mtd>
         <mtd class="align-even">
          <msup>
           <mrow>
            <mrow>
             <mo fence="true" form="prefix">
              (
             </mo>
             <mrow>
              <mn>
               1
              </mn>
              <mo class="MathClass-bin" stretchy="false">
               −
              </mo>
              <msup>
               <mrow>
                <mrow>
                 <mo fence="true" form="prefix">
                  (
                 </mo>
                 <mrow>
                  <mn>
                   1
                  </mn>
                  <mo class="MathClass-bin" stretchy="false">
                   −
                  </mo>
                  <mn>
                   2
                  </mn>
                  <mo class="MathClass-bin" stretchy="false">
                   ×
                  </mo>
                  <mn>
                   0
                  </mn>
                  <mo class="MathClass-punc" stretchy="false">
                   .
                  </mo>
                  <mn>
                   0
                  </mn>
                  <mn>
                   0
                  </mn>
                  <mn>
                   1
                  </mn>
                 </mrow>
                 <mo fence="true" form="postfix">
                  )
                 </mo>
                </mrow>
               </mrow>
               <mrow>
                <mn>
                 1
                </mn>
                <mn>
                 0
                </mn>
                <mo class="MathClass-punc" stretchy="false">
                 ,
                </mo>
                <mn>
                 0
                </mn>
                <mn>
                 0
                </mn>
                <mn>
                 0
                </mn>
               </mrow>
              </msup>
             </mrow>
             <mo fence="true" form="postfix">
              )
             </mo>
            </mrow>
           </mrow>
           <mrow>
           </mrow>
          </msup>
          <mo class="MathClass-bin" stretchy="false">
           ×
          </mo>
          <mn>
           1
          </mn>
          <mn>
           0
          </mn>
          <mn>
           0
          </mn>
          <mi>
           %
          </mi>
          <mo class="MathClass-rel" stretchy="false">
           ∼
          </mo>
          <mn>
           9
          </mn>
          <mn>
           9
          </mn>
          <mo class="MathClass-punc" stretchy="false">
           .
          </mo>
          <mn>
           9
          </mn>
          <mn>
           9
          </mn>
          <mn>
           9
          </mn>
          <mn>
           9
          </mn>
          <mn>
           9
          </mn>
          <mn>
           9
          </mn>
          <mn>
           7
          </mn>
          <mn>
           9
          </mn>
          <mn>
           8
          </mn>
          <mi>
           %
          </mi>
          <mspace class="quad" width="1em">
          </mspace>
          <mo class="MathClass-ord">
           ■
          </mo>
          <mspace width="2em">
          </mspace>
         </mtd>
         <mtd class="align-label" columnalign="right">
         </mtd>
         <mtd class="align-label">
          <mspace width="2em">
          </mspace>
         </mtd>
         <mtd class="align-label" columnalign="right">
         </mtd>
         <mtd class="align-label">
          <mspace width="2em">
          </mspace>
         </mtd>
        </mtr>
       </mtable>
      </math>
     </div>
    </div>
    <div class="informationblock" id="tcolobox-27">
     <div class="title">
      <p class="noindent">
       <span id="bold" style="font-weight:bold;">
        Information
       </span>
       : n-dimensional Geometry
      </p>
     </div>
     <div class="box-content">
      <p class="noindent">
       When dimensions become detached from real-life, the mathematics and our intuition becomes more divergent.
There are many problems in mathematics where as the geometry becomes n-dimensional the results get even
more complicated.
      </p>
     </div>
    </div>
    <p class="noindent">
     Here is another example.
    </p>
    <p class="noindent">
     If we were to pick two <alert style="color: #821131;">(2)</alert> points randomly in a unit square, the distance between these two points will be, on average,
roughly 0.52. If we were to pick two random points in a 3D unit cube instead, the average distance will be roughly
0.66.
    </p>
    <p class="noindent">
     But what about two points picked randomly in a 1,000,000-dimensional unit hypercube?
    </p>
    <p class="noindent">
     The average distance, believe it or not, will be about 408.25. This is a problem known as
     <span id="bold" style="font-weight:bold;">
      Hypercube Line Picking
     </span>
    </p>
    <div class="theoryblock" id="tcolobox-28">
     <div class="title">
      <a id="x6-44004r4">
      </a>
      <p class="noindent">
       <span id="bold" style="font-weight:bold;">
        Theory
       </span>
       4.4: Hypercube Line Picking
      </p>
     </div>
     <div class="box-content">
      <p class="noindent">
       Let two points
       <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
        <mi>
         x
        </mi>
       </math>
       and
       <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
        <mi>
         y
        </mi>
       </math>
       be
picked randomly from a unit n-dimensional hypercube. The expected distance between the points
       <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
        <mi mathvariant="normal">
         Δ
        </mi>
        <msup>
         <mrow>
          <mrow>
           <mo fence="true" form="prefix">
            (
           </mo>
           <mrow>
            <mi>
             n
            </mi>
           </mrow>
           <mo fence="true" form="postfix">
            )
           </mo>
          </mrow>
         </mrow>
         <mrow>
         </mrow>
        </msup>
       </math>
       , i.e., the
mean line segment length, is then:
      </p>
      <table class="equation-star">
       <tr>
        <td>
         <math class="equation" display="block" xmlns="http://www.w3.org/1998/Math/MathML">
          <mi mathvariant="normal">
           Δ
          </mi>
          <msup>
           <mrow>
            <mrow>
             <mo fence="true" form="prefix">
              (
             </mo>
             <mrow>
              <mi>
               n
              </mi>
             </mrow>
             <mo fence="true" form="postfix">
              )
             </mo>
            </mrow>
           </mrow>
           <mrow>
           </mrow>
          </msup>
          <mo class="MathClass-rel" stretchy="false">
           =
          </mo>
          <msubsup>
           <mrow>
            <mo>
             ∫
            </mo>
           </mrow>
           <mrow>
            <mn>
             0
            </mn>
           </mrow>
           <mrow>
            <mn>
             1
            </mn>
           </mrow>
          </msubsup>
          <mo class="MathClass-rel" stretchy="false">
           ⋯
          </mo>
          <msubsup>
           <mrow>
            <mo>
             ∫
            </mo>
           </mrow>
           <mrow>
            <mn>
             0
            </mn>
           </mrow>
           <mrow>
            <mn>
             1
            </mn>
           </mrow>
          </msubsup>
          <msqrt>
           <mrow>
            <msup>
             <mrow>
              <mrow>
               <mo fence="true" form="prefix">
                (
               </mo>
               <mrow>
                <msubsup>
                 <mrow>
                  <mi>
                   x
                  </mi>
                 </mrow>
                 <mrow>
                  <mn>
                   1
                  </mn>
                 </mrow>
                 <mrow>
                 </mrow>
                </msubsup>
                <mo class="MathClass-bin" stretchy="false">
                 −
                </mo>
                <msubsup>
                 <mrow>
                  <mi>
                   y
                  </mi>
                 </mrow>
                 <mrow>
                  <mn>
                   1
                  </mn>
                 </mrow>
                 <mrow>
                 </mrow>
                </msubsup>
               </mrow>
               <mo fence="true" form="postfix">
                )
               </mo>
              </mrow>
             </mrow>
             <mrow>
              <mn>
               2
              </mn>
             </mrow>
            </msup>
            <mo class="MathClass-bin" stretchy="false">
             +
            </mo>
            <msup>
             <mrow>
              <mrow>
               <mo fence="true" form="prefix">
                (
               </mo>
               <mrow>
                <msubsup>
                 <mrow>
                  <mi>
                   x
                  </mi>
                 </mrow>
                 <mrow>
                  <mn>
                   2
                  </mn>
                 </mrow>
                 <mrow>
                 </mrow>
                </msubsup>
                <mo class="MathClass-bin" stretchy="false">
                 −
                </mo>
                <msubsup>
                 <mrow>
                  <mi>
                   y
                  </mi>
                 </mrow>
                 <mrow>
                  <mn>
                   2
                  </mn>
                 </mrow>
                 <mrow>
                 </mrow>
                </msubsup>
               </mrow>
               <mo fence="true" form="postfix">
                )
               </mo>
              </mrow>
             </mrow>
             <mrow>
              <mn>
               2
              </mn>
             </mrow>
            </msup>
            <mo class="MathClass-rel" stretchy="false">
             ⋯
            </mo>
            <msup>
             <mrow>
              <mrow>
               <mo fence="true" form="prefix">
                (
               </mo>
               <mrow>
                <msubsup>
                 <mrow>
                  <mi>
                   x
                  </mi>
                 </mrow>
                 <mrow>
                  <mi>
                   n
                  </mi>
                 </mrow>
                 <mrow>
                 </mrow>
                </msubsup>
                <mo class="MathClass-bin" stretchy="false">
                 −
                </mo>
                <msubsup>
                 <mrow>
                  <mi>
                   y
                  </mi>
                 </mrow>
                 <mrow>
                  <mi>
                   n
                  </mi>
                 </mrow>
                 <mrow>
                 </mrow>
                </msubsup>
               </mrow>
               <mo fence="true" form="postfix">
                )
               </mo>
              </mrow>
             </mrow>
             <mrow>
              <mn>
               2
              </mn>
             </mrow>
            </msup>
           </mrow>
          </msqrt>
          <mspace class="thinspace" width="0.17em">
          </mspace>
          <mstyle class="text">
           <mtext>
            d
           </mtext>
          </mstyle>
          <msubsup>
           <mrow>
            <mi>
             x
            </mi>
           </mrow>
           <mrow>
            <mn>
             1
            </mn>
           </mrow>
           <mrow>
           </mrow>
          </msubsup>
          <mo class="MathClass-rel" stretchy="false">
           ⋯
          </mo>
          <mspace class="thinspace" width="0.17em">
          </mspace>
          <mstyle class="text">
           <mtext>
            d
           </mtext>
          </mstyle>
          <msubsup>
           <mrow>
            <mi>
             x
            </mi>
           </mrow>
           <mrow>
            <mi>
             n
            </mi>
           </mrow>
           <mrow>
           </mrow>
          </msubsup>
          <mspace class="thinspace" width="0.17em">
          </mspace>
          <mstyle class="text">
           <mtext>
            d
           </mtext>
          </mstyle>
          <msubsup>
           <mrow>
            <mi>
             y
            </mi>
           </mrow>
           <mrow>
            <mn>
             1
            </mn>
           </mrow>
           <mrow>
           </mrow>
          </msubsup>
          <mo class="MathClass-rel" stretchy="false">
           ⋯
          </mo>
          <mspace class="thinspace" width="0.17em">
          </mspace>
          <mstyle class="text">
           <mtext>
            d
           </mtext>
          </mstyle>
          <msubsup>
           <mrow>
            <mi>
             y
            </mi>
           </mrow>
           <mrow>
            <mi>
             n
            </mi>
           </mrow>
           <mrow>
           </mrow>
          </msubsup>
         </math>
        </td>
       </tr>
      </table>
      <p class="noindent">
       The first few values for
       <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
        <mi mathvariant="normal">
         Δ
        </mi>
        <msup>
         <mrow>
          <mrow>
           <mo fence="true" form="prefix">
            (
           </mo>
           <mrow>
            <mi>
             n
            </mi>
           </mrow>
           <mo fence="true" form="postfix">
            )
           </mo>
          </mrow>
         </mrow>
         <mrow>
         </mrow>
        </msup>
       </math>
       are given in the following table.
      </p>
      <div class="center">
       <p class="noindent">
       </p>
       <table class="tabularray tblr" id="tbl-5">
        <tr id="row-5-1-">
         <td id="cell5-1-1" style="text-align:center;vertical-align:top;">
          <span id="bold" style="font-weight:bold;">
           n
          </span>
         </td>
         <td id="cell5-1-2" style="vertical-align:top;">
          <span id="bold" style="font-weight:bold;">
           OEIS
          </span>
         </td>
         <td id="cell5-1-3" style="vertical-align:top;">
          <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
           <mi mathvariant="normal">
            Δ
           </mi>
           <msup>
            <mrow>
             <mrow>
              <mo fence="true" form="prefix">
               (
              </mo>
              <mrow>
               <mi>
                n
               </mi>
              </mrow>
              <mo fence="true" form="postfix">
               )
              </mo>
             </mrow>
            </mrow>
            <mrow>
            </mrow>
           </msup>
          </math>
         </td>
         <td id="cell5-1-4" style="text-align:justify;vertical-align:top;">
          <span id="bold" style="font-weight:bold;">
           n
          </span>
         </td>
         <td id="cell5-1-5" style="text-align:justify;vertical-align:top;">
          <span id="bold" style="font-weight:bold;">
           OEIS
          </span>
         </td>
         <td id="cell5-1-6" style="text-align:justify;vertical-align:top;">
          <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
           <mi mathvariant="normal">
            Δ
           </mi>
           <msup>
            <mrow>
             <mrow>
              <mo fence="true" form="prefix">
               (
              </mo>
              <mrow>
               <mi>
                n
               </mi>
              </mrow>
              <mo fence="true" form="postfix">
               )
              </mo>
             </mrow>
            </mrow>
            <mrow>
            </mrow>
           </msup>
          </math>
         </td>
        </tr>
        <tr id="row-5-2-">
         <td id="cell5-2-1" style="text-align:center;vertical-align:top;">
          1
         </td>
         <td id="cell5-2-2" style="vertical-align:top;">
          –
         </td>
         <td id="cell5-2-3" style="vertical-align:top;">
          0.3333333333…
         </td>
         <td id="cell5-2-4" style="text-align:justify;vertical-align:top;">
          5
         </td>
         <td id="cell5-2-5" style="text-align:justify;vertical-align:top;">
          A103984
         </td>
         <td id="cell5-2-6" style="text-align:justify;vertical-align:top;">
          0.8785309152…
         </td>
        </tr>
        <tr id="row-5-3-">
         <td id="cell5-3-1" style="text-align:center;vertical-align:top;">
          2
         </td>
         <td id="cell5-3-2" style="vertical-align:top;">
          A091505
         </td>
         <td id="cell5-3-3" style="vertical-align:top;">
          0.5214054331…
         </td>
         <td id="cell5-3-4" style="text-align:justify;vertical-align:top;">
          6
         </td>
         <td id="cell5-3-5" style="text-align:justify;vertical-align:top;">
          A103985
         </td>
         <td id="cell5-3-6" style="text-align:justify;vertical-align:top;">
          0.9689420830…
         </td>
        </tr>
        <tr id="row-5-4-">
         <td id="cell5-4-1" style="text-align:center;vertical-align:top;">
          3
         </td>
         <td id="cell5-4-2" style="vertical-align:top;">
          A073012
         </td>
         <td id="cell5-4-3" style="vertical-align:top;">
          0.6617071822…
         </td>
         <td id="cell5-4-4" style="text-align:justify;vertical-align:top;">
          7
         </td>
         <td id="cell5-4-5" style="text-align:justify;vertical-align:top;">
          A103986
         </td>
         <td id="cell5-4-6" style="text-align:justify;vertical-align:top;">
          1.0515838734…
         </td>
        </tr>
        <tr id="row-5-5-">
         <td id="cell5-5-1" style="text-align:center;vertical-align:top;">
          4
         </td>
         <td id="cell5-5-2" style="vertical-align:top;">
          A103983
         </td>
         <td id="cell5-5-3" style="vertical-align:top;">
          0.7776656535…
         </td>
         <td id="cell5-5-4" style="text-align:justify;vertical-align:top;">
          8
         </td>
         <td id="cell5-5-5" style="text-align:justify;vertical-align:top;">
          A103987
         </td>
         <td id="cell5-5-6" style="text-align:justify;vertical-align:top;">
          1.1281653402…
         </td>
        </tr>
       </table>
      </div>
     </div>
    </div>
    <p class="noindent">
     This is counterintuitive: how can two points be so far apart when they both lie within the same unit hypercube? Well,
     <span id="bold" style="font-weight:bold;">
      there’s
just plenty of space in high dimensions
     </span>
     . As a result, high-dimensional datasets are at risk of being very
sparse:
    </p>
    <p class="noindent">
    </p>
    <div class="quoteblock">
     <p class="noindent">
      most
training
instances
are
likely
to
be
far
away
from
each
other.
     </p>
    </div>
    <p class="noindent">
     This also means a new instance will likely be far away from any training instance, making predictions much less reliable than in
lower dimensions, since they will be based on much larger extrapolations.
    </p>
    <div class="warning">
     <p class="noindent">
      The more dimensions the training set has, the greater the risk of overfitting it.
     </p>
    </div>
    <p class="noindent">
     In theory, one solution to the curse of dimensionality could be to increase the size of the training set to reach a sufficient density
of training instances. Unfortunately, in practice, the number of training instances required to reach a given density grows
exponentially with the number of dimensions.
    </p>
    <p class="noindent">
     With just 100 features, which is significantly fewer than in the MNIST problem, all ranging from 0 to 1, you would need more
training instances than atoms in the observable universe in order for training instances to be within 0.1 of each other on average,
assuming they were spread out uniformly across all dimensions.
    </p>
    <p class="noindent">
    </p>
    <h2 class="sectionHead">
     <a aria-label="Link to this section in PDF version" class="pdf-link" href="DataScienceIILectureBook.pdf#section.4.2" style="color: red;  position: absolute;  left: -30px;" target="_blank">
      <svg class="bi bi-file-pdf" fill="currentColor" height="20" viewbox="0 0 16 16" width="20" xmlns="http://www.w3.org/2000/svg">
       <path d="M4 0a2 2 0 0 0-2 2v12a2 2 0 0 0 2 2h8a2 2 0 0 0 2-2V2a2 2 0 0 0-2-2H4zm0 1h8a1 1 0 0 1 1 1v12a1 1 0 0 1-1 1H4a1 1 0 0 1-1-1V2a1 1 0 0 1 1-1z">
       </path>
       <path d="M4.603 12.087a.81.81 0 0 1-.438-.42c-.195-.388-.13-.776.08-1.102.198-.307.526-.568.897-.787a7.68 7.68 0 0 1 1.482-.645 19.701 19.701 0 0 0 1.062-2.227 7.269 7.269 0 0 1-.43-1.295c-.086-.4-.119-.796-.046-1.136.075-.354.274-.672.65-.823.192-.077.4-.12.602-.077a.7.7 0 0 1 .477.365c.088.164.12.356.127.538.007.187-.012.395-.047.614-.084.51-.27 1.134-.52 1.794a10.954 10.954 0 0 0 .98 1.686 5.753 5.753 0 0 1 1.334.05c.364.065.734.195.96.465.12.144.193.32.2.518.007.192-.047.382-.138.563a1.04 1.04 0 0 1-.354.416.856.856 0 0 1-.51.138c-.331-.014-.654-.196-.933-.417a5.716 5.716 0 0 1-.911-.95 11.642 11.642 0 0 0-1.997.406 11.311 11.311 0 0 1-1.021 1.51c-.29.35-.608.655-.926.787a.793.793 0 0 1-.58.029zm1.379-1.901c-.166.076-.32.156-.459.238-.328.194-.541.383-.647.547-.094.145-.096.25-.04.361.01.022.02.036.026.044a.27.27 0 0 0 .035-.012c.137-.056.355-.235.635-.572a8.18 8.18 0 0 0 .45-.606zm1.64-1.33a12.647 12.647 0 0 1 1.01-.193 11.666 11.666 0 0 1-.51-.858 20.741 20.741 0 0 1-.5 1.05zm2.446.45c.15.162.296.3.435.41.24.19.407.253.498.256a.107.107 0 0 0 .07-.015.307.307 0 0 0 .094-.125.436.436 0 0 0 .059-.2.095.095 0 0 0-.026-.063c-.052-.062-.2-.152-.518-.209a3.881 3.881 0 0 0-.612-.053zM8.078 5.8a6.7 6.7 0 0 0 .2-.828c.031-.188.043-.343.038-.465a.613.613 0 0 0-.032-.198.517.517 0 0 0-.145.04c-.087.035-.158.106-.196.283-.04.192-.03.469.046.822.024.111.054.227.09.346z">
       </path>
      </svg>
     </a>
     <small class="titlemark">
      4.2
     </small>
     <a id="x6-450004.2">
     </a>
     Main Approaches to Dimensionality Reduction
    </h2>
    <p class="noindent">
     Before we dive into different dimensionality reduction algorithms, let’s take a look at the two <alert style="color: #821131;">(2)</alert> main approaches to reducing
dimensionality:
    </p>
    <dl class="enumerate-enumitem">
     <dt class="enumerate-enumitem">
      1.
     </dt>
     <dd class="enumerate-enumitem">
      projection,
and
     </dd>
     <dt class="enumerate-enumitem">
      2.
     </dt>
     <dd class="enumerate-enumitem">
      manifold
learning.
     </dd>
    </dl>
    <h3 class="subsectionHead">
     <a aria-label="Link to this section in PDF version" class="pdf-link" href="DataScienceIILectureBook.pdf#subsection.4.2.1" style="color: red;  position: absolute;  left: -30px;" target="_blank">
      <svg class="bi bi-file-pdf" fill="currentColor" height="20" viewbox="0 0 16 16" width="20" xmlns="http://www.w3.org/2000/svg">
       <path d="M4 0a2 2 0 0 0-2 2v12a2 2 0 0 0 2 2h8a2 2 0 0 0 2-2V2a2 2 0 0 0-2-2H4zm0 1h8a1 1 0 0 1 1 1v12a1 1 0 0 1-1 1H4a1 1 0 0 1-1-1V2a1 1 0 0 1 1-1z">
       </path>
       <path d="M4.603 12.087a.81.81 0 0 1-.438-.42c-.195-.388-.13-.776.08-1.102.198-.307.526-.568.897-.787a7.68 7.68 0 0 1 1.482-.645 19.701 19.701 0 0 0 1.062-2.227 7.269 7.269 0 0 1-.43-1.295c-.086-.4-.119-.796-.046-1.136.075-.354.274-.672.65-.823.192-.077.4-.12.602-.077a.7.7 0 0 1 .477.365c.088.164.12.356.127.538.007.187-.012.395-.047.614-.084.51-.27 1.134-.52 1.794a10.954 10.954 0 0 0 .98 1.686 5.753 5.753 0 0 1 1.334.05c.364.065.734.195.96.465.12.144.193.32.2.518.007.192-.047.382-.138.563a1.04 1.04 0 0 1-.354.416.856.856 0 0 1-.51.138c-.331-.014-.654-.196-.933-.417a5.716 5.716 0 0 1-.911-.95 11.642 11.642 0 0 0-1.997.406 11.311 11.311 0 0 1-1.021 1.51c-.29.35-.608.655-.926.787a.793.793 0 0 1-.58.029zm1.379-1.901c-.166.076-.32.156-.459.238-.328.194-.541.383-.647.547-.094.145-.096.25-.04.361.01.022.02.036.026.044a.27.27 0 0 0 .035-.012c.137-.056.355-.235.635-.572a8.18 8.18 0 0 0 .45-.606zm1.64-1.33a12.647 12.647 0 0 1 1.01-.193 11.666 11.666 0 0 1-.51-.858 20.741 20.741 0 0 1-.5 1.05zm2.446.45c.15.162.296.3.435.41.24.19.407.253.498.256a.107.107 0 0 0 .07-.015.307.307 0 0 0 .094-.125.436.436 0 0 0 .059-.2.095.095 0 0 0-.026-.063c-.052-.062-.2-.152-.518-.209a3.881 3.881 0 0 0-.612-.053zM8.078 5.8a6.7 6.7 0 0 0 .2-.828c.031-.188.043-.343.038-.465a.613.613 0 0 0-.032-.198.517.517 0 0 0-.145.04c-.087.035-.158.106-.196.283-.04.192-.03.469.046.822.024.111.054.227.09.346z">
       </path>
      </svg>
     </a>
     <small class="titlemark">
      4.2.1
     </small>
     <a id="x6-460004.2.1">
     </a>
     Projection
    </h3>
    <p class="noindent">
     In most real-world problems, training instances are
     <span id="bold" style="font-weight:bold;">
      NOT
     </span>
     spread out uniformly across all dimensions. Many features are almost
constant, while others are highly correlated. As a result, all training instances lie within a much lower-dimensional subspace of
the high-dimensional space. This sounds very abstract, so let’s look at an example.
    </p>
    <p class="noindent">
     In
     <span id="bold" style="font-weight:bold;">
      Fig.
     </span>
     <a href="#x6-46001r1">
      4.0a
     </a>
     we can see a 3D dataset represented by
     <span id="bold" style="font-weight:bold;">
      small spheres
     </span>
     .
    </p>
    <figure class="figure">
     <div class="subfigure" style="width:49%;">
      <p class="noindent">
      </p>
      <p class="noindent">
       <img alt="PIC" height="" src="codes/images/Dimensionality-Reduction/dataset_3d_plot-.svg" width=""/>
       <a id="x6-46001r1">
       </a>
      </p>
      <div class="caption">
       <span class="id">
        (a)
       </span>
       <span class="content">
       </span>
      </div>
     </div>
     <div class="subfigure" style="width:49%;">
      <p class="noindent">
      </p>
      <p class="noindent">
       <img alt="PIC" height="" src="codes/images/Dimensionality-Reduction/dataset_2d_plot-.svg" width=""/>
       <a id="x6-46002r2">
       </a>
      </p>
      <div class="caption">
       <span class="id">
        (b)
       </span>
       <span class="content">
       </span>
      </div>
     </div>
     <a id="x6-46003r1">
     </a>
     <div class="caption">
      <span class="id">
       Figure 4.1:
      </span>
      <span class="content">
       (a) A 3D dataset lying close to a 2D subspace (b) The new 2D dataset after projection
      </span>
     </div>
    </figure>
    <p class="noindent">
     As you can see, almost all training instances lie close to a plane:
    </p>
    <div class="quoteblock">
     <p class="noindent">
      this
is
a
lower-dimensional
(2D)
subspace
of
the
higher-dimensional
(3D)
space.
     </p>
    </div>
    <p class="noindent">
     If
we
were
to
project
every
training
instance
perpendicularly
onto
this
subspace,
     <alert style="color: #821131;">
      <span id="bold" style="font-weight:bold;">
       <sup class="textsuperscript">
        3
       </sup>
      </span>
     </alert>
     <span class="marginnote">
      <alert style="color: #821131;">
       <span id="bold" style="font-weight:bold;">
        <sup class="textsuperscript">
         3
        </sup>
       </span>
      </alert>
      <span style="color:#0063B2;">
       as
represented
by
the
short
dashed
lines
connecting
the
instances
to
the
plane
      </span>
     </span>
     we
get
the
new
2D
dataset
shown
in
     <span id="bold" style="font-weight:bold;">
      Fig.
     </span>
     <a href="#x6-46002r2">
      4.0b
     </a>.
    </p>
    <p class="noindent">
     We
have
just
reduced
the
dataset’s
dimensionality
from
3D
to
2D.
Please
observe
                                                                                
                                                                                
the
axes
corresponding
to
new
features
     <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
      <msubsup>
       <mrow>
        <mi>
         z
        </mi>
       </mrow>
       <mrow>
        <mn>
         1
        </mn>
       </mrow>
       <mrow>
       </mrow>
      </msubsup>
     </math>
     and
     <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
      <msubsup>
       <mrow>
        <mi>
         z
        </mi>
       </mrow>
       <mrow>
        <mn>
         2
        </mn>
       </mrow>
       <mrow>
       </mrow>
      </msubsup>
     </math>
     which
are
the
coordinates
of
the
projections
on
the
plane.
    </p>
    <h3 class="subsectionHead">
     <a aria-label="Link to this section in PDF version" class="pdf-link" href="DataScienceIILectureBook.pdf#subsection.4.2.2" style="color: red;  position: absolute;  left: -30px;" target="_blank">
      <svg class="bi bi-file-pdf" fill="currentColor" height="20" viewbox="0 0 16 16" width="20" xmlns="http://www.w3.org/2000/svg">
       <path d="M4 0a2 2 0 0 0-2 2v12a2 2 0 0 0 2 2h8a2 2 0 0 0 2-2V2a2 2 0 0 0-2-2H4zm0 1h8a1 1 0 0 1 1 1v12a1 1 0 0 1-1 1H4a1 1 0 0 1-1-1V2a1 1 0 0 1 1-1z">
       </path>
       <path d="M4.603 12.087a.81.81 0 0 1-.438-.42c-.195-.388-.13-.776.08-1.102.198-.307.526-.568.897-.787a7.68 7.68 0 0 1 1.482-.645 19.701 19.701 0 0 0 1.062-2.227 7.269 7.269 0 0 1-.43-1.295c-.086-.4-.119-.796-.046-1.136.075-.354.274-.672.65-.823.192-.077.4-.12.602-.077a.7.7 0 0 1 .477.365c.088.164.12.356.127.538.007.187-.012.395-.047.614-.084.51-.27 1.134-.52 1.794a10.954 10.954 0 0 0 .98 1.686 5.753 5.753 0 0 1 1.334.05c.364.065.734.195.96.465.12.144.193.32.2.518.007.192-.047.382-.138.563a1.04 1.04 0 0 1-.354.416.856.856 0 0 1-.51.138c-.331-.014-.654-.196-.933-.417a5.716 5.716 0 0 1-.911-.95 11.642 11.642 0 0 0-1.997.406 11.311 11.311 0 0 1-1.021 1.51c-.29.35-.608.655-.926.787a.793.793 0 0 1-.58.029zm1.379-1.901c-.166.076-.32.156-.459.238-.328.194-.541.383-.647.547-.094.145-.096.25-.04.361.01.022.02.036.026.044a.27.27 0 0 0 .035-.012c.137-.056.355-.235.635-.572a8.18 8.18 0 0 0 .45-.606zm1.64-1.33a12.647 12.647 0 0 1 1.01-.193 11.666 11.666 0 0 1-.51-.858 20.741 20.741 0 0 1-.5 1.05zm2.446.45c.15.162.296.3.435.41.24.19.407.253.498.256a.107.107 0 0 0 .07-.015.307.307 0 0 0 .094-.125.436.436 0 0 0 .059-.2.095.095 0 0 0-.026-.063c-.052-.062-.2-.152-.518-.209a3.881 3.881 0 0 0-.612-.053zM8.078 5.8a6.7 6.7 0 0 0 .2-.828c.031-.188.043-.343.038-.465a.613.613 0 0 0-.032-.198.517.517 0 0 0-.145.04c-.087.035-.158.106-.196.283-.04.192-.03.469.046.822.024.111.054.227.09.346z">
       </path>
      </svg>
     </a>
     <small class="titlemark">
      4.2.2
     </small>
     <a id="x6-470004.2.2">
     </a>
     Manifold Learning
    </h3>
    <p class="noindent">
     It
is
worth
mentioning
that,
projection
is
     <alert style="color: #821131;">
      not
always
the
best
approach
to
dimensionality
reduction
     </alert>
     .
In
many
cases
the
subspace
may
     <span id="bold" style="font-weight:bold;">
      twist
     </span>
     and
     <span id="bold" style="font-weight:bold;">
      turn
     </span>
     ,
such
as
in
the
famous
Swiss
roll
toy
dataset
     <alert style="color: #821131;">
      <span id="bold" style="font-weight:bold;">
       <sup class="textsuperscript">
        4
       </sup>
      </span>
     </alert>
     <span class="marginnote">
      <alert style="color: #821131;">
       <span id="bold" style="font-weight:bold;">
        <sup class="textsuperscript">
         4
        </sup>
       </span>
      </alert>
      <span style="color:#0063B2;">
       The
Swiss
roll
is
a
toy
dataset
                                                                                
                                                                                
in <span style="color:#054C5C;"><code class="verb">sklearn</code></span>
       that
is
commonly
used
for
testing
and
demonstrating
nonlinear
dimensionality
reduction
algorithms.
It
consists
of
a
set
of
points
in
three
dimensions,
arranged
in
a
       <alert style="color: #821131;">
        roll
       </alert>
       shape,
such
that
the
points
on
the
roll
are
mapped
to
a
two-dimensional
plane
in
a
nonlinear
fashion
      </span>
     </span>
     represented
in
     <span id="bold" style="font-weight:bold;">
      Fig.
     </span>
     <a href="#x6-47001r2">
      4.2
     </a>.
    </p>
    <div class="figure">
     <p class="noindent">
      <img alt="PIC" height="" src="codes/images/Dimensionality-Reduction/swiss-roll-plot-.svg" width="150%"/>
      <a id="x6-47001r2">
      </a>
     </p>
     <figcaption class="caption">
      <span class="id">
       Figure 4.2:
      </span>
      <span class="content">
       The Swiss roll dataset
      </span>
     </figcaption>
    </div>
    <p class="noindent">
     Simply
projecting
onto
a
plane
     <alert style="color: #821131;">
      <span id="bold" style="font-weight:bold;">
       <sup class="textsuperscript">
        5
       </sup>
      </span>
     </alert>
     <span class="marginnote">
      <alert style="color: #821131;">
       <span id="bold" style="font-weight:bold;">
        <sup class="textsuperscript">
         5
        </sup>
       </span>
      </alert>
      <span style="color:#0063B2;">
       e.g.,
by
dropping
the
dimension
       <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
        <msubsup>
         <mrow>
          <mi>
           x
          </mi>
         </mrow>
         <mrow>
          <mn>
           3
          </mn>
         </mrow>
         <mrow>
         </mrow>
        </msubsup>
       </math>
      </span>
     </span>
     would
     <alert style="color: #821131;">
      squash
     </alert>
     different
layers
of
the
Swiss
roll
together,
as
shown
on
the
left
side
of
     <span id="bold" style="font-weight:bold;">
      Fig.
     </span>
     <a href="#x6-47002r3">
      4.3
     </a>.
What
we
probably
want
instead
is
to
unroll
the
Swiss
roll
to
obtain
the
2D
dataset
on
the
right
side
of
     <span id="bold" style="font-weight:bold;">
      Fig.
     </span>
     <a href="#x6-47002r3">
      4.3
     </a>.
    </p>
    <p class="noindent">
     The
Swiss
roll
is
an
example
of
a
2D
manifold.
To
put
simply,
a
2D
                                                                                
                                                                                
manifold
is
a
2D
shape
that
can
be
bent
and
twisted
in
a
higher-dimensional
space.
     <alert style="color: #821131;">
      <span id="bold" style="font-weight:bold;">
       <sup class="textsuperscript">
        6
       </sup>
      </span>
     </alert>
     <span class="marginnote">
      <img alt="PIC" height="" src="figures/Dimensionality-Reduction/figures-1.svg" width="100%"/>
      <alert style="color: #821131;">
       <span id="bold" style="font-weight:bold;">
        <sup class="textsuperscript">
         6
        </sup>
       </span>
      </alert>
      <span style="color:#0063B2;">
       A
torus
can
be
taught
of
as
a
2D
manifold
as
the
entire
surface
is
defined
in
a
2D
space.
      </span>
     </span>
     More
generally,
a
     <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
      <mi>
       d
      </mi>
     </math>
     -dimensional
manifold
is
a
part
of
an
     <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
      <mi>
       n
      </mi>
     </math>
     -dimensional
space
(where
     <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
      <mi>
       d
      </mi>
      <mo class="MathClass-rel" stretchy="false">
       &lt;
      </mo>
      <mi>
       n
      </mi>
     </math>
     )
that
locally
resembles
a
     <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
      <mi>
       d
      </mi>
     </math>
     -dimensional
hyperplane.
    </p>
    <p class="noindent">
     In
the
case
of
the
Swiss
roll,
     <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
      <mi>
       d
      </mi>
     </math>
     =
2
                                                                                
                                                                                
and
     <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
      <mi>
       n
      </mi>
     </math>
     =
3.
    </p>
    <div class="knowledge">
     <p class="noindent">
      It locally resembles a 2D plane, but it is rolled in the
      <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
       <msup>
        <mrow>
         <mstyle class="text">
          <mtext>
           3
          </mtext>
         </mstyle>
        </mrow>
        <mrow>
         <mstyle class="text">
          <mtext>
           rd
          </mtext>
         </mstyle>
        </mrow>
       </msup>
      </math>
      dimension.
     </p>
    </div>
    <div class="figure">
     <p class="noindent">
      <img alt="PIC" height="" src="codes/images/Dimensionality-Reduction/squished-swiss-roll-plot-.svg" width="150%"/>
      <a id="x6-47002r3">
      </a>
     </p>
     <figcaption class="caption">
      <span class="id">
       Figure 4.3:
      </span>
      <span class="content">
       Squashing by projecting onto a plane (left) versus unrolling the Swiss roll (right)
      </span>
     </figcaption>
    </div>
    <p class="noindent">
     Many dimensionality reduction algorithms work by modelling the manifold on which the training instances lie. This is called
     <alert style="color: #821131;">
      manifold learning
     </alert> <a id="x6-47003"></a><a href="#X0-izenman2012introduction">[8]</a>. It relies on the manifold assumption, also called the
     <italic>
      manifold hypothesis
     </italic>
     :
    </p>
    <div class="quoteblock">
     <p class="noindent">
      Most
real-world
high-dimensional
datasets
lie
close
to
a
much
lower
dimensional
manifold.
     </p>
    </div>
    <p class="noindent">
     This
assumption
is
very
often
empirically
     <alert style="color: #821131;">
      <span id="bold" style="font-weight:bold;">
       <sup class="textsuperscript">
        7
       </sup>
      </span>
     </alert>
     <span class="marginnote">
      <alert style="color: #821131;">
       <span id="bold" style="font-weight:bold;">
        <sup class="textsuperscript">
         7
        </sup>
       </span>
      </alert>
      <span style="color:#0063B2;">
       This
means
it
is
based
on,
concerned
with,
or
verifiable
by
observation
or
experience
rather
than
theory
or
pure
logic
      </span>
     </span>
     observed.
    </p>
    <p class="noindent">
     Once
again,
let
us
look
back
at
the
MNIST
dataset:
all
handwritten
digit
images
have
some
similarities.
They
                                                                                
                                                                                
are
made
of
connected
lines,
the
borders
are
white,
and
they
are
more
or
less
centered.
If
you
randomly
generated
images,
only
a
ridiculously
tiny
fraction
of
them
would
look
like
handwritten
digits.
    </p>
    <p class="noindent">
     In
other
words,
the
degrees
of
freedom
available
to
you
if
you
try
to
create
a
digit
image
are
dramatically
lower
than
the
degrees
of
freedom
you
have
if
you
are
allowed
to
generate
                                                                                
                                                                                
any
image
you
want.
    </p>
    <div class="knowledge">
     <p class="noindent">
      These constraints tend to squeeze the dataset into a lower-dimensional manifold.
     </p>
    </div>
    <div class="figure">
     <div class="subfigure" style="width:24%;">
      <p class="noindent">
      </p>
      <p class="noindent">
       <img alt="PIC" height="" src="codes/images/Dimensionality-Reduction/manifold-decision-boundary-plot1-.svg" width=""/>
       <a id="x6-47004r1">
       </a>
      </p>
      <div class="caption">
       <span class="id">
        (a)
       </span>
       <span class="content">
       </span>
      </div>
     </div>
     <div class="subfigure" style="width:24%;">
      <p class="noindent">
      </p>
      <p class="noindent">
       <img alt="PIC" height="" src="codes/images/Dimensionality-Reduction/manifold-decision-boundary-plot2-.svg" width=""/>
       <a id="x6-47005r2">
       </a>
      </p>
      <div class="caption">
       <span class="id">
        (b)
       </span>
       <span class="content">
       </span>
      </div>
     </div>
     <div class="subfigure" style="width:24%;">
      <p class="noindent">
      </p>
      <p class="noindent">
       <img alt="PIC" height="" src="codes/images/Dimensionality-Reduction/manifold-decision-boundary-plot3-.svg" width=""/>
       <a id="x6-47006r3">
       </a>
      </p>
      <div class="caption">
       <span class="id">
        (c)
       </span>
       <span class="content">
       </span>
      </div>
     </div>
     <div class="subfigure" style="width:24%;">
      <p class="noindent">
      </p>
      <p class="noindent">
       <img alt="PIC" height="" src="codes/images/Dimensionality-Reduction/manifold-decision-boundary-plot4-.svg" width=""/>
       <a id="x6-47007r4">
       </a>
      </p>
      <div class="caption">
       <span class="id">
        (d)
       </span>
       <span class="content">
       </span>
      </div>
     </div>
     <a id="x6-47008r4">
     </a>
     <div class="caption">
      <span class="id">
       Figure 4.4:
      </span>
      <span class="content">
       The decision boundary may not always be simpler with lower dimensions.
      </span>
     </div>
    </div>
    <p class="noindent">
     The manifold assumption is often accompanied by another
     <span id="bold" style="font-weight:bold;">
      implicit
     </span>
     assumption:
    </p>
    <div class="quoteblock">
     <p class="noindent">
      The
task
at
hand
(e.g.,
classification
or
regression)
will
be
simpler
if
expressed
in
the
lower-dimensional
space
of
the
manifold.
     </p>
    </div>
    <p class="noindent">
     For example, in
     <span id="bold" style="font-weight:bold;">
      Fig.
     </span>
     <a href="#x6-47006r3">
      4.3c
     </a>, the Swiss roll is split into two <alert style="color: #821131;">(2)</alert> classes. In the 3D space
     <span id="bold" style="font-weight:bold;">
      Fig.
     </span>
     <a href="#x6-47004r1">
      4.3a
     </a>
     the decision boundary
would be fairly complex, but in the 2D unrolled manifold space (on the right) the decision boundary is a straight
line.
    </p>
    <p class="noindent">
     However, this
     <alert style="color: #821131;">
      implicit assumption does not always hold
     </alert>
     . For example, in the bottom row of Figure 8-6, the decision boundary is
located at
     <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
      <msubsup>
       <mrow>
        <mi>
         x
        </mi>
       </mrow>
       <mrow>
        <mn>
         1
        </mn>
       </mrow>
       <mrow>
       </mrow>
      </msubsup>
     </math>
     =
5. This decision boundary looks very simple in the original 3D space (a vertical plane), but it looks more complex in the unrolled
manifold (a collection of four independent line segments).
    </p>
    <div class="warning">
     <p class="noindent">
      Reducing the dimensionality of your training set before training a model will usually speed up training, but it may not always
lead to a better or simpler solution; it all depends on the dataset.
     </p>
    </div>
    <p class="noindent">
     We now have a good sense of what the curse of dimensionality is and how dimensionality reduction algorithms can fight it,
especially when the manifold assumption holds. The rest of this chapter will go through some of the most popular algorithms for
dimensionality reduction.
    </p>
    <p class="noindent">
    </p>
    <h2 class="sectionHead">
     <a aria-label="Link to this section in PDF version" class="pdf-link" href="DataScienceIILectureBook.pdf#section.4.3" style="color: red;  position: absolute;  left: -30px;" target="_blank">
      <svg class="bi bi-file-pdf" fill="currentColor" height="20" viewbox="0 0 16 16" width="20" xmlns="http://www.w3.org/2000/svg">
       <path d="M4 0a2 2 0 0 0-2 2v12a2 2 0 0 0 2 2h8a2 2 0 0 0 2-2V2a2 2 0 0 0-2-2H4zm0 1h8a1 1 0 0 1 1 1v12a1 1 0 0 1-1 1H4a1 1 0 0 1-1-1V2a1 1 0 0 1 1-1z">
       </path>
       <path d="M4.603 12.087a.81.81 0 0 1-.438-.42c-.195-.388-.13-.776.08-1.102.198-.307.526-.568.897-.787a7.68 7.68 0 0 1 1.482-.645 19.701 19.701 0 0 0 1.062-2.227 7.269 7.269 0 0 1-.43-1.295c-.086-.4-.119-.796-.046-1.136.075-.354.274-.672.65-.823.192-.077.4-.12.602-.077a.7.7 0 0 1 .477.365c.088.164.12.356.127.538.007.187-.012.395-.047.614-.084.51-.27 1.134-.52 1.794a10.954 10.954 0 0 0 .98 1.686 5.753 5.753 0 0 1 1.334.05c.364.065.734.195.96.465.12.144.193.32.2.518.007.192-.047.382-.138.563a1.04 1.04 0 0 1-.354.416.856.856 0 0 1-.51.138c-.331-.014-.654-.196-.933-.417a5.716 5.716 0 0 1-.911-.95 11.642 11.642 0 0 0-1.997.406 11.311 11.311 0 0 1-1.021 1.51c-.29.35-.608.655-.926.787a.793.793 0 0 1-.58.029zm1.379-1.901c-.166.076-.32.156-.459.238-.328.194-.541.383-.647.547-.094.145-.096.25-.04.361.01.022.02.036.026.044a.27.27 0 0 0 .035-.012c.137-.056.355-.235.635-.572a8.18 8.18 0 0 0 .45-.606zm1.64-1.33a12.647 12.647 0 0 1 1.01-.193 11.666 11.666 0 0 1-.51-.858 20.741 20.741 0 0 1-.5 1.05zm2.446.45c.15.162.296.3.435.41.24.19.407.253.498.256a.107.107 0 0 0 .07-.015.307.307 0 0 0 .094-.125.436.436 0 0 0 .059-.2.095.095 0 0 0-.026-.063c-.052-.062-.2-.152-.518-.209a3.881 3.881 0 0 0-.612-.053zM8.078 5.8a6.7 6.7 0 0 0 .2-.828c.031-.188.043-.343.038-.465a.613.613 0 0 0-.032-.198.517.517 0 0 0-.145.04c-.087.035-.158.106-.196.283-.04.192-.03.469.046.822.024.111.054.227.09.346z">
       </path>
      </svg>
     </a>
     <small class="titlemark">
      4.3
     </small>
     <a id="x6-480004.3">
     </a>
     Principal Component Analysis (PCA)
    </h2>
    <p class="noindent">
     <a class="glossary" href="DataScienceIILectureBookli2.html#glo:pca">
      PCA
     </a>
     is
by
far
the
most
popular
dimensionality
reduction
algorithm,
invented
in
1901
by
Karl
Pearson.
     <alert style="color: #821131;">
      <span id="bold" style="font-weight:bold;">
       <sup class="textsuperscript">
        8
       </sup>
      </span>
     </alert>
     <span class="marginnote">
      <img alt="PIC" height="" src="figures/Dimensionality-Reduction/raster/portrait-karl.jpg" width="100%"/>
      <alert style="color: #821131;">
       <span id="bold" style="font-weight:bold;">
        <sup class="textsuperscript">
         8
        </sup>
       </span>
      </alert>
      <span style="color:#0063B2;">
       <span id="bold" style="font-weight:bold;">
        Karl
Pearson
       </span>
       <italic>
        (1857
 1936)
       </italic>
       An
English
biostatistician
and
mathematician.
He
has
been
credited
with
establishing
the
discipline
of
mathematical
statistics.
He
founded
the
world’s
first
university
statistics
department
at
University
College
London
in
1911,
and
contributed
significantly
to
the
field
of
                                                                                
                                                                                
biometrics
and
meteorology.
      </span>
     </span>
     First
it
identifies
the
hyperplane
that
lies
closest
to
the
data,
and
then
it
projects
the
data
onto
it,
just
like
in
     <span id="bold" style="font-weight:bold;">
      Fig.
     </span>
     <a href="#x6-46001r1">
      4.0a
     </a>.
    </p>
    <h3 class="subsectionHead">
     <a aria-label="Link to this section in PDF version" class="pdf-link" href="DataScienceIILectureBook.pdf#subsection.4.3.1" style="color: red;  position: absolute;  left: -30px;" target="_blank">
      <svg class="bi bi-file-pdf" fill="currentColor" height="20" viewbox="0 0 16 16" width="20" xmlns="http://www.w3.org/2000/svg">
       <path d="M4 0a2 2 0 0 0-2 2v12a2 2 0 0 0 2 2h8a2 2 0 0 0 2-2V2a2 2 0 0 0-2-2H4zm0 1h8a1 1 0 0 1 1 1v12a1 1 0 0 1-1 1H4a1 1 0 0 1-1-1V2a1 1 0 0 1 1-1z">
       </path>
       <path d="M4.603 12.087a.81.81 0 0 1-.438-.42c-.195-.388-.13-.776.08-1.102.198-.307.526-.568.897-.787a7.68 7.68 0 0 1 1.482-.645 19.701 19.701 0 0 0 1.062-2.227 7.269 7.269 0 0 1-.43-1.295c-.086-.4-.119-.796-.046-1.136.075-.354.274-.672.65-.823.192-.077.4-.12.602-.077a.7.7 0 0 1 .477.365c.088.164.12.356.127.538.007.187-.012.395-.047.614-.084.51-.27 1.134-.52 1.794a10.954 10.954 0 0 0 .98 1.686 5.753 5.753 0 0 1 1.334.05c.364.065.734.195.96.465.12.144.193.32.2.518.007.192-.047.382-.138.563a1.04 1.04 0 0 1-.354.416.856.856 0 0 1-.51.138c-.331-.014-.654-.196-.933-.417a5.716 5.716 0 0 1-.911-.95 11.642 11.642 0 0 0-1.997.406 11.311 11.311 0 0 1-1.021 1.51c-.29.35-.608.655-.926.787a.793.793 0 0 1-.58.029zm1.379-1.901c-.166.076-.32.156-.459.238-.328.194-.541.383-.647.547-.094.145-.096.25-.04.361.01.022.02.036.026.044a.27.27 0 0 0 .035-.012c.137-.056.355-.235.635-.572a8.18 8.18 0 0 0 .45-.606zm1.64-1.33a12.647 12.647 0 0 1 1.01-.193 11.666 11.666 0 0 1-.51-.858 20.741 20.741 0 0 1-.5 1.05zm2.446.45c.15.162.296.3.435.41.24.19.407.253.498.256a.107.107 0 0 0 .07-.015.307.307 0 0 0 .094-.125.436.436 0 0 0 .059-.2.095.095 0 0 0-.026-.063c-.052-.062-.2-.152-.518-.209a3.881 3.881 0 0 0-.612-.053zM8.078 5.8a6.7 6.7 0 0 0 .2-.828c.031-.188.043-.343.038-.465a.613.613 0 0 0-.032-.198.517.517 0 0 0-.145.04c-.087.035-.158.106-.196.283-.04.192-.03.469.046.822.024.111.054.227.09.346z">
       </path>
      </svg>
     </a>
     <small class="titlemark">
      4.3.1
     </small>
     <a id="x6-490004.3.1">
     </a>
     Preserving the Variance
    </h3>
    <p class="noindent">
     Before
we
can
project
the
training
set
onto
a
lower-dimensional
hyperplane,
we
first
need
to
choose
the
     <alert style="color: #821131;">
      correct
hyperplane
     </alert>
     .
As
an
example,
a
simple
2D
dataset
is
represented
on
the
left
in
     <span id="bold" style="font-weight:bold;">
      Fig.
     </span>
     <a href="#x6-49004r5">
      4.5
     </a>
     along
with
three
different
axes.
     <alert style="color: #821131;">
      <span id="bold" style="font-weight:bold;">
       <sup class="textsuperscript">
        9
       </sup>
      </span>
     </alert>
     <span class="marginnote">
      <alert style="color: #821131;">
       <span id="bold" style="font-weight:bold;">
        <sup class="textsuperscript">
         9
        </sup>
       </span>
      </alert>
      <span style="color:#0063B2;">
       i.e.,
1D
hyperplanes.
      </span>
     </span>
    </p>
    <p class="noindent">
     On the right is the result of the projection of the dataset onto each of these axes. As we can see:
    </p>
    <dl class="description">
     <dt class="description">
      <a id="x6-49001x4.3.1">
      </a>
      Top
     </dt>
     <dd class="description">
      <p class="noindent">
       The
projection
onto
the
solid
line
preserves
the
maximum
variance,
      </p>
     </dd>
     <dt class="description">
      <a id="x6-49002x4.3.1">
      </a>
      Bottom
     </dt>
     <dd class="description">
      <p class="noindent">
       The
projection
onto
the
dotted
line
preserves
very
little
variance
      </p>
     </dd>
     <dt class="description">
      <a id="x6-49003x4.3.1">
      </a>
      Middle
     </dt>
     <dd class="description">
      <p class="noindent">
       The
projection
onto
the
dashed
line
preserves
an
intermediate
amount
of
variance
      </p>
     </dd>
    </dl>
    <div class="figure">
     <p class="noindent">
      <img alt="PIC" height="" src="codes/images/Dimensionality-Reduction/pca-best-projection-plot-.svg" width="150%"/>
      <a id="x6-49004r5">
      </a>
     </p>
     <figcaption class="caption">
      <span class="id">
       Figure 4.5:
      </span>
      <span class="content">
       Selecting the subspace on which to project.
      </span>
     </figcaption>
    </div>
    <p class="noindent">
     It seems reasonable to select the
     <span id="bold" style="font-weight:bold;">
      axis preserving the maximum amount of variance
     </span>
     , as it will most likely lose less
information than the other projections.
    </p>
    <div class="knowledge">
     <p class="noindent">
      Another way to justify this choice is that it is the axis that minimizes the mean squared distance between the original dataset
and its projection onto that axis.
     </p>
    </div>
    <p class="noindent">
     This is
     <span id="bold" style="font-weight:bold;">
      core idea
     </span>
     behind
     <a class="glossary" href="DataScienceIILectureBookli2.html#glo:pca">
      PCA
     </a>.
    </p>
    <h3 class="subsectionHead">
     <a aria-label="Link to this section in PDF version" class="pdf-link" href="DataScienceIILectureBook.pdf#subsection.4.3.2" style="color: red;  position: absolute;  left: -30px;" target="_blank">
      <svg class="bi bi-file-pdf" fill="currentColor" height="20" viewbox="0 0 16 16" width="20" xmlns="http://www.w3.org/2000/svg">
       <path d="M4 0a2 2 0 0 0-2 2v12a2 2 0 0 0 2 2h8a2 2 0 0 0 2-2V2a2 2 0 0 0-2-2H4zm0 1h8a1 1 0 0 1 1 1v12a1 1 0 0 1-1 1H4a1 1 0 0 1-1-1V2a1 1 0 0 1 1-1z">
       </path>
       <path d="M4.603 12.087a.81.81 0 0 1-.438-.42c-.195-.388-.13-.776.08-1.102.198-.307.526-.568.897-.787a7.68 7.68 0 0 1 1.482-.645 19.701 19.701 0 0 0 1.062-2.227 7.269 7.269 0 0 1-.43-1.295c-.086-.4-.119-.796-.046-1.136.075-.354.274-.672.65-.823.192-.077.4-.12.602-.077a.7.7 0 0 1 .477.365c.088.164.12.356.127.538.007.187-.012.395-.047.614-.084.51-.27 1.134-.52 1.794a10.954 10.954 0 0 0 .98 1.686 5.753 5.753 0 0 1 1.334.05c.364.065.734.195.96.465.12.144.193.32.2.518.007.192-.047.382-.138.563a1.04 1.04 0 0 1-.354.416.856.856 0 0 1-.51.138c-.331-.014-.654-.196-.933-.417a5.716 5.716 0 0 1-.911-.95 11.642 11.642 0 0 0-1.997.406 11.311 11.311 0 0 1-1.021 1.51c-.29.35-.608.655-.926.787a.793.793 0 0 1-.58.029zm1.379-1.901c-.166.076-.32.156-.459.238-.328.194-.541.383-.647.547-.094.145-.096.25-.04.361.01.022.02.036.026.044a.27.27 0 0 0 .035-.012c.137-.056.355-.235.635-.572a8.18 8.18 0 0 0 .45-.606zm1.64-1.33a12.647 12.647 0 0 1 1.01-.193 11.666 11.666 0 0 1-.51-.858 20.741 20.741 0 0 1-.5 1.05zm2.446.45c.15.162.296.3.435.41.24.19.407.253.498.256a.107.107 0 0 0 .07-.015.307.307 0 0 0 .094-.125.436.436 0 0 0 .059-.2.095.095 0 0 0-.026-.063c-.052-.062-.2-.152-.518-.209a3.881 3.881 0 0 0-.612-.053zM8.078 5.8a6.7 6.7 0 0 0 .2-.828c.031-.188.043-.343.038-.465a.613.613 0 0 0-.032-.198.517.517 0 0 0-.145.04c-.087.035-.158.106-.196.283-.04.192-.03.469.046.822.024.111.054.227.09.346z">
       </path>
      </svg>
     </a>
     <small class="titlemark">
      4.3.2
     </small>
     <a id="x6-500004.3.2">
     </a>
     Principal Components
    </h3>
    <p class="noindent">
     <a class="glossary" href="DataScienceIILectureBookli2.html#glo:pca">
      PCA
     </a>
     identifies the axis accounting for the
     <span id="bold" style="font-weight:bold;">
      largest amount of variance
     </span>
     in the training set. In
     <span id="bold" style="font-weight:bold;">
      Fig.
     </span>
     <a href="#x6-49004r5">
      4.5
     </a>, it is the solid
line.
    </p>
    <p class="noindent">
     It also finds a second axis,
     <span id="bold" style="font-weight:bold;">
      orthogonal
     </span>
     to the first one, which accounts for the largest amount of the remaining variance. In this
2D example there is no choice as it is the dotted line. If it were a higher-dimensional dataset, PCA would also find a
     <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
      <msup>
       <mrow>
        <mstyle class="text">
         <mtext>
          3
         </mtext>
        </mstyle>
       </mrow>
       <mrow>
        <mstyle class="text">
         <mtext>
          rd
         </mtext>
        </mstyle>
       </mrow>
      </msup>
     </math>
     axis,
orthogonal to both previous axes, and a fourth, a fifth, and so on, as many axes as the number of dimensions in the
dataset.
    </p>
    <p class="noindent">
     The
     <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
      <msubsup>
       <mrow>
        <mi>
         i
        </mi>
       </mrow>
       <mrow>
       </mrow>
       <mrow>
        <mi>
         t
        </mi>
        <mi>
         h
        </mi>
       </mrow>
      </msubsup>
     </math>
     axis
is called the
     <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
      <msup>
       <mrow>
        <mstyle class="text">
         <mtext>
          i
         </mtext>
        </mstyle>
       </mrow>
       <mrow>
        <mstyle class="text">
         <mtext>
          th
         </mtext>
        </mstyle>
       </mrow>
      </msup>
     </math>
     <a class="glossary" href="DataScienceIILectureBookli2.html#glo:pc">
      Principal Component (PC)
     </a>
     of the data. In
     <span id="bold" style="font-weight:bold;">
      Fig.
     </span>
     <a href="#x6-49004r5">
      4.5
     </a>, the first
     <a class="glossary" href="DataScienceIILectureBookli2.html#glo:pc">
      PC
     </a>
     is the axis on which vector
     <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
      <msubsup>
       <mrow>
        <mi>
         c
        </mi>
       </mrow>
       <mrow>
        <mn>
         1
        </mn>
       </mrow>
       <mrow>
       </mrow>
      </msubsup>
     </math>
     lies, and the second
     <a class="glossary" href="DataScienceIILectureBookli2.html#glo:pc">
      PC
     </a>
     is
the axis on which vector
     <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
      <msubsup>
       <mrow>
        <mi>
         c
        </mi>
       </mrow>
       <mrow>
        <mn>
         2
        </mn>
       </mrow>
       <mrow>
       </mrow>
      </msubsup>
     </math>
     lies.
    </p>
    <p class="noindent">
     In
     <span id="bold" style="font-weight:bold;">
      Fig.
     </span>
     <a href="#x6-46001r1">
      4.0a
     </a>
     the first two
     <a class="glossary" href="DataScienceIILectureBookli2.html#glo:pc">
      PC
     </a>
     s are on the projection plane, and the third
     <a class="glossary" href="DataScienceIILectureBookli2.html#glo:pc">
      PC
     </a>
     is the axis
orthogonal to that plane. After the projection, in
     <span id="bold" style="font-weight:bold;">
      Fig.
     </span>
     <a href="#x6-46002r2">
      4.0b
     </a>, the first
     <a class="glossary" href="DataScienceIILectureBookli2.html#glo:pc">
      PC
     </a>
     corresponds to the
     <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
      <msubsup>
       <mrow>
        <mi>
         z
        </mi>
       </mrow>
       <mrow>
        <mn>
         1
        </mn>
       </mrow>
       <mrow>
       </mrow>
      </msubsup>
     </math>
     axis, and the second
     <a class="glossary" href="DataScienceIILectureBookli2.html#glo:pc">
      PC
     </a>
     corresponds to the
     <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
      <msubsup>
       <mrow>
        <mi>
         z
        </mi>
       </mrow>
       <mrow>
        <mn>
         2
        </mn>
       </mrow>
       <mrow>
       </mrow>
      </msubsup>
     </math>
     axis.
    </p>
    <div class="informationblock" id="tcolobox-29">
     <div class="title">
      <p class="noindent">
       <span id="bold" style="font-weight:bold;">
        Information
       </span>
       : The Idea Behind PCA
      </p>
     </div>
     <div class="box-content">
      <p class="noindent">
       For
each
       <a class="glossary" href="DataScienceIILectureBookli2.html#glo:pc">
        PC
       </a>,
       <a class="glossary" href="DataScienceIILectureBookli2.html#glo:pca">
        PCA
       </a>
       finds
a
zero-centered
unit
vector
pointing
in
the
direction
of
the
       <a class="glossary" href="DataScienceIILectureBookli2.html#glo:pc">
        PC
       </a>.
As
two
opposing
unit
vectors
lie
on
the
same
axis,
the
direction
of
the
unit
vectors
returned
by
PCA
is
not
stable:
if
you
perturb
the
training
set
slightly
and
run
PCA
again,
the
unit
vectors
may
point
in
the
opposite
direction
as
the
original
vectors.
However,
they
will
generally
still
lie
on
the
same
axes.
In
some
cases,
a
pair
of
unit
vectors
may
even
rotate
or
swap,
       <alert style="color: #821131;">
        <span id="bold" style="font-weight:bold;">
         <sup class="textsuperscript">
          10
         </sup>
        </span>
       </alert>
       <span class="marginnote">
        <alert style="color: #821131;">
         <span id="bold" style="font-weight:bold;">
          <sup class="textsuperscript">
           10
          </sup>
         </span>
        </alert>
        <span style="color:#0063B2;">
         if
the
variances
along
these
two
axes
are
very
close
        </span>
       </span>
       but
the
plane
they
define
will
generally
remain
the
same.
      </p>
     </div>
    </div>
    <p class="noindent">
     So how can we find the principal components of a training set?
    </p>
    <p class="noindent">
     Luckily, there is a standard matrix factorisation technique called
     <a class="glossary" href="DataScienceIILectureBookli2.html#glo:svd">
      Singular Value Decomposition (SVD)
     </a>
     which decomposes the training set matrix
     <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
      <mover accent="true">
       <mrow>
        <mi>
         X
        </mi>
       </mrow>
       <mo accent="true">
        →
       </mo>
      </mover>
     </math>
     into the matrix multiplication
of three <alert style="color: #821131;">(3)</alert> matrices
     <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
      <mover accent="true">
       <mrow>
        <mi>
         U
        </mi>
       </mrow>
       <mo accent="true">
        →
       </mo>
      </mover>
      <mstyle mathvariant="bold">
       <mi mathvariant="normal">
        Σ
       </mi>
      </mstyle>
      <msubsup>
       <mrow>
        <mover accent="true">
         <mrow>
          <mi>
           V
          </mi>
         </mrow>
         <mo accent="true">
          →
         </mo>
        </mover>
       </mrow>
       <mrow>
       </mrow>
       <mrow>
        <mi>
         T
        </mi>
       </mrow>
      </msubsup>
     </math>
     ,
where
     <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
      <mover accent="true">
       <mrow>
        <mi>
         V
        </mi>
       </mrow>
       <mo accent="true">
        →
       </mo>
      </mover>
     </math>
     contains the unit vectors that define all the principal components that you are looking for, as shown below:
    </p>
    <table class="equation-star">
     <tr>
      <td>
       <math class="equation" display="block" xmlns="http://www.w3.org/1998/Math/MathML">
        <mover accent="true">
         <mrow>
          <mi>
           V
          </mi>
         </mrow>
         <mo accent="true">
          →
         </mo>
        </mover>
        <mo class="MathClass-rel" stretchy="false">
         =
        </mo>
        <mrow>
         <mo fence="true" form="prefix">
          [
         </mo>
         <mrow>
          <mtable align="axis" class="array" columnlines="none none none none none none none none none" displaystyle="true" equalcolumns="false" equalrows="false" style="">
           <mtr class="array-row">
            <mtd class="array-td" columnalign="center">
             <msubsup>
              <mrow>
               <mover accent="true">
                <mrow>
                 <mi>
                  c
                 </mi>
                </mrow>
                <mo accent="true">
                 →
                </mo>
               </mover>
              </mrow>
              <mrow>
               <mn>
                1
               </mn>
              </mrow>
              <mrow>
              </mrow>
             </msubsup>
            </mtd>
            <mtd class="array-td" columnalign="center">
             <msubsup>
              <mrow>
               <mover accent="true">
                <mrow>
                 <mi>
                  c
                 </mi>
                </mrow>
                <mo accent="true">
                 →
                </mo>
               </mover>
              </mrow>
              <mrow>
               <mn>
                2
               </mn>
              </mrow>
              <mrow>
              </mrow>
             </msubsup>
            </mtd>
            <mtd class="array-td" columnalign="center">
             <mo class="MathClass-rel" stretchy="false">
              ⋯
             </mo>
            </mtd>
            <mtd class="array-td" columnalign="center">
             <msubsup>
              <mrow>
               <mover accent="true">
                <mrow>
                 <mi>
                  c
                 </mi>
                </mrow>
                <mo accent="true">
                 →
                </mo>
               </mover>
              </mrow>
              <mrow>
               <mi>
                n
               </mi>
              </mrow>
              <mrow>
              </mrow>
             </msubsup>
            </mtd>
           </mtr>
          </mtable>
         </mrow>
         <mo fence="true" form="postfix">
          ]
         </mo>
        </mrow>
       </math>
      </td>
     </tr>
    </table>
    <p class="noindent">
     The following Python code uses NumPy’s <span style="color:#054C5C;"><code class="verb">svd()</code></span>
     function to obtain all the principal components of the 3D training set represented
in
     <span id="bold" style="font-weight:bold;">
      Fig.
     </span>
     <a href="#x6-46001r1">
      4.0a
     </a>, then it extracts the two unit vectors that define the first two
     <a class="glossary" href="DataScienceIILectureBookli2.html#glo:pc">
      PC
     </a>
     s:
    </p>
    <p class="noindent">
    </p>
    <pre><div id="fancyvrb50" style="padding:20px;border-radius: 3px;"><a id="x6-50002r365"></a><span style="color:#2B2BFF;">import</span><span style="color:#BABABA;"> </span>numpy<span style="color:#BABABA;"> </span><span style="color:#2B2BFF;">as</span><span style="color:#BABABA;"> </span>np 
<a id="x6-50004r366"></a> 
<a id="x6-50006r367"></a>X_centered = X - X.mean(axis=<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">0</span></span>) 
<a id="x6-50008r368"></a>U, s, Vt = np.linalg.svd(X_centered) 
<a id="x6-50010r369"></a>c1 = Vt[<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">0</span></span>] 
<a id="x6-50012r370"></a>c2 = Vt[<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">1</span></span>]</div></pre>
    <div class="warning">
     <p class="noindent">
      <a class="glossary" href="DataScienceIILectureBookli2.html#glo:pca">
       PCA
      </a>
      assumes that the dataset is centered around the origin. As we will see, <span style="color:#054C5C;"><code class="verb">sklearn</code></span>
      ’s
      <a class="glossary" href="DataScienceIILectureBookli2.html#glo:pca">
       PCA
      </a>
      classes take care of centering the data
for us. If we were to implement
      <a class="glossary" href="DataScienceIILectureBookli2.html#glo:pca">
       PCA
      </a>
      ourselves, or if we used other libraries, we shouldn’t forget to center the data first.
     </p>
    </div>
    <h3 class="subsectionHead">
     <a aria-label="Link to this section in PDF version" class="pdf-link" href="DataScienceIILectureBook.pdf#subsection.4.3.3" style="color: red;  position: absolute;  left: -30px;" target="_blank">
      <svg class="bi bi-file-pdf" fill="currentColor" height="20" viewbox="0 0 16 16" width="20" xmlns="http://www.w3.org/2000/svg">
       <path d="M4 0a2 2 0 0 0-2 2v12a2 2 0 0 0 2 2h8a2 2 0 0 0 2-2V2a2 2 0 0 0-2-2H4zm0 1h8a1 1 0 0 1 1 1v12a1 1 0 0 1-1 1H4a1 1 0 0 1-1-1V2a1 1 0 0 1 1-1z">
       </path>
       <path d="M4.603 12.087a.81.81 0 0 1-.438-.42c-.195-.388-.13-.776.08-1.102.198-.307.526-.568.897-.787a7.68 7.68 0 0 1 1.482-.645 19.701 19.701 0 0 0 1.062-2.227 7.269 7.269 0 0 1-.43-1.295c-.086-.4-.119-.796-.046-1.136.075-.354.274-.672.65-.823.192-.077.4-.12.602-.077a.7.7 0 0 1 .477.365c.088.164.12.356.127.538.007.187-.012.395-.047.614-.084.51-.27 1.134-.52 1.794a10.954 10.954 0 0 0 .98 1.686 5.753 5.753 0 0 1 1.334.05c.364.065.734.195.96.465.12.144.193.32.2.518.007.192-.047.382-.138.563a1.04 1.04 0 0 1-.354.416.856.856 0 0 1-.51.138c-.331-.014-.654-.196-.933-.417a5.716 5.716 0 0 1-.911-.95 11.642 11.642 0 0 0-1.997.406 11.311 11.311 0 0 1-1.021 1.51c-.29.35-.608.655-.926.787a.793.793 0 0 1-.58.029zm1.379-1.901c-.166.076-.32.156-.459.238-.328.194-.541.383-.647.547-.094.145-.096.25-.04.361.01.022.02.036.026.044a.27.27 0 0 0 .035-.012c.137-.056.355-.235.635-.572a8.18 8.18 0 0 0 .45-.606zm1.64-1.33a12.647 12.647 0 0 1 1.01-.193 11.666 11.666 0 0 1-.51-.858 20.741 20.741 0 0 1-.5 1.05zm2.446.45c.15.162.296.3.435.41.24.19.407.253.498.256a.107.107 0 0 0 .07-.015.307.307 0 0 0 .094-.125.436.436 0 0 0 .059-.2.095.095 0 0 0-.026-.063c-.052-.062-.2-.152-.518-.209a3.881 3.881 0 0 0-.612-.053zM8.078 5.8a6.7 6.7 0 0 0 .2-.828c.031-.188.043-.343.038-.465a.613.613 0 0 0-.032-.198.517.517 0 0 0-.145.04c-.087.035-.158.106-.196.283-.04.192-.03.469.046.822.024.111.054.227.09.346z">
       </path>
      </svg>
     </a>
     <small class="titlemark">
      4.3.3
     </small>
     <a id="x6-510004.3.3">
     </a>
     Downgrading Dimensions
    </h3>
    <p class="noindent">
     Once we identified all the
     <a class="glossary" href="DataScienceIILectureBookli2.html#glo:pc">
      PC
     </a>
     s, we can reduce the dimensionality of the dataset down to
     <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
      <mi>
       d
      </mi>
     </math>
     dimensions by projecting it onto
the hyperplane defined by the first
     <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
      <mi>
       d
      </mi>
     </math>
     principal components. Selecting this hyperplane ensures that the projection will preserve as much variance as
possible.
    </p>
    <p class="noindent">
     For example, in
     <span id="bold" style="font-weight:bold;">
      Fig.
     </span>
     <a href="#x6-46001r1">
      4.0a
     </a>
     the 3D dataset is projected down to the 2D plane defined by the first two <alert style="color: #821131;">(2)</alert> principal components,
preserving a large part of the dataset’s variance. As a result, the 2D projection looks very much like the original 3D
dataset.
    </p>
    <p class="noindent">
     To project the training set onto the hyperplane and obtain a reduced dataset
     <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
      <msubsup>
       <mrow>
        <mover accent="true">
         <mrow>
          <mi>
           X
          </mi>
         </mrow>
         <mo accent="true">
          →
         </mo>
        </mover>
       </mrow>
       <mrow>
        <mi>
         d
        </mi>
        <mo class="MathClass-bin" stretchy="false">
         −
        </mo>
        <mi>
         p
        </mi>
        <mi>
         r
        </mi>
        <mi>
         o
        </mi>
        <mi>
         j
        </mi>
       </mrow>
       <mrow>
       </mrow>
      </msubsup>
     </math>
     of dimensionality
     <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
      <mi>
       d
      </mi>
     </math>
     , compute the matrix multiplication
of the training set matrix
     <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
      <mover accent="true">
       <mrow>
        <mi>
         X
        </mi>
       </mrow>
       <mo accent="true">
        →
       </mo>
      </mover>
     </math>
     by
the matrix
     <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
      <msubsup>
       <mrow>
        <mi>
         W
        </mi>
       </mrow>
       <mrow>
        <mi>
         d
        </mi>
       </mrow>
       <mrow>
       </mrow>
      </msubsup>
     </math>
     , defined as the
matrix containing the first
     <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
      <mi>
       d
      </mi>
     </math>
     columns of
     <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
      <mover accent="true">
       <mrow>
        <mi>
         V
        </mi>
       </mrow>
       <mo accent="true">
        →
       </mo>
      </mover>
     </math>
     :
    </p>
    <table class="equation-star">
     <tr>
      <td>
       <math class="equation" display="block" xmlns="http://www.w3.org/1998/Math/MathML">
        <msubsup>
         <mrow>
          <mover accent="true">
           <mrow>
            <mi>
             X
            </mi>
           </mrow>
           <mo accent="true">
            →
           </mo>
          </mover>
         </mrow>
         <mrow>
          <mi>
           d
          </mi>
          <mo class="MathClass-bin" stretchy="false">
           −
          </mo>
          <mi>
           p
          </mi>
          <mi>
           r
          </mi>
          <mi>
           o
          </mi>
          <mi>
           j
          </mi>
         </mrow>
         <mrow>
         </mrow>
        </msubsup>
        <mo class="MathClass-rel" stretchy="false">
         =
        </mo>
        <mover accent="true">
         <mrow>
          <mi>
           X
          </mi>
         </mrow>
         <mo accent="true">
          →
         </mo>
        </mover>
        <msubsup>
         <mrow>
          <mover accent="true">
           <mrow>
            <mi>
             W
            </mi>
           </mrow>
           <mo accent="true">
            →
           </mo>
          </mover>
         </mrow>
         <mrow>
          <mi>
           d
          </mi>
         </mrow>
         <mrow>
         </mrow>
        </msubsup>
       </math>
      </td>
     </tr>
    </table>
    <p class="noindent">
     The following Python code projects the training set onto the plane defined by the first two principal components:
    </p>
    <p class="noindent">
    </p>
    <pre><div id="fancyvrb51" style="padding:20px;border-radius: 3px;"><a id="x6-51002r377"></a>W2 = Vt[:<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">2</span></span>].T 
<a id="x6-51004r378"></a>X2D = X_centered @ W2</div></pre>
    <p class="noindent">
     We now know how to reduce the dimensionality of any dataset by projecting it down to any number of dimensions, while
preserving as much variance as possible.
     <a id="paragraph*.4">
     </a>
    </p>
    <p class="noindent">
     <span class="paragraphHead">
      <a id="x6-52000">
      </a>
      Using sklearn
     </span>
     The <span style="color:#054C5C;"><code class="verb">PCA</code></span>
     class uses
     <a class="glossary" href="DataScienceIILectureBookli2.html#glo:svd">
      SVD
     </a>
     to implement
     <a class="glossary" href="DataScienceIILectureBookli2.html#glo:pca">
      PCA
     </a>, just like we did earlier. The following code applies
     <a class="glossary" href="DataScienceIILectureBookli2.html#glo:pca">
      PCA
     </a>
     to reduce the dimensionality
of the dataset down to two <alert style="color: #821131;">(2)</alert> dimensions:
    </p>
    <div class="warning">
     <p class="noindent"> <span style="color:#054C5C;"><code class="verb">sklearn</code></span>
      ’s <span style="color:#054C5C;"><code class="verb">PCA</code></span>
      also automatically takes care of centering the data.
     </p>
    </div>
    <p class="noindent">
    </p>
    <pre><div id="fancyvrb52" style="padding:20px;border-radius: 3px;"><a id="x6-52002r385"></a><span style="color:#2B2BFF;">from</span><span style="color:#BABABA;"> </span>sklearn.decomposition<span style="color:#BABABA;"> </span><span style="color:#2B2BFF;">import</span> PCA 
<a id="x6-52004r386"></a> 
<a id="x6-52006r387"></a>pca = PCA(n_components=<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">2</span></span>) 
<a id="x6-52008r388"></a>X2D = pca.fit_transform(X)</div></pre>
    <p class="noindent">
     After fitting the
     <a class="glossary" href="DataScienceIILectureBookli2.html#glo:pca">
      PCA
     </a>
     transformer to the dataset, its <span style="color:#054C5C;"><code class="verb">components_</code></span>
     attribute holds the transpose of
     <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
      <msubsup>
       <mrow>
        <mover accent="true">
         <mrow>
          <mi>
           W
          </mi>
         </mrow>
         <mo accent="true">
          →
         </mo>
        </mover>
       </mrow>
       <mrow>
        <mi>
         d
        </mi>
       </mrow>
       <mrow>
       </mrow>
      </msubsup>
     </math>
     : it
contains one row for each of the first d principal components.
     <a id="subsubsection*.5">
     </a>
    </p>
    <h5 class="subsubsectionHead">
     <a id="x6-53000">
     </a>
     Explained Variance Ratio
    </h5>
    <p class="noindent">
     Another useful piece of information is the explained variance ratio of each principal component, available via the <span style="color:#054C5C;"><code class="verb">explained_variance_ratio_</code></span>
     variable. The ratio indicates the proportion of the dataset’s variance that lies along each principal
component.
    </p>
    <p class="noindent">
     For example, let’s look at the explained variance ratios of the first two components of the 3D dataset represented in
     <span id="bold" style="font-weight:bold;">
      Fig.
     </span>
     <a href="#x6-46001r1">
      4.0a
     </a>
     :
    </p>
    <p class="noindent">
    </p>
    <pre><div id="fancyvrb53" style="padding:20px;border-radius: 3px;"><a id="x6-53002r395"></a><span style="color:#2B2BFF;">print</span>(pca.explained_variance_ratio_)</div></pre>
    <div class="tcolorbox tcolorbox" id="tcolobox-30">
     <div class="tcolorbox-content">
      <pre><div id="fancyvrb54" style="padding:20px;border-radius: 3px;"><a id="x6-53004r1"></a>[0.17974135 0.1177597 ]</div></pre>
     </div>
    </div>
    <p class="noindent">
     This output tells us that about 76% of the dataset’s variance lies along the first PC, and about 15% lies along the second
PC. This leaves about 9% for the third PC, so it is reasonable to assume that the third PC probably carries little
information.
    </p>
    <h3 class="subsectionHead">
     <a aria-label="Link to this section in PDF version" class="pdf-link" href="DataScienceIILectureBook.pdf#subsection.4.3.4" style="color: red;  position: absolute;  left: -30px;" target="_blank">
      <svg class="bi bi-file-pdf" fill="currentColor" height="20" viewbox="0 0 16 16" width="20" xmlns="http://www.w3.org/2000/svg">
       <path d="M4 0a2 2 0 0 0-2 2v12a2 2 0 0 0 2 2h8a2 2 0 0 0 2-2V2a2 2 0 0 0-2-2H4zm0 1h8a1 1 0 0 1 1 1v12a1 1 0 0 1-1 1H4a1 1 0 0 1-1-1V2a1 1 0 0 1 1-1z">
       </path>
       <path d="M4.603 12.087a.81.81 0 0 1-.438-.42c-.195-.388-.13-.776.08-1.102.198-.307.526-.568.897-.787a7.68 7.68 0 0 1 1.482-.645 19.701 19.701 0 0 0 1.062-2.227 7.269 7.269 0 0 1-.43-1.295c-.086-.4-.119-.796-.046-1.136.075-.354.274-.672.65-.823.192-.077.4-.12.602-.077a.7.7 0 0 1 .477.365c.088.164.12.356.127.538.007.187-.012.395-.047.614-.084.51-.27 1.134-.52 1.794a10.954 10.954 0 0 0 .98 1.686 5.753 5.753 0 0 1 1.334.05c.364.065.734.195.96.465.12.144.193.32.2.518.007.192-.047.382-.138.563a1.04 1.04 0 0 1-.354.416.856.856 0 0 1-.51.138c-.331-.014-.654-.196-.933-.417a5.716 5.716 0 0 1-.911-.95 11.642 11.642 0 0 0-1.997.406 11.311 11.311 0 0 1-1.021 1.51c-.29.35-.608.655-.926.787a.793.793 0 0 1-.58.029zm1.379-1.901c-.166.076-.32.156-.459.238-.328.194-.541.383-.647.547-.094.145-.096.25-.04.361.01.022.02.036.026.044a.27.27 0 0 0 .035-.012c.137-.056.355-.235.635-.572a8.18 8.18 0 0 0 .45-.606zm1.64-1.33a12.647 12.647 0 0 1 1.01-.193 11.666 11.666 0 0 1-.51-.858 20.741 20.741 0 0 1-.5 1.05zm2.446.45c.15.162.296.3.435.41.24.19.407.253.498.256a.107.107 0 0 0 .07-.015.307.307 0 0 0 .094-.125.436.436 0 0 0 .059-.2.095.095 0 0 0-.026-.063c-.052-.062-.2-.152-.518-.209a3.881 3.881 0 0 0-.612-.053zM8.078 5.8a6.7 6.7 0 0 0 .2-.828c.031-.188.043-.343.038-.465a.613.613 0 0 0-.032-.198.517.517 0 0 0-.145.04c-.087.035-.158.106-.196.283-.04.192-.03.469.046.822.024.111.054.227.09.346z">
       </path>
      </svg>
     </a>
     <small class="titlemark">
      4.3.4
     </small>
     <a id="x6-540004.3.4">
     </a>
     The Right Number of Dimensions
    </h3>
    <p class="noindent">
     Instead of randomly choosing the number of dimensions to reduce down to, it is simpler to choose the number of dimensions
which add up to the required summed variance, for example, 95%.
    </p>
    <div class="warning">
     <p class="noindent">
      An exception to this rule, of course, is if you are reducing dimensionality for data visualization, in which case you will want to
reduce the dimensionality down to 2 or 3.
     </p>
    </div>
    <p class="noindent">
     Let’s load and splits the MNIST dataset and performs
     <a class="glossary" href="DataScienceIILectureBookli2.html#glo:pca">
      PCA
     </a>
     without reducing dimensionality, Then compute the minimum
number of dimensions required to preserve 95% of the training set’s variance:
    </p>
    <p class="noindent">
    </p>
    <pre><div id="fancyvrb55" style="padding:20px;border-radius: 3px;"><a id="x6-54002r407"></a><span style="color:#2B2BFF;">from</span><span style="color:#BABABA;"> </span>sklearn.datasets<span style="color:#BABABA;"> </span><span style="color:#2B2BFF;">import</span> fetch_openml 
<a id="x6-54004r408"></a> 
<a id="x6-54006r409"></a>mnist = fetch_openml(<span style="color:#800080;">'mnist_784'</span>, as_frame=<span style="color:#2B2BFF;"><span id="bold" style="font-weight:bold;">False</span></span>, parser=<span style="color:#800080;">"auto"</span>) 
<a id="x6-54008r410"></a>X_train, y_train = mnist.data[:<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">60_000</span></span>], mnist.target[:<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">60_000</span></span>] 
<a id="x6-54010r411"></a>X_test, y_test = mnist.data[<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">60_000</span></span>:], mnist.target[<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">60_000</span></span>:] 
<a id="x6-54012r412"></a> 
<a id="x6-54014r413"></a>pca = PCA() 
<a id="x6-54016r414"></a>pca.fit(X_train) 
<a id="x6-54018r415"></a>cumsum = np.cumsum(pca.explained_variance_ratio_) 
<a id="x6-54020r416"></a>d = np.argmax(cumsum &gt;= <span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">0.95</span></span>) + <span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">1</span></span>  <span style="color:#008700;"><italic># d equals 154</italic></span></div></pre>
    <p class="noindent">
     We could then set <span style="color:#054C5C;"><code class="verb">n_components=d</code></span>
     and run
     <a class="glossary" href="DataScienceIILectureBookli2.html#glo:pca">
      PCA
     </a>
     again, but there’s a better option. Instead of specifying the number of principal
components you want to preserve, you can set <span style="color:#054C5C;"><code class="verb">n_components</code></span>
     to be a float between 0.0 and 1.0, indicating the ratio of variance you
wish to preserve:
    </p>
    <p class="noindent">
    </p>
    <pre><div id="fancyvrb56" style="padding:20px;border-radius: 3px;"><a id="x6-54022r424"></a>pca = PCA(n_components=<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">0.95</span></span>) 
<a id="x6-54024r425"></a>X_reduced = pca.fit_transform(X_train)</div></pre>
    <p class="noindent">
     The actual number of components is determined during training, and it is stored in the <span style="color:#054C5C;"><code class="verb">n_components_</code></span>
     attribute:
    </p>
    <p class="noindent">
    </p>
    <pre><div id="fancyvrb57" style="padding:20px;border-radius: 3px;"><a id="x6-54026r432"></a><span style="color:#2B2BFF;">print</span>(pca.n_components_)</div></pre>
    <div class="tcolorbox tcolorbox" id="tcolobox-31">
     <div class="tcolorbox-content">
      <pre><div id="fancyvrb58" style="padding:20px;border-radius: 3px;"><a id="x6-54028r1"></a>154</div></pre>
     </div>
    </div>
    <p class="noindent">
     A different option is to plot the explained variance as a function of the number of dimensions which you can see in
     <span id="bold" style="font-weight:bold;">
      Fig.
     </span>
     <a href="#x6-54029r6">
      4.6
     </a>. There will usually be an elbow in the curve, where the explained variance stops growing fast. In this case,
we can see that reducing the dimensionality down to about 100 dimensions wouldn’t lose too much explained
variance.
    </p>
    <div class="figure">
     <p class="noindent">
      <img alt="PIC" height="" src="codes/images/Dimensionality-Reduction/explained-variance-plot-.svg" width="150%"/>
      <a id="x6-54029r6">
      </a>
     </p>
     <figcaption class="caption">
      <span class="id">
       Figure 4.6:
      </span>
      <span class="content">
       Explained variance as a function of the number of dimensions.
      </span>
     </figcaption>
    </div>
    <p class="noindent">
     Finally, if we are using dimensionality reduction as a
     <alert style="color: #821131;">
      pre-processing
     </alert>
     step for a supervised learning task, then we can tune the
number of dimensions as you would any other hyperparameter.
    </p>
    <p class="noindent">
     For example, the following code example creates a two-step pipeline, first reducing dimensionality using
     <a class="glossary" href="DataScienceIILectureBookli2.html#glo:pca">
      PCA
     </a>, then classifying
using a random forest. Next, it uses <span style="color:#054C5C;"><code class="verb">RandomizedSearchCV</code></span>
     to find a good combination of hyperparameters for both
     <a class="glossary" href="DataScienceIILectureBookli2.html#glo:pca">
      PCA
     </a>
     and the
random forest classifier.
    </p>
    <p class="noindent">
     This example does a quick search, tuning only two <alert style="color: #821131;">(2)</alert> hyperparameters, training on just 1,000 instances, and running for just 10
iterations:
    </p>
    <pre><div id="fancyvrb59" style="padding:20px;border-radius: 3px;"><a id="x6-54031r457"></a><span style="color:#2B2BFF;">from</span><span style="color:#BABABA;"> </span>sklearn.ensemble<span style="color:#BABABA;"> </span><span style="color:#2B2BFF;">import</span> RandomForestClassifier 
<a id="x6-54033r458"></a><span style="color:#2B2BFF;">from</span><span style="color:#BABABA;"> </span>sklearn.model_selection<span style="color:#BABABA;"> </span><span style="color:#2B2BFF;">import</span> RandomizedSearchCV 
<a id="x6-54035r459"></a><span style="color:#2B2BFF;">from</span><span style="color:#BABABA;"> </span>sklearn.pipeline<span style="color:#BABABA;"> </span><span style="color:#2B2BFF;">import</span> make_pipeline 
<a id="x6-54037r460"></a> 
<a id="x6-54039r461"></a>clf = make_pipeline(PCA(random_state=<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">42</span></span>), 
<a id="x6-54041r462"></a>              RandomForestClassifier(random_state=<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">42</span></span>)) 
<a id="x6-54043r463"></a>param_distrib = { 
<a id="x6-54045r464"></a>   <span style="color:#800080;">"pca__n_components"</span>: np.arange(<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">10</span></span>, <span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">80</span></span>), 
<a id="x6-54047r465"></a>   <span style="color:#800080;">"randomforestclassifier__n_estimators"</span>: np.arange(<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">50</span></span>, <span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">500</span></span>) 
<a id="x6-54049r466"></a>} 
<a id="x6-54051r467"></a>rnd_search = RandomizedSearchCV(clf, param_distrib, n_iter=<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">10</span></span>, cv=<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">3</span></span>, 
<a id="x6-54053r468"></a>                       random_state=<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">42</span></span>) 
<a id="x6-54055r469"></a>rnd_search.fit(X_train[:<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">1000</span></span>], y_train[:<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">1000</span></span>]) 
<a id="x6-54057r470"></a> 
<a id="x6-54059r471"></a></div></pre>
    <p class="noindent">
     Let’s look at the best hyperparameters found:
    </p>
    <pre><div id="fancyvrb60" style="padding:20px;border-radius: 3px;"><a id="x6-54061r478"></a><span style="color:#2B2BFF;">print</span>(rnd_search.best_params_)</div></pre>
    <div class="tcolorbox tcolorbox" id="tcolobox-32">
     <div class="tcolorbox-content">
      <pre><div id="fancyvrb61" style="padding:20px;border-radius: 3px;"><a id="x6-54063r1"></a>{'randomforestclassifier__n_estimators': 475, 'pca__n_components': 57}</div></pre>
     </div>
    </div>
    <p class="noindent">
     It’s interesting to see how low the optimal number of components is:
    </p>
    <div class="quoteblock">
     <p class="noindent">
      we
reduced
a
784-dimensional
dataset
to
just
57
dimensions.
     </p>
    </div>
    <p class="noindent">
     This is tied to the fact that we used a random forest, which is a pretty powerful model. If we used a linear model instead, such as
an <span style="color:#054C5C;"><code class="verb">SGDClassifier</code></span>, the search would find that we need to preserve more dimensions.
    </p>
    <h3 class="subsectionHead">
     <a aria-label="Link to this section in PDF version" class="pdf-link" href="DataScienceIILectureBook.pdf#subsection.4.3.5" style="color: red;  position: absolute;  left: -30px;" target="_blank">
      <svg class="bi bi-file-pdf" fill="currentColor" height="20" viewbox="0 0 16 16" width="20" xmlns="http://www.w3.org/2000/svg">
       <path d="M4 0a2 2 0 0 0-2 2v12a2 2 0 0 0 2 2h8a2 2 0 0 0 2-2V2a2 2 0 0 0-2-2H4zm0 1h8a1 1 0 0 1 1 1v12a1 1 0 0 1-1 1H4a1 1 0 0 1-1-1V2a1 1 0 0 1 1-1z">
       </path>
       <path d="M4.603 12.087a.81.81 0 0 1-.438-.42c-.195-.388-.13-.776.08-1.102.198-.307.526-.568.897-.787a7.68 7.68 0 0 1 1.482-.645 19.701 19.701 0 0 0 1.062-2.227 7.269 7.269 0 0 1-.43-1.295c-.086-.4-.119-.796-.046-1.136.075-.354.274-.672.65-.823.192-.077.4-.12.602-.077a.7.7 0 0 1 .477.365c.088.164.12.356.127.538.007.187-.012.395-.047.614-.084.51-.27 1.134-.52 1.794a10.954 10.954 0 0 0 .98 1.686 5.753 5.753 0 0 1 1.334.05c.364.065.734.195.96.465.12.144.193.32.2.518.007.192-.047.382-.138.563a1.04 1.04 0 0 1-.354.416.856.856 0 0 1-.51.138c-.331-.014-.654-.196-.933-.417a5.716 5.716 0 0 1-.911-.95 11.642 11.642 0 0 0-1.997.406 11.311 11.311 0 0 1-1.021 1.51c-.29.35-.608.655-.926.787a.793.793 0 0 1-.58.029zm1.379-1.901c-.166.076-.32.156-.459.238-.328.194-.541.383-.647.547-.094.145-.096.25-.04.361.01.022.02.036.026.044a.27.27 0 0 0 .035-.012c.137-.056.355-.235.635-.572a8.18 8.18 0 0 0 .45-.606zm1.64-1.33a12.647 12.647 0 0 1 1.01-.193 11.666 11.666 0 0 1-.51-.858 20.741 20.741 0 0 1-.5 1.05zm2.446.45c.15.162.296.3.435.41.24.19.407.253.498.256a.107.107 0 0 0 .07-.015.307.307 0 0 0 .094-.125.436.436 0 0 0 .059-.2.095.095 0 0 0-.026-.063c-.052-.062-.2-.152-.518-.209a3.881 3.881 0 0 0-.612-.053zM8.078 5.8a6.7 6.7 0 0 0 .2-.828c.031-.188.043-.343.038-.465a.613.613 0 0 0-.032-.198.517.517 0 0 0-.145.04c-.087.035-.158.106-.196.283-.04.192-.03.469.046.822.024.111.054.227.09.346z">
       </path>
      </svg>
     </a>
     <small class="titlemark">
      4.3.5
     </small>
     <a id="x6-550004.3.5">
     </a>
     PCA for Compression
    </h3>
    <p class="noindent">
     After dimensionality reduction, the training set takes up much less space. For example, after applying
     <a class="glossary" href="DataScienceIILectureBookli2.html#glo:pca">
      PCA
     </a>
     to the
MNIST dataset while preserving 95% of its variance, we are left with 154 features, instead of the original 784
features. So the dataset is now less than 20% of its original size, and we only lost 5% of its variance. This is a
reasonable compression ratio, and it’s easy to see how such a size reduction would speed up a classification algorithm
tremendously.
    </p>
    <p class="noindent">
     It is also possible to decompress the reduced dataset back to 784 dimensions by applying the inverse transformation of the
     <a class="glossary" href="DataScienceIILectureBookli2.html#glo:pca">
      PCA
     </a>
     projection. This won’t give you back the original data, since the projection lost a bit of information (within the 5%
variance that was dropped), but it will likely be close to the original data. The mean squared distance between
the original data and the reconstructed data (compressed and then decompressed) is called the
     <span id="bold" style="font-weight:bold;">
      reconstruction
error
     </span> <a id="x6-55001"></a><a href="#X0-nayak2021comprehensive">[9]</a>.
    </p>
    <div class="figure">
     <p class="noindent">
      <img alt="PIC" height="" src="codes/images/Dimensionality-Reduction/mnist-compression-plot-.svg" width="150%"/>
      <a id="x6-55002r7">
      </a>
     </p>
     <figcaption class="caption">
      <span class="id">
       Figure 4.7:
      </span>
      <span class="content">
       MNIST compression that preserves 95% of the variance
      </span>
     </figcaption>
    </div>
    <p class="noindent">
     The <span style="color:#054C5C;"><code class="verb">inverse_transform()</code></span>
     method lets us decompress the reduced MNIST dataset back to 784 dimensions:
    </p>
    <pre><div id="fancyvrb62" style="padding:20px;border-radius: 3px;"><a id="x6-55004r496"></a>X_recovered = pca.inverse_transform(X_reduced)</div></pre>
    <p class="noindent">
     <span id="bold" style="font-weight:bold;">
      Fig.
     </span>
     <a href="#x6-55002r7">
      4.7
     </a>
     shows a few digits from the original training set, seen on the left, and the corresponding digits after compression and
decompression. You can see that there is a slight image quality loss, but the digits are still mostly intact. The equation for the
inverse transformation is shown below.
    </p>
    <table class="equation-star">
     <tr>
      <td>
       <math class="equation" display="block" xmlns="http://www.w3.org/1998/Math/MathML">
        <msubsup>
         <mrow>
          <mover accent="true">
           <mrow>
            <mi>
             X
            </mi>
           </mrow>
           <mo accent="true">
            →
           </mo>
          </mover>
         </mrow>
         <mrow>
          <mi>
           r
          </mi>
          <mi>
           e
          </mi>
          <mi>
           c
          </mi>
          <mi>
           o
          </mi>
          <mi>
           v
          </mi>
          <mi>
           e
          </mi>
          <mi>
           r
          </mi>
          <mi>
           e
          </mi>
          <mi>
           d
          </mi>
         </mrow>
         <mrow>
         </mrow>
        </msubsup>
        <mo class="MathClass-rel" stretchy="false">
         =
        </mo>
        <msubsup>
         <mrow>
          <mover accent="true">
           <mrow>
            <mi>
             X
            </mi>
           </mrow>
           <mo accent="true">
            →
           </mo>
          </mover>
         </mrow>
         <mrow>
          <mi>
           d
          </mi>
          <mo class="MathClass-bin" stretchy="false">
           −
          </mo>
          <mi>
           p
          </mi>
          <mi>
           r
          </mi>
          <mi>
           o
          </mi>
          <mi>
           j
          </mi>
         </mrow>
         <mrow>
         </mrow>
        </msubsup>
        <msubsup>
         <mrow>
          <mover accent="true">
           <mrow>
            <mi>
             W
            </mi>
           </mrow>
           <mo accent="true">
            →
           </mo>
          </mover>
         </mrow>
         <mrow>
          <mi>
           d
          </mi>
         </mrow>
         <mrow>
          <mi>
           T
          </mi>
         </mrow>
        </msubsup>
       </math>
      </td>
     </tr>
    </table>
    <h3 class="subsectionHead">
     <a aria-label="Link to this section in PDF version" class="pdf-link" href="DataScienceIILectureBook.pdf#subsection.4.3.6" style="color: red;  position: absolute;  left: -30px;" target="_blank">
      <svg class="bi bi-file-pdf" fill="currentColor" height="20" viewbox="0 0 16 16" width="20" xmlns="http://www.w3.org/2000/svg">
       <path d="M4 0a2 2 0 0 0-2 2v12a2 2 0 0 0 2 2h8a2 2 0 0 0 2-2V2a2 2 0 0 0-2-2H4zm0 1h8a1 1 0 0 1 1 1v12a1 1 0 0 1-1 1H4a1 1 0 0 1-1-1V2a1 1 0 0 1 1-1z">
       </path>
       <path d="M4.603 12.087a.81.81 0 0 1-.438-.42c-.195-.388-.13-.776.08-1.102.198-.307.526-.568.897-.787a7.68 7.68 0 0 1 1.482-.645 19.701 19.701 0 0 0 1.062-2.227 7.269 7.269 0 0 1-.43-1.295c-.086-.4-.119-.796-.046-1.136.075-.354.274-.672.65-.823.192-.077.4-.12.602-.077a.7.7 0 0 1 .477.365c.088.164.12.356.127.538.007.187-.012.395-.047.614-.084.51-.27 1.134-.52 1.794a10.954 10.954 0 0 0 .98 1.686 5.753 5.753 0 0 1 1.334.05c.364.065.734.195.96.465.12.144.193.32.2.518.007.192-.047.382-.138.563a1.04 1.04 0 0 1-.354.416.856.856 0 0 1-.51.138c-.331-.014-.654-.196-.933-.417a5.716 5.716 0 0 1-.911-.95 11.642 11.642 0 0 0-1.997.406 11.311 11.311 0 0 1-1.021 1.51c-.29.35-.608.655-.926.787a.793.793 0 0 1-.58.029zm1.379-1.901c-.166.076-.32.156-.459.238-.328.194-.541.383-.647.547-.094.145-.096.25-.04.361.01.022.02.036.026.044a.27.27 0 0 0 .035-.012c.137-.056.355-.235.635-.572a8.18 8.18 0 0 0 .45-.606zm1.64-1.33a12.647 12.647 0 0 1 1.01-.193 11.666 11.666 0 0 1-.51-.858 20.741 20.741 0 0 1-.5 1.05zm2.446.45c.15.162.296.3.435.41.24.19.407.253.498.256a.107.107 0 0 0 .07-.015.307.307 0 0 0 .094-.125.436.436 0 0 0 .059-.2.095.095 0 0 0-.026-.063c-.052-.062-.2-.152-.518-.209a3.881 3.881 0 0 0-.612-.053zM8.078 5.8a6.7 6.7 0 0 0 .2-.828c.031-.188.043-.343.038-.465a.613.613 0 0 0-.032-.198.517.517 0 0 0-.145.04c-.087.035-.158.106-.196.283-.04.192-.03.469.046.822.024.111.054.227.09.346z">
       </path>
      </svg>
     </a>
     <small class="titlemark">
      4.3.6
     </small>
     <a id="x6-560004.3.6">
     </a>
     Randomized PCA
    </h3>
    <p class="noindent">
     If you set the <span style="color:#054C5C;"><code class="verb">svd_solver</code></span>
     hyperparameter to <span style="color:#054C5C;"><code class="verb">randomized</code></span>, <span style="color:#054C5C;"><code class="verb">sklearn</code></span>
     uses a stochastic algorithm called randomised
     <a class="glossary" href="DataScienceIILectureBookli2.html#glo:pca">
      PCA
     </a>
     that quickly finds an
approximation of the first
     <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
      <mi>
       d
      </mi>
     </math>
     principal components. Its computational complexity is:
    </p>
    <table class="equation">
     <tr>
      <td>
       <math class="equation" display="block" xmlns="http://www.w3.org/1998/Math/MathML">
        <mstyle class="label" id="x6-56001r1">
        </mstyle>
        <mstyle mathvariant="script">
         <mi>
          O
         </mi>
        </mstyle>
        <msup>
         <mrow>
          <mrow>
           <mo fence="true" form="prefix">
            (
           </mo>
           <mrow>
            <mi>
             m
            </mi>
            <mo class="MathClass-bin" stretchy="false">
             ×
            </mo>
            <msubsup>
             <mrow>
              <mi>
               d
              </mi>
             </mrow>
             <mrow>
             </mrow>
             <mrow>
              <mn>
               2
              </mn>
             </mrow>
            </msubsup>
           </mrow>
           <mo fence="true" form="postfix">
            )
           </mo>
          </mrow>
         </mrow>
         <mrow>
         </mrow>
        </msup>
        <mo class="MathClass-bin" stretchy="false">
         +
        </mo>
        <mstyle mathvariant="script">
         <mi>
          O
         </mi>
        </mstyle>
        <msup>
         <mrow>
          <mrow>
           <mo fence="true" form="prefix">
            (
           </mo>
           <mrow>
            <msubsup>
             <mrow>
              <mi>
               d
              </mi>
             </mrow>
             <mrow>
             </mrow>
             <mrow>
              <mn>
               3
              </mn>
             </mrow>
            </msubsup>
           </mrow>
           <mo fence="true" form="postfix">
            )
           </mo>
          </mrow>
         </mrow>
         <mrow>
         </mrow>
        </msup>
        <mspace class="qquad" width="2em">
        </mspace>
        <mstyle class="text">
         <mtext>
          Instead of
         </mtext>
        </mstyle>
        <mspace class="qquad" width="2em">
        </mspace>
        <mspace class="qquad" width="2em">
        </mspace>
        <mstyle mathvariant="script">
         <mi>
          O
         </mi>
        </mstyle>
        <msup>
         <mrow>
          <mrow>
           <mo fence="true" form="prefix">
            (
           </mo>
           <mrow>
            <mi>
             m
            </mi>
            <mo class="MathClass-bin" stretchy="false">
             ×
            </mo>
            <msubsup>
             <mrow>
              <mi>
               n
              </mi>
             </mrow>
             <mrow>
             </mrow>
             <mrow>
              <mn>
               2
              </mn>
             </mrow>
            </msubsup>
           </mrow>
           <mo fence="true" form="postfix">
            )
           </mo>
          </mrow>
         </mrow>
         <mrow>
         </mrow>
        </msup>
        <mo class="MathClass-bin" stretchy="false">
         +
        </mo>
        <mstyle mathvariant="script">
         <mi>
          O
         </mi>
        </mstyle>
        <msup>
         <mrow>
          <mrow>
           <mo fence="true" form="prefix">
            (
           </mo>
           <mrow>
            <msubsup>
             <mrow>
              <mi>
               n
              </mi>
             </mrow>
             <mrow>
             </mrow>
             <mrow>
              <mn>
               3
              </mn>
             </mrow>
            </msubsup>
           </mrow>
           <mo fence="true" form="postfix">
            )
           </mo>
          </mrow>
         </mrow>
         <mrow>
         </mrow>
        </msup>
       </math>
      </td>
      <td class="eq-no">
       (4.1)
      </td>
     </tr>
    </table>
    <p class="noindent">
    </p>
    <p class="noindent">
     for full
     <a class="glossary" href="DataScienceIILectureBookli2.html#glo:svd">
      SVD
     </a>
     approach, therefore it is faster than full
     <a class="glossary" href="DataScienceIILectureBookli2.html#glo:svd">
      SVD
     </a>
     when
     <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
      <mi>
       d
      </mi>
     </math>
     is much
smaller than
     <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
      <mi>
       n
      </mi>
     </math>
     :
    </p>
    <p class="noindent">
    </p>
    <pre><div id="fancyvrb63" style="padding:20px;border-radius: 3px;"><a id="x6-56003r521"></a>rnd_pca = PCA(n_components=<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">154</span></span>, svd_solver=<span style="color:#800080;">"randomized"</span>, random_state=<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">42</span></span>) 
<a id="x6-56005r522"></a>X_reduced = rnd_pca.fit_transform(X_train)</div></pre>
    <div class="knowledge">
     <p class="noindent">
      By default, <span style="color:#054C5C;"><code class="verb">svd_solver</code></span>
      is set to <span style="color:#054C5C;"><code class="verb">"auto"</code></span>, where <span style="color:#054C5C;"><code class="verb">sklearn</code></span>
      automatically uses the randomised
      <a class="glossary" href="DataScienceIILectureBookli2.html#glo:pca">
       PCA
      </a>
      algorithm if
      <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
       <mi>
        m
       </mi>
       <mi>
        a
       </mi>
       <mi>
        x
       </mi>
       <mo class="MathClass-open" stretchy="false">
        (
       </mo>
       <mi>
        m
       </mi>
       <mo class="MathClass-punc" stretchy="false">
        ,
       </mo>
       <mi>
        n
       </mi>
       <mo class="MathClass-close" stretchy="false">
        )
       </mo>
       <mo class="MathClass-rel" stretchy="false">
        &gt;
       </mo>
       <mn>
        5
       </mn>
       <mn>
        0
       </mn>
       <mn>
        0
       </mn>
      </math>
      and <span style="color:#054C5C;"><code class="verb">n_components</code></span>
      is an integer
smaller than 80% of
      <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
       <mi>
        m
       </mi>
       <mi>
        i
       </mi>
       <mi>
        n
       </mi>
       <mo class="MathClass-open" stretchy="false">
        (
       </mo>
       <mi>
        m
       </mi>
       <mo class="MathClass-punc" stretchy="false">
        ,
       </mo>
       <mi>
        n
       </mi>
       <mo class="MathClass-close" stretchy="false">
        )
       </mo>
      </math>
      , or else
it uses the full
      <a class="glossary" href="DataScienceIILectureBookli2.html#glo:svd">
       SVD
      </a>
      approach. So the preceding code would use the randomized
      <a class="glossary" href="DataScienceIILectureBookli2.html#glo:pca">
       PCA
      </a>
      algorithm even if you removed the <span style="color:#054C5C;"><code class="verb">svd_solver="randomized"</code></span>
      argument, as
      <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
       <mn>
        1
       </mn>
       <mn>
        5
       </mn>
       <mn>
        4
       </mn>
       <mo class="MathClass-rel" stretchy="false">
        &lt;
       </mo>
       <mn>
        0
       </mn>
       <mo class="MathClass-punc" stretchy="false">
        .
       </mo>
       <mn>
        8
       </mn>
       <mo class="MathClass-bin" stretchy="false">
        ×
       </mo>
       <mn>
        7
       </mn>
       <mn>
        8
       </mn>
       <mn>
        4
       </mn>
      </math>
      .
      If we want to force <span style="color:#054C5C;"><code class="verb">sklearn</code></span>
      to use full
      <a class="glossary" href="DataScienceIILectureBookli2.html#glo:svd">
       SVD
      </a>
      for a slightly more precise result, you can set the <span style="color:#054C5C;"><code class="verb">svd_solver</code></span>
      hyperparameter to <span style="color:#054C5C;"><code class="verb">"full"</code></span>.
     </p>
    </div>
    <h3 class="subsectionHead">
     <a aria-label="Link to this section in PDF version" class="pdf-link" href="DataScienceIILectureBook.pdf#subsection.4.3.7" style="color: red;  position: absolute;  left: -30px;" target="_blank">
      <svg class="bi bi-file-pdf" fill="currentColor" height="20" viewbox="0 0 16 16" width="20" xmlns="http://www.w3.org/2000/svg">
       <path d="M4 0a2 2 0 0 0-2 2v12a2 2 0 0 0 2 2h8a2 2 0 0 0 2-2V2a2 2 0 0 0-2-2H4zm0 1h8a1 1 0 0 1 1 1v12a1 1 0 0 1-1 1H4a1 1 0 0 1-1-1V2a1 1 0 0 1 1-1z">
       </path>
       <path d="M4.603 12.087a.81.81 0 0 1-.438-.42c-.195-.388-.13-.776.08-1.102.198-.307.526-.568.897-.787a7.68 7.68 0 0 1 1.482-.645 19.701 19.701 0 0 0 1.062-2.227 7.269 7.269 0 0 1-.43-1.295c-.086-.4-.119-.796-.046-1.136.075-.354.274-.672.65-.823.192-.077.4-.12.602-.077a.7.7 0 0 1 .477.365c.088.164.12.356.127.538.007.187-.012.395-.047.614-.084.51-.27 1.134-.52 1.794a10.954 10.954 0 0 0 .98 1.686 5.753 5.753 0 0 1 1.334.05c.364.065.734.195.96.465.12.144.193.32.2.518.007.192-.047.382-.138.563a1.04 1.04 0 0 1-.354.416.856.856 0 0 1-.51.138c-.331-.014-.654-.196-.933-.417a5.716 5.716 0 0 1-.911-.95 11.642 11.642 0 0 0-1.997.406 11.311 11.311 0 0 1-1.021 1.51c-.29.35-.608.655-.926.787a.793.793 0 0 1-.58.029zm1.379-1.901c-.166.076-.32.156-.459.238-.328.194-.541.383-.647.547-.094.145-.096.25-.04.361.01.022.02.036.026.044a.27.27 0 0 0 .035-.012c.137-.056.355-.235.635-.572a8.18 8.18 0 0 0 .45-.606zm1.64-1.33a12.647 12.647 0 0 1 1.01-.193 11.666 11.666 0 0 1-.51-.858 20.741 20.741 0 0 1-.5 1.05zm2.446.45c.15.162.296.3.435.41.24.19.407.253.498.256a.107.107 0 0 0 .07-.015.307.307 0 0 0 .094-.125.436.436 0 0 0 .059-.2.095.095 0 0 0-.026-.063c-.052-.062-.2-.152-.518-.209a3.881 3.881 0 0 0-.612-.053zM8.078 5.8a6.7 6.7 0 0 0 .2-.828c.031-.188.043-.343.038-.465a.613.613 0 0 0-.032-.198.517.517 0 0 0-.145.04c-.087.035-.158.106-.196.283-.04.192-.03.469.046.822.024.111.054.227.09.346z">
       </path>
      </svg>
     </a>
     <small class="titlemark">
      4.3.7
     </small>
     <a id="x6-570004.3.7">
     </a>
     Incremental PCA
    </h3>
    <p class="noindent">
     A
problem
with
the
preceding
implementations
of
     <a class="glossary" href="DataScienceIILectureBookli2.html#glo:pca">
      PCA
     </a>
     is
that
they
require
the
whole
training
set
to
fit
in
memory
in
order
for
the
algorithm
to
run.
Fortunately,
incremental
     <a class="glossary" href="DataScienceIILectureBookli2.html#glo:pca">
      PCA
     </a>
     algorithms
have
been
developed
that
allow
you
to
split
the
training
set
into
mini-batches
and
feed
these
in
one
mini-batch
at
a
time.
This
is
useful
for
large
training
sets
and
for
applying
     <a class="glossary" href="DataScienceIILectureBookli2.html#glo:pca">
      PCA
     </a>
     online.
     <alert style="color: #821131;">
      <span id="bold" style="font-weight:bold;">
       <sup class="textsuperscript">
        11
       </sup>
      </span>
     </alert>
     <span class="marginnote">
      <alert style="color: #821131;">
       <span id="bold" style="font-weight:bold;">
        <sup class="textsuperscript">
         11
        </sup>
       </span>
      </alert>
      <span style="color:#0063B2;">
       i.e.,
on
the
fly,
as
new
instances
arrive.
      </span>
     </span>
    </p>
    <p class="noindent">
     The
following
splits
the
MNIST
training
set
into
100
mini-batches
     <alert style="color: #821131;">
      <span id="bold" style="font-weight:bold;">
       <sup class="textsuperscript">
        12
       </sup>
      </span>
     </alert>
     <span class="marginnote">
      <alert style="color: #821131;">
       <span id="bold" style="font-weight:bold;">
        <sup class="textsuperscript">
         12
        </sup>
       </span>
      </alert>
      <span style="color:#0063B2;">
       using
NumPy’s <span style="color:#054C5C;"><code class="verb">array_split()</code></span>
       function
      </span>
     </span>
     and
feeds
them
to <span style="color:#054C5C;"><code class="verb">sklearn</code></span>
     ’s
Incremental
     <a class="glossary" href="DataScienceIILectureBookli2.html#glo:pca">
      PCA
     </a>
     class
to
reduce
the
dimensionality
of
the
MNIST
dataset
down
to
154
dimensions,
just
like
before.
    </p>
    <div class="warning">
     <p class="noindent">
      We must call the <span style="color:#054C5C;"><code class="verb">partial_fit()</code></span>
      method with each mini-batch, rather than the <span style="color:#054C5C;"><code class="verb">fit()</code></span>
      method with the whole training set.
     </p>
    </div>
    <p class="noindent">
    </p>
    <pre><div id="fancyvrb64" style="padding:20px;border-radius: 3px;"><a id="x6-57002r529"></a><span style="color:#2B2BFF;">from</span><span style="color:#BABABA;"> </span>sklearn.decomposition<span style="color:#BABABA;"> </span><span style="color:#2B2BFF;">import</span> IncrementalPCA 
<a id="x6-57004r530"></a> 
<a id="x6-57006r531"></a>n_batches = <span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">100</span></span> 
<a id="x6-57008r532"></a>inc_pca = IncrementalPCA(n_components=<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">154</span></span>) 
<a id="x6-57010r533"></a><span style="color:#2B2BFF;">for</span> X_batch in np.array_split(X_train, n_batches): 
<a id="x6-57012r534"></a>   inc_pca.partial_fit(X_batch) 
<a id="x6-57014r535"></a> 
<a id="x6-57016r536"></a>X_reduced = inc_pca.transform(X_train)</div></pre>
    <p class="noindent">
     Alternatively,
we
can
use
NumPy’s <span style="color:#054C5C;"><code class="verb">memmap</code></span>
     class
     <alert style="color: #821131;">
      <span id="bold" style="font-weight:bold;">
       <sup class="textsuperscript">
        13
       </sup>
      </span>
     </alert>
     <span class="marginnote">
      <alert style="color: #821131;">
       <span id="bold" style="font-weight:bold;">
        <sup class="textsuperscript">
         13
        </sup>
       </span>
      </alert>
      <span style="color:#0063B2;">
       Memory-mapped
files
are
                                                                                
                                                                                
used
for
accessing
small
segments
of
large
files
on
disk,
without
reading
the
entire
file
into
memory
      </span>
     </span>
     ,
which
allows
you
to
manipulate
a
large
array
stored
in
a
binary
file
on
disk
as
if
it
were
entirely
in
memory.
    </p>
    <p class="noindent">
     To
demonstrate
this,
let’s
first
create
a
memory-mapped
(
     <span id="bold" style="font-weight:bold;">
      memmap
     </span>
     )
file
and
copy
the
MNIST
training
set
to
it,
then
call <span style="color:#054C5C;"><code class="verb">flush()</code></span>
     to
ensure
that
any
data
still
in
                                                                                
                                                                                
the
cache
gets
saved
to
disk.
In
real
life, <span style="color:#054C5C;"><code class="verb">X_train</code></span>
     would
typically
not
fit
in
memory,
so
you
would
load
it
chunk
by
chunk
and
save
each
chunk
to
the
right
part
of
the
memmap
array:
    </p>
    <p class="noindent">
    </p>
    <pre><div id="fancyvrb65" style="padding:20px;border-radius: 3px;"><a id="x6-57018r541"></a>filename = <span style="color:#800080;">"my_mnist.mmap"</span> 
<a id="x6-57020r542"></a>X_mmap = np.memmap(filename, dtype=<span style="color:#800080;">'float32'</span>, mode=<span style="color:#800080;">'write'</span>, shape=X_train.shape) 
<a id="x6-57022r543"></a>X_mmap[:] = X_train  <span style="color:#008700;"><italic># could be a loop instead, saving the data chunk by chunk</italic></span> 
<a id="x6-57024r544"></a>X_mmap.flush()</div></pre>
    <p class="noindent">
     Next,
we
can
load
the
memmap
file
and
use
it
like
a
regular
NumPy
array.
Let’s
use
the <span style="color:#054C5C;"><code class="verb">IncrementalPCA</code></span>
     class
to
reduce
its
dimensionality.
Since
this
algorithm
                                                                                
                                                                                
uses
only
a
small
part
of
the
array
at
any
given
time,
memory
usage
remains
under
control.
This
makes
it
possible
to
call
the
usual <span style="color:#054C5C;"><code class="verb">fit()</code></span>
     method
instead
of <span style="color:#054C5C;"><code class="verb">partial_fit()</code></span>,
which
is
quite
convenient:
    </p>
    <p class="noindent">
    </p>
    <pre><div id="fancyvrb66" style="padding:20px;border-radius: 3px;"><a id="x6-57026r549"></a>X_mmap = np.memmap(filename, dtype=<span style="color:#800080;">"float32"</span>, mode=<span style="color:#800080;">"readonly"</span>).reshape(-<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">1</span></span>, <span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">784</span></span>) 
<a id="x6-57028r550"></a>batch_size = X_mmap.shape[<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">0</span></span>] // n_batches 
<a id="x6-57030r551"></a>inc_pca = IncrementalPCA(n_components=<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">154</span></span>, batch_size=batch_size) 
<a id="x6-57032r552"></a>inc_pca.fit(X_mmap)</div></pre>
    <div class="warning">
     <p class="noindent">
      Only the raw binary data is saved to disk, so specify the data type and shape of the array when you load it.
      If you omit the shape, <span style="color:#054C5C;"><code class="verb">np.memmap()</code></span>
      returns a 1D array.
     </p>
    </div>
    <p class="noindent">
     For very high-dimensional datasets,
     <a class="glossary" href="DataScienceIILectureBookli2.html#glo:pca">
      PCA
     </a>
     can be too slow. As we saw previously, even if you use randomized
     <a class="glossary" href="DataScienceIILectureBookli2.html#glo:pca">
      PCA
     </a>
     its
computational complexity is still
    </p>
    <table class="equation-star">
     <tr>
      <td>
       <math class="equation" display="block" xmlns="http://www.w3.org/1998/Math/MathML">
        <mstyle mathvariant="script">
         <mi>
          O
         </mi>
        </mstyle>
        <msup>
         <mrow>
          <mrow>
           <mo fence="true" form="prefix">
            (
           </mo>
           <mrow>
            <mi>
             m
            </mi>
            <mo class="MathClass-bin" stretchy="false">
             ×
            </mo>
            <msubsup>
             <mrow>
              <mi>
               d
              </mi>
             </mrow>
             <mrow>
             </mrow>
             <mrow>
              <mn>
               2
              </mn>
             </mrow>
            </msubsup>
           </mrow>
           <mo fence="true" form="postfix">
            )
           </mo>
          </mrow>
         </mrow>
         <mrow>
         </mrow>
        </msup>
        <mo class="MathClass-bin" stretchy="false">
         +
        </mo>
        <mstyle mathvariant="script">
         <mi>
          O
         </mi>
        </mstyle>
        <msup>
         <mrow>
          <mrow>
           <mo fence="true" form="prefix">
            (
           </mo>
           <mrow>
            <msubsup>
             <mrow>
              <mi>
               d
              </mi>
             </mrow>
             <mrow>
             </mrow>
             <mrow>
              <mn>
               3
              </mn>
             </mrow>
            </msubsup>
           </mrow>
           <mo fence="true" form="postfix">
            )
           </mo>
          </mrow>
         </mrow>
         <mrow>
         </mrow>
        </msup>
        <mo class="MathClass-punc" stretchy="false">
         ,
        </mo>
       </math>
      </td>
     </tr>
    </table>
    <p class="noindent">
     so
the
target
number
of
dimensions
     <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
      <mi>
       d
      </mi>
     </math>
     cannot
                                                                                
                                                                                
be
too
large.
If
we
are
dealing
with
a
dataset
with
tens
of
thousands
of
features
or
more,
     <alert style="color: #821131;">
      <span id="bold" style="font-weight:bold;">
       <sup class="textsuperscript">
        14
       </sup>
      </span>
     </alert>
     <span class="marginnote">
      <alert style="color: #821131;">
       <span id="bold" style="font-weight:bold;">
        <sup class="textsuperscript">
         14
        </sup>
       </span>
      </alert>
      <span style="color:#0063B2;">
       For
example
images.
      </span>
     </span>
     then
training
may
become
much
too
slow:
in
this
case,
you
should
consider
using
random
projection
instead.
    </p>
    <p class="noindent">
    </p>
    <h2 class="sectionHead">
     <a aria-label="Link to this section in PDF version" class="pdf-link" href="DataScienceIILectureBook.pdf#section.4.4" style="color: red;  position: absolute;  left: -30px;" target="_blank">
      <svg class="bi bi-file-pdf" fill="currentColor" height="20" viewbox="0 0 16 16" width="20" xmlns="http://www.w3.org/2000/svg">
       <path d="M4 0a2 2 0 0 0-2 2v12a2 2 0 0 0 2 2h8a2 2 0 0 0 2-2V2a2 2 0 0 0-2-2H4zm0 1h8a1 1 0 0 1 1 1v12a1 1 0 0 1-1 1H4a1 1 0 0 1-1-1V2a1 1 0 0 1 1-1z">
       </path>
       <path d="M4.603 12.087a.81.81 0 0 1-.438-.42c-.195-.388-.13-.776.08-1.102.198-.307.526-.568.897-.787a7.68 7.68 0 0 1 1.482-.645 19.701 19.701 0 0 0 1.062-2.227 7.269 7.269 0 0 1-.43-1.295c-.086-.4-.119-.796-.046-1.136.075-.354.274-.672.65-.823.192-.077.4-.12.602-.077a.7.7 0 0 1 .477.365c.088.164.12.356.127.538.007.187-.012.395-.047.614-.084.51-.27 1.134-.52 1.794a10.954 10.954 0 0 0 .98 1.686 5.753 5.753 0 0 1 1.334.05c.364.065.734.195.96.465.12.144.193.32.2.518.007.192-.047.382-.138.563a1.04 1.04 0 0 1-.354.416.856.856 0 0 1-.51.138c-.331-.014-.654-.196-.933-.417a5.716 5.716 0 0 1-.911-.95 11.642 11.642 0 0 0-1.997.406 11.311 11.311 0 0 1-1.021 1.51c-.29.35-.608.655-.926.787a.793.793 0 0 1-.58.029zm1.379-1.901c-.166.076-.32.156-.459.238-.328.194-.541.383-.647.547-.094.145-.096.25-.04.361.01.022.02.036.026.044a.27.27 0 0 0 .035-.012c.137-.056.355-.235.635-.572a8.18 8.18 0 0 0 .45-.606zm1.64-1.33a12.647 12.647 0 0 1 1.01-.193 11.666 11.666 0 0 1-.51-.858 20.741 20.741 0 0 1-.5 1.05zm2.446.45c.15.162.296.3.435.41.24.19.407.253.498.256a.107.107 0 0 0 .07-.015.307.307 0 0 0 .094-.125.436.436 0 0 0 .059-.2.095.095 0 0 0-.026-.063c-.052-.062-.2-.152-.518-.209a3.881 3.881 0 0 0-.612-.053zM8.078 5.8a6.7 6.7 0 0 0 .2-.828c.031-.188.043-.343.038-.465a.613.613 0 0 0-.032-.198.517.517 0 0 0-.145.04c-.087.035-.158.106-.196.283-.04.192-.03.469.046.822.024.111.054.227.09.346z">
       </path>
      </svg>
     </a>
     <small class="titlemark">
      4.4
     </small>
     <a id="x6-580004.4">
     </a>
     Random Projection
    </h2>
    <p class="noindent">
     As its name suggests, the random projection algorithm projects the data to a lower-dimensional space using a random linear
projection. This may sound counter intuitive, but turns out such a random projection is actually very likely to preserve distances
fairly well, as was demonstrated mathematically by William B. Johnson and Joram Lindenstrauss in a famous lemma shown in an
abridged form below <a id="x6-58001"></a><a href="#X0-matouvsek2008variants">[10]</a>.
    </p>
    <div class="theoryblock" id="tcolobox-33">
     <div class="title">
      <a id="x6-58003r6">
      </a>
      <p class="noindent">
       <span id="bold" style="font-weight:bold;">
        Theory
       </span>
       4.6: Johnson - Lindenstrauss Lemma
      </p>
     </div>
     <div class="box-content">
      <p class="noindent">
       For
       <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
        <mn>
         0
        </mn>
        <mo class="MathClass-rel" stretchy="false">
         &lt;
        </mo>
        <mi>
         𝜖
        </mi>
        <mo class="MathClass-rel" stretchy="false">
         &lt;
        </mo>
        <mn>
         1
        </mn>
       </math>
       , let
       <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
        <mi>
         V
        </mi>
        <mo class="MathClass-rel" stretchy="false">
         =
        </mo>
        <msup>
         <mrow>
          <mrow>
           <mo fence="true" form="prefix">
            {
           </mo>
           <mrow>
            <msubsup>
             <mrow>
              <mi>
               x
              </mi>
             </mrow>
             <mrow>
              <mi>
               i
              </mi>
             </mrow>
             <mrow>
             </mrow>
            </msubsup>
            <mo class="MathClass-rel" stretchy="false">
             :
            </mo>
            <mi>
             i
            </mi>
            <mo class="MathClass-rel" stretchy="false">
             =
            </mo>
            <mn>
             1
            </mn>
            <mo class="MathClass-punc" stretchy="false">
             ,
            </mo>
            <mo>
             …
            </mo>
            <mi>
             M
            </mi>
           </mrow>
           <mo fence="true" form="postfix">
            }
           </mo>
          </mrow>
         </mrow>
         <mrow>
         </mrow>
        </msup>
        <mo class="MathClass-rel" stretchy="false">
         ∈
        </mo>
        <mi>
         ℝ
        </mi>
       </math>
       be a set of
points in
       <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
        <msup>
         <mrow>
          <mi>
           ℝ
          </mi>
         </mrow>
         <mrow>
          <mi>
           m
          </mi>
         </mrow>
        </msup>
       </math>
       where
       <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
        <mi>
         m
        </mi>
       </math>
       is
the number of dimensions of the original dataset. If we define a lower dimension such as:
      </p>
      <table class="equation-star">
       <tr>
        <td>
         <math class="equation" display="block" xmlns="http://www.w3.org/1998/Math/MathML">
          <mi>
           n
          </mi>
          <mo class="MathClass-rel" stretchy="false">
           ≥
          </mo>
          <mfrac>
           <mrow>
            <mi>
             c
            </mi>
           </mrow>
           <mrow>
            <msup>
             <mrow>
              <mi>
               𝜖
              </mi>
             </mrow>
             <mrow>
              <mn>
               2
              </mn>
             </mrow>
            </msup>
           </mrow>
          </mfrac>
          <mi class="loglike">
           log
          </mi>
          <mo>
           ⁡
          </mo>
          <mi>
           M
          </mi>
         </math>
        </td>
       </tr>
      </table>
      <p class="noindent">
       Then there exist a linear mapping of
       <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
        <msup>
         <mrow>
          <mi>
           ℝ
          </mi>
         </mrow>
         <mrow>
          <mi>
           m
          </mi>
         </mrow>
        </msup>
        <mo class="MathClass-rel" stretchy="false">
         →
        </mo>
        <msup>
         <mrow>
          <mi>
           ℝ
          </mi>
         </mrow>
         <mrow>
          <mi>
           n
          </mi>
         </mrow>
        </msup>
       </math>
       such for all
       <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
        <mi>
         i
        </mi>
        <mo class="MathClass-rel" stretchy="false">
         ≠
        </mo>
        <mi>
         j
        </mi>
       </math>
       :
      </p>
      <table class="equation-star">
       <tr>
        <td>
         <math class="equation" display="block" xmlns="http://www.w3.org/1998/Math/MathML">
          <mn>
           1
          </mn>
          <mo class="MathClass-bin" stretchy="false">
           −
          </mo>
          <mi>
           𝜖
          </mi>
          <mo class="MathClass-rel" stretchy="false">
           ≤
          </mo>
          <mfrac>
           <mrow>
            <mo class="MathClass-rel" stretchy="false">
             ∥
            </mo>
            <mi>
             A
            </mi>
            <msup>
             <mrow>
              <mrow>
               <mo fence="true" form="prefix">
                (
               </mo>
               <mrow>
                <msubsup>
                 <mrow>
                  <mi>
                   x
                  </mi>
                 </mrow>
                 <mrow>
                  <mi>
                   i
                  </mi>
                 </mrow>
                 <mrow>
                 </mrow>
                </msubsup>
               </mrow>
               <mo fence="true" form="postfix">
                )
               </mo>
              </mrow>
             </mrow>
             <mrow>
             </mrow>
            </msup>
            <mo class="MathClass-bin" stretchy="false">
             −
            </mo>
            <mi>
             A
            </mi>
            <msup>
             <mrow>
              <mrow>
               <mo fence="true" form="prefix">
                (
               </mo>
               <mrow>
                <msubsup>
                 <mrow>
                  <mi>
                   x
                  </mi>
                 </mrow>
                 <mrow>
                  <mi>
                   j
                  </mi>
                 </mrow>
                 <mrow>
                 </mrow>
                </msubsup>
               </mrow>
               <mo fence="true" form="postfix">
                )
               </mo>
              </mrow>
             </mrow>
             <mrow>
             </mrow>
            </msup>
            <mo class="MathClass-rel" stretchy="false">
             ∥
            </mo>
           </mrow>
           <mrow>
            <mo class="MathClass-rel" stretchy="false">
             ∥
            </mo>
            <msubsup>
             <mrow>
              <mi>
               x
              </mi>
             </mrow>
             <mrow>
              <mi>
               i
              </mi>
             </mrow>
             <mrow>
             </mrow>
            </msubsup>
            <mo class="MathClass-bin" stretchy="false">
             −
            </mo>
            <msubsup>
             <mrow>
              <mi>
               x
              </mi>
             </mrow>
             <mrow>
              <mi>
               j
              </mi>
             </mrow>
             <mrow>
             </mrow>
            </msubsup>
            <mo class="MathClass-rel" stretchy="false">
             ∥
            </mo>
           </mrow>
          </mfrac>
          <mo class="MathClass-rel" stretchy="false">
           ≤
          </mo>
          <mn>
           1
          </mn>
          <mo class="MathClass-bin" stretchy="false">
           +
          </mo>
          <mi>
           𝜖
          </mi>
         </math>
        </td>
       </tr>
      </table>
      <p class="noindent">
       The Theorem states that after fixing an error level, one can map a collection of points from one Euclidean space, no matter how
high its dimension
       <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
        <mi>
         m
        </mi>
       </math>
       is to a smaller Euclidean space while only changing the distance between any two points by a factor of
       <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
        <mn>
         1
        </mn>
        <mo class="MathClass-bin" stretchy="false">
         ±
        </mo>
        <mi>
         𝜖
        </mi>
       </math>
       .
The dimension of the image space is only dependent on the error and the number of points.
Given that the dimension is very large, one can achieve significant dimension reduction
       <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
        <mspace class="quad" width="1em">
        </mspace>
        <mo class="MathClass-ord">
         ■
        </mo>
       </math>
      </p>
     </div>
    </div>
    <p class="noindent">
     So, two similar instances will remain similar after the projection, and two very different instances will remain very
different.
    </p>
    <p class="noindent">
     As intuition goes, the more dimensions we drop, the more information is lost, and the more distances get distorted.
    </p>
    <p class="noindent">
    </p>
    <div class="quoteblock">
     <p class="noindent">
      So
how
can
we
choose
the
optimal
number
of
dimensions?
     </p>
    </div>
    <p class="noindent">
     Well, the progenitors of the aforementioned lemma defined a method which determines the minimum number of
dimensions to preserve in order to ensure-with high probability-that distances won’t change by more than a given
tolerance.
    </p>
    <p class="noindent">
     For example, if we have a dataset containing
     <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
      <mi>
       m
      </mi>
      <mo class="MathClass-rel" stretchy="false">
       =
      </mo>
      <mn>
       5
      </mn>
      <mo class="MathClass-punc" stretchy="false">
       ,
      </mo>
      <mn>
       0
      </mn>
      <mn>
       0
      </mn>
      <mn>
       0
      </mn>
     </math>
     instances with
     <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
      <mi>
       n
      </mi>
      <mo class="MathClass-rel" stretchy="false">
       =
      </mo>
      <mn>
       2
      </mn>
      <mn>
       0
      </mn>
      <mo class="MathClass-punc" stretchy="false">
       ,
      </mo>
      <mn>
       0
      </mn>
      <mn>
       0
      </mn>
      <mn>
       0
      </mn>
     </math>
     features each, and don’t want the squared distance between any two instances to change by more than
     <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
      <mi>
       𝜖
      </mi>
     </math>
     = 10%, then we should
project the data down to
     <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
      <mi>
       d
      </mi>
     </math>
     dimensions, with:
    </p>
    <table class="equation-star">
     <tr>
      <td>
       <math class="equation" display="block" xmlns="http://www.w3.org/1998/Math/MathML">
        <mi>
         d
        </mi>
        <mo class="MathClass-rel" stretchy="false">
         ≥
        </mo>
        <mn>
         4
        </mn>
        <mfrac>
         <mrow>
          <mi class="loglike">
           log
          </mi>
          <mo>
           ⁡
          </mo>
          <mi>
           m
          </mi>
         </mrow>
         <mrow>
          <mstyle class="text">
           <mtext>
           </mtext>
           <mstyle class="math">
            <mn>
             1
            </mn>
           </mstyle>
           <mtext>
           </mtext>
           <mstyle class="math">
           </mstyle>
           <mtext>
           </mtext>
           <mstyle class="math">
            <mo class="MathClass-bin" stretchy="false">
             ∕
            </mo>
           </mstyle>
           <mtext>
           </mtext>
           <mstyle class="math">
           </mstyle>
           <mtext>
           </mtext>
           <mstyle class="math">
            <mn>
             2
            </mn>
           </mstyle>
           <mtext>
           </mtext>
          </mstyle>
          <msup>
           <mrow>
            <mi>
             𝜖
            </mi>
           </mrow>
           <mrow>
            <mn>
             2
            </mn>
           </mrow>
          </msup>
          <mo class="MathClass-bin" stretchy="false">
           −
          </mo>
          <mstyle class="text">
           <mtext>
           </mtext>
           <mstyle class="math">
            <mn>
             1
            </mn>
           </mstyle>
           <mtext>
           </mtext>
           <mstyle class="math">
           </mstyle>
           <mtext>
           </mtext>
           <mstyle class="math">
            <mo class="MathClass-bin" stretchy="false">
             ∕
            </mo>
           </mstyle>
           <mtext>
           </mtext>
           <mstyle class="math">
           </mstyle>
           <mtext>
           </mtext>
           <mstyle class="math">
            <mn>
             3
            </mn>
           </mstyle>
           <mtext>
           </mtext>
          </mstyle>
          <msup>
           <mrow>
            <mi>
             𝜖
            </mi>
           </mrow>
           <mrow>
            <mn>
             3
            </mn>
           </mrow>
          </msup>
         </mrow>
        </mfrac>
       </math>
      </td>
     </tr>
    </table>
    <p class="noindent">
     which is 7,300 dimensions, which is a significant dimensionality reduction. Please observe that the equation does not use
     <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
      <mi>
       n
      </mi>
     </math>
     , it only
relies on
     <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
      <mi>
       m
      </mi>
     </math>
     and
     <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
      <mi>
       𝜖
      </mi>
     </math>
     . This
equation is implemented by the <span style="color:#054C5C;"><code class="verb">johnson_lindenstrauss_min_dim()</code></span>
     function:
    </p>
    <p class="noindent">
     Now we can just generate a random matrix
     <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
      <mover accent="true">
       <mrow>
        <mi>
         P
        </mi>
       </mrow>
       <mo accent="true">
        →
       </mo>
      </mover>
     </math>
     of shape
     <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
      <mo class="MathClass-open" stretchy="false">
       (
      </mo>
      <mi>
       d
      </mi>
      <mo class="MathClass-punc" stretchy="false">
       ,
      </mo>
      <mi>
       n
      </mi>
      <mo class="MathClass-close" stretchy="false">
       )
      </mo>
     </math>
     ,
where each item is sampled randomly from a Gaussian distribution with
     <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
      <mo class="MathClass-open" stretchy="false">
       (
      </mo>
      <mi>
       μ
      </mi>
      <mo class="MathClass-rel" stretchy="false">
       =
      </mo>
      <mn>
       0
      </mn>
      <mo class="MathClass-punc" stretchy="false">
       ,
      </mo>
      <mspace class="thinspace" width="0.17em">
      </mspace>
      <msubsup>
       <mrow>
        <mi>
         σ
        </mi>
       </mrow>
       <mrow>
       </mrow>
       <mrow>
        <mn>
         2
        </mn>
       </mrow>
      </msubsup>
      <mo class="MathClass-rel" stretchy="false">
       =
      </mo>
      <mstyle class="text">
       <mtext>
       </mtext>
       <mstyle class="math">
        <mn>
         1
        </mn>
       </mstyle>
       <mtext>
       </mtext>
       <mstyle class="math">
       </mstyle>
       <mtext>
       </mtext>
       <mstyle class="math">
        <mo class="MathClass-bin" stretchy="false">
         ∕
        </mo>
       </mstyle>
       <mtext>
       </mtext>
       <mstyle class="math">
       </mstyle>
       <mtext>
       </mtext>
       <mstyle class="math">
        <mi>
         d
        </mi>
       </mstyle>
       <mtext>
       </mtext>
      </mstyle>
      <mo class="MathClass-close" stretchy="false">
       )
      </mo>
     </math>
     , and use it to project a dataset
from n dimensions down to
     <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
      <mi>
       d
      </mi>
     </math>
     :
    </p>
    <p class="noindent">
     Simple and efficient, and no training is required. The only thing the algorithm needs to create the random matrix is the dataset’s
shape. The data itself is not used at all.
    </p>
    <p class="noindent">
     For
implementation <span style="color:#054C5C;"><code class="verb">sklearn</code></span>
     offers
a <span style="color:#054C5C;"><code class="verb">GaussianRandomProjection</code></span>
     class.
When
we
call
its <span style="color:#054C5C;"><code class="verb">fit()</code></span>
     method,
it
uses <span style="color:#054C5C;"><code class="verb">johnson_lindenstrauss_min_dim()</code></span>
     to
                                                                                
                                                                                
determine
the
output
dimensionality,
then
it
generates
a
random
matrix,
which
it
stores
in
the <span style="color:#054C5C;"><code class="verb">components_</code></span>
     attribute.
Then
when
we
call <span style="color:#054C5C;"><code class="verb">transform()</code></span>,
it
uses
this
matrix
to
perform
the
projection.
When
creating
the
transformer,
we
can
set <span style="color:#054C5C;"><code class="verb">eps</code></span>
     if
we
want
to
tweak
     <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
      <mi>
       𝜖
      </mi>
     </math>
     ,
     <alert style="color: #821131;">
      <span id="bold" style="font-weight:bold;">
       <sup class="textsuperscript">
        15
       </sup>
      </span>
     </alert>
     <span class="marginnote">
      <alert style="color: #821131;">
       <span id="bold" style="font-weight:bold;">
        <sup class="textsuperscript">
         15
        </sup>
       </span>
      </alert>
      <span style="color:#0063B2;">
       The
default
value
is
0.1
      </span>
     </span>
     and <span style="color:#054C5C;"><code class="verb">n_components</code></span>
     if
we
want
to
force
a
specific
target
dimensionality
     <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
      <mi>
       d
      </mi>
     </math>
     .
    </p>
    <p class="noindent">
     The
following
code
example
gives
                                                                                
                                                                                
the
same
result
as
the
preceding
code
which
we
can
also
use
to
verify
that <span style="color:#054C5C;"><code class="verb">gaussian_rnd_proj.components_</code></span>
     is
equal
to
P→
:
    </p>
    <p class="noindent"> <span style="color:#054C5C;"><code class="verb">sklearn</code></span>
     also
provides
a
second
random
projection
transformer,
known
as <span style="color:#054C5C;"><code class="verb">SparseRandomProjection</code></span>.
It
determines
the
target
dimensionality
in
the
same
way,
generates
a
random
matrix
of
the
same
shape,
and
performs
the
projection
identically.
The
main
difference
is
that
the
random
matrix
is
sparse.
This
means
it
                                                                                
                                                                                
uses
much
less
memory:
about
25
MB
instead
of
almost
1.2
GB
in
the
preceding
example!
And
it’s
also
much
faster,
both
to
generate
the
random
matrix
and
to
reduce
dimensionality:
about
50%
faster
in
this
case.
Moreover,
if
the
input
is
sparse,
the
transformation
keeps
it
sparse
(unless
you
set <span style="color:#054C5C;"><code class="verb">dense_output=True</code></span>
     ).
    </p>
    <p class="noindent">
     Finally,
it
enjoys
the
same
distance-preserving
property
as
the
previous
approach,
and
the
quality
of
                                                                                
                                                                                
the
dimensionality
reduction
is
comparable.
In
short,
it’s
usually
preferable
to
use
this
transformer
instead
of
the
first
one,
especially
for
large
or
sparse
datasets.
    </p>
    <p class="noindent">
     The
ratio
(
     <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
      <mi>
       r
      </mi>
     </math>
     )
of
nonzero
items
in
the
sparse
random
matrix
is
called
its
     <span id="bold" style="font-weight:bold;">
      density
     </span>
     .
By
default,
it
is
equal
to
     <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
      <mstyle class="text">
       <mtext>
       </mtext>
       <mstyle class="math">
        <mn>
         1
        </mn>
       </mstyle>
       <mtext>
       </mtext>
       <mstyle class="math">
       </mstyle>
       <mtext>
       </mtext>
       <mstyle class="math">
        <mo class="MathClass-bin" stretchy="false">
         ∕
        </mo>
       </mstyle>
       <mtext>
       </mtext>
       <mstyle class="math">
       </mstyle>
       <mtext>
       </mtext>
       <mstyle class="math">
        <mi>
         n
        </mi>
       </mstyle>
       <mtext>
       </mtext>
      </mstyle>
     </math>
     .
With
20,000
features,
this
means
that
roughly
only
     <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
      <mstyle class="text">
       <mtext>
       </mtext>
       <mstyle class="math">
        <mn>
         1
        </mn>
       </mstyle>
       <mtext>
       </mtext>
       <mstyle class="math">
       </mstyle>
       <mtext>
       </mtext>
       <mstyle class="math">
        <mo class="MathClass-bin" stretchy="false">
         ∕
        </mo>
       </mstyle>
       <mtext>
       </mtext>
       <mstyle class="math">
       </mstyle>
       <mtext>
       </mtext>
       <mstyle class="math">
        <mn>
         1
        </mn>
        <mn>
         4
        </mn>
        <mn>
         1
        </mn>
       </mstyle>
       <mtext>
       </mtext>
      </mstyle>
     </math>
     cells
in
the
random
matrix
is
non-zero.
     <alert style="color: #821131;">
      <span id="bold" style="font-weight:bold;">
       <sup class="textsuperscript">
        16
       </sup>
      </span>
     </alert>
     <span class="marginnote">
      <alert style="color: #821131;">
       <span id="bold" style="font-weight:bold;">
        <sup class="textsuperscript">
         16
        </sup>
       </span>
      </alert>
      <span style="color:#0063B2;">
       There
is
a
                                                                                
                                                                                
reason
why
it
is
called
a
sparse
matrix.
      </span>
     </span>
     We
can
set
this
density
hyperparameter
to
another
value
if
we
prefer.
    </p>
    <p class="noindent">
     Each
cell
in
the
sparse
random
matrix
has
a
probability
     <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
      <mi>
       r
      </mi>
     </math>
     of
being
non-zero,
and
each
non-zero
value
is
either
     <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
      <mo class="MathClass-bin" stretchy="false">
       −
      </mo>
      <mi>
       v
      </mi>
     </math>
     or
     <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
      <mo class="MathClass-bin" stretchy="false">
       +
      </mo>
      <mi>
       v
      </mi>
     </math>
     ,
     <alert style="color: #821131;">
      <span id="bold" style="font-weight:bold;">
       <sup class="textsuperscript">
        17
       </sup>
      </span>
     </alert>
     <span class="marginnote">
      <alert style="color: #821131;">
       <span id="bold" style="font-weight:bold;">
        <sup class="textsuperscript">
         17
        </sup>
       </span>
      </alert>
      <span style="color:#0063B2;">
       Which
are
both
equally
likely.
      </span>
     </span>
     where
     <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
      <mi>
       v
      </mi>
      <mo class="MathClass-rel" stretchy="false">
       =
      </mo>
      <mstyle class="text">
       <mtext>
       </mtext>
       <mstyle class="math">
        <mn>
         1
        </mn>
       </mstyle>
       <mtext>
       </mtext>
       <mstyle class="math">
       </mstyle>
       <mtext>
       </mtext>
       <mstyle class="math">
        <mo class="MathClass-bin" stretchy="false">
         ∕
        </mo>
       </mstyle>
       <mtext>
       </mtext>
       <mstyle class="math">
       </mstyle>
       <mtext>
       </mtext>
       <mstyle class="math">
        <mi>
         d
        </mi>
        <mi>
         r
        </mi>
       </mstyle>
       <mtext>
       </mtext>
      </mstyle>
     </math>
     .
    </p>
    <p class="noindent">
     If
we
want
to
perform
the
inverse
transform,
you
first
need
to
compute
the
pseudo-inverse
of
                                                                                
                                                                                
the
components
matrix
using
SciPy’s <span style="color:#054C5C;"><code class="verb">pinv()</code></span>
     function,
then
multiply
the
reduced
data
by
the
transpose
of
the
pseudo-inverse:
    </p>
    <p class="noindent">
     Random
projection
is
a
simple,
fast,
memory-efficient,
and
powerful
dimensionality
reduction
algorithm
that
we
should
keep
in
mind,
especially
when
dealing
with
high-dimensional
datasets.
    </p>
    <p class="noindent">
    </p>
    <h2 class="sectionHead">
     <a aria-label="Link to this section in PDF version" class="pdf-link" href="DataScienceIILectureBook.pdf#section.4.5" style="color: red;  position: absolute;  left: -30px;" target="_blank">
      <svg class="bi bi-file-pdf" fill="currentColor" height="20" viewbox="0 0 16 16" width="20" xmlns="http://www.w3.org/2000/svg">
       <path d="M4 0a2 2 0 0 0-2 2v12a2 2 0 0 0 2 2h8a2 2 0 0 0 2-2V2a2 2 0 0 0-2-2H4zm0 1h8a1 1 0 0 1 1 1v12a1 1 0 0 1-1 1H4a1 1 0 0 1-1-1V2a1 1 0 0 1 1-1z">
       </path>
       <path d="M4.603 12.087a.81.81 0 0 1-.438-.42c-.195-.388-.13-.776.08-1.102.198-.307.526-.568.897-.787a7.68 7.68 0 0 1 1.482-.645 19.701 19.701 0 0 0 1.062-2.227 7.269 7.269 0 0 1-.43-1.295c-.086-.4-.119-.796-.046-1.136.075-.354.274-.672.65-.823.192-.077.4-.12.602-.077a.7.7 0 0 1 .477.365c.088.164.12.356.127.538.007.187-.012.395-.047.614-.084.51-.27 1.134-.52 1.794a10.954 10.954 0 0 0 .98 1.686 5.753 5.753 0 0 1 1.334.05c.364.065.734.195.96.465.12.144.193.32.2.518.007.192-.047.382-.138.563a1.04 1.04 0 0 1-.354.416.856.856 0 0 1-.51.138c-.331-.014-.654-.196-.933-.417a5.716 5.716 0 0 1-.911-.95 11.642 11.642 0 0 0-1.997.406 11.311 11.311 0 0 1-1.021 1.51c-.29.35-.608.655-.926.787a.793.793 0 0 1-.58.029zm1.379-1.901c-.166.076-.32.156-.459.238-.328.194-.541.383-.647.547-.094.145-.096.25-.04.361.01.022.02.036.026.044a.27.27 0 0 0 .035-.012c.137-.056.355-.235.635-.572a8.18 8.18 0 0 0 .45-.606zm1.64-1.33a12.647 12.647 0 0 1 1.01-.193 11.666 11.666 0 0 1-.51-.858 20.741 20.741 0 0 1-.5 1.05zm2.446.45c.15.162.296.3.435.41.24.19.407.253.498.256a.107.107 0 0 0 .07-.015.307.307 0 0 0 .094-.125.436.436 0 0 0 .059-.2.095.095 0 0 0-.026-.063c-.052-.062-.2-.152-.518-.209a3.881 3.881 0 0 0-.612-.053zM8.078 5.8a6.7 6.7 0 0 0 .2-.828c.031-.188.043-.343.038-.465a.613.613 0 0 0-.032-.198.517.517 0 0 0-.145.04c-.087.035-.158.106-.196.283-.04.192-.03.469.046.822.024.111.054.227.09.346z">
       </path>
      </svg>
     </a>
     <small class="titlemark">
      4.5
     </small>
     <a id="x6-590004.5">
     </a>
     Locally Linear Embedding
    </h2>
    <p class="noindent">
     Locally linear embedding(LLE) is a non-linear dimensionality reduction (NLDR) technique. It is a
     <alert style="color: #821131;">
      manifold learning technique
     </alert>
     which does
     <span id="bold" style="font-weight:bold;">
      NOT
     </span>
     rely on projections, unlike
     <a class="glossary" href="DataScienceIILectureBookli2.html#glo:pca">
      PCA
     </a>
     and random projection. In a nutshell, LLE works by first measuring how each
training instance linearly relates to its nearest neighbors, and then looking for a low-dimensional representation of the training
set where these local relationships are best preserved.
    </p>
    <p class="noindent">
     This approach makes it good at unrolling twisted manifolds, especially when there is not too much noise. The following code
makes a Swiss roll, then uses <span style="color:#054C5C;"><code class="verb">sklearn</code></span>
     ’s <span style="color:#054C5C;"><code class="verb">LocallyLinearEmbedding</code></span>
     class to unroll it:
    </p>
    <p class="noindent">
     The variable <span style="color:#054C5C;"><code class="verb">t</code></span>
     is a 1D NumPy array containing the position of each instance along the rolled axis of the Swiss roll. We don’t use
it in this example, but it can be used as a target for a nonlinear regression task. The resulting 2D dataset is shown
in Figure 8-10. As you can see, the Swiss roll is completely unrolled, and the distances between instances are
locally well preserved. However, distances are not preserved on a larger scale: the unrolled Swiss roll should be a
rectangle, not this kind of stretched and twisted band. Nevertheless, LLE did a pretty good job of modeling the
manifold.
     <a id="subsubsection*.6">
     </a>
    </p>
    <h5 class="subsubsectionHead">
     <a id="x6-60000">
     </a>
     Operation Principle
    </h5>
    <p class="noindent">
     For each training instance
     <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
      <msubsup>
       <mrow>
        <mover accent="true">
         <mrow>
          <mi>
           x
          </mi>
         </mrow>
         <mo accent="true">
          →
         </mo>
        </mover>
       </mrow>
       <mrow>
        <mo class="MathClass-open" stretchy="false">
         (
        </mo>
        <mi>
         i
        </mi>
        <mo class="MathClass-close" stretchy="false">
         )
        </mo>
       </mrow>
       <mrow>
       </mrow>
      </msubsup>
     </math>
     ,
the algorithm identifies its k-nearest neighbours, then tries to reconstruct
     <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
      <msubsup>
       <mrow>
        <mover accent="true">
         <mrow>
          <mi>
           x
          </mi>
         </mrow>
         <mo accent="true">
          →
         </mo>
        </mover>
       </mrow>
       <mrow>
        <mo class="MathClass-open" stretchy="false">
         (
        </mo>
        <mi>
         i
        </mi>
        <mo class="MathClass-close" stretchy="false">
         )
        </mo>
       </mrow>
       <mrow>
       </mrow>
      </msubsup>
     </math>
     as a linear function of these neighbors. More specifically, it tries to find the weights
     <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
      <msubsup>
       <mrow>
        <mi>
         w
        </mi>
       </mrow>
       <mrow>
        <mi>
         i
        </mi>
        <mo class="MathClass-punc" stretchy="false">
         ,
        </mo>
        <mi>
         j
        </mi>
       </mrow>
       <mrow>
       </mrow>
      </msubsup>
     </math>
     such that the squared
distance between
     <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
      <msubsup>
       <mrow>
        <mover accent="true">
         <mrow>
          <mi>
           x
          </mi>
         </mrow>
         <mo accent="true">
          →
         </mo>
        </mover>
       </mrow>
       <mrow>
        <mo class="MathClass-open" stretchy="false">
         (
        </mo>
        <mi>
         i
        </mi>
        <mo class="MathClass-close" stretchy="false">
         )
        </mo>
       </mrow>
       <mrow>
       </mrow>
      </msubsup>
     </math>
     and:
    </p>
    <table class="equation-star">
     <tr>
      <td>
       <math class="equation" display="block" xmlns="http://www.w3.org/1998/Math/MathML">
        <munderover accent="false" accentunder="false">
         <mrow>
          <mo>
           ∑
          </mo>
         </mrow>
         <mrow>
          <mi>
           j
          </mi>
          <mo class="MathClass-rel" stretchy="false">
           =
          </mo>
          <mn>
           1
          </mn>
         </mrow>
         <mrow>
          <mi>
           m
          </mi>
         </mrow>
        </munderover>
        <msubsup>
         <mrow>
          <mi>
           w
          </mi>
         </mrow>
         <mrow>
          <mi>
           i
          </mi>
          <mo class="MathClass-punc" stretchy="false">
           ,
          </mo>
          <mi>
           j
          </mi>
         </mrow>
         <mrow>
         </mrow>
        </msubsup>
        <msubsup>
         <mrow>
          <mover accent="true">
           <mrow>
            <mi>
             x
            </mi>
           </mrow>
           <mo accent="true">
            →
           </mo>
          </mover>
         </mrow>
         <mrow>
         </mrow>
         <mrow>
          <mo class="MathClass-open" stretchy="false">
           (
          </mo>
          <mn>
           1
          </mn>
          <mo class="MathClass-close" stretchy="false">
           )
          </mo>
         </mrow>
        </msubsup>
       </math>
      </td>
     </tr>
    </table>
    <p class="noindent">
     is as
     <span id="bold" style="font-weight:bold;">
      small as possible
     </span>
     , assuming
     <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
      <msubsup>
       <mrow>
        <mi>
         w
        </mi>
       </mrow>
       <mrow>
        <mi>
         i
        </mi>
        <mo class="MathClass-punc" stretchy="false">
         ,
        </mo>
        <mi>
         j
        </mi>
       </mrow>
       <mrow>
       </mrow>
      </msubsup>
      <mo class="MathClass-rel" stretchy="false">
       =
      </mo>
      <mn>
       0
      </mn>
     </math>
     if
     <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
      <msubsup>
       <mrow>
        <mover accent="true">
         <mrow>
          <mi>
           x
          </mi>
         </mrow>
         <mo accent="true">
          →
         </mo>
        </mover>
       </mrow>
       <mrow>
       </mrow>
       <mrow>
        <mo class="MathClass-open" stretchy="false">
         (
        </mo>
        <mi>
         i
        </mi>
        <mo class="MathClass-close" stretchy="false">
         )
        </mo>
       </mrow>
      </msubsup>
     </math>
     is not one of the
k-nearest neighbors of
     <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
      <msubsup>
       <mrow>
        <mover accent="true">
         <mrow>
          <mi>
           x
          </mi>
         </mrow>
         <mo accent="true">
          →
         </mo>
        </mover>
       </mrow>
       <mrow>
        <mo class="MathClass-open" stretchy="false">
         (
        </mo>
        <mi>
         i
        </mi>
        <mo class="MathClass-close" stretchy="false">
         )
        </mo>
       </mrow>
       <mrow>
       </mrow>
      </msubsup>
     </math>
     .
Thus the first step of LLE is the constrained optimization problem:
    </p>
    <table class="equation-star">
     <tr>
      <td>
       <math class="equation" display="block" xmlns="http://www.w3.org/1998/Math/MathML">
        <msubsup>
         <mrow>
          <mover accent="true">
           <mrow>
            <mover accent="true">
             <mrow>
              <mi>
               W
              </mi>
             </mrow>
             <mo accent="true">
              →
             </mo>
            </mover>
           </mrow>
           <mo accent="true">
            ^
           </mo>
          </mover>
         </mrow>
         <mrow>
         </mrow>
         <mrow>
         </mrow>
        </msubsup>
        <mo class="MathClass-rel" stretchy="false">
         =
        </mo>
       </math>
      </td>
     </tr>
    </table>
    <p class="noindent">
     , where W is the weight matrix containing all the weights wi,j. The second constraint simply normalizes the weights for each
training instance x(i).
    </p>
    <p class="noindent">
     After this step, the weight matrix W(containing the weights w i,j) encodes the local linear relationships between the training
instances. The second step is to map the training instances into a d-dimensional space (where d &lt; n) while preserving these
local relationships as much as possible. If z(i) is the image of x(i) in this d-dimensional space, then we want the
squared distance between z(i) and  j=1 m w i,j z (j) to be as small as possible. This idea leads to the unconstrained
optimization problem described in Equation 8-5. It looks very similar to the first step, but instead of keeping the
instances fixed and finding the optimal weights, we are doing the reverse: keeping the weights fixed and finding the
optimal position of the instances’ images in the low-dimensional space. Note that Z is the matrix containing all
z(i).
    </p>
    <p class="noindent"> <span style="color:#054C5C;"><code class="verb">sklearn</code></span>
     ’s LLE implementation has the following computational complexity: O(m log(m)n log(k)) for finding the k-nearest
neighbors, O(mnk3) for optimizing the weights, and O(dm2) for constructing the low-dimensional representations. Unfortunately,
the m2 in the last term makes this algorithm scale poorly to very large datasets. As you can see, LLE is quite different from the
projection techniques, and it’s significantly more complex, but it can also construct much better low- dimensional representations,
especially if the data is nonlinear.
    </p>
   </article>
   <footer>
    <div class="next">
     <p class="noindent">
      <a href="DataScienceIILectureBookch5.html" style="float: right;">
       Next Chapter →
      </a>
      <a href="DataScienceIILectureBookch3.html" style="float: left;">
       ← Previous Chapter
      </a>
      <a href="index.html">
       TOP
      </a>
     </p>
    </div>
   </footer>
  </div>
  <p class="noindent">
   <a id="tailDataScienceIILectureBookch4.html">
   </a>
  </p>
 </body>
</html>
