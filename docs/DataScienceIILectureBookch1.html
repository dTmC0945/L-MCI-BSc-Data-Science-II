<!DOCTYPE html>
<html lang="en-US" xml:lang="en-US">
 <head>
  <title>
   1 Support Vector Machines
  </title>
    <link rel='icon' type='image/x-icon' href='figures/logo.svg'></link>
  <meta charset="utf-8"/>
  <meta content="ParSnip" name="generator"/>
  <meta content="width=device-width,initial-scale=1" name="viewport"/>
  <link href="DataScienceIILectureBook.css" rel="stylesheet" type="text/css"/>
  <meta content="DataScienceIILectureBook.tex" name="src"/>
  <script async="async" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/mml-chtml.js" type="text/javascript">
  </script>
  <link href="flatweb.css" rel="stylesheet" type="text/css"/>
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.7.1/jquery.min.js">
  </script>
  <script src="flatjs.js">
  </script>
  <link href="http://fonts.googleapis.com/css?family=Roboto" rel="stylesheet" type="text/css"/>
  <script type="module">
   import pdfjsDist from 'https://cdn.jsdelivr.net/npm/pdfjs-dist@5.3.93/+esm'
  </script>
  <script>
   function moveTOC() { 
var htmlShow = document.getElementsByClassName("wide"); 
if (htmlShow[0].style.display === "none") { 
htmlShow[0].style.display = "block"; 
} else { 
htmlShow[0].style.display = "none"; 
} 
}
  </script>
  <link href="https://unpkg.com/nord-highlightjs@0.1.0/dist/nord.css" rel="stylesheet" type="text/css"/>
  <script src="https://unpkg.com/@highlightjs/cdn-assets@11.3.1/highlight.min.js">
  </script>
  <script src="https://unpkg.com/highlightjs-copy/dist/highlightjs-copy.min.js">
  </script>
  <link href="https://unpkg.com/highlightjs-copy/dist/highlightjs-copy.min.css" rel="stylesheet"/>
  <script src="flatjs.js">
  </script>
  <header>
  </header>
 </head>
 <body id="top">
  <nav class="wide">
   <div class="contents">
    <h3 style="font-weight:lighter; padding: 20px 0 20px 0;">
     1
   
     
   

   Support Vector Machines
    </h3>
    <h4 style="font-weight:lighter">
     Chapter Table of Contents
    </h4>
    <ul>
     <div class="chapterTOCS">
      <li>
       <span class="sectionToc">
        <small>
         1.1
        </small>
        <a href="#x3-30001.1">
         Introduction
        </a>
       </span>
      </li>
      <li>
       <span class="sectionToc">
        <small>
         1.2
        </small>
        <a href="#x3-40001.2">
         Linear SVM Classification
        </a>
       </span>
      </li>
      <li>
       <span class="subsectionToc" style="font-size:12px;margin:0px;">
        <small>
         1.2.1
        </small>
        <a href="#x3-50001.2.1">
         Soft Margin Classification
        </a>
       </span>
      </li>
      <li>
       <span class="sectionToc">
        <small>
         1.3
        </small>
        <a href="#x3-60001.3">
         Nonlinear SVM Classification
        </a>
       </span>
      </li>
      <li>
       <span class="subsectionToc" style="font-size:12px;margin:0px;">
        <small>
         1.3.1
        </small>
        <a href="#x3-70001.3.1">
         Polynomial Kernel
        </a>
       </span>
      </li>
      <li>
       <span class="subsectionToc" style="font-size:12px;margin:0px;">
        <small>
         1.3.2
        </small>
        <a href="#x3-80001.3.2">
         Similarity Features
        </a>
       </span>
      </li>
      <li>
       <span class="subsectionToc" style="font-size:12px;margin:0px;">
        <small>
         1.3.3
        </small>
        <a href="#x3-90001.3.3">
         Gaussian RBF Kernel
        </a>
       </span>
      </li>
      <li>
       <span class="sectionToc">
        <small>
         1.4
        </small>
        <a href="#x3-100001.4">
         SVM Regression
        </a>
       </span>
      </li>
     </div>
    </ul>
    <div class="prev-next" style="text-align: center">
     <p class="noindent">
      <a href="DataScienceIILectureBookch2.html" style="float:right; font-size:10px">
       NEXT →
      </a>
      <a href="index.html" style="float:left; font-size:10px">
       ← PREV
      </a>
      <a href="index.html">
       TOP
      </a>
     </p>
    </div>
    <div id="author-info" style="bottom: 0; padding-bottom: 10px;">
     <div id="author-logo">
      <img src="figures/logo.svg" style="margin: 10px 0;"/>
      <div>
       <div>
        <h4 style="color:#7aa0b8; font-weight:lighter;">
         WebBook | D. T. McGuiness, P.hD
        </h4>
       </div>
      </div>
      <div id="author-text" style="bottom: 0; padding-top: 50px;border-top: solid 1px #3b4b5e;font-size: 12px;text-align: right;">
       <p>
        <b>
         Authors Note
        </b>
        The website you are viewing is auto-generated
    using ParSnip and therefore subject to slight errors in
    typography and formatting. When in doubt, please consult the
    LectureBook.
       </p>
      </div>
     </div>
    </div>
   </div>
  </nav>
  <div class="page">
   <article class="chapter">
    <h1 class="chapterHead">
     <div class="number">
      1
     </div>
     <a id="x3-20001">
     </a>
     Support Vector Machines
    </h1><button id='toc-button' onclick='moveTOC()'>TOC</button>
    <div class="section-control" style="text-align: center">
     <a id="prev-section" onclick="goPrev()" style="float:left;">
      ← Previous Section
     </a>
     <a id="next-section" onclick="goNext()" style="float:right;">
      Next Section →
     </a>
    </div>
    <h2 class="sectionHead">
     <a aria-label="Link to this section in PDF version" class="pdf-link" href="DataScienceIILectureBook.pdf#section.1.1" style="color: red;  position: absolute;  left: -30px;" target="_blank">
      <svg class="bi bi-file-pdf" fill="currentColor" height="20" viewbox="0 0 16 16" width="20" xmlns="http://www.w3.org/2000/svg">
       <path d="M4 0a2 2 0 0 0-2 2v12a2 2 0 0 0 2 2h8a2 2 0 0 0 2-2V2a2 2 0 0 0-2-2H4zm0 1h8a1 1 0 0 1 1 1v12a1 1 0 0 1-1 1H4a1 1 0 0 1-1-1V2a1 1 0 0 1 1-1z">
       </path>
       <path d="M4.603 12.087a.81.81 0 0 1-.438-.42c-.195-.388-.13-.776.08-1.102.198-.307.526-.568.897-.787a7.68 7.68 0 0 1 1.482-.645 19.701 19.701 0 0 0 1.062-2.227 7.269 7.269 0 0 1-.43-1.295c-.086-.4-.119-.796-.046-1.136.075-.354.274-.672.65-.823.192-.077.4-.12.602-.077a.7.7 0 0 1 .477.365c.088.164.12.356.127.538.007.187-.012.395-.047.614-.084.51-.27 1.134-.52 1.794a10.954 10.954 0 0 0 .98 1.686 5.753 5.753 0 0 1 1.334.05c.364.065.734.195.96.465.12.144.193.32.2.518.007.192-.047.382-.138.563a1.04 1.04 0 0 1-.354.416.856.856 0 0 1-.51.138c-.331-.014-.654-.196-.933-.417a5.716 5.716 0 0 1-.911-.95 11.642 11.642 0 0 0-1.997.406 11.311 11.311 0 0 1-1.021 1.51c-.29.35-.608.655-.926.787a.793.793 0 0 1-.58.029zm1.379-1.901c-.166.076-.32.156-.459.238-.328.194-.541.383-.647.547-.094.145-.096.25-.04.361.01.022.02.036.026.044a.27.27 0 0 0 .035-.012c.137-.056.355-.235.635-.572a8.18 8.18 0 0 0 .45-.606zm1.64-1.33a12.647 12.647 0 0 1 1.01-.193 11.666 11.666 0 0 1-.51-.858 20.741 20.741 0 0 1-.5 1.05zm2.446.45c.15.162.296.3.435.41.24.19.407.253.498.256a.107.107 0 0 0 .07-.015.307.307 0 0 0 .094-.125.436.436 0 0 0 .059-.2.095.095 0 0 0-.026-.063c-.052-.062-.2-.152-.518-.209a3.881 3.881 0 0 0-.612-.053zM8.078 5.8a6.7 6.7 0 0 0 .2-.828c.031-.188.043-.343.038-.465a.613.613 0 0 0-.032-.198.517.517 0 0 0-.145.04c-.087.035-.158.106-.196.283-.04.192-.03.469.046.822.024.111.054.227.09.346z">
       </path>
      </svg>
     </a>
     <small class="titlemark">
      1.1
     </small>
     <a id="x3-30001.1">
     </a>
     Introduction
    </h2>
    <p class="noindent">
     One of the important task a
     <a class="glossary" href="DataScienceIILectureBookli2.html#glo:ml">
      Machine Learning (ML)
     </a>
     has to accomplish is to
     <alert style="color: #821131;">
      classify data
     </alert>
     . An a
useful method for this task is called
     <a class="glossary" href="DataScienceIILectureBookli2.html#glo:svm">
      SVM
     </a>.
    </p>
    <p class="noindent">
     A
powerful
     <a class="glossary" href="DataScienceIILectureBookli2.html#glo:ml">
      ML
     </a>
     model,
capable
of
performing
     <alert style="color: #821131;">
      linear
     </alert>
     or
     <alert style="color: #821131;">
      non-linear
classification
     </alert>
     ,
regression.
They
are
classified
as
     <alert style="color: #821131;">
      supervised
learning
     </alert>
     which
utilises
statistical
learning
     <alert style="color: #821131;">
      <span id="bold" style="font-weight:bold;">
       <sup class="textsuperscript">
        1
       </sup>
      </span>
     </alert>
     <span class="marginnote">
      <alert style="color: #821131;">
       <span id="bold" style="font-weight:bold;">
        <sup class="textsuperscript">
         1
        </sup>
       </span>
      </alert>
      <span style="color:#0063B2;">
       a
       framework
       for
       machine
       learning
       drawing
       from
       the
       fields
       of
       statistics
       and
       functional
       analysis.[1][2][3]
       Statistical
       learning
       theory
       deals
       with
       the
       statistical
       inference
       problem
       of
       finding
       a
       predictive
       function
       based
       on
       data.
       Statistical
       learning
       theory
       has
       led
       to
       successful
       applications
       in
       fields
       such
       as
       computer
       vision,
       speech
       recognition,
       and
       bioinformatics.
      </span>
     </span>
     which
     can
     be
     used
     for
     pattern
     recognition
     and
     regression.
     It
     can
     also
     identify
     precisely
     the
     factors
     which
     need
     to
     be
     taken
     into
     account
     to
     learn
     successfully
     certain
     simple
     types
     of
     algorithms,
     however,
     real-world
     applications
     usually
     need
     more
     complex
     models
     and
     algorithms
     (such
     as
     neural
     networks),
     that
     makes
     them
     much
     harder
     to
     analyse
     theoretically.
    </p>
    <p class="noindent">
     SVMs can be seen as lying at the intersection of learning theory and practice. They construct models that are complex enough
     (containing a large class of neural networks for instance) and yet that are simple enough to be analysed mathematically.
    </p>
    <div class="knowledge">
     <p class="noindent">
      It is even possible to implement
      <span id="bold" style="font-weight:bold;">
       novelty detection
      </span>
      using
      <a class="glossary" href="DataScienceIILectureBookli2.html#glo:svm">
       SVM
      </a>
     </p>
    </div>
    <a class="glossary" href="DataScienceIILectureBookli2.html#glo:svm">
     SVM
    </a>
    s most suitable for small to medium sized
    <alert style="color: #821131;">
     non-linear
    </alert>
    datasets (i.e., hundreds to thousands of instances), especially for classification tasks.
    <p class="noindent">
    </p>
    <h2 class="sectionHead">
     <a aria-label="Link to this section in PDF version" class="pdf-link" href="DataScienceIILectureBook.pdf#section.1.2" style="color: red;  position: absolute;  left: -30px;" target="_blank">
      <svg class="bi bi-file-pdf" fill="currentColor" height="20" viewbox="0 0 16 16" width="20" xmlns="http://www.w3.org/2000/svg">
       <path d="M4 0a2 2 0 0 0-2 2v12a2 2 0 0 0 2 2h8a2 2 0 0 0 2-2V2a2 2 0 0 0-2-2H4zm0 1h8a1 1 0 0 1 1 1v12a1 1 0 0 1-1 1H4a1 1 0 0 1-1-1V2a1 1 0 0 1 1-1z">
       </path>
       <path d="M4.603 12.087a.81.81 0 0 1-.438-.42c-.195-.388-.13-.776.08-1.102.198-.307.526-.568.897-.787a7.68 7.68 0 0 1 1.482-.645 19.701 19.701 0 0 0 1.062-2.227 7.269 7.269 0 0 1-.43-1.295c-.086-.4-.119-.796-.046-1.136.075-.354.274-.672.65-.823.192-.077.4-.12.602-.077a.7.7 0 0 1 .477.365c.088.164.12.356.127.538.007.187-.012.395-.047.614-.084.51-.27 1.134-.52 1.794a10.954 10.954 0 0 0 .98 1.686 5.753 5.753 0 0 1 1.334.05c.364.065.734.195.96.465.12.144.193.32.2.518.007.192-.047.382-.138.563a1.04 1.04 0 0 1-.354.416.856.856 0 0 1-.51.138c-.331-.014-.654-.196-.933-.417a5.716 5.716 0 0 1-.911-.95 11.642 11.642 0 0 0-1.997.406 11.311 11.311 0 0 1-1.021 1.51c-.29.35-.608.655-.926.787a.793.793 0 0 1-.58.029zm1.379-1.901c-.166.076-.32.156-.459.238-.328.194-.541.383-.647.547-.094.145-.096.25-.04.361.01.022.02.036.026.044a.27.27 0 0 0 .035-.012c.137-.056.355-.235.635-.572a8.18 8.18 0 0 0 .45-.606zm1.64-1.33a12.647 12.647 0 0 1 1.01-.193 11.666 11.666 0 0 1-.51-.858 20.741 20.741 0 0 1-.5 1.05zm2.446.45c.15.162.296.3.435.41.24.19.407.253.498.256a.107.107 0 0 0 .07-.015.307.307 0 0 0 .094-.125.436.436 0 0 0 .059-.2.095.095 0 0 0-.026-.063c-.052-.062-.2-.152-.518-.209a3.881 3.881 0 0 0-.612-.053zM8.078 5.8a6.7 6.7 0 0 0 .2-.828c.031-.188.043-.343.038-.465a.613.613 0 0 0-.032-.198.517.517 0 0 0-.145.04c-.087.035-.158.106-.196.283-.04.192-.03.469.046.822.024.111.054.227.09.346z">
       </path>
      </svg>
     </a>
     <small class="titlemark">
      1.2
     </small>
     <a id="x3-40001.2">
     </a>
     Linear
     <a class="glossary" href="DataScienceIILectureBookli2.html#glo:svm">
      SVM
     </a>
     Classification
    </h2>
    <p class="noindent">
     As
     with
     most
     engineering
     and
     abstract
     concepts,
     the
     idea
     behind
     <a class="glossary" href="DataScienceIILectureBookli2.html#glo:svm">
      SVM
     </a>
     is
     best
     explained
     using
     programming
     and
     some
     visuals.
     <span id="bold" style="font-weight:bold;">
      Fig.
     </span>
     <a href="#x3-4001r1">
      1.1
     </a>
     shows
     part
     of
     the
     iris
     dataset
     <alert style="color: #821131;">
      <span id="bold" style="font-weight:bold;">
       <sup class="textsuperscript">
        2
       </sup>
      </span>
     </alert>
     <span class="marginnote">
      <img alt="PIC" height="" src="figures/Support-Vector-Machines/raster/portrait-iris-flower.jpg" width="100%"/>
      <alert style="color: #821131;">
       <span id="bold" style="font-weight:bold;">
        <sup class="textsuperscript">
         2
        </sup>
       </span>
      </alert>
      <span style="color:#0063B2;">
       The
       Iris
       flower
       data
       set
       or
       Fisher’s
       Iris
       data
       set
       is
       a
       multivariate
       data
       set
       used
       and
       made
       famous
       by
       the
       British
       statistician
       and
       biologist
       Ronald
       Fisher
       in
       his
       1936
       paper
       The
       use
       of
       multiple
       measurements
       in
       taxonomic
       problems
       as
       an
       example
       of
       linear
       discriminant
       analysis.[1]
       It
       is
       sometimes
       called
       Anderson’s
       Iris
       data
       set
       because
       Edgar
       Anderson
       collected
       the
       data
       to
       quantify
       the
       morphologic
       variation
       of
       Iris
       flowers
       of
       three
       related
       species.
      </span>
     </span>
     which
     was
     briefly
     discussed
     in
     <span id="bold" style="font-weight:bold;">
      Data
Science
I
     </span>
     .
    </p>
    <div class="figure">
     <p class="noindent">
      <img alt="PIC" height="" src="codes/images/Support-Vector-Machines/large-margin-classification-plot-.svg" width="150%"/>
      <a id="x3-4001r1">
      </a>
     </p>
     <figcaption class="caption">
      <span class="id">
       Figure
       1.1:
      </span>
      <span class="content">
       Large Margin Classifier
      </span>
     </figcaption>
    </div>
    <p class="noindent">
     The two <alert style="color: #821131;">(2)</alert>
     classes can easily and clearly separated with a
     <alert style="color: #821131;">
      straight line
     </alert>
     .
    </p>
    <div class="warning">
     <p class="noindent">
      This means the data is
      <span id="bold" style="font-weight:bold;">
       linearly separable
      </span>
      .
     </p>
    </div>
    <p class="noindent">
     The left plot shows the decision boundaries of three <alert style="color: #821131;">(3)</alert>
     possible linear classifiers. The model whose decision boundary is
     represented by the dashed line (
     <span style="color:#00FF00;">
      - -
     </span>
     ) is so bad it does not even separate the classes properly.
    </p>
    <p class="noindent">
     The other two models work perfectly on this training set, but their decision boundaries come so close to the instances that these
     models will probably
     <span id="bold" style="font-weight:bold;">
      NOT
     </span>
     perform as well on new instances.
    </p>
    <p class="noindent">
     In contrast, the black solid line in the plot on the right represents the
     <span id="bold" style="font-weight:bold;">
      decision boundary
     </span>
     of an
     <span id="bold" style="font-weight:bold;">
      <a class="glossary" href="DataScienceIILectureBookli2.html#glo:svm">
       SVM
      </a>
      classifier
     </span>
     . This line not
     only separates the two classes but also stays as far away from the closest training instances as possible. Think of an
     <a class="glossary" href="DataScienceIILectureBookli2.html#glo:svm">
      SVM
     </a>
     classifier as fitting the widest possible street, which in the plot is represented by the parallel dashed lines, between the
     classes.
    </p>
    <p class="noindent">
     This is called
     <span id="bold" style="font-weight:bold;">
      large margin classification
     </span>
     .
    </p>
    <div class="informationblock" id="tcolobox-1">
     <div class="title">
      <p class="noindent">
       <span id="bold" style="font-weight:bold;">
        Information
       </span>
       : Margin Classifier
      </p>
     </div>
     <div class="box-content">
      <p class="noindent">
       A classifier which is able to give an associated distance from the decision boundary for each example. For instance,
       if a linear classifier is used, the distance of an example from the separating hyperplane is the margin of that
       example.
      </p>
     </div>
    </div>
    <p class="noindent">
     Please
     observe
     that
     adding
     more
     training
     instances
     will
     <span id="bold" style="font-weight:bold;">
      NOT
     </span>
     affect
     the
     decision
     boundary
     at
     all
     as
     the
     boundaries
     are
     fully
     determined
     <alert style="color: #821131;">
      <span id="bold" style="font-weight:bold;">
       <sup class="textsuperscript">
        3
       </sup>
      </span>
     </alert>
     <span class="marginnote">
      <alert style="color: #821131;">
       <span id="bold" style="font-weight:bold;">
        <sup class="textsuperscript">
         3
        </sup>
       </span>
      </alert>
      <span style="color:#0063B2;">
       or
       supported.
      </span>
     </span>
     by
     the
     instances
     located
     on
     the
     edge
     of
     the
     boundaries.
     These
     instances
     are
     called
     the
     <span id="bold" style="font-weight:bold;">
      support
vectors
     </span>
     .
    </p>
    <div class="warning">
     <p class="noindent">
      <a class="glossary" href="DataScienceIILectureBookli2.html#glo:svm">
       SVM
      </a>
      s are
      <span id="bold" style="font-weight:bold;">
       sensitive to the feature scales
      </span>
      , as you can see in Figure
      <a href="#x3-4002r2">
       1.2
      </a>. In the left plot, the vertical scale is much larger than
      the horizontal scale, so the widest possible street is close to horizontal. After feature scaling (e.g., using <span style="color:#054C5C;"><code class="verb">sklearn</code></span>
      ’s <span style="color:#054C5C;"><code class="verb">StandardScaler</code></span>
      ), the decision boundary in the right plot looks much better.
     </p>
    </div>
    <div class="figure">
     <p class="noindent">
      <img alt="PIC" height="" src="codes/images/Support-Vector-Machines/sensitivity_to_feature_scales_plot-.svg" width="150%"/>
      <a id="x3-4002r2">
      </a>
     </p>
     <figcaption class="caption">
      <span class="id">
       Figure
       1.2:
      </span>
      <span class="content">
       Sensitivity to feature scales.
      </span>
     </figcaption>
    </div>
    <h3 class="subsectionHead">
     <a aria-label="Link to this section in PDF version" class="pdf-link" href="DataScienceIILectureBook.pdf#subsection.1.2.1" style="color: red;  position: absolute;  left: -30px;" target="_blank">
      <svg class="bi bi-file-pdf" fill="currentColor" height="20" viewbox="0 0 16 16" width="20" xmlns="http://www.w3.org/2000/svg">
       <path d="M4 0a2 2 0 0 0-2 2v12a2 2 0 0 0 2 2h8a2 2 0 0 0 2-2V2a2 2 0 0 0-2-2H4zm0 1h8a1 1 0 0 1 1 1v12a1 1 0 0 1-1 1H4a1 1 0 0 1-1-1V2a1 1 0 0 1 1-1z">
       </path>
       <path d="M4.603 12.087a.81.81 0 0 1-.438-.42c-.195-.388-.13-.776.08-1.102.198-.307.526-.568.897-.787a7.68 7.68 0 0 1 1.482-.645 19.701 19.701 0 0 0 1.062-2.227 7.269 7.269 0 0 1-.43-1.295c-.086-.4-.119-.796-.046-1.136.075-.354.274-.672.65-.823.192-.077.4-.12.602-.077a.7.7 0 0 1 .477.365c.088.164.12.356.127.538.007.187-.012.395-.047.614-.084.51-.27 1.134-.52 1.794a10.954 10.954 0 0 0 .98 1.686 5.753 5.753 0 0 1 1.334.05c.364.065.734.195.96.465.12.144.193.32.2.518.007.192-.047.382-.138.563a1.04 1.04 0 0 1-.354.416.856.856 0 0 1-.51.138c-.331-.014-.654-.196-.933-.417a5.716 5.716 0 0 1-.911-.95 11.642 11.642 0 0 0-1.997.406 11.311 11.311 0 0 1-1.021 1.51c-.29.35-.608.655-.926.787a.793.793 0 0 1-.58.029zm1.379-1.901c-.166.076-.32.156-.459.238-.328.194-.541.383-.647.547-.094.145-.096.25-.04.361.01.022.02.036.026.044a.27.27 0 0 0 .035-.012c.137-.056.355-.235.635-.572a8.18 8.18 0 0 0 .45-.606zm1.64-1.33a12.647 12.647 0 0 1 1.01-.193 11.666 11.666 0 0 1-.51-.858 20.741 20.741 0 0 1-.5 1.05zm2.446.45c.15.162.296.3.435.41.24.19.407.253.498.256a.107.107 0 0 0 .07-.015.307.307 0 0 0 .094-.125.436.436 0 0 0 .059-.2.095.095 0 0 0-.026-.063c-.052-.062-.2-.152-.518-.209a3.881 3.881 0 0 0-.612-.053zM8.078 5.8a6.7 6.7 0 0 0 .2-.828c.031-.188.043-.343.038-.465a.613.613 0 0 0-.032-.198.517.517 0 0 0-.145.04c-.087.035-.158.106-.196.283-.04.192-.03.469.046.822.024.111.054.227.09.346z">
       </path>
      </svg>
     </a>
     <small class="titlemark">
      1.2.1
     </small>
     <a id="x3-50001.2.1">
     </a>
     Soft Margin Classification
    </h3>
    <p class="noindent">
     If we impose all instances must be off the street and on the correct side, this is called
     <span id="bold" style="font-weight:bold;">
      hard margin classification
     </span>
     . There are
     two <alert style="color: #821131;">(2)</alert>
     major issues with hard margin classification:
    </p>
    <dl class="description">
     <dt class="description">
      <a id="x3-5001x1.2.1">
      </a>
      <span class="ec-lmssbx-10x-x-70">
       It is sensitive to outliers
      </span>
     </dt>
     <dd class="description">
      <p class="noindent">
       Hard
       margin
       SVM
       is
       highly
       sensitive
       to
       outliers
       or
       noisy
       data
       points.
       Even
       a
       single
       mislabeled
       point
       can
       significantly
       affect
       the
       position
       of
       the
       decision
       boundary
       and
       lead
       to
       poor
       generalization
       on
       unseen
       data.
      </p>
     </dd>
     <dt class="description">
      <a id="x3-5002x1.2.1">
      </a>
      <span class="ec-lmssbx-10x-x-70">
       Not Suitable for Non-linear Data
      </span>
     </dt>
     <dd class="description">
      <p class="noindent">
       When
       the
       data
       is
       not
       linearly
       separable,
       hard
       margin
       SVM
       fails
       to
       find
       a
       valid
       solution,
       rendering
       it
       impractical
       for
       many
       real-world
       datasets.
      </p>
     </dd>
    </dl>
    <div class="informationblock" id="tcolobox-2">
     <div class="title">
      <p class="noindent">
       <span id="bold" style="font-weight:bold;">
        Information
       </span>
       : Outlier
      </p>
     </div>
     <div class="box-content">
      <p class="noindent">
       A data point that differs significantly from other observations. An outlier may be due to a variability in the
       measurement, an indication of novel data, or it may be the result of experimental error.
      </p>
     </div>
    </div>
    <p class="noindent">
     <span id="bold" style="font-weight:bold;">
      Fig.
     </span>
     <a href="#x3-5003r3">
      1.3
     </a>
     shows the iris dataset with just one <alert style="color: #821131;">(1)</alert>
     additional outlier:
    </p>
    <div class="quoteblock">
     <p class="noindent">
      on
      the
      left,
      it
      is
      impossible
      to
      find
      a
      hard
      margin
      whereas
      on
      the
      right,
      the
      decision
      boundary
      ends
      up
      very
      different
      from
      the
      one
      we
      saw
      in
      <span id="bold" style="font-weight:bold;">
       Fig.
      </span>
      <a href="#x3-4001r1">
       1.1
      </a>
      without
      the
      outlier,
      and
      the
      model
      will
      probably
      not
      generalize
      as
      well.
     </p>
    </div>
    <div class="figure">
     <p class="noindent">
      <img alt="PIC" height="" src="codes/images/Support-Vector-Machines/sensitivity_to_outliers_plot-.svg" width="150%"/>
      <a id="x3-5003r3">
      </a>
     </p>
     <figcaption class="caption">
      <span class="id">
       Figure
       1.3:
      </span>
      <span class="content">
       Hard margin sensitivity to outliers.
      </span>
     </figcaption>
    </div>
    <p class="noindent">
     To
     avoid
     these
     aforementioned
     issues,
     we
     need
     to
     use
     a
     more
     flexible
     model.
     The
     idea
     is
     to
     find
     a
     good
     balance
     between
     keeping
     the
     street
     as
     large
     as
     possible
     and
     limiting
     the
     margin
     violations.
     <alert style="color: #821131;">
      <span id="bold" style="font-weight:bold;">
       <sup class="textsuperscript">
        4
       </sup>
      </span>
     </alert>
     <span class="marginnote">
      <alert style="color: #821131;">
       <span id="bold" style="font-weight:bold;">
        <sup class="textsuperscript">
         4
        </sup>
       </span>
      </alert>
      <span style="color:#0063B2;">
       i.e.,
       instances
       that
       end
       up
       in
       the
       middle
       of
       the
       street
       or
       even
       on
       the
       wrong
       side
      </span>
     </span>
     This
     is
     called
     <alert style="color: #821131;">
      soft
margin
classification
     </alert>
     .
    </p>
    <p class="noindent">
     As
     mentioned,
     soft
     margin
     <a class="glossary" href="DataScienceIILectureBookli2.html#glo:svm">
      SVM
     </a>
     introduces
     flexibility
     by
     allowing
     some
     margin
     violations,
     misclassifications,
     to
     handle
     cases
     where
     the
     data
     is
     <span id="bold" style="font-weight:bold;">
      NOT
     </span>
     perfectly
     separable.
    </p>
    <div class="knowledge">
     <p class="noindent">
      This is suitable for scenarios where the data may contain noise or outliers.
     </p>
    </div>
    <p class="noindent">
     It introduces a
     <alert style="color: #821131;">
      penalty term
     </alert>
     for misclassifications, allowing for a trade-off between a wider margin and a few
     misclassifications.
    </p>
    <p class="noindent">
     Soft margin
     <a class="glossary" href="DataScienceIILectureBookli2.html#glo:svm">
      SVM
     </a>
     allows for some margin violations, meaning that it permits certain data points to fall within the margin or even
     on the wrong side of the decision boundary. This adaptability is managed by a factor called <span style="color:#054C5C;"><code class="verb">C</code></span>, also called the “regularisation
     parameter”, which helps find a balance between making the gap as big as possible and reducing mistakes in grouping
     things.
    </p>
    <p class="noindent">
     When creating an
     <a class="glossary" href="DataScienceIILectureBookli2.html#glo:svm">
      SVM
     </a>
     model using <span style="color:#054C5C;"><code class="verb">sklearn</code></span>, you can specify several hyperparameters, including the regularisation
     hyperparameter <span style="color:#054C5C;"><code class="verb">C</code></span>.
    </p>
    <div class="informationblock" id="tcolobox-3">
     <div class="title">
      <p class="noindent">
       <span id="bold" style="font-weight:bold;">
        Information
       </span>
       : Hyperparameter <span style="color:#054C5C;"><code class="verb">C</code></span>
      </p>
     </div>
     <div class="box-content">
      <p class="noindent">
       Regularisation parameter. The strength of the regularisation is inversely proportional to <span style="color:#054C5C;"><code class="verb">C</code></span>
       and must be
       <span id="bold" style="font-weight:bold;">
        strictly
positive
       </span>
       .                     The                     penalty                     is                     a                     squared
       <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
        <msub>
         <mrow>
          <mi>
           ℓ
          </mi>
         </mrow>
         <mrow>
          <mn>
           2
          </mn>
         </mrow>
        </msub>
       </math>
       penalty. <span style="color:#054C5C;"><code class="verb">float, default=1.0</code></span>
      </p>
     </div>
    </div>
    <p class="noindent">
     Setting the <span style="color:#054C5C;"><code class="verb">C</code></span>
     to a low value, then we end up with the model on the left of
     <span id="bold" style="font-weight:bold;">
      Fig.
     </span>
     <a href="#x3-5004r4">
      1.4
     </a>. However, if we were to set a high value, we
     get the model on the right. As can be seen, reducing <span style="color:#054C5C;"><code class="verb">C</code></span>
     makes the street larger, but it also leads to more margin
     violations.
    </p>
    <p class="noindent">
     In other words, reducing <span style="color:#054C5C;"><code class="verb">C</code></span>
     results in more instances supporting the street, so there’s less risk of overfitting. But if you reduce it
     too much, then the model ends up underfitting, as seems to be the case here:
    </p>
    <div class="quoteblock">
     <p class="noindent">
      the
      model
      with <span style="color:#054C5C;"><code class="verb">C=100</code></span>
      looks
      like
      it
      will
      generalise
      better
      than
      the
      one
      with <span style="color:#054C5C;"><code class="verb">C=1</code></span>.
     </p>
    </div>
    <div class="figure">
     <p class="noindent">
      <img alt="PIC" height="" src="codes/images/Support-Vector-Machines/regularization_plot-.svg" width="150%"/>
      <a id="x3-5004r4">
      </a>
     </p>
     <figcaption class="caption">
      <span class="id">
       Figure
       1.4:
      </span>
      <span class="content">
       Large margin (left) v. fewer margin violations (right).
      </span>
     </figcaption>
    </div>
    <div class="knowledge">
     <p class="noindent">
      If your
      <a class="glossary" href="DataScienceIILectureBookli2.html#glo:svm">
       SVM
      </a>
      model is overfitting, you can try regularizing it by reducing <span style="color:#054C5C;"><code class="verb">C</code></span>.
     </p>
    </div>
    <p class="noindent">
     The following <span style="color:#054C5C;"><code class="verb">sklearn</code></span>
     code loads the iris dataset and trains a linear
     <a class="glossary" href="DataScienceIILectureBookli2.html#glo:svm">
      SVM
     </a>
     classifier to detect
     <italic>
      Iris virginica
     </italic>
     flowers. The pipeline
     first scales the features, then uses a <span style="color:#054C5C;"><code class="verb">LinearSVC</code></span>
     with <span style="color:#054C5C;"><code class="verb">C=1</code></span>
     :
    </p>
    <pre><div id="fancyvrb1" style="padding:20px;border-radius: 3px;"><a id="x3-5006r218"></a><span style="color:#2B2BFF;">from</span><span style="color:#BABABA;"> </span>sklearn.datasets<span style="color:#BABABA;"> </span><span style="color:#2B2BFF;">import</span> load_iris 
<a id="x3-5008r219"></a><span style="color:#2B2BFF;">from</span><span style="color:#BABABA;"> </span>sklearn.pipeline<span style="color:#BABABA;"> </span><span style="color:#2B2BFF;">import</span> make_pipeline 
<a id="x3-5010r220"></a><span style="color:#2B2BFF;">from</span><span style="color:#BABABA;"> </span>sklearn.preprocessing<span style="color:#BABABA;"> </span><span style="color:#2B2BFF;">import</span> StandardScaler 
<a id="x3-5012r221"></a><span style="color:#2B2BFF;">from</span><span style="color:#BABABA;"> </span>sklearn.svm<span style="color:#BABABA;"> </span><span style="color:#2B2BFF;">import</span> LinearSVC 
<a id="x3-5014r222"></a>iris = load_iris(as_frame=<span style="color:#2B2BFF;"><span id="bold" style="font-weight:bold;">True</span></span>) 
<a id="x3-5016r223"></a>X = iris.data[[<span style="color:#800080;">"petal length (cm)"</span>, <span style="color:#800080;">"petal width (cm)"</span>]].values 
<a id="x3-5018r224"></a>y = (iris.target == <span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">2</span></span>) <span style="color:#008700;"><italic># Iris virginica</italic></span> 
<a id="x3-5020r225"></a>svm_clf = make_pipeline(StandardScaler(), 
<a id="x3-5022r226"></a>LinearSVC(C=<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">1</span></span>, random_state=<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">42</span></span>)) 
<a id="x3-5024r227"></a>svm_clf.fit(X, y)</div></pre>
    <p class="noindent">
     The resulting model is represented on the left in
     <span id="bold" style="font-weight:bold;">
      Fig.
     </span>
     <a href="#x3-5004r4">
      1.4
     </a>. Then, as usual, we can use the model to make predictions:
    </p>
    <pre><div id="fancyvrb2" style="padding:20px;border-radius: 3px;"><a id="x3-5026r234"></a>X_new = [[<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">5.5</span></span>, <span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">1.7</span></span>], [<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">5.0</span></span>, <span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">1.5</span></span>]] 
<a id="x3-5028r235"></a><span style="color:#2B2BFF;">print</span>(svm_clf.predict(X_new))</div></pre>
    <div class="tcolorbox tcolorbox" id="tcolobox-4">
     <div class="tcolorbox-content">
      <pre><div id="fancyvrb3" style="padding:20px;border-radius: 3px;"><a id="x3-5030r1"></a>[ True False]</div></pre>
     </div>
    </div>
    <p class="noindent">
     The first plant is classified as an
     <italic>
      Iris virginica
     </italic>
     , while the second is
     <span id="bold" style="font-weight:bold;">
      NOT
     </span>
     . Let us look at the scores that the
     <a class="glossary" href="DataScienceIILectureBookli2.html#glo:svm">
      SVM
     </a>
     used to make these predictions. These measure the signed distance between each instance and the decision
boundary:
    </p>
    <pre><div id="fancyvrb4" style="padding:20px;border-radius: 3px;"><a id="x3-5032r245"></a><span style="color:#2B2BFF;">print</span>(svm_clf.decision_function(X_new))</div></pre>
    <div class="tcolorbox tcolorbox" id="tcolobox-5">
     <div class="tcolorbox-content">
      <pre><div id="fancyvrb5" style="padding:20px;border-radius: 3px;"><a id="x3-5034r1"></a>[ 0.66163816 -0.22035761]</div></pre>
     </div>
    </div>
    <p class="noindent">
     Unlike the <span style="color:#054C5C;"><code class="verb">LogisticRegression</code></span>
     class, <span style="color:#054C5C;"><code class="verb">LinearSVC</code></span>
     doesn’t have a <span style="color:#054C5C;"><code class="verb">predict_proba()</code></span>
     method to estimate the class probabilities. That
said, if you use the SVC class instead of <span style="color:#054C5C;"><code class="verb">LinearSVC</code></span>, and if you set its probability hyperparameter to <span style="color:#054C5C;"><code class="verb">True</code></span>, then
the model will fit an extra model at the end of training to map the
     <a class="glossary" href="DataScienceIILectureBookli2.html#glo:svm">
      SVM
     </a>
     decision function scores to estimated
probabilities.
    </p>
    <p class="noindent">
     Under the hood, this requires using 5-fold cross-validation to generate out-of-sample predictions for every instance in the training
set, then training a <span style="color:#054C5C;"><code class="verb">LogisticRegression</code></span>
     model, so it will slow down training considerably. After that, the <span style="color:#054C5C;"><code class="verb">predict_proba()</code></span>
     and <span style="color:#054C5C;"><code class="verb">predict_log_proba()</code></span>
     methods will be available.
    </p>
    <p class="noindent">
    </p>
    <h2 class="sectionHead">
     <a aria-label="Link to this section in PDF version" class="pdf-link" href="DataScienceIILectureBook.pdf#section.1.3" style="color: red;  position: absolute;  left: -30px;" target="_blank">
      <svg class="bi bi-file-pdf" fill="currentColor" height="20" viewbox="0 0 16 16" width="20" xmlns="http://www.w3.org/2000/svg">
       <path d="M4 0a2 2 0 0 0-2 2v12a2 2 0 0 0 2 2h8a2 2 0 0 0 2-2V2a2 2 0 0 0-2-2H4zm0 1h8a1 1 0 0 1 1 1v12a1 1 0 0 1-1 1H4a1 1 0 0 1-1-1V2a1 1 0 0 1 1-1z">
       </path>
       <path d="M4.603 12.087a.81.81 0 0 1-.438-.42c-.195-.388-.13-.776.08-1.102.198-.307.526-.568.897-.787a7.68 7.68 0 0 1 1.482-.645 19.701 19.701 0 0 0 1.062-2.227 7.269 7.269 0 0 1-.43-1.295c-.086-.4-.119-.796-.046-1.136.075-.354.274-.672.65-.823.192-.077.4-.12.602-.077a.7.7 0 0 1 .477.365c.088.164.12.356.127.538.007.187-.012.395-.047.614-.084.51-.27 1.134-.52 1.794a10.954 10.954 0 0 0 .98 1.686 5.753 5.753 0 0 1 1.334.05c.364.065.734.195.96.465.12.144.193.32.2.518.007.192-.047.382-.138.563a1.04 1.04 0 0 1-.354.416.856.856 0 0 1-.51.138c-.331-.014-.654-.196-.933-.417a5.716 5.716 0 0 1-.911-.95 11.642 11.642 0 0 0-1.997.406 11.311 11.311 0 0 1-1.021 1.51c-.29.35-.608.655-.926.787a.793.793 0 0 1-.58.029zm1.379-1.901c-.166.076-.32.156-.459.238-.328.194-.541.383-.647.547-.094.145-.096.25-.04.361.01.022.02.036.026.044a.27.27 0 0 0 .035-.012c.137-.056.355-.235.635-.572a8.18 8.18 0 0 0 .45-.606zm1.64-1.33a12.647 12.647 0 0 1 1.01-.193 11.666 11.666 0 0 1-.51-.858 20.741 20.741 0 0 1-.5 1.05zm2.446.45c.15.162.296.3.435.41.24.19.407.253.498.256a.107.107 0 0 0 .07-.015.307.307 0 0 0 .094-.125.436.436 0 0 0 .059-.2.095.095 0 0 0-.026-.063c-.052-.062-.2-.152-.518-.209a3.881 3.881 0 0 0-.612-.053zM8.078 5.8a6.7 6.7 0 0 0 .2-.828c.031-.188.043-.343.038-.465a.613.613 0 0 0-.032-.198.517.517 0 0 0-.145.04c-.087.035-.158.106-.196.283-.04.192-.03.469.046.822.024.111.054.227.09.346z">
       </path>
      </svg>
     </a>
     <small class="titlemark">
      1.3
     </small>
     <a id="x3-60001.3">
     </a>
     Nonlinear
     <a class="glossary" href="DataScienceIILectureBookli2.html#glo:svm">
      SVM
     </a>
     Classification
    </h2>
    <p class="noindent">
     Although linear
     <a class="glossary" href="DataScienceIILectureBookli2.html#glo:svm">
      SVM
     </a>
     classifiers are efficient and often work surprisingly well, many datasets are not even close to
being linearly separable. One approach to handling nonlinear datasets is to add more features, such as polynomial
features.
    </p>
    <p class="noindent">
     In some cases this can result in a linearly separable dataset. Consider the LHS plot in
     <span id="bold" style="font-weight:bold;">
      Fig.
     </span>
     <a href="#x3-6001r5">
      1.5
     </a>
     : it represents a simple dataset with just
one feature,
     <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
      <msubsup>
       <mrow>
        <mi>
         x
        </mi>
       </mrow>
       <mrow>
        <mn>
         1
        </mn>
       </mrow>
       <mrow>
       </mrow>
      </msubsup>
     </math>
     .
This dataset is not linearly separable, as you can see. But adding a second feature
     <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
      <msubsup>
       <mrow>
        <mi>
         x
        </mi>
       </mrow>
       <mrow>
        <mn>
         2
        </mn>
       </mrow>
       <mrow>
       </mrow>
      </msubsup>
      <mo class="MathClass-rel" stretchy="false">
       =
      </mo>
      <msubsup>
       <mrow>
        <mi>
         x
        </mi>
       </mrow>
       <mrow>
        <mn>
         1
        </mn>
       </mrow>
       <mrow>
        <mn>
         2
        </mn>
       </mrow>
      </msubsup>
     </math>
     , the
resulting 2D dataset is perfectly linearly separable.
    </p>
    <div class="figure">
     <p class="noindent">
      <img alt="PIC" height="" src="codes/images/Support-Vector-Machines/higher_dimensions_plot-.svg" width="150%"/>
      <a id="x3-6001r5">
      </a>
     </p>
     <figcaption class="caption">
      <span class="id">
       Figure 1.5:
      </span>
      <span class="content">
       Adding features to make a dataset linearly separable.
      </span>
     </figcaption>
    </div>
    <p class="noindent">
     To implement this idea using <span style="color:#054C5C;"><code class="verb">sklearn</code></span>, we can create a pipeline containing a <span style="color:#054C5C;"><code class="verb">PolynomialFeatures</code></span>
     transformer, followed by a <span style="color:#054C5C;"><code class="verb">StandardScaler</code></span>
     and a <span style="color:#054C5C;"><code class="verb">LinearSVC</code></span>
     classifier.
    </p>
    <div class="informationblock" id="tcolobox-6">
     <div class="title">
      <p class="noindent">
       <span id="bold" style="font-weight:bold;">
        Information
       </span>
       : Pipeline
      </p>
     </div>
     <div class="box-content">
      <p class="noindent">
       A series of interconnected data processing and modeling steps for streamlining the process of working with
       <a class="glossary" href="DataScienceIILectureBookli2.html#glo:ml">
        ML
       </a>
       models.
      </p>
     </div>
    </div>
    <p class="noindent">
     Let’s test this on the
     <alert style="color: #821131;">
      moons dataset
     </alert>
     , a toy dataset for binary classification in which the data points are shaped as two <alert style="color: #821131;">(2)</alert>
interleaving crescent moons.
    </p>
    <p class="noindent">
     We can generate this dataset using the <span style="color:#054C5C;"><code class="verb">make_moons()</code></span>
     function:
    </p>
    <pre><div id="fancyvrb6" style="padding:20px;border-radius: 3px;"><a id="x3-6003r315"></a>plt.figure(figsize=(<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">10</span></span>, <span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">3</span></span>)) 
<a id="x3-6005r316"></a> 
<a id="x3-6007r317"></a>plt.subplot(<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">121</span></span>) 
<a id="x3-6009r318"></a>plt.grid(<span style="color:#2B2BFF;"><span id="bold" style="font-weight:bold;">True</span></span>) 
<a id="x3-6011r319"></a>plt.axhline(y=<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">0</span></span>, color=<span style="color:#800080;">'k'</span>) 
<a id="x3-6013r320"></a>plt.plot(X1D[:, <span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">0</span></span>][y==<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">0</span></span>], np.zeros(<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">4</span></span>), <span style="color:#800080;">"bs"</span>) 
<a id="x3-6015r321"></a>plt.plot(X1D[:, <span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">0</span></span>][y==<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">1</span></span>], np.zeros(<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">5</span></span>), <span style="color:#800080;">"g^"</span>) 
<a id="x3-6017r322"></a>plt.gca().get_yaxis().set_ticks([]) 
<a id="x3-6019r323"></a>plt.xlabel(<span style="color:#800080;">"$x_1$"</span>) 
<a id="x3-6021r324"></a>plt.axis([-<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">4.5</span></span>, <span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">4.5</span></span>, -<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">0.2</span></span>, <span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">0.2</span></span>]) 
<a id="x3-6023r325"></a></div></pre>
    <div class="figure">
     <p class="noindent">
      <img alt="PIC" height="" src="codes/images/Support-Vector-Machines/moons_polynomial_svc_plot-.svg" width="150%"/>
      <a id="x3-6024r6">
      </a>
     </p>
     <figcaption class="caption">
      <span class="id">
       Figure 1.6:
      </span>
      <span class="content">
       Linear
       <a class="glossary" href="DataScienceIILectureBookli2.html#glo:svm">
        SVM
       </a>
       Classifier using polynomial features.
      </span>
     </figcaption>
    </div>
    <h3 class="subsectionHead">
     <a aria-label="Link to this section in PDF version" class="pdf-link" href="DataScienceIILectureBook.pdf#subsection.1.3.1" style="color: red;  position: absolute;  left: -30px;" target="_blank">
      <svg class="bi bi-file-pdf" fill="currentColor" height="20" viewbox="0 0 16 16" width="20" xmlns="http://www.w3.org/2000/svg">
       <path d="M4 0a2 2 0 0 0-2 2v12a2 2 0 0 0 2 2h8a2 2 0 0 0 2-2V2a2 2 0 0 0-2-2H4zm0 1h8a1 1 0 0 1 1 1v12a1 1 0 0 1-1 1H4a1 1 0 0 1-1-1V2a1 1 0 0 1 1-1z">
       </path>
       <path d="M4.603 12.087a.81.81 0 0 1-.438-.42c-.195-.388-.13-.776.08-1.102.198-.307.526-.568.897-.787a7.68 7.68 0 0 1 1.482-.645 19.701 19.701 0 0 0 1.062-2.227 7.269 7.269 0 0 1-.43-1.295c-.086-.4-.119-.796-.046-1.136.075-.354.274-.672.65-.823.192-.077.4-.12.602-.077a.7.7 0 0 1 .477.365c.088.164.12.356.127.538.007.187-.012.395-.047.614-.084.51-.27 1.134-.52 1.794a10.954 10.954 0 0 0 .98 1.686 5.753 5.753 0 0 1 1.334.05c.364.065.734.195.96.465.12.144.193.32.2.518.007.192-.047.382-.138.563a1.04 1.04 0 0 1-.354.416.856.856 0 0 1-.51.138c-.331-.014-.654-.196-.933-.417a5.716 5.716 0 0 1-.911-.95 11.642 11.642 0 0 0-1.997.406 11.311 11.311 0 0 1-1.021 1.51c-.29.35-.608.655-.926.787a.793.793 0 0 1-.58.029zm1.379-1.901c-.166.076-.32.156-.459.238-.328.194-.541.383-.647.547-.094.145-.096.25-.04.361.01.022.02.036.026.044a.27.27 0 0 0 .035-.012c.137-.056.355-.235.635-.572a8.18 8.18 0 0 0 .45-.606zm1.64-1.33a12.647 12.647 0 0 1 1.01-.193 11.666 11.666 0 0 1-.51-.858 20.741 20.741 0 0 1-.5 1.05zm2.446.45c.15.162.296.3.435.41.24.19.407.253.498.256a.107.107 0 0 0 .07-.015.307.307 0 0 0 .094-.125.436.436 0 0 0 .059-.2.095.095 0 0 0-.026-.063c-.052-.062-.2-.152-.518-.209a3.881 3.881 0 0 0-.612-.053zM8.078 5.8a6.7 6.7 0 0 0 .2-.828c.031-.188.043-.343.038-.465a.613.613 0 0 0-.032-.198.517.517 0 0 0-.145.04c-.087.035-.158.106-.196.283-.04.192-.03.469.046.822.024.111.054.227.09.346z">
       </path>
      </svg>
     </a>
     <small class="titlemark">
      1.3.1
     </small>
     <a id="x3-70001.3.1">
     </a>
     Polynomial Kernel
    </h3>
    <p class="noindent">
     Adding polynomial features is simple to implement and can work great with all sorts of
     <a class="glossary" href="DataScienceIILectureBookli2.html#glo:ml">
      ML
     </a>
     algorithms, and not just
     <a class="glossary" href="DataScienceIILectureBookli2.html#glo:svm">
      SVM
     </a>
     s. That
said, at a low polynomial degree this method cannot deal with very complex datasets, and with a high polynomial degree it
creates a huge number of features, making the model too slow.
    </p>
    <p class="noindent">
     Fortunately, when using
     <a class="glossary" href="DataScienceIILectureBookli2.html#glo:svm">
      SVM
     </a>
     s you can apply a mathematical technique called the kernel trick. The kernel trick makes it possible
to get the same result as if you had added many polynomial features, even with a very high degree, without actually having to
add them.
    </p>
    <div class="informationblock" id="tcolobox-7">
     <div class="title">
      <p class="noindent">
       <span id="bold" style="font-weight:bold;">
        Information
       </span>
       : Kernel Trick
      </p>
     </div>
     <div class="box-content">
      <p class="noindent">
       A method used in
       <a class="glossary" href="DataScienceIILectureBookli2.html#glo:svm">
        SVM
       </a>
       s to enable them to classify non-linear data using a linear classifier. By applying a
kernel function, SVMs can implicitly map input data into a higher-dimensional space where a linear separator
(hyperplane) can be used to divide the classes. This mapping is computationally efficient because it avoids the
direct calculation of the coordinates in this higher space.
      </p>
     </div>
    </div>
    <div class="warning">
     <p class="noindent">
      This means there’s no combinatorial explosion of the number of features.
     </p>
    </div>
    <p class="noindent">
     This trick is implemented by the SVC class. Let’s test it on the moons dataset:
    </p>
    <p class="noindent">
    </p>
    <pre><div id="fancyvrb7" style="padding:20px;border-radius: 3px;"><a id="x3-7002r356"></a><span style="color:#008700;"><italic>#+end_src</italic></span> 
<a id="x3-7004r357"></a> 
<a id="x3-7006r358"></a><span style="color:#008700;"><italic>#+NAME: SVM-A4</italic></span> 
<a id="x3-7008r359"></a><span style="color:#008700;"><italic>#+begin_src python :session svm :results none</italic></span> 
<a id="x3-7010r360"></a><span style="color:#2B2BFF;">def</span><span style="color:#BABABA;"> </span><italic><span id="bold" style="font-weight:bold;">plot_dataset</span></italic>(X, y, axes): 
<a id="x3-7012r361"></a>   plt.plot(X[:, <span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">0</span></span>][y==<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">0</span></span>], X[:, <span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">1</span></span>][y==<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">0</span></span>], <span style="color:#800080;">"bs"</span>) 
<a id="x3-7014r362"></a>   plt.plot(X[:, <span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">0</span></span>][y==<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">1</span></span>], X[:, <span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">1</span></span>][y==<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">1</span></span>], <span style="color:#800080;">"g^"</span>)</div></pre>
    <p class="noindent">
     This code trains an
     <a class="glossary" href="DataScienceIILectureBookli2.html#glo:svm">
      SVM
     </a>
     classifier using a third-degree polynomial kernel, represented on the left in Figure
     <a href="#x3-7015r7">
      1.7
     </a>. On the right is another
     <a class="glossary" href="DataScienceIILectureBookli2.html#glo:svm">
      SVM
     </a>
     classifier using a
     <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
      <msup>
       <mrow>
        <mstyle class="text">
         <mtext>
          10
         </mtext>
        </mstyle>
       </mrow>
       <mrow>
        <mstyle class="text">
         <mtext>
          th
         </mtext>
        </mstyle>
       </mrow>
      </msup>
     </math>
     degree polynomial kernel. Obviously, if your model is overfitting, you might want to reduce the polynomial degree. Conversely, if
it is underfitting, you can try increasing it. The hyperparameter <span style="color:#054C5C;"><code class="verb">coef0</code></span>
     controls how much the model is influenced by high-degree
terms versus low-degree terms.
    </p>
    <div class="figure">
     <p class="noindent">
      <img alt="PIC" height="" src="codes/images/Support-Vector-Machines/moons_kernelized_polynomial_svc_plot-.svg" width="150%"/>
      <a id="x3-7015r7">
      </a>
     </p>
     <figcaption class="caption">
      <span class="id">
       Figure 1.7:
      </span>
      <span class="content">
       <a class="glossary" href="DataScienceIILectureBookli2.html#glo:svm">
        SVM
       </a>
       classifiers with a polynomial kernel.
      </span>
     </figcaption>
    </div>
    <div class="knowledge">
     <p class="noindent">
      Although hyperparameters will generally be tuned automatically (e.g., using randomised search), it’s good to have a sense of
what each hyperparameter actually does and how it may interact with other hyperparameters: this way, you can narrow the
search to a much smaller space.
     </p>
    </div>
    <h3 class="subsectionHead">
     <a aria-label="Link to this section in PDF version" class="pdf-link" href="DataScienceIILectureBook.pdf#subsection.1.3.2" style="color: red;  position: absolute;  left: -30px;" target="_blank">
      <svg class="bi bi-file-pdf" fill="currentColor" height="20" viewbox="0 0 16 16" width="20" xmlns="http://www.w3.org/2000/svg">
       <path d="M4 0a2 2 0 0 0-2 2v12a2 2 0 0 0 2 2h8a2 2 0 0 0 2-2V2a2 2 0 0 0-2-2H4zm0 1h8a1 1 0 0 1 1 1v12a1 1 0 0 1-1 1H4a1 1 0 0 1-1-1V2a1 1 0 0 1 1-1z">
       </path>
       <path d="M4.603 12.087a.81.81 0 0 1-.438-.42c-.195-.388-.13-.776.08-1.102.198-.307.526-.568.897-.787a7.68 7.68 0 0 1 1.482-.645 19.701 19.701 0 0 0 1.062-2.227 7.269 7.269 0 0 1-.43-1.295c-.086-.4-.119-.796-.046-1.136.075-.354.274-.672.65-.823.192-.077.4-.12.602-.077a.7.7 0 0 1 .477.365c.088.164.12.356.127.538.007.187-.012.395-.047.614-.084.51-.27 1.134-.52 1.794a10.954 10.954 0 0 0 .98 1.686 5.753 5.753 0 0 1 1.334.05c.364.065.734.195.96.465.12.144.193.32.2.518.007.192-.047.382-.138.563a1.04 1.04 0 0 1-.354.416.856.856 0 0 1-.51.138c-.331-.014-.654-.196-.933-.417a5.716 5.716 0 0 1-.911-.95 11.642 11.642 0 0 0-1.997.406 11.311 11.311 0 0 1-1.021 1.51c-.29.35-.608.655-.926.787a.793.793 0 0 1-.58.029zm1.379-1.901c-.166.076-.32.156-.459.238-.328.194-.541.383-.647.547-.094.145-.096.25-.04.361.01.022.02.036.026.044a.27.27 0 0 0 .035-.012c.137-.056.355-.235.635-.572a8.18 8.18 0 0 0 .45-.606zm1.64-1.33a12.647 12.647 0 0 1 1.01-.193 11.666 11.666 0 0 1-.51-.858 20.741 20.741 0 0 1-.5 1.05zm2.446.45c.15.162.296.3.435.41.24.19.407.253.498.256a.107.107 0 0 0 .07-.015.307.307 0 0 0 .094-.125.436.436 0 0 0 .059-.2.095.095 0 0 0-.026-.063c-.052-.062-.2-.152-.518-.209a3.881 3.881 0 0 0-.612-.053zM8.078 5.8a6.7 6.7 0 0 0 .2-.828c.031-.188.043-.343.038-.465a.613.613 0 0 0-.032-.198.517.517 0 0 0-.145.04c-.087.035-.158.106-.196.283-.04.192-.03.469.046.822.024.111.054.227.09.346z">
       </path>
      </svg>
     </a>
     <small class="titlemark">
      1.3.2
     </small>
     <a id="x3-80001.3.2">
     </a>
     Similarity Features
    </h3>
    <p class="noindent">
     Another technique to tackle non-linear problems is to add features computed using a similarity function, which measures how
much each instance resembles a particular landmark.
    </p>
    <div class="figure">
     <p class="noindent">
      <img alt="PIC" height="" src="codes/images/Support-Vector-Machines/kernel_method_plot-.svg" width="150%"/>
      <a id="x3-8001r8">
      </a>
     </p>
     <figcaption class="caption">
      <span class="id">
       Figure 1.8:
      </span>
      <span class="content">
       Similarity features using the Gaussian RBF.
      </span>
     </figcaption>
    </div>
    <p class="noindent">
     For example, let’s take the 1D dataset from earlier and add two landmarks to it at
     <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
      <msubsup>
       <mrow>
        <mi>
         x
        </mi>
       </mrow>
       <mrow>
        <mn>
         1
        </mn>
       </mrow>
       <mrow>
       </mrow>
      </msubsup>
      <mo class="MathClass-rel" stretchy="false">
       =
      </mo>
      <mo class="MathClass-bin" stretchy="false">
       −
      </mo>
      <mn>
       2
      </mn>
     </math>
     and
     <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
      <msubsup>
       <mrow>
        <mi>
         x
        </mi>
       </mrow>
       <mrow>
        <mn>
         1
        </mn>
       </mrow>
       <mrow>
       </mrow>
      </msubsup>
      <mo class="MathClass-rel" stretchy="false">
       =
      </mo>
      <mn>
       1
      </mn>
     </math>
     (see the left plot in
     <span id="bold" style="font-weight:bold;">
      Fig.
     </span>
     <a href="#x3-8001r8">
      1.8
     </a>
     ). Next, we’ll define the similarity function to be the Gaussian RBF with
     <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
      <mi>
       γ
      </mi>
      <mo class="MathClass-rel" stretchy="false">
       =
      </mo>
      <mn>
       0
      </mn>
      <mo class="MathClass-punc" stretchy="false">
       .
      </mo>
      <mn>
       3
      </mn>
     </math>
     . As it is
a Gaussian function, it is a bell-shaped function varying from 0 (very far away from the landmark) to 1 (at the landmark).
    </p>
    <table class="equation-star">
     <tr>
      <td>
       <math class="equation" display="block" xmlns="http://www.w3.org/1998/Math/MathML">
        <msub>
         <mrow>
          <mi>
           ϕ
          </mi>
         </mrow>
         <mrow>
          <mi>
           γ
          </mi>
         </mrow>
        </msub>
        <msup>
         <mrow>
          <mrow>
           <mo fence="true" form="prefix">
            (
           </mo>
           <mrow>
            <mover accent="true">
             <mrow>
              <mi>
               x
              </mi>
             </mrow>
             <mo accent="true">
              →
             </mo>
            </mover>
            <mo class="MathClass-punc" stretchy="false">
             ,
            </mo>
            <mspace class="thinspace" width="0.17em">
            </mspace>
            <mi>
             ℓ
            </mi>
           </mrow>
           <mo fence="true" form="postfix">
            )
           </mo>
          </mrow>
         </mrow>
         <mrow>
         </mrow>
        </msup>
        <mo class="MathClass-rel" stretchy="false">
         =
        </mo>
        <mi class="loglike">
         exp
        </mi>
        <mo>
         ⁡
        </mo>
        <msup>
         <mrow>
          <mrow>
           <mo fence="true" form="prefix">
            (
           </mo>
           <mrow>
            <mo class="MathClass-bin" stretchy="false">
             −
            </mo>
            <mi>
             γ
            </mi>
            <mo class="MathClass-rel" stretchy="false">
             ∣
            </mo>
            <mo class="MathClass-rel" stretchy="false">
             ∣
            </mo>
            <mover accent="true">
             <mrow>
              <mi>
               x
              </mi>
             </mrow>
             <mo accent="true">
              →
             </mo>
            </mover>
            <mo class="MathClass-bin" stretchy="false">
             −
            </mo>
            <mi>
             ℓ
            </mi>
            <mo class="MathClass-rel" stretchy="false">
             ∣
            </mo>
            <msup>
             <mrow>
              <mo class="MathClass-rel" stretchy="false">
               ∣
              </mo>
             </mrow>
             <mrow>
              <mn>
               2
              </mn>
             </mrow>
            </msup>
           </mrow>
           <mo fence="true" form="postfix">
            )
           </mo>
          </mrow>
         </mrow>
         <mrow>
         </mrow>
        </msup>
       </math>
      </td>
     </tr>
    </table>
    <p class="noindent">
     Now we are ready to compute the new features. For example, let’s look at the instance
     <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
      <msubsup>
       <mrow>
        <mi>
         x
        </mi>
       </mrow>
       <mrow>
        <mn>
         1
        </mn>
       </mrow>
       <mrow>
       </mrow>
      </msubsup>
      <mo class="MathClass-rel" stretchy="false">
       =
      </mo>
      <mo class="MathClass-bin" stretchy="false">
       −
      </mo>
      <mn>
       1
      </mn>
     </math>
     . It is
located at a distance of 1 from the first landmark and 2 from the second landmark. Therefore, its new features are:
    </p>
    <table class="equation-star">
     <tr>
      <td>
       <math class="equation" display="block" xmlns="http://www.w3.org/1998/Math/MathML">
        <msubsup>
         <mrow>
          <mi>
           x
          </mi>
         </mrow>
         <mrow>
          <mn>
           2
          </mn>
         </mrow>
         <mrow>
         </mrow>
        </msubsup>
        <mo class="MathClass-rel" stretchy="false">
         =
        </mo>
        <msubsup>
         <mrow>
          <mi>
           e
          </mi>
         </mrow>
         <mrow>
         </mrow>
         <mrow>
          <mo class="MathClass-bin" stretchy="false">
           −
          </mo>
          <mn>
           0
          </mn>
          <mo class="MathClass-punc" stretchy="false">
           .
          </mo>
          <mn>
           3
          </mn>
          <mo class="MathClass-bin" stretchy="false">
           ×
          </mo>
          <mn>
           1
          </mn>
         </mrow>
        </msubsup>
        <mo class="MathClass-rel" stretchy="false">
         =
        </mo>
        <mn>
         0
        </mn>
        <mo class="MathClass-punc" stretchy="false">
         .
        </mo>
        <mn>
         7
        </mn>
        <mn>
         5
        </mn>
        <mspace class="qquad" width="2em">
        </mspace>
        <mspace class="qquad" width="2em">
        </mspace>
        <msubsup>
         <mrow>
          <mi>
           x
          </mi>
         </mrow>
         <mrow>
          <mn>
           3
          </mn>
         </mrow>
         <mrow>
         </mrow>
        </msubsup>
        <mo class="MathClass-rel" stretchy="false">
         =
        </mo>
        <msubsup>
         <mrow>
          <mi>
           e
          </mi>
         </mrow>
         <mrow>
         </mrow>
         <mrow>
          <mo class="MathClass-bin" stretchy="false">
           −
          </mo>
          <mn>
           0
          </mn>
          <mo class="MathClass-punc" stretchy="false">
           .
          </mo>
          <mn>
           3
          </mn>
          <mo class="MathClass-bin" stretchy="false">
           ×
          </mo>
          <mn>
           4
          </mn>
         </mrow>
        </msubsup>
        <mo class="MathClass-rel" stretchy="false">
         =
        </mo>
        <mn>
         0
        </mn>
        <mo class="MathClass-punc" stretchy="false">
         .
        </mo>
        <mn>
         3
        </mn>
       </math>
      </td>
     </tr>
    </table>
    <p class="noindent">
     The plot on the right in Figure
     <a href="#x3-8001r8">
      1.8
     </a>
     shows the transformed dataset (dropping the original features). As you can see, it is now
linearly separable.
    </p>
    <p class="noindent">
     You may wonder how to select the landmarks. The simplest approach is to create a landmark at the location of each and every
instance in the dataset. Doing that creates many dimensions and thus increases the chances that the transformed training set will
be linearly separable.
    </p>
    <p class="noindent">
     The downside is that a training set with
     <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
      <mi>
       m
      </mi>
     </math>
     instances and
     <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
      <mi>
       n
      </mi>
     </math>
     features gets
transformed into a training set with
     <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
      <mi>
       m
      </mi>
     </math>
     instances and
     <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
      <mi>
       m
      </mi>
     </math>
     features (assuming you drop the original features).
    </p>
    <div class="warning">
     <p class="noindent">
      A very large training set, ends up with an equally large number of features.
     </p>
    </div>
    <h3 class="subsectionHead">
     <a aria-label="Link to this section in PDF version" class="pdf-link" href="DataScienceIILectureBook.pdf#subsection.1.3.3" style="color: red;  position: absolute;  left: -30px;" target="_blank">
      <svg class="bi bi-file-pdf" fill="currentColor" height="20" viewbox="0 0 16 16" width="20" xmlns="http://www.w3.org/2000/svg">
       <path d="M4 0a2 2 0 0 0-2 2v12a2 2 0 0 0 2 2h8a2 2 0 0 0 2-2V2a2 2 0 0 0-2-2H4zm0 1h8a1 1 0 0 1 1 1v12a1 1 0 0 1-1 1H4a1 1 0 0 1-1-1V2a1 1 0 0 1 1-1z">
       </path>
       <path d="M4.603 12.087a.81.81 0 0 1-.438-.42c-.195-.388-.13-.776.08-1.102.198-.307.526-.568.897-.787a7.68 7.68 0 0 1 1.482-.645 19.701 19.701 0 0 0 1.062-2.227 7.269 7.269 0 0 1-.43-1.295c-.086-.4-.119-.796-.046-1.136.075-.354.274-.672.65-.823.192-.077.4-.12.602-.077a.7.7 0 0 1 .477.365c.088.164.12.356.127.538.007.187-.012.395-.047.614-.084.51-.27 1.134-.52 1.794a10.954 10.954 0 0 0 .98 1.686 5.753 5.753 0 0 1 1.334.05c.364.065.734.195.96.465.12.144.193.32.2.518.007.192-.047.382-.138.563a1.04 1.04 0 0 1-.354.416.856.856 0 0 1-.51.138c-.331-.014-.654-.196-.933-.417a5.716 5.716 0 0 1-.911-.95 11.642 11.642 0 0 0-1.997.406 11.311 11.311 0 0 1-1.021 1.51c-.29.35-.608.655-.926.787a.793.793 0 0 1-.58.029zm1.379-1.901c-.166.076-.32.156-.459.238-.328.194-.541.383-.647.547-.094.145-.096.25-.04.361.01.022.02.036.026.044a.27.27 0 0 0 .035-.012c.137-.056.355-.235.635-.572a8.18 8.18 0 0 0 .45-.606zm1.64-1.33a12.647 12.647 0 0 1 1.01-.193 11.666 11.666 0 0 1-.51-.858 20.741 20.741 0 0 1-.5 1.05zm2.446.45c.15.162.296.3.435.41.24.19.407.253.498.256a.107.107 0 0 0 .07-.015.307.307 0 0 0 .094-.125.436.436 0 0 0 .059-.2.095.095 0 0 0-.026-.063c-.052-.062-.2-.152-.518-.209a3.881 3.881 0 0 0-.612-.053zM8.078 5.8a6.7 6.7 0 0 0 .2-.828c.031-.188.043-.343.038-.465a.613.613 0 0 0-.032-.198.517.517 0 0 0-.145.04c-.087.035-.158.106-.196.283-.04.192-.03.469.046.822.024.111.054.227.09.346z">
       </path>
      </svg>
     </a>
     <small class="titlemark">
      1.3.3
     </small>
     <a id="x3-90001.3.3">
     </a>
     Gaussian RBF Kernel
    </h3>
    <p class="noindent">
     Just like the polynomial features method, the similarity features method can be useful with any
     <a class="glossary" href="DataScienceIILectureBookli2.html#glo:ml">
      ML
     </a>
     algorithm, but it may be
computationally expensive to compute all the additional features (especially on large training sets). Once again the kernel trick
can be used here, making it possible to obtain a similar result as if you had added many similarity features, but without actually
doing so. Let’s try the SVC class with the Gaussian RBF kernel:
    </p>
    <p class="noindent">
    </p>
    <pre><div id="fancyvrb8" style="padding:20px;border-radius: 3px;"><a id="x3-9002r491"></a>rbf_kernel_svm_clf = make_pipeline( 
<a id="x3-9004r492"></a>   StandardScaler(), 
<a id="x3-9006r493"></a>   SVC(kernel=<span style="color:#800080;">"rbf"</span>, gamma=<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">5</span></span>, C=<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">0.001</span></span>) 
<a id="x3-9008r494"></a>) 
<a id="x3-9010r495"></a>rbf_kernel_svm_clf.fit(X, y)</div></pre>
    <p class="noindent">
     This model is represented at the bottom left in Figure
     <a href="#x3-9011r9">
      1.9
     </a>. The other plots show models trained with different values of hyperparameters
gamma (
     <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
      <mi>
       γ
      </mi>
     </math>
     )
and <span style="color:#054C5C;"><code class="verb">C</code></span>.
    </p>
    <p class="noindent">
     Increasing gamma makes the bell-shaped curve narrower (see the LHS plots in Figure
     <a href="#x3-9011r9">
      1.9
     </a>
     ). As a result, each instance’s range of
influence is
     <span id="bold" style="font-weight:bold;">
      smaller
     </span>
     . The decision boundary ends up being more irregular, wiggling around individual instances. Conversely, a
small gamma value makes the bell-shaped curve wider: instances have a larger range of influence, and the decision boundary ends
up smoother.
    </p>
    <p class="noindent">
     Therefore
     <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
      <mi>
       γ
      </mi>
     </math>
     acts like a
     <span id="bold" style="font-weight:bold;">
      regularisation hyperparameter
     </span>
     : if your model is overfitting, you should reduce
     <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
      <mi>
       γ
      </mi>
     </math>
     ; if it is underfitting,
you should increase
     <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
      <mi>
       γ
      </mi>
     </math>
     (similar to the <span style="color:#054C5C;"><code class="verb">C</code></span>
     hyperparameter).
    </p>
    <div class="figure">
     <p class="noindent">
      <img alt="PIC" height="" src="codes/images/Support-Vector-Machines/moons_rbf_svc_plot-.svg" width="150%"/>
      <a id="x3-9011r9">
      </a>
     </p>
     <figcaption class="caption">
      <span class="id">
       Figure 1.9:
      </span>
      <span class="content">
       <a class="glossary" href="DataScienceIILectureBookli2.html#glo:svm">
        SVM
       </a>
       classifiers using an RBF kernel.
      </span>
     </figcaption>
    </div>
    <p class="noindent">
     Other kernels exist but are used much more rarely. Some kernels are specialized for specific data structures. String kernels are
sometimes used when classifying text documents or DNA sequences (e.g., using the string subsequence kernel or kernels based on
the Levenshtein distance).
    </p>
    <div class="knowledge">
     <p class="noindent">
      You need to choose the right kernel for the job. As a rule of thumb, you should always try the linear kernel first. The <span style="color:#054C5C;"><code class="verb">LinearSVC</code></span>
      class is much faster than <span style="color:#054C5C;"><code class="verb">SVC(kernel="linear")</code></span>, especially if the training set is very large. If it is not too large,
you should also try kernelised
      <a class="glossary" href="DataScienceIILectureBookli2.html#glo:svm">
       SVM
      </a>
      s, starting with the Gaussian RBF kernel; it often works really well. Then, if
you have spare time and computing power, you can experiment with a few other kernels using hyperparameter
search.
      If there are kernels specialized for your training set’s data structure, make sure to give them a try too
     </p>
    </div>
    <p class="noindent">
    </p>
    <h2 class="sectionHead">
     <a aria-label="Link to this section in PDF version" class="pdf-link" href="DataScienceIILectureBook.pdf#section.1.4" style="color: red;  position: absolute;  left: -30px;" target="_blank">
      <svg class="bi bi-file-pdf" fill="currentColor" height="20" viewbox="0 0 16 16" width="20" xmlns="http://www.w3.org/2000/svg">
       <path d="M4 0a2 2 0 0 0-2 2v12a2 2 0 0 0 2 2h8a2 2 0 0 0 2-2V2a2 2 0 0 0-2-2H4zm0 1h8a1 1 0 0 1 1 1v12a1 1 0 0 1-1 1H4a1 1 0 0 1-1-1V2a1 1 0 0 1 1-1z">
       </path>
       <path d="M4.603 12.087a.81.81 0 0 1-.438-.42c-.195-.388-.13-.776.08-1.102.198-.307.526-.568.897-.787a7.68 7.68 0 0 1 1.482-.645 19.701 19.701 0 0 0 1.062-2.227 7.269 7.269 0 0 1-.43-1.295c-.086-.4-.119-.796-.046-1.136.075-.354.274-.672.65-.823.192-.077.4-.12.602-.077a.7.7 0 0 1 .477.365c.088.164.12.356.127.538.007.187-.012.395-.047.614-.084.51-.27 1.134-.52 1.794a10.954 10.954 0 0 0 .98 1.686 5.753 5.753 0 0 1 1.334.05c.364.065.734.195.96.465.12.144.193.32.2.518.007.192-.047.382-.138.563a1.04 1.04 0 0 1-.354.416.856.856 0 0 1-.51.138c-.331-.014-.654-.196-.933-.417a5.716 5.716 0 0 1-.911-.95 11.642 11.642 0 0 0-1.997.406 11.311 11.311 0 0 1-1.021 1.51c-.29.35-.608.655-.926.787a.793.793 0 0 1-.58.029zm1.379-1.901c-.166.076-.32.156-.459.238-.328.194-.541.383-.647.547-.094.145-.096.25-.04.361.01.022.02.036.026.044a.27.27 0 0 0 .035-.012c.137-.056.355-.235.635-.572a8.18 8.18 0 0 0 .45-.606zm1.64-1.33a12.647 12.647 0 0 1 1.01-.193 11.666 11.666 0 0 1-.51-.858 20.741 20.741 0 0 1-.5 1.05zm2.446.45c.15.162.296.3.435.41.24.19.407.253.498.256a.107.107 0 0 0 .07-.015.307.307 0 0 0 .094-.125.436.436 0 0 0 .059-.2.095.095 0 0 0-.026-.063c-.052-.062-.2-.152-.518-.209a3.881 3.881 0 0 0-.612-.053zM8.078 5.8a6.7 6.7 0 0 0 .2-.828c.031-.188.043-.343.038-.465a.613.613 0 0 0-.032-.198.517.517 0 0 0-.145.04c-.087.035-.158.106-.196.283-.04.192-.03.469.046.822.024.111.054.227.09.346z">
       </path>
      </svg>
     </a>
     <small class="titlemark">
      1.4
     </small>
     <a id="x3-100001.4">
     </a>
     <a class="glossary" href="DataScienceIILectureBookli2.html#glo:svm">
      SVM
     </a>
     Regression
    </h2>
    <p class="noindent">
     To use
     <a class="glossary" href="DataScienceIILectureBookli2.html#glo:svm">
      SVM
     </a>
     s for regression instead of classification, the main idea is to tweak the objective: instead of trying to fit the largest
possible street between two classes while limiting margin violations,
     <a class="glossary" href="DataScienceIILectureBookli2.html#glo:svm">
      SVM
     </a>
     regression tries to fit as many instances as possible on
the street while limiting margin violations (i.e., instances off the street).
     The width of the street is controlled by a hyperparameter,
     <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
      <mi>
       𝜖
      </mi>
     </math>
     .
Figure
     <a href="#x3-10001r10">
      1.10
     </a>
     shows two <alert style="color: #821131;">(2)</alert> linear
     <a class="glossary" href="DataScienceIILectureBookli2.html#glo:svm">
      SVM
     </a>
     regression models trained on some linear data, one with a small margin
(
     <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
      <mi>
       𝜖
      </mi>
     </math>
     = 0.5) and the other
with a larger margin (
     <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
      <mi>
       𝜖
      </mi>
     </math>
     = 1.2).
    </p>
    <div class="figure">
     <p class="noindent">
      <img alt="PIC" height="" src="codes/images/Support-Vector-Machines/svm_regression_plot-.svg" width="150%"/>
      <a id="x3-10001r10">
      </a>
     </p>
     <figcaption class="caption">
      <span class="id">
       Figure 1.10:
      </span>
      <span class="content">
       <a class="glossary" href="DataScienceIILectureBookli2.html#glo:svm">
        SVM
       </a>
       regression.
      </span>
     </figcaption>
    </div>
    <p class="noindent">
     Reducing
     <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
      <mi>
       𝜖
      </mi>
     </math>
     increases the number of support vectors, which regularises the model. Moreover, if you add more training
instances within the margin, it will not affect the model’s predictions; therefore, the model is said to be
     <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
      <mi>
       𝜖
      </mi>
     </math>
     -insensitive.
    </p>
    <p class="noindent">
     You can use <span style="color:#054C5C;"><code class="verb">sklearn</code></span>
     ’s <span style="color:#054C5C;"><code class="verb">LinearSVR</code></span>
     class to perform linear
     <a class="glossary" href="DataScienceIILectureBookli2.html#glo:svm">
      SVM
     </a>
     regression. The following code produces the model represented on
the left in Figure
     <a href="#x3-10001r10">
      1.10
     </a>.
    </p>
    <pre><div id="fancyvrb9" style="padding:20px;border-radius: 3px;"><a id="x3-10003r537"></a><span style="color:#2B2BFF;">from</span><span style="color:#BABABA;"> </span>sklearn.svm<span style="color:#BABABA;"> </span><span style="color:#2B2BFF;">import</span> LinearSVR 
<a id="x3-10005r538"></a> 
<a id="x3-10007r539"></a>np.random.seed(<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">42</span></span>) 
<a id="x3-10009r540"></a>X = <span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">2</span></span> * np.random.rand(<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">50</span></span>, <span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">1</span></span>) 
<a id="x3-10011r541"></a>y = <span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">4</span></span> + <span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">3</span></span> * X[:, <span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">0</span></span>] + np.random.randn(<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">50</span></span>) 
<a id="x3-10013r542"></a> 
<a id="x3-10015r543"></a>svm_reg = make_pipeline( 
<a id="x3-10017r544"></a>   StandardScaler(), 
<a id="x3-10019r545"></a>   LinearSVR(epsilon=<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">0.5</span></span>, dual=<span style="color:#2B2BFF;"><span id="bold" style="font-weight:bold;">True</span></span>, random_state=<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">42</span></span>) 
<a id="x3-10021r546"></a>) 
<a id="x3-10023r547"></a>svm_reg.fit(X, y)</div></pre>
    <p class="noindent">
     To tackle non-linear regression tasks, you can use a kernelized
     <a class="glossary" href="DataScienceIILectureBookli2.html#glo:svm">
      SVM
     </a>
     model. Figure
     <a href="#x3-10024r11">
      1.11
     </a>
     shows
     <a class="glossary" href="DataScienceIILectureBookli2.html#glo:svm">
      SVM
     </a>
     regression on a random
quadratic training set, using a second-degree polynomial kernel. There is some regularisation in the left plot (i.e., a small <span style="color:#054C5C;"><code class="verb">C</code></span>
     value), and much less in the right plot (i.e., a large <span style="color:#054C5C;"><code class="verb">C</code></span>
     value).
    </p>
    <div class="figure">
     <p class="noindent">
      <img alt="PIC" height="" src="codes/images/Support-Vector-Machines/svm_with_polynomial_kernel_plot-.svg" width="150%"/>
      <a id="x3-10024r11">
      </a>
     </p>
     <figcaption class="caption">
      <span class="id">
       Figure 1.11:
      </span>
      <span class="content">
       <a class="glossary" href="DataScienceIILectureBookli2.html#glo:svm">
        SVM
       </a>
       regression using a second-degree polynomial kernel.
      </span>
     </figcaption>
    </div>
    <p class="noindent">
     The following code uses <span style="color:#054C5C;"><code class="verb">sklearn</code></span>
     ’s SVR class (which supports the kernel trick) to produce the model represented on the left in
Figure
     <a href="#x3-10024r11">
      1.11
     </a>
     :
    </p>
    <pre><div id="fancyvrb10" style="padding:20px;border-radius: 3px;"><a id="x3-10026r604"></a><span style="color:#2B2BFF;">from</span><span style="color:#BABABA;"> </span>sklearn.svm<span style="color:#BABABA;"> </span><span style="color:#2B2BFF;">import</span> SVR 
<a id="x3-10028r605"></a> 
<a id="x3-10030r606"></a><span style="color:#008700;"><italic># extra code  these 3 lines generate a simple quadratic dataset</italic></span> 
<a id="x3-10032r607"></a>np.random.seed(<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">42</span></span>) 
<a id="x3-10034r608"></a>X = <span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">2</span></span> * np.random.rand(<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">50</span></span>, <span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">1</span></span>) - <span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">1</span></span> 
<a id="x3-10036r609"></a>y = <span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">0.2</span></span> + <span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">0.1</span></span> * X[:, <span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">0</span></span>] + <span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">0.5</span></span> * X[:, <span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">0</span></span>] ** <span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">2</span></span> + np.random.randn(<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">50</span></span>) / <span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">10</span></span> 
<a id="x3-10038r610"></a> 
<a id="x3-10040r611"></a>svm_poly_reg = make_pipeline( 
<a id="x3-10042r612"></a>   StandardScaler(), 
<a id="x3-10044r613"></a>   SVR(kernel=<span style="color:#800080;">"poly"</span>, degree=<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">2</span></span>, C=<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">0.01</span></span>, epsilon=<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">0.1</span></span>) 
<a id="x3-10046r614"></a>) 
<a id="x3-10048r615"></a> 
<a id="x3-10050r616"></a>svm_poly_reg.fit(X, y)</div></pre>
    <p class="noindent">
     The SVR class is the regression equivalent of the SVC class, and the <span style="color:#054C5C;"><code class="verb">LinearSVR</code></span>
     class is the regression equivalent of the <span style="color:#054C5C;"><code class="verb">LinearSVC</code></span>
     class. The <span style="color:#054C5C;"><code class="verb">LinearSVR</code></span>
     class scales linearly with the size of the training set (just like the <span style="color:#054C5C;"><code class="verb">LinearSVC</code></span>
     class), while the SVR class gets
much too slow when the training set grows very large (just like the SVC class).
    </p>
   </article>
   <footer>
    <div class="next">
     <p class="noindent">
      <a href="DataScienceIILectureBookch2.html" style="float: right;">
       Next Chapter →
      </a>
      <a href="index.html" style="float: left;">
       ← Previous Chapter
      </a>
      <a href="index.html">
       TOP
      </a>
     </p>
    </div>
   </footer>
  </div>
  <p class="noindent">
   <a id="tailDataScienceIILectureBookch1.html">
   </a>
  </p>
 </body>
</html>
