<!DOCTYPE html>
<html lang="en-US" xml:lang="en-US">
 <head>
  <title>
   7 Computer Vision using Convolutional Neural Networks
  </title>
    <link rel='icon' type='image/x-icon' href='figures/logo.svg'></link>
  <meta charset="utf-8"/>
  <meta content="ParSnip" name="generator"/>
  <meta content="width=device-width,initial-scale=1" name="viewport"/>
  <link href="DataScienceIILectureBook.css" rel="stylesheet" type="text/css"/>
  <meta content="DataScienceIILectureBook.tex" name="src"/>
  <script async="async" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/mml-chtml.js" type="text/javascript">
  </script>
  <link href="flatweb.css" rel="stylesheet" type="text/css"/>
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.7.1/jquery.min.js">
  </script>
  <script src="flatjs.js">
  </script>
  <link href="http://fonts.googleapis.com/css?family=Roboto" rel="stylesheet" type="text/css"/>
  <script type="module">
   import pdfjsDist from 'https://cdn.jsdelivr.net/npm/pdfjs-dist@5.3.93/+esm'
  </script>
  <script>
   function moveTOC() { 
var htmlShow = document.getElementsByClassName("wide"); 
if (htmlShow[0].style.display === "none") { 
htmlShow[0].style.display = "block"; 
} else { 
htmlShow[0].style.display = "none"; 
} 
}
  </script>
  <link href="https://unpkg.com/nord-highlightjs@0.1.0/dist/nord.css" rel="stylesheet" type="text/css"/>
  <script src="https://unpkg.com/@highlightjs/cdn-assets@11.3.1/highlight.min.js">
  </script>
  <script src="https://unpkg.com/highlightjs-copy/dist/highlightjs-copy.min.js">
  </script>
  <link href="https://unpkg.com/highlightjs-copy/dist/highlightjs-copy.min.css" rel="stylesheet"/>
  <script src="flatjs.js">
  </script>
  <header>
  </header>
 </head>
 <body id="top">
  <nav class="wide">
   <div class="contents">
    <h3 style="font-weight:lighter; padding: 20px 0 20px 0;">
     7
   
     
   

   Computer Vision using Convolutional Neural Networks
    </h3>
    <h4 style="font-weight:lighter">
     Chapter Table of Contents
    </h4>
    <ul>
     <div class="chapterTOCS">
      <li>
       <span class="subsectionToc" style="font-size:12px;margin:0px;">
        <small>
         6.3.3
        </small>
        <a href="DataScienceIILectureBookch6.html#x8-1000006.3.3">
         Building a Regression MLP Using the Sequential API
        </a>
       </span>
      </li>
     </div>
    </ul>
    <div class="prev-next" style="text-align: center">
     <p class="noindent">
      <a href="DataScienceIILectureBookch6.html" style="float:left; font-size:10px">
       ← PREV
      </a>
      <a href="index.html">
       TOP
      </a>
     </p>
    </div>
    <div id="author-info" style="bottom: 0; padding-bottom: 10px;">
     <div id="author-logo">
      <img src="figures/logo.svg" style="margin: 10px 0;"/>
      <div>
       <div>
        <h4 style="color:#7aa0b8; font-weight:lighter;">
         WebBook | D. T. McGuiness, P.hD
        </h4>
       </div>
      </div>
      <div id="author-text" style="bottom: 0; padding-top: 50px;border-top: solid 1px #3b4b5e;font-size: 12px;text-align: right;">
       <p>
        <b>
         Authors Note
        </b>
        The website you are viewing is auto-generated
    using ParSnip and therefore subject to slight errors in
    typography and formatting. When in doubt, please consult the
    LectureBook.
       </p>
      </div>
     </div>
    </div>
   </div>
  </nav>
  <div class="page">
   <article class="chapter">
    <h1 class="chapterHead">
     <div class="number">
      7
     </div>
     <a id="x9-1010007">
     </a>
     Computer Vision using Convolutional Neural Networks
    </h1><button id='toc-button' onclick='moveTOC()'>TOC</button>
    <div class="section-control" style="text-align: center">
     <a id="prev-section" onclick="goPrev()" style="float:left;">
      ← Previous Section
     </a>
     <a id="next-section" onclick="goNext()" style="float:right;">
      Next Section →
     </a>
    </div>
    <h2 class="sectionHead">
     <a aria-label="Link to this section in PDF version" class="pdf-link" href="DataScienceIILectureBook.pdf#section.7.1" style="color: red;  position: absolute;  left: -30px;" target="_blank">
      <svg class="bi bi-file-pdf" fill="currentColor" height="20" viewbox="0 0 16 16" width="20" xmlns="http://www.w3.org/2000/svg">
       <path d="M4 0a2 2 0 0 0-2 2v12a2 2 0 0 0 2 2h8a2 2 0 0 0 2-2V2a2 2 0 0 0-2-2H4zm0 1h8a1 1 0 0 1 1 1v12a1 1 0 0 1-1 1H4a1 1 0 0 1-1-1V2a1 1 0 0 1 1-1z">
       </path>
       <path d="M4.603 12.087a.81.81 0 0 1-.438-.42c-.195-.388-.13-.776.08-1.102.198-.307.526-.568.897-.787a7.68 7.68 0 0 1 1.482-.645 19.701 19.701 0 0 0 1.062-2.227 7.269 7.269 0 0 1-.43-1.295c-.086-.4-.119-.796-.046-1.136.075-.354.274-.672.65-.823.192-.077.4-.12.602-.077a.7.7 0 0 1 .477.365c.088.164.12.356.127.538.007.187-.012.395-.047.614-.084.51-.27 1.134-.52 1.794a10.954 10.954 0 0 0 .98 1.686 5.753 5.753 0 0 1 1.334.05c.364.065.734.195.96.465.12.144.193.32.2.518.007.192-.047.382-.138.563a1.04 1.04 0 0 1-.354.416.856.856 0 0 1-.51.138c-.331-.014-.654-.196-.933-.417a5.716 5.716 0 0 1-.911-.95 11.642 11.642 0 0 0-1.997.406 11.311 11.311 0 0 1-1.021 1.51c-.29.35-.608.655-.926.787a.793.793 0 0 1-.58.029zm1.379-1.901c-.166.076-.32.156-.459.238-.328.194-.541.383-.647.547-.094.145-.096.25-.04.361.01.022.02.036.026.044a.27.27 0 0 0 .035-.012c.137-.056.355-.235.635-.572a8.18 8.18 0 0 0 .45-.606zm1.64-1.33a12.647 12.647 0 0 1 1.01-.193 11.666 11.666 0 0 1-.51-.858 20.741 20.741 0 0 1-.5 1.05zm2.446.45c.15.162.296.3.435.41.24.19.407.253.498.256a.107.107 0 0 0 .07-.015.307.307 0 0 0 .094-.125.436.436 0 0 0 .059-.2.095.095 0 0 0-.026-.063c-.052-.062-.2-.152-.518-.209a3.881 3.881 0 0 0-.612-.053zM8.078 5.8a6.7 6.7 0 0 0 .2-.828c.031-.188.043-.343.038-.465a.613.613 0 0 0-.032-.198.517.517 0 0 0-.145.04c-.087.035-.158.106-.196.283-.04.192-.03.469.046.822.024.111.054.227.09.346z">
       </path>
      </svg>
     </a>
     <small class="titlemark">
      7.1
     </small>
     <a id="x9-1020007.1">
     </a>
     Introduction
    </h2>
    <p class="noindent">
     It
wasn’t
until
recently
computers
were
able
to
reliably
perform
seemingly
easy
tasks
such
as
detecting
a
cat
     <alert style="color: #821131;">
      <span id="bold" style="font-weight:bold;">
       <sup class="textsuperscript">
        1
       </sup>
      </span>
     </alert>
     <span class="marginnote">
      <span style="color:#999999;">
      </span>
      <alert style="color: #821131;">
       <span id="bold" style="font-weight:bold;">
        <sup class="textsuperscript">
         1
        </sup>
       </span>
      </alert>
      <span style="color:#0063B2;">
       As
it
is
known,
internet
was
invented
for
sharing
cat
pictures,
therefore
their
detection
is
paramount.
      </span>
     </span>
     in
a
picture
or
recognising
spoken
words.
     Why
are
these
tasks
                                                                                
                                                                                
so
effortless
to
us
humans?
     The answer lies in the fact that perception largely takes place
     <alert style="color: #821131;">
      outside the realm of our control
     </alert>
     , within specialized visual,
auditory, and other sensory modules in our brains. By the time sensory information reaches our consciousness, it is already
imbued with
     <alert style="color: #821131;">
      high-level features
     </alert>
     . For example:
    </p>
    
    <div class="quoteblock">
     <p class="noindent">
      When
we
look
at
a
picture
of
a
cute
puppy,
we
cannot
choose
not
to
see
the
puppy,
not
to
notice
its
cuteness.
Nor
can
we
explain
how
we
recognize
a
cute
puppy;
it’s
just
obvious
to
you.
     </p>
    </div>
    <p class="noindent">
     Therefore, we cannot trust our subjective experience.
    </p>
    <p class="noindent">
     Perception is
     <span id="bold" style="font-weight:bold;">
      NOT
     </span>
     a trivial task, and to understand it we must look at how our sensory modules work.
     <a class="glossary" href="DataScienceIILectureBookli2.html#glo:cnn">
      CNN
     </a>
     s emerged from
the study of the brain’s visual cortex, and they have been used in computer image recognition since the 1980s <a id="x9-102001"></a><a href="#X0-zarandy2015overview">[57]</a>.
     Over the last 10 years, thanks to the increase in computational power, the amount of available training data, and a much better
methods developed for training deep nets,
     <a class="glossary" href="DataScienceIILectureBookli2.html#glo:cnn">
      CNN
     </a>
     s have managed to achieve impressive performance on some complex visual tasks.
Some examples of their application include:
    </p>
    <ul class="itemize1">
     <li class="itemize">
      <p class="noindent">
       <span id="bold" style="font-weight:bold;">
        Search
services:
       </span>
       Such
as
                                                                                
                                                                                
     
connecting
users
in
the
web
to
the
software
they
need
to
find <a id="x9-102002"></a><a href="#X0-huang2020combination">[58]</a>,
       <a id="x9-102003">
       </a> <a href="#X0-li2021hybrid">[59]</a>,
      </p>
     </li>
     <li class="itemize">
      <p class="noindent">
       <span id="bold" style="font-weight:bold;">
        Self-driving
cars:
       </span>
       Such
as
detecting
the
colours
on
a
traffic
light <a id="x9-102004"></a><a href="#X0-ouyang2019deep">[60]</a>,
or
detecting
the
road
lines
during
driving <a id="x9-102005"></a><a href="#X0-nugraha2017towards">[61]</a>,
      </p>
     </li>
     <li class="itemize">
      <p class="noindent">
       <span id="bold" style="font-weight:bold;">
        Automatic
video
classification
       </span>
       :
i.e.,
detecting
videos
and
categorising
based
on
content <a id="x9-102006"></a><a href="#X0-ye2015evaluating">[62]</a>,
      </p>
     </li>
     <li class="itemize">
      <p class="noindent">
       <span id="bold" style="font-weight:bold;">
        Object
Detection:
       </span>
       Such
as
detecting
objects
in
an
                                                                                
                                                                                
     
image <a id="x9-102007"></a><a href="#X0-zhiqiang2017review">[20]</a>.
      </p>
     </li>
    </ul>
    <p class="noindent">
     In addition,
     <a class="glossary" href="DataScienceIILectureBookli2.html#glo:cnn">
      CNN
     </a>
     s are not restricted to visual perception:
    </p>
    <p class="noindent">
    </p>
    <div class="quoteblock">
     <p class="noindent">
      They
are
also
successful
at
many
other
tasks,
such
as
voice
recognition
and
natural
language
processing.
     </p>
    </div>
    <p class="noindent">
     However,
as
our
topic
is
     <span id="bold" style="font-weight:bold;">
      Image
Processing
     </span>
     ,
we
will
focus
on
its
visual
applications.
In
this
chapter
we
will
explore
where
     <a class="glossary" href="DataScienceIILectureBookli2.html#glo:cnn">
      CNN
     </a>
     s
came
from,
what
their
building
blocks
look
like,
and
how
to
implement
them
using <span style="color:#054C5C;"><code class="verb">tf.keras</code></span>.
Then
we
will
                                                                                
                                                                                
discuss
some
of
the
best
     <a class="glossary" href="DataScienceIILectureBookli2.html#glo:cnn">
      CNN
     </a>
     architectures,
as
well
as
other
visual
tasks,
including
object
detection
     <alert style="color: #821131;">
      <span id="bold" style="font-weight:bold;">
       <sup class="textsuperscript">
        2
       </sup>
      </span>
     </alert>
     <span class="marginnote">
      <span style="color:#999999;">
      </span>
      <alert style="color: #821131;">
       <span id="bold" style="font-weight:bold;">
        <sup class="textsuperscript">
         2
        </sup>
       </span>
      </alert>
      <span style="color:#0063B2;">
       classifying
multiple
objects
in
an
image
and
placing
bounding
boxes
around
them
      </span>
     </span>
     and
semantic
segmentation..
    </p>
    
    <div class="figure">
     <p class="noindent">
      <img alt="PIC" height="" src="figures/Computer-Vision-using-Convolutional-Neural-Networks/figures-1.svg" width="150%"/>
      <a id="x9-102008r1">
      </a>
     </p>
     <figcaption class="caption">
      <span class="id">
       Figure 7.1:
      </span>
      <span class="content">
       A standard
       <a class="glossary" href="DataScienceIILectureBookli2.html#glo:cnn">
        CNN
       </a>
       architecture. Don’t worry if it looks too confusing at the moment as by the end of this
         chapter we will have the knowledge to understand and build your very own
       <a class="glossary" href="DataScienceIILectureBookli2.html#glo:cnn">
        CNN
       </a>.
      </span>
     </figcaption>
    </div>
    <p class="noindent">
    </p>
    <h2 class="sectionHead">
     <a aria-label="Link to this section in PDF version" class="pdf-link" href="DataScienceIILectureBook.pdf#section.7.2" style="color: red;  position: absolute;  left: -30px;" target="_blank">
      <svg class="bi bi-file-pdf" fill="currentColor" height="20" viewbox="0 0 16 16" width="20" xmlns="http://www.w3.org/2000/svg">
       <path d="M4 0a2 2 0 0 0-2 2v12a2 2 0 0 0 2 2h8a2 2 0 0 0 2-2V2a2 2 0 0 0-2-2H4zm0 1h8a1 1 0 0 1 1 1v12a1 1 0 0 1-1 1H4a1 1 0 0 1-1-1V2a1 1 0 0 1 1-1z">
       </path>
       <path d="M4.603 12.087a.81.81 0 0 1-.438-.42c-.195-.388-.13-.776.08-1.102.198-.307.526-.568.897-.787a7.68 7.68 0 0 1 1.482-.645 19.701 19.701 0 0 0 1.062-2.227 7.269 7.269 0 0 1-.43-1.295c-.086-.4-.119-.796-.046-1.136.075-.354.274-.672.65-.823.192-.077.4-.12.602-.077a.7.7 0 0 1 .477.365c.088.164.12.356.127.538.007.187-.012.395-.047.614-.084.51-.27 1.134-.52 1.794a10.954 10.954 0 0 0 .98 1.686 5.753 5.753 0 0 1 1.334.05c.364.065.734.195.96.465.12.144.193.32.2.518.007.192-.047.382-.138.563a1.04 1.04 0 0 1-.354.416.856.856 0 0 1-.51.138c-.331-.014-.654-.196-.933-.417a5.716 5.716 0 0 1-.911-.95 11.642 11.642 0 0 0-1.997.406 11.311 11.311 0 0 1-1.021 1.51c-.29.35-.608.655-.926.787a.793.793 0 0 1-.58.029zm1.379-1.901c-.166.076-.32.156-.459.238-.328.194-.541.383-.647.547-.094.145-.096.25-.04.361.01.022.02.036.026.044a.27.27 0 0 0 .035-.012c.137-.056.355-.235.635-.572a8.18 8.18 0 0 0 .45-.606zm1.64-1.33a12.647 12.647 0 0 1 1.01-.193 11.666 11.666 0 0 1-.51-.858 20.741 20.741 0 0 1-.5 1.05zm2.446.45c.15.162.296.3.435.41.24.19.407.253.498.256a.107.107 0 0 0 .07-.015.307.307 0 0 0 .094-.125.436.436 0 0 0 .059-.2.095.095 0 0 0-.026-.063c-.052-.062-.2-.152-.518-.209a3.881 3.881 0 0 0-.612-.053zM8.078 5.8a6.7 6.7 0 0 0 .2-.828c.031-.188.043-.343.038-.465a.613.613 0 0 0-.032-.198.517.517 0 0 0-.145.04c-.087.035-.158.106-.196.283-.04.192-.03.469.046.822.024.111.054.227.09.346z">
       </path>
      </svg>
     </a>
     <small class="titlemark">
      7.2
     </small>
     <a id="x9-1030007.2">
     </a>
     Visual Cortex Architecture
    </h2>
    <p class="noindent">
     <italic>
      David
H.
Hubel
     </italic>
     and
     <italic>
      Torsten
Wiesel
     </italic>
     performed
a
series
of
experiments
on
cats
in
1958
and
1959
(and
a
few
years
later
on
monkeys),
giving
crucial
insights
into
the
structure
of
the
visual
cortex
     <alert style="color: #821131;">
      <span id="bold" style="font-weight:bold;">
       <sup class="textsuperscript">
        3
       </sup>
      </span>
     </alert>
     <span class="marginnote">
      <span style="color:#999999;">
      </span>
      <alert style="color: #821131;">
       <span id="bold" style="font-weight:bold;">
        <sup class="textsuperscript">
         3
        </sup>
       </span>
      </alert>
      <span style="color:#0063B2;">
       the
authors
received
the
Nobel
Prize
in
Physiology
or
Medicine
in
1981
for
their
work
      </span>
     </span> <a id="x9-103001"></a><a href="#X0-hubel1962receptive">[63]</a>.
What
is
important
to
us
is,
they
showed
                                                                                
                                                                                
that
many
neurons
in
the
visual
cortex
have
a
     <alert style="color: #821131;">
      small
local
receptive
field
     </alert>
     ,
meaning
they
     <alert style="color: #821131;">
      react
only
to
visual
stimuli
located
in
a
limited
region
of
the
visual
field
     </alert>
     .
A
diagram
showcasing
this
phenomenon
can
be
seen
in
     <span id="bold" style="font-weight:bold;">
      Fig.
     </span>
     <a href="#x9-103002r2">
      7.2
     </a>,
in
which
the
local
receptive
fields
of
five
neurons
are
represented
by
dashed
circles.
The
receptive
fields
of
different
neurons
may
overlap,
and
together
they
tile
the
whole
                                                                                
                                                                                
visual
field.
    </p>
    
    <div class="figure">
     <p class="noindent">
      <img alt="PIC" height="" src="figures/Computer-Vision-using-Convolutional-Neural-Networks/raster/perception.png" width="150%"/>
      <a id="x9-103002r2">
      </a>
     </p>
     <figcaption class="caption">
      <span class="id">
       Figure 7.2:
      </span>
      <span class="content">
       Biological neurons in the visual cortex respond to specific patterns in small regions of the visual field called
         receptive fields; as the visual signal makes its way through consecutive brain modules, neurons respond to
         more complex patterns in larger receptive fields
      </span>
     </figcaption>
    </div>
    <p class="noindent">
     In
addition
to
the
previous
revelation
of
how
neurons
work,
they
showed
some
neurons
react
     <span id="bold" style="font-weight:bold;">
      ONLY
     </span>
     to
images
of
horizontal
lines,
while
others
react
     <span id="bold" style="font-weight:bold;">
      ONLY
     </span>
     to
lines
with
different
orientations
     <alert style="color: #821131;">
      <span id="bold" style="font-weight:bold;">
       <sup class="textsuperscript">
        4
       </sup>
      </span>
     </alert>
     <span class="marginnote">
      <span style="color:#999999;">
      </span>
      <alert style="color: #821131;">
       <span id="bold" style="font-weight:bold;">
        <sup class="textsuperscript">
         4
        </sup>
       </span>
      </alert>
      <span style="color:#0063B2;">
       two
neurons
may
have
the
same
receptive
field
but
react
to
different
line
orientations
      </span>
     </span>
     .
They
also
noticed
that
some
neurons
have
larger
receptive
fields,
and
they
react
to
more
complex
patterns
that
are
combinations
of
                                                                                
                                                                                
the
lower-level
patterns.
These
observations
led
to
the
idea
that
the
higher-level
neurons
are
based
on
the
outputs
of
neighboring
lower-level
neurons.
(in
     <span id="bold" style="font-weight:bold;">
      Fig.
     </span>
     <a href="#x9-103002r2">
      7.2
     </a>,
observe
that
each
neuron
is
connected
only
to
nearby
neurons
from
the
previous
layer).
     This
powerful
architecture
is
able
to
detect
all
sorts
of
complex
patterns
in
any
area
of
the
visual
field.
These
studies
of
the
visual
cortex
inspired
the
     <alert style="color: #821131;">
      neocognitron
     </alert> <a id="x9-103003"></a><a href="#X0-fukushima1980neocognitron">[64]</a>,
introduced
in
1980,
which
gradually
evolved
into
what
we
now
call
     <a class="glossary" href="DataScienceIILectureBookli2.html#glo:cnn">
      CNN
     </a>.
     An important milestone was a 1998 paper by
     <italic>
      Yann LeCun et.al.
     </italic>
     which introduced the famous
     <alert style="color: #821131;">
      LeNet-5
     </alert>
     architecture, which
became widely used by banks to recognize handwritten digits on checks <a id="x9-103004"></a><a href="#X0-lecun1998gradient">[65]</a>. This architecture has some building blocks were are
familiar with:
    </p>
    
    <ul class="itemize1">
     <li class="itemize">
      <p class="noindent">
       Fully
connected
layers,
      </p>
     </li>
     <li class="itemize">
      <p class="noindent">
       Sigmoid
activation
functions
       <alert style="color: #821131;">
        <span id="bold" style="font-weight:bold;">
         <sup class="textsuperscript">
          5
         </sup>
        </span>
       </alert>
       <span class="marginnote">
        <span style="color:#999999;">
        </span>
        <alert style="color: #821131;">
         <span id="bold" style="font-weight:bold;">
          <sup class="textsuperscript">
           5
          </sup>
         </span>
        </alert>
        <span style="color:#0063B2;">
         A
function
allowing
non-linear
properties
for
the
neural
network.
        </span>
       </span>
       ,
      </p>
      
     </li>
    </ul>
    <p class="noindent">
     but it also introduces two new building blocks:
    </p>
    <ul class="itemize1">
     <li class="itemize">
      <p class="noindent">
       convolutional
layers
      </p>
     </li>
     <li class="itemize">
      <p class="noindent">
       pooling
layers.
      </p>
     </li>
    </ul>
    <p class="noindent">
     Which will, of course, will be our focus of attention this chapter.
    </p>
    <div class="informationblock" id="tcolobox-75">
     <div class="title">
      <p class="noindent">
       <span id="bold" style="font-weight:bold;">
        Information
       </span>
       : Limits of DNN
      </p>
     </div>
     <div class="box-content">
      <p class="noindent">
       Why not simply use a
       <a class="glossary" href="DataScienceIILectureBookli2.html#glo:dnn">
        DNN
       </a>
       with fully connected layers for image recognition tasks? Why do we need a new
method ?
      </p>
      <p class="noindent">
       Unfortunately, although this works fine for small images (e.g., MNIST), it breaks down for larger images due to
the
       <alert style="color: #821131;">
        huge number of parameters
       </alert>
       it requires.
       For example, a 100-by-100 pixel image has 10,000 pixels, and if the first layer has just 1,000 neurons (which
already severely restricts the amount of information transmitted to the next layer), this means a total of 10
million connections. And that’s just the first layer.
      </p>
      <p class="noindent">
       <a class="glossary" href="DataScienceIILectureBookli2.html#glo:cnn">
        CNN
       </a>
       s solve this problem using partially connected layers and weight sharing.
      </p>
     </div>
    </div>
    <p class="noindent">
    </p>
    <h2 class="sectionHead">
     <a aria-label="Link to this section in PDF version" class="pdf-link" href="DataScienceIILectureBook.pdf#section.7.3" style="color: red;  position: absolute;  left: -30px;" target="_blank">
      <svg class="bi bi-file-pdf" fill="currentColor" height="20" viewbox="0 0 16 16" width="20" xmlns="http://www.w3.org/2000/svg">
       <path d="M4 0a2 2 0 0 0-2 2v12a2 2 0 0 0 2 2h8a2 2 0 0 0 2-2V2a2 2 0 0 0-2-2H4zm0 1h8a1 1 0 0 1 1 1v12a1 1 0 0 1-1 1H4a1 1 0 0 1-1-1V2a1 1 0 0 1 1-1z">
       </path>
       <path d="M4.603 12.087a.81.81 0 0 1-.438-.42c-.195-.388-.13-.776.08-1.102.198-.307.526-.568.897-.787a7.68 7.68 0 0 1 1.482-.645 19.701 19.701 0 0 0 1.062-2.227 7.269 7.269 0 0 1-.43-1.295c-.086-.4-.119-.796-.046-1.136.075-.354.274-.672.65-.823.192-.077.4-.12.602-.077a.7.7 0 0 1 .477.365c.088.164.12.356.127.538.007.187-.012.395-.047.614-.084.51-.27 1.134-.52 1.794a10.954 10.954 0 0 0 .98 1.686 5.753 5.753 0 0 1 1.334.05c.364.065.734.195.96.465.12.144.193.32.2.518.007.192-.047.382-.138.563a1.04 1.04 0 0 1-.354.416.856.856 0 0 1-.51.138c-.331-.014-.654-.196-.933-.417a5.716 5.716 0 0 1-.911-.95 11.642 11.642 0 0 0-1.997.406 11.311 11.311 0 0 1-1.021 1.51c-.29.35-.608.655-.926.787a.793.793 0 0 1-.58.029zm1.379-1.901c-.166.076-.32.156-.459.238-.328.194-.541.383-.647.547-.094.145-.096.25-.04.361.01.022.02.036.026.044a.27.27 0 0 0 .035-.012c.137-.056.355-.235.635-.572a8.18 8.18 0 0 0 .45-.606zm1.64-1.33a12.647 12.647 0 0 1 1.01-.193 11.666 11.666 0 0 1-.51-.858 20.741 20.741 0 0 1-.5 1.05zm2.446.45c.15.162.296.3.435.41.24.19.407.253.498.256a.107.107 0 0 0 .07-.015.307.307 0 0 0 .094-.125.436.436 0 0 0 .059-.2.095.095 0 0 0-.026-.063c-.052-.062-.2-.152-.518-.209a3.881 3.881 0 0 0-.612-.053zM8.078 5.8a6.7 6.7 0 0 0 .2-.828c.031-.188.043-.343.038-.465a.613.613 0 0 0-.032-.198.517.517 0 0 0-.145.04c-.087.035-.158.106-.196.283-.04.192-.03.469.046.822.024.111.054.227.09.346z">
       </path>
      </svg>
     </a>
     <small class="titlemark">
      7.3
     </small>
     <a id="x9-1040007.3">
     </a>
     Convolutional Layers
    </h2>
    <p class="noindent">
     The most important building block of a
     <a class="glossary" href="DataScienceIILectureBookli2.html#glo:cnn">
      CNN
     </a>
     is the
     <alert style="color: #821131;">
      convolutional layer
     </alert>
     :
    </p>
    <p class="noindent">
    </p>
    <div class="quoteblock">
     <p class="noindent">
      Neurons
in
the
first
convolutional
layer
are
      <span id="bold" style="font-weight:bold;">
       NOT
      </span>
      connected
to
every
single
pixel
in
the
input
image
      <alert style="color: #821131;">
       <span id="bold" style="font-weight:bold;">
        <sup class="textsuperscript">
         6
        </sup>
       </span>
      </alert>
      <span class="marginnote">
       <span style="color:#999999;">
       </span>
       <alert style="color: #821131;">
        <span id="bold" style="font-weight:bold;">
         <sup class="textsuperscript">
          6
         </sup>
        </span>
       </alert>
       <span style="color:#0063B2;">
        This
approach
is
going
against
the
previously
discussed
        <a class="glossary" href="DataScienceIILectureBookli2.html#glo:ann">
         ANN
        </a>
        s
and
        <a class="glossary" href="DataScienceIILectureBookli2.html#glo:dnn">
         DNN
        </a>
        s
       </span>
      </span>
      .
     </p>
     
    </div>
    <p class="noindent">
     The neurons are only connected to pixels in their
     <alert style="color: #821131;">
      receptive fields
     </alert>
     which its graphical representation can be seen in
     <span id="bold" style="font-weight:bold;">
      Fig.
     </span>
     <a href="#x9-104001r3">
      7.3
     </a>.
    </p>
    <div class="figure">
     <p class="noindent">
      <img alt="PIC" height="" src="figures/Computer-Vision-using-Convolutional-Neural-Networks/figures-2.svg" width="150%"/>
      <a id="x9-104001r3">
      </a>
     </p>
     <figcaption class="caption">
      <span class="id">
       Figure 7.3:
      </span>
      <span class="content">
       <a class="glossary" href="DataScienceIILectureBookli2.html#glo:cnn">
        CNN
       </a>
       layers with rectangular local receptive fields.
      </span>
     </figcaption>
    </div>
    <div class="knowledge">
     <p class="noindent">
      In a
      <a class="glossary" href="DataScienceIILectureBookli2.html#glo:cnn">
       CNN
      </a>, each layer is represented in 2D, which makes it easier to match neurons with their corresponding inputs as our images
are also 2D.
     </p>
    </div>
    <p class="noindent">
     In turn, each neuron in the second convolutional layer is connected only to neurons located within a small rectangle in the first
layer. This architecture allows the network to concentrate on small low-level features in the first hidden layer, then assemble them
into larger higher-level features in the next hidden layer, and so on. This hierarchical structure is common in real-world images,
which is one of the reasons why
     <a class="glossary" href="DataScienceIILectureBookli2.html#glo:cnn">
      CNN
     </a>
     s work so well for image recognition.
     A neuron located in row
     <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
      <mi>
       i
      </mi>
     </math>
     ,
column
     <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
      <mi>
       j
      </mi>
     </math>
     of a given layer is connected to the outputs of the neurons in the previous layer located in rows
     <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
      <mi>
       i
      </mi>
     </math>
     to
     <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
      <mi>
       i
      </mi>
      <mo class="MathClass-bin" stretchy="false">
       +
      </mo>
      <msubsup>
       <mrow>
        <mi>
         f
        </mi>
       </mrow>
       <mrow>
        <mi>
         h
        </mi>
       </mrow>
       <mrow>
       </mrow>
      </msubsup>
      <mo class="MathClass-bin" stretchy="false">
       −
      </mo>
      <mn>
       1
      </mn>
     </math>
     , columns
     <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
      <mi>
       j
      </mi>
     </math>
     to
     <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
      <mi>
       j
      </mi>
      <mo class="MathClass-bin" stretchy="false">
       +
      </mo>
      <msubsup>
       <mrow>
        <mi>
         f
        </mi>
       </mrow>
       <mrow>
        <mi>
         w
        </mi>
       </mrow>
       <mrow>
       </mrow>
      </msubsup>
      <mo class="MathClass-bin" stretchy="false">
       −
      </mo>
      <mn>
       1
      </mn>
     </math>
     , where
     <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
      <msubsup>
       <mrow>
        <mi>
         f
        </mi>
       </mrow>
       <mrow>
        <mi>
         h
        </mi>
       </mrow>
       <mrow>
       </mrow>
      </msubsup>
     </math>
     and
     <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
      <msubsup>
       <mrow>
        <mi>
         f
        </mi>
       </mrow>
       <mrow>
        <mi>
         w
        </mi>
       </mrow>
       <mrow>
       </mrow>
      </msubsup>
     </math>
     are the
height and width of the receptive field which can be observed in
     <span id="bold" style="font-weight:bold;">
      Fig.
     </span>
     ??.
    </p>
    <div class="figure">
     <p class="noindent">
      <img alt="PIC" height="" src="figures/Computer-Vision-using-Convolutional-Neural-Networks/figures-3.svg" width="150%"/>
      <a id="x9-104002r4">
      </a>
     </p>
     <figcaption class="caption">
      <span class="id">
       Figure 7.4:
      </span>
      <span class="content">
      </span>
     </figcaption>
    </div>
    <div class="knowledge">
     <p class="noindent">
      For a layer to have the same height and width as the previous layer, it is common to add zeros around the inputs, which is called
      <italic>
       zero padding
      </italic>
      .
     </p>
    </div>
    <p class="noindent">
     It is also possible to connect a large input layer to a much smaller layer by spacing out the receptive fields, shown in
     <span id="bold" style="font-weight:bold;">
      Fig.
     </span>
     ??.
    </p>
    <p class="noindent">
     This
     <alert style="color: #821131;">
      significantly
decreases
     </alert>
     the
model’s
computational
complexity.
The
horizontal
or
vertical
step
size
from
one
receptive
field
to
the
next
is
called
the
     <span id="bold" style="font-weight:bold;">
      stride
     </span>
     .
In
     <span id="bold" style="font-weight:bold;">
      Fig.
     </span>
     ??,
a
5-by-7
input
layer
(plus
zero
padding)
is
connected
to
a
3-by-4
layer,
using
3-by-3
receptive
fields
and
a
stride
of
2
     <alert style="color: #821131;">
      <span id="bold" style="font-weight:bold;">
       <sup class="textsuperscript">
        7
       </sup>
      </span>
     </alert>
     <span class="marginnote">
      <span style="color:#999999;">
      </span>
      <alert style="color: #821131;">
       <span id="bold" style="font-weight:bold;">
        <sup class="textsuperscript">
         7
        </sup>
       </span>
      </alert>
      <span style="color:#0063B2;">
       in
this
example
the
stride
is
the
same
in
both
directions,
but
                                                                                
                                                                                
it
does
not
have
to
be
so
      </span>
     </span>
     .
A
neuron
located
in
row
     <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
      <mi>
       i
      </mi>
     </math>
     ,
column
     <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
      <mi>
       j
      </mi>
     </math>
     in
the
upper
layer
is
connected
to
the
outputs
of
the
neurons
in
the
previous
layer
located
in
rows
iŒsh
toiŒsh
+fh
-1,columnsjŒsw
to
j
Œ
sw
+
fw
-
1,
where
sh
and
sw
are
the
vertical
and
horizontal
strides.
    </p>
    
    <h3 class="subsectionHead">
     <a aria-label="Link to this section in PDF version" class="pdf-link" href="DataScienceIILectureBook.pdf#subsection.7.3.1" style="color: red;  position: absolute;  left: -30px;" target="_blank">
      <svg class="bi bi-file-pdf" fill="currentColor" height="20" viewbox="0 0 16 16" width="20" xmlns="http://www.w3.org/2000/svg">
       <path d="M4 0a2 2 0 0 0-2 2v12a2 2 0 0 0 2 2h8a2 2 0 0 0 2-2V2a2 2 0 0 0-2-2H4zm0 1h8a1 1 0 0 1 1 1v12a1 1 0 0 1-1 1H4a1 1 0 0 1-1-1V2a1 1 0 0 1 1-1z">
       </path>
       <path d="M4.603 12.087a.81.81 0 0 1-.438-.42c-.195-.388-.13-.776.08-1.102.198-.307.526-.568.897-.787a7.68 7.68 0 0 1 1.482-.645 19.701 19.701 0 0 0 1.062-2.227 7.269 7.269 0 0 1-.43-1.295c-.086-.4-.119-.796-.046-1.136.075-.354.274-.672.65-.823.192-.077.4-.12.602-.077a.7.7 0 0 1 .477.365c.088.164.12.356.127.538.007.187-.012.395-.047.614-.084.51-.27 1.134-.52 1.794a10.954 10.954 0 0 0 .98 1.686 5.753 5.753 0 0 1 1.334.05c.364.065.734.195.96.465.12.144.193.32.2.518.007.192-.047.382-.138.563a1.04 1.04 0 0 1-.354.416.856.856 0 0 1-.51.138c-.331-.014-.654-.196-.933-.417a5.716 5.716 0 0 1-.911-.95 11.642 11.642 0 0 0-1.997.406 11.311 11.311 0 0 1-1.021 1.51c-.29.35-.608.655-.926.787a.793.793 0 0 1-.58.029zm1.379-1.901c-.166.076-.32.156-.459.238-.328.194-.541.383-.647.547-.094.145-.096.25-.04.361.01.022.02.036.026.044a.27.27 0 0 0 .035-.012c.137-.056.355-.235.635-.572a8.18 8.18 0 0 0 .45-.606zm1.64-1.33a12.647 12.647 0 0 1 1.01-.193 11.666 11.666 0 0 1-.51-.858 20.741 20.741 0 0 1-.5 1.05zm2.446.45c.15.162.296.3.435.41.24.19.407.253.498.256a.107.107 0 0 0 .07-.015.307.307 0 0 0 .094-.125.436.436 0 0 0 .059-.2.095.095 0 0 0-.026-.063c-.052-.062-.2-.152-.518-.209a3.881 3.881 0 0 0-.612-.053zM8.078 5.8a6.7 6.7 0 0 0 .2-.828c.031-.188.043-.343.038-.465a.613.613 0 0 0-.032-.198.517.517 0 0 0-.145.04c-.087.035-.158.106-.196.283-.04.192-.03.469.046.822.024.111.054.227.09.346z">
       </path>
      </svg>
     </a>
     <small class="titlemark">
      7.3.1
     </small>
     <a id="x9-1050007.3.1">
     </a>
     Filters
    </h3>
    <p class="noindent">
     A
neuron’s
weights
can
                                                                                
                                                                                
be
represented
as
a
small
image
the
size
of
the
receptive
field.
For
example,
     <span id="bold" style="font-weight:bold;">
      Fig.
     </span>
     <a href="#x9-105001r5">
      7.5
     </a>
     shows
two
<alert style="color: #821131;">(2)</alert>
possible
sets
of
weights,
called
filters
     <alert style="color: #821131;">
      <span id="bold" style="font-weight:bold;">
       <sup class="textsuperscript">
        8
       </sup>
      </span>
     </alert>
     <span class="marginnote">
      <span style="color:#999999;">
      </span>
      <alert style="color: #821131;">
       <span id="bold" style="font-weight:bold;">
        <sup class="textsuperscript">
         8
        </sup>
       </span>
      </alert>
      <span style="color:#0063B2;">
       The
terms
convolution
kernels,
or
kernels
are
also
used
      </span>
     </span>
     .
    </p>
    
    <div class="figure">
     <p class="noindent">
      <img alt="PIC" height="" src="figures/Computer-Vision-using-Convolutional-Neural-Networks/raster/cnn-filters.png" width="150%"/>
      <a id="x9-105001r5">
      </a>
     </p>
     <figcaption class="caption">
      <span class="id">
       Figure 7.5:
      </span>
      <span class="content">
       An example image showcasing the effect of applying filters to get feature maps.
      </span>
     </figcaption>
    </div>
    <p class="noindent">
     The first one is represented as a black square with a vertical white line in the middle.
    </p>
    <div class="knowledge">
     <p class="noindent">
      It’s a 7-by-7 matrix full of 0s except for the central column, which is full of 1s.
     </p>
    </div>
    <p class="noindent">
     Neurons
using
these
weights
will
ignore
everything
in
their
receptive
field
except
for
the
central
vertical
line
     <alert style="color: #821131;">
      <span id="bold" style="font-weight:bold;">
       <sup class="textsuperscript">
        9
       </sup>
      </span>
     </alert>
     <span class="marginnote">
      <span style="color:#999999;">
      </span>
      <alert style="color: #821131;">
       <span id="bold" style="font-weight:bold;">
        <sup class="textsuperscript">
         9
        </sup>
       </span>
      </alert>
      <span style="color:#0063B2;">
       since
all
inputs
will
be
multiplied
by
0,
except
for
the
ones
in
the
central
vertical
line
      </span>
     </span>
     .
The
second
filter
is
a
black
square
with
a
horizontal
white
line
in
the
middle.
Neurons
using
these
weights
will
ignore
everything
in
their
receptive
field
except
                                                                                
                                                                                
for
the
central
horizontal
line.
     If
all
neurons
in
a
layer
use
the
same
vertical
line
filter
(and
the
same
bias
term),
and
we
feed
the
network
the
input
image
shown
in
     <span id="bold" style="font-weight:bold;">
      Fig.
     </span>
     <a href="#x9-105001r5">
      7.5
     </a>
     (the
bottom
image),
the
layer
will
output
the
top-left
image.
Notice
that
the
vertical
white
lines
get
enhanced
while
the
rest
gets
blurred.
Similarly,
the
upper-right
image
is
what
we
get
if
all
neurons
                                                                                
                                                                                
use
the
same
horizontal
line
filter.
Notice
the
horizontal
white
lines
get
enhanced
while
the
rest
is
blurred
out.
Therefore,
a
layer
full
of
neurons
using
the
same
filter
outputs
a
     <alert style="color: #821131;">
      feature
map
     </alert>
     ,
which
highlights
the
areas
in
an
image
that
activate
the
filter
the
most.
    </p>
    
    <div class="knowledge">
     <p class="noindent">
      We won’t have to define the filters manually: instead, during training the convolutional layer will automatically learn
the most useful filters for its task, and the layers above will learn to combine them into more complex patterns.
     </p>
    </div>
    <h3 class="subsectionHead">
     <a aria-label="Link to this section in PDF version" class="pdf-link" href="DataScienceIILectureBook.pdf#subsection.7.3.2" style="color: red;  position: absolute;  left: -30px;" target="_blank">
      <svg class="bi bi-file-pdf" fill="currentColor" height="20" viewbox="0 0 16 16" width="20" xmlns="http://www.w3.org/2000/svg">
       <path d="M4 0a2 2 0 0 0-2 2v12a2 2 0 0 0 2 2h8a2 2 0 0 0 2-2V2a2 2 0 0 0-2-2H4zm0 1h8a1 1 0 0 1 1 1v12a1 1 0 0 1-1 1H4a1 1 0 0 1-1-1V2a1 1 0 0 1 1-1z">
       </path>
       <path d="M4.603 12.087a.81.81 0 0 1-.438-.42c-.195-.388-.13-.776.08-1.102.198-.307.526-.568.897-.787a7.68 7.68 0 0 1 1.482-.645 19.701 19.701 0 0 0 1.062-2.227 7.269 7.269 0 0 1-.43-1.295c-.086-.4-.119-.796-.046-1.136.075-.354.274-.672.65-.823.192-.077.4-.12.602-.077a.7.7 0 0 1 .477.365c.088.164.12.356.127.538.007.187-.012.395-.047.614-.084.51-.27 1.134-.52 1.794a10.954 10.954 0 0 0 .98 1.686 5.753 5.753 0 0 1 1.334.05c.364.065.734.195.96.465.12.144.193.32.2.518.007.192-.047.382-.138.563a1.04 1.04 0 0 1-.354.416.856.856 0 0 1-.51.138c-.331-.014-.654-.196-.933-.417a5.716 5.716 0 0 1-.911-.95 11.642 11.642 0 0 0-1.997.406 11.311 11.311 0 0 1-1.021 1.51c-.29.35-.608.655-.926.787a.793.793 0 0 1-.58.029zm1.379-1.901c-.166.076-.32.156-.459.238-.328.194-.541.383-.647.547-.094.145-.096.25-.04.361.01.022.02.036.026.044a.27.27 0 0 0 .035-.012c.137-.056.355-.235.635-.572a8.18 8.18 0 0 0 .45-.606zm1.64-1.33a12.647 12.647 0 0 1 1.01-.193 11.666 11.666 0 0 1-.51-.858 20.741 20.741 0 0 1-.5 1.05zm2.446.45c.15.162.296.3.435.41.24.19.407.253.498.256a.107.107 0 0 0 .07-.015.307.307 0 0 0 .094-.125.436.436 0 0 0 .059-.2.095.095 0 0 0-.026-.063c-.052-.062-.2-.152-.518-.209a3.881 3.881 0 0 0-.612-.053zM8.078 5.8a6.7 6.7 0 0 0 .2-.828c.031-.188.043-.343.038-.465a.613.613 0 0 0-.032-.198.517.517 0 0 0-.145.04c-.087.035-.158.106-.196.283-.04.192-.03.469.046.822.024.111.054.227.09.346z">
       </path>
      </svg>
     </a>
     <small class="titlemark">
      7.3.2
     </small>
     <a id="x9-1060007.3.2">
     </a>
     Stacking Multiple Feature Maps
    </h3>
    <p class="noindent">
     Up
to
now,
for
simplicity,
We
represented
the
output
of
                                                                                
                                                                                
each
convolutional
layer
as
a
2D
layer,
but
in
reality
a
convolutional
layer
has
multiple
filters
     <alert style="color: #821131;">
      <span id="bold" style="font-weight:bold;">
       <sup class="textsuperscript">
        10
       </sup>
      </span>
     </alert>
     <span class="marginnote">
      <span style="color:#999999;">
      </span>
      <alert style="color: #821131;">
       <span id="bold" style="font-weight:bold;">
        <sup class="textsuperscript">
         10
        </sup>
       </span>
      </alert>
      <span style="color:#0063B2;">
       The
number
of
filters
is
left
up
to
the
programmer
      </span>
     </span>
     and
outputs
one
feature
map
per
filter,
so
it
is
more
accurately
represented
in
3D,
which
can
be
seen
in
     <span id="bold" style="font-weight:bold;">
      Fig.
     </span>
     <a href="#x9-106001r6">
      7.6
     </a>.
     It
has
one
neuron
per
pixel
in
each
feature
map,
and
all
neurons
within
a
given
feature
map
share
                                                                                
                                                                                
the
same
parameters
(i.e.,
the
same
kernel
and
bias
term).
Neurons
in
different
feature
maps
use
different
parameters.
A
neuron’s
receptive
field
is
the
same
as
described
earlier,
but
it
extends
across
all
the
feature
maps
of
the
previous
layer.
In
short,
a
convolutional
layer
simultaneously
applies
multiple
trainable
filters
to
its
inputs,
making
it
capable
of
detecting
multiple
features
anywhere
in
its
inputs.
    </p>
    
    <div class="figure">
     <p class="noindent">
      <img alt="PIC" height="" src="figures/Computer-Vision-using-Convolutional-Neural-Networks/raster/cnn-multilayer.png" width="150%"/>
      <a id="x9-106001r6">
      </a>
     </p>
     <figcaption class="caption">
      <span class="id">
       Figure 7.6:
      </span>
      <span class="content">
      </span>
     </figcaption>
    </div>
    <div class="knowledge">
     <p class="noindent">
      The fact that all neurons in a feature map share the same parameters dramatically reduces the number of parameters in the
model. Once the
      <a class="glossary" href="DataScienceIILectureBookli2.html#glo:cnn">
       CNN
      </a>
      has learned to recognize a pattern in one location, it can recognize it in any other location. In contrast,
once a fully connected neural network has learned to recognize a pattern in one location, it can only recognize it in that
particular location.
     </p>
    </div>
    <p class="noindent">
     Input images are also composed of multiple sublayers: one per color channel. As we already now, there are typically three: red,
green, and blue (RGB). Grayscale images have just one channel, but some images may have many more-for example, satellite
images that capture extra light frequencies
    </p>
    <p class="noindent">
     Specifically, a neuron located in row
     <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
      <mi>
       i
      </mi>
     </math>
     ,
column
     <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
      <mi>
       j
      </mi>
     </math>
     of the feature map
     <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
      <mi>
       k
      </mi>
     </math>
     in a given convolutional layer
     <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
      <mi>
       l
      </mi>
     </math>
     is connected to the outputs of
the neurons in the previous layer
     <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
      <mi>
       l
      </mi>
      <mo class="MathClass-bin" stretchy="false">
       −
      </mo>
      <mn>
       1
      </mn>
     </math>
     ,
located in rows
     <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
      <mi>
       i
      </mi>
      <mo class="MathClass-bin" stretchy="false">
       ×
      </mo>
      <msubsup>
       <mrow>
        <mi>
         s
        </mi>
       </mrow>
       <mrow>
        <mi>
         h
        </mi>
       </mrow>
       <mrow>
       </mrow>
      </msubsup>
     </math>
     to
     <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
      <mi>
       i
      </mi>
      <mo class="MathClass-bin" stretchy="false">
       ×
      </mo>
      <msubsup>
       <mrow>
        <mi>
         s
        </mi>
       </mrow>
       <mrow>
        <mi>
         h
        </mi>
       </mrow>
       <mrow>
       </mrow>
      </msubsup>
      <mo class="MathClass-bin" stretchy="false">
       +
      </mo>
      <msubsup>
       <mrow>
        <mi>
         f
        </mi>
       </mrow>
       <mrow>
        <mi>
         h
        </mi>
       </mrow>
       <mrow>
       </mrow>
      </msubsup>
      <mo class="MathClass-bin" stretchy="false">
       −
      </mo>
      <mn>
       1
      </mn>
     </math>
     and
columns
     <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
      <mi>
       j
      </mi>
      <mo class="MathClass-bin" stretchy="false">
       ×
      </mo>
      <msubsup>
       <mrow>
        <mi>
         s
        </mi>
       </mrow>
       <mrow>
        <mi>
         w
        </mi>
       </mrow>
       <mrow>
       </mrow>
      </msubsup>
     </math>
     to
     <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
      <mi>
       j
      </mi>
      <mo class="MathClass-bin" stretchy="false">
       ×
      </mo>
      <msubsup>
       <mrow>
        <mi>
         s
        </mi>
       </mrow>
       <mrow>
        <mi>
         w
        </mi>
       </mrow>
       <mrow>
       </mrow>
      </msubsup>
      <mo class="MathClass-bin" stretchy="false">
       +
      </mo>
      <msubsup>
       <mrow>
        <mi>
         f
        </mi>
       </mrow>
       <mrow>
        <mi>
         w
        </mi>
       </mrow>
       <mrow>
       </mrow>
      </msubsup>
      <mo class="MathClass-bin" stretchy="false">
       −
      </mo>
      <mn>
       1
      </mn>
     </math>
     ,across all feature
maps( in layer
     <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
      <mi>
       l
      </mi>
      <mo class="MathClass-bin" stretchy="false">
       −
      </mo>
      <mn>
       1
      </mn>
     </math>
     ).
    </p>
    <div class="warning">
     <p class="noindent">
      Within a layer, all neurons located in the same row i and column j but in different feature maps are connected to the outputs of
the exact same neurons in the previous layer.
     </p>
    </div>
    <p class="noindent">
     This definition can be summarised in one big mathematical equation: it shows how to compute the output of a given neuron in a
convolutional layer.
    </p>
    <p class="noindent">
     Let’s try to understand the variables:
    </p>
    <ul class="itemize1">
     <li class="itemize">
      <p class="noindent">
       <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
        <msubsup>
         <mrow>
          <mi>
           z
          </mi>
         </mrow>
         <mrow>
          <mi>
           i
          </mi>
          <mo class="MathClass-punc" stretchy="false">
           ,
          </mo>
          <mi>
           j
          </mi>
          <mo class="MathClass-punc" stretchy="false">
           ,
          </mo>
          <mi>
           k
          </mi>
         </mrow>
         <mrow>
         </mrow>
        </msubsup>
       </math>
       is
the
output
of
the
neuron
located
in
row
       <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
        <mi>
         i
        </mi>
       </math>
       ,
column
       <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
        <mi>
         j
        </mi>
       </math>
       in
feature
map
       <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
        <mi>
         k
        </mi>
       </math>
       of
the
convolutional
layer
(layer
       <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
        <mi>
         l
        </mi>
       </math>
       ).
      </p>
     </li>
     <li class="itemize">
      <p class="noindent">
       <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
        <msubsup>
         <mrow>
          <mi>
           s
          </mi>
         </mrow>
         <mrow>
          <mi>
           h
          </mi>
         </mrow>
         <mrow>
         </mrow>
        </msubsup>
        <mo class="MathClass-punc" stretchy="false">
         ,
        </mo>
        <mspace class="thinspace" width="0.17em">
        </mspace>
        <msubsup>
         <mrow>
          <mi>
           s
          </mi>
         </mrow>
         <mrow>
          <mi>
           w
          </mi>
         </mrow>
         <mrow>
         </mrow>
        </msubsup>
       </math>
       are
the
vertical
and
horizontal
strides,
       <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
        <msubsup>
         <mrow>
          <mi>
           f
          </mi>
         </mrow>
         <mrow>
          <mi>
           h
          </mi>
         </mrow>
         <mrow>
         </mrow>
        </msubsup>
        <mo class="MathClass-punc" stretchy="false">
         ,
        </mo>
        <mspace class="thinspace" width="0.17em">
        </mspace>
        <msubsup>
         <mrow>
          <mi>
           f
          </mi>
         </mrow>
         <mrow>
          <mi>
           w
          </mi>
         </mrow>
         <mrow>
         </mrow>
        </msubsup>
       </math>
       are
the
                                                                                
                                                                                
     
height
and
width
of
the
receptive
field,
and
       <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
        <msubsup>
         <mrow>
          <mi>
           f
          </mi>
         </mrow>
         <mrow>
          <mi>
           n
          </mi>
         </mrow>
         <mrow>
          <mi>
           ′
          </mi>
         </mrow>
        </msubsup>
       </math>
       is
the
number
of
feature
maps
in
the
previous
layer
(
       <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
        <mi>
         l
        </mi>
        <mo class="MathClass-bin" stretchy="false">
         −
        </mo>
        <mn>
         1
        </mn>
       </math>
       ).
      </p>
     </li>
     <li class="itemize">
      <p class="noindent">
       <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
        <msubsup>
         <mrow>
          <mi>
           x
          </mi>
         </mrow>
         <mrow>
          <msubsup>
           <mrow>
            <mi>
             i
            </mi>
           </mrow>
           <mrow>
           </mrow>
           <mrow>
            <mi>
             ′
            </mi>
           </mrow>
          </msubsup>
          <mo class="MathClass-punc" stretchy="false">
           ,
          </mo>
          <msubsup>
           <mrow>
            <mi>
             j
            </mi>
           </mrow>
           <mrow>
           </mrow>
           <mrow>
            <mi>
             ′
            </mi>
           </mrow>
          </msubsup>
          <mo class="MathClass-punc" stretchy="false">
           ,
          </mo>
          <msubsup>
           <mrow>
            <mi>
             k
            </mi>
           </mrow>
           <mrow>
           </mrow>
           <mrow>
            <mi>
             ′
            </mi>
           </mrow>
          </msubsup>
         </mrow>
         <mrow>
         </mrow>
        </msubsup>
       </math>
       is
the
output
of
the
neuron
located
in
layer
       <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
        <mi>
         l
        </mi>
        <mo class="MathClass-bin" stretchy="false">
         −
        </mo>
        <mn>
         1
        </mn>
       </math>
       ,
row
       <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
        <msubsup>
         <mrow>
          <mi>
           i
          </mi>
         </mrow>
         <mrow>
         </mrow>
         <mrow>
          <mi>
           ′
          </mi>
         </mrow>
        </msubsup>
       </math>
       ,
column
       <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
        <msubsup>
         <mrow>
          <mi>
           j
          </mi>
         </mrow>
         <mrow>
         </mrow>
         <mrow>
          <mi>
           ′
          </mi>
         </mrow>
        </msubsup>
       </math>
       ,
feature
map
       <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
        <msubsup>
         <mrow>
          <mi>
           k
          </mi>
         </mrow>
         <mrow>
         </mrow>
         <mrow>
          <mi>
           ′
          </mi>
         </mrow>
        </msubsup>
       </math>
       ,
or
channel
       <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
        <msubsup>
         <mrow>
          <mi>
           k
          </mi>
         </mrow>
         <mrow>
         </mrow>
         <mrow>
          <mi>
           ′
          </mi>
         </mrow>
        </msubsup>
       </math>
       if
the
previous
layer
is
the
input
layer.
      </p>
     </li>
     <li class="itemize">
      <p class="noindent">
       <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
        <munder accent="true">
         <mrow>
          <mo class="MathClass-open" stretchy="false">
           [
          </mo>
         </mrow>
         <mo accent="true">
          ¯
         </mo>
        </munder>
        <mi>
         k
        </mi>
        <mo class="MathClass-close" stretchy="false">
         ]
        </mo>
       </math>
       is
the
bias
term
for
feature
map
       <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
        <mi>
         k
        </mi>
       </math>
       (in
                                                                                
                                                                                
     
layer
       <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
        <mi>
         l
        </mi>
       </math>
       ).
Think
of
it
as
a
knob
that
tweaks
the
overall
brightness
of
the
feature
map
       <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
        <mi>
         k
        </mi>
       </math>
       .
      </p>
     </li>
     <li class="itemize">
      <p class="noindent">
       <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
        <msubsup>
         <mrow>
          <mi>
           w
          </mi>
         </mrow>
         <mrow>
          <mi>
           u
          </mi>
          <mo class="MathClass-punc" stretchy="false">
           ,
          </mo>
          <mi>
           v
          </mi>
          <mo class="MathClass-punc" stretchy="false">
           ,
          </mo>
          <msubsup>
           <mrow>
            <mi>
             k
            </mi>
           </mrow>
           <mrow>
           </mrow>
           <mrow>
            <mi>
             ′
            </mi>
           </mrow>
          </msubsup>
          <mo class="MathClass-punc" stretchy="false">
           ,
          </mo>
          <msubsup>
           <mrow>
            <mi>
             k
            </mi>
           </mrow>
           <mrow>
           </mrow>
           <mrow>
           </mrow>
          </msubsup>
         </mrow>
         <mrow>
         </mrow>
        </msubsup>
       </math>
       is
the
connection
weight
between
any
neuron
in
feature
map
       <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
        <mi>
         k
        </mi>
       </math>
       of
the
layer
       <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
        <mi>
         l
        </mi>
       </math>
       and
its
input
located
at
row
       <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
        <mi>
         u
        </mi>
       </math>
       ,
column
       <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
        <mi>
         v
        </mi>
       </math>
       (relative
to
the
neuron’s
receptive
field),
and
feature
map
       <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
        <msubsup>
         <mrow>
          <mi>
           k
          </mi>
         </mrow>
         <mrow>
         </mrow>
         <mrow>
          <mi>
           ′
          </mi>
         </mrow>
        </msubsup>
       </math>
       .
      </p>
     </li>
    </ul>
    <h3 class="subsectionHead">
     <a aria-label="Link to this section in PDF version" class="pdf-link" href="DataScienceIILectureBook.pdf#subsection.7.3.3" style="color: red;  position: absolute;  left: -30px;" target="_blank">
      <svg class="bi bi-file-pdf" fill="currentColor" height="20" viewbox="0 0 16 16" width="20" xmlns="http://www.w3.org/2000/svg">
       <path d="M4 0a2 2 0 0 0-2 2v12a2 2 0 0 0 2 2h8a2 2 0 0 0 2-2V2a2 2 0 0 0-2-2H4zm0 1h8a1 1 0 0 1 1 1v12a1 1 0 0 1-1 1H4a1 1 0 0 1-1-1V2a1 1 0 0 1 1-1z">
       </path>
       <path d="M4.603 12.087a.81.81 0 0 1-.438-.42c-.195-.388-.13-.776.08-1.102.198-.307.526-.568.897-.787a7.68 7.68 0 0 1 1.482-.645 19.701 19.701 0 0 0 1.062-2.227 7.269 7.269 0 0 1-.43-1.295c-.086-.4-.119-.796-.046-1.136.075-.354.274-.672.65-.823.192-.077.4-.12.602-.077a.7.7 0 0 1 .477.365c.088.164.12.356.127.538.007.187-.012.395-.047.614-.084.51-.27 1.134-.52 1.794a10.954 10.954 0 0 0 .98 1.686 5.753 5.753 0 0 1 1.334.05c.364.065.734.195.96.465.12.144.193.32.2.518.007.192-.047.382-.138.563a1.04 1.04 0 0 1-.354.416.856.856 0 0 1-.51.138c-.331-.014-.654-.196-.933-.417a5.716 5.716 0 0 1-.911-.95 11.642 11.642 0 0 0-1.997.406 11.311 11.311 0 0 1-1.021 1.51c-.29.35-.608.655-.926.787a.793.793 0 0 1-.58.029zm1.379-1.901c-.166.076-.32.156-.459.238-.328.194-.541.383-.647.547-.094.145-.096.25-.04.361.01.022.02.036.026.044a.27.27 0 0 0 .035-.012c.137-.056.355-.235.635-.572a8.18 8.18 0 0 0 .45-.606zm1.64-1.33a12.647 12.647 0 0 1 1.01-.193 11.666 11.666 0 0 1-.51-.858 20.741 20.741 0 0 1-.5 1.05zm2.446.45c.15.162.296.3.435.41.24.19.407.253.498.256a.107.107 0 0 0 .07-.015.307.307 0 0 0 .094-.125.436.436 0 0 0 .059-.2.095.095 0 0 0-.026-.063c-.052-.062-.2-.152-.518-.209a3.881 3.881 0 0 0-.612-.053zM8.078 5.8a6.7 6.7 0 0 0 .2-.828c.031-.188.043-.343.038-.465a.613.613 0 0 0-.032-.198.517.517 0 0 0-.145.04c-.087.035-.158.106-.196.283-.04.192-.03.469.046.822.024.111.054.227.09.346z">
       </path>
      </svg>
     </a>
     <small class="titlemark">
      7.3.3
     </small>
     <a id="x9-1070007.3.3">
     </a>
     Implementing Convolutional Layers with Keras
    </h3>
    <p class="noindent">
     As with every
     <a class="glossary" href="DataScienceIILectureBookli2.html#glo:ml">
      ML
     </a>
     application, we load and preprocess a couple of sample images, using <span style="color:#054C5C;"><code class="verb">sklearn</code></span>
     ’s <span style="color:#054C5C;"><code class="verb">load_sample_image()</code></span>
     function
and Keras’s CenterCrop and Rescaling layers:
    </p>
    <p class="noindent">
    </p>
    <pre><div id="fancyvrb162" style="padding:20px;border-radius: 3px;"><a id="x9-107002r30"></a><span style="color:#2B2BFF;">from</span><span style="color:#BABABA;"> </span>sklearn.datasets<span style="color:#BABABA;"> </span><span style="color:#2B2BFF;">import</span> load_sample_images 
<a id="x9-107004r31"></a><span style="color:#2B2BFF;">import</span><span style="color:#BABABA;"> </span>tensorflow<span style="color:#BABABA;"> </span><span style="color:#2B2BFF;">as</span><span style="color:#BABABA;"> </span>tf 
<a id="x9-107006r32"></a> 
<a id="x9-107008r33"></a>images = load_sample_images()[<span style="color:#800080;">"images"</span>] 
<a id="x9-107010r34"></a>images = tf.keras.layers.CenterCrop(height=<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">70</span></span>, width=<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">120</span></span>)(images) 
<a id="x9-107012r35"></a>images = tf.keras.layers.Rescaling(scale=<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">1</span></span> / <span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">255</span></span>)(images)</div></pre>
    <p class="noindent">
     Let’s
look
at
the
shape
of
the
images
tensor
     <alert style="color: #821131;">
      <span id="bold" style="font-weight:bold;">
       <sup class="textsuperscript">
        11
       </sup>
      </span>
     </alert>
     <span class="marginnote">
      <span style="color:#999999;">
      </span>
      <alert style="color: #821131;">
       <span id="bold" style="font-weight:bold;">
        <sup class="textsuperscript">
         11
        </sup>
       </span>
      </alert>
      <span style="color:#0063B2;">
       A
tensor
is
a
generalization
of
vectors
and
matrices
to
potentially
higher
dimensions.
      </span>
     </span>
     :
    </p>
    
    <p class="noindent">
    </p>
    <pre><div id="fancyvrb163" style="padding:20px;border-radius: 3px;"><a id="x9-107014r42"></a><span style="color:#2B2BFF;">print</span>(images.shape)</div></pre>
    <div class="tcolorbox tcolorbox" id="tcolobox-76">
     <div class="tcolorbox-content">
      <pre><div id="fancyvrb164" style="padding:20px;border-radius: 3px;"><a id="x9-107016r1"></a>(2, 70, 120, 3)</div></pre>
     </div>
    </div>
    <p class="noindent">
     It’s
a
4D
tensor.
Let’s
see
why
this
is
a
4D
matrix.
There
are
two
<alert style="color: #821131;">(2)</alert>
sample
images,
which
explains
the
first
dimension.
Then
each
image
is
     <alert style="color: #821131;">
      70
     </alert>
     -by-
     <alert style="color: #821131;">
      120
     </alert>
     ,
as
that’s
                                                                                
                                                                                
the
size
we
specified
when
creating
the
CenterCrop
layer
     <alert style="color: #821131;">
      <span id="bold" style="font-weight:bold;">
       <sup class="textsuperscript">
        12
       </sup>
      </span>
     </alert>
     <span class="marginnote">
      <span style="color:#999999;">
      </span>
      <alert style="color: #821131;">
       <span id="bold" style="font-weight:bold;">
        <sup class="textsuperscript">
         12
        </sup>
       </span>
      </alert>
      <span style="color:#0063B2;">
       the
original
images
were
427-by-640
      </span>
     </span>
     .
This
explains
the
second
and
third
dimensions.
And
lastly,
each
pixel
holds
one
value
per
colour
channel,
and
there
are
three
<alert style="color: #821131;">(3)</alert>
of
them:
    </p>
    
    <p class="noindent">
    </p>
    <div class="quoteblock">
     <p class="noindent">
      Red,
Green,
and
Blue.
     </p>
    </div>
    <p class="noindent">
     Now let’s create a 2D convolutional layer and feed it these images to see what comes out. For this, Keras provides a
Convolution2D layer, alias <span style="color:#054C5C;"><code class="verb">Conv2D</code></span>. Under the hood, this layer relies on TensorFlow’s <span style="color:#054C5C;"><code class="verb">tf.nn.conv2d()</code></span>
     operation.
    </p>
    <p class="noindent">
     Let’s create a convolutional layer with 32 filters, each of size 7-by-7 (using <span style="color:#054C5C;"><code class="verb">kernel_size=7</code></span>, which is equivalent to using <span style="color:#054C5C;"><code class="verb">kernel_size=(7 , 7)</code></span>
     ), and apply this layer to our small batch of two <alert style="color: #821131;">(2)</alert> images:
    </p>
    <p class="noindent">
    </p>
    <pre><div id="fancyvrb165" style="padding:20px;border-radius: 3px;"><a id="x9-107018r52"></a>tf.random.set_seed(<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">42</span></span>)  <span style="color:#008700;"><italic># extra code  ensures reproducibility</italic></span> 
<a id="x9-107020r53"></a>conv_layer = tf.keras.layers.Conv2D(filters=<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">32</span></span>, kernel_size=<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">7</span></span>) 
<a id="x9-107022r54"></a>fmaps = conv_layer(images)</div></pre>
    <p class="noindent">
     Now let’s look at the output’s shape:
    </p>
    <p class="noindent">
    </p>
    <pre><div id="fancyvrb166" style="padding:20px;border-radius: 3px;"><a id="x9-107024r61"></a><span style="color:#2B2BFF;">print</span>(fmaps.shape)</div></pre>
    <div class="tcolorbox tcolorbox" id="tcolobox-77">
     <div class="tcolorbox-content">
      <pre><div id="fancyvrb167" style="padding:20px;border-radius: 3px;"><a id="x9-107026r1"></a>(2, 64, 114, 32)</div></pre>
     </div>
    </div>
    <p class="noindent">
     The output shape is similar to the input shape, with two <alert style="color: #821131;">(2)</alert> main differences.
    </p>
    <dl class="enumerate-enumitem">
     <dt class="enumerate-enumitem">
      1.
     </dt>
     <dd class="enumerate-enumitem">
      There
are
32
channels
instead
of
3.
This
is
because
we
set <span style="color:#054C5C;"><code class="verb">filters=32</code></span>,
so
we
get
32
output
feature
maps:
instead
of
the
intensity
of
red,
green,
and
blue
at
each
location,
we
now
have
the
intensity
of
each
feature
at
each
location.
     </dd>
     <dt class="enumerate-enumitem">
      2.
     </dt>
     <dd class="enumerate-enumitem">
      The
height
and
width
have
both
shrunk
by
6
pixels.
This
is
due
to
the
fact
that
                                                                                
                                                                                
     
the
Conv2D
layer
does
      <span id="bold" style="font-weight:bold;">
       NOT
      </span>
      use
any
zero-padding
by
default,
which
means
that
we
lose
a
few
pixels
on
the
sides
of
the
output
feature
maps,
depending
on
the
size
of
the
filters.
In
this
case,
since
the
kernel
size
is
7,
we
lose
6
pixels
horizontally
and
6
pixels
vertically
(i.e.,
3
pixels
on
each
side).
     </dd>
    </dl>
    <p class="noindent">
     If instead we set <span style="color:#054C5C;"><code class="verb">padding="same"</code></span>, then the inputs are padded with enough zeros on all sides to ensure that the output feature
maps end up with the same size as the inputs (hence the name of this option):
    </p>
    <div class="figure">
     <p class="noindent">
      <img alt="PIC" height="" src="codes/images/Computer-Vision-using-Convolutional-Neural-Networks/example-feature-maps-.svg" width="150%"/>
      <a id="x9-107029r7">
      </a>
     </p>
     <figcaption class="caption">
      <span class="id">
       Figure 7.7:
      </span>
      <span class="content">
      </span>
     </figcaption>
    </div>
    <p class="noindent">
     These two padding options are illustrated in
     <span id="bold" style="font-weight:bold;">
      Fig.
     </span>
     <a href="#x9-107030r8">
      7.8
     </a>. For simplicity, only the horizontal dimension is shown here, but of course
the same logic applies to the vertical dimension as well.
    </p>
    <p class="noindent">
     If the stride is greater than 1 (in any direction), then the output size will not be equal to the input size, even if <span style="color:#054C5C;"><code class="verb">padding="same"</code></span>.
For example, if we set <span style="color:#054C5C;"><code class="verb">strides=2</code></span>
     (or equivalently <span style="color:#054C5C;"><code class="verb">strides=(2, 2)</code></span>
     ), then the output feature maps will be 35-by-60: halved both
vertically and horizontally.
    </p>
    <p class="noindent">
     <span id="bold" style="font-weight:bold;">
      Fig.
     </span>
     <a href="#x9-107030r8">
      7.8
     </a>
     shows what happens when
     <span id="bold" style="font-weight:bold;">
      strides=2
     </span>
     , with both padding options.
    </p>
    <div class="figure">
     <p class="noindent">
      <img alt="PIC" height="" src="figures/Computer-Vision-using-Convolutional-Neural-Networks/raster/cnn-strides-comparison.png" width="150%"/>
      <a id="x9-107030r8">
      </a>
     </p>
     <figcaption class="caption">
      <span class="id">
       Figure 7.8:
      </span>
      <span class="content">
       Showcasing different padding options.
      </span>
     </figcaption>
    </div>
    <p class="noindent">
     If we are curious, this is how the output size is computed:
    </p>
    <ul class="itemize1">
     <li class="itemize">
      <p class="noindent">
       With <span style="color:#054C5C;"><code class="verb">padding="valid"</code></span>,
if
the
width
of
the
input
is
ih,
then
the
output
width
is
equal
to
(ih
-
fh
+
sh)
/
sh,
rounded
down.
Recall
that
fh
is
the
kernel
width,
and
sh
is
the
horizontal
stride.
Any
remainder
in
the
division
corresponds
to
ignored
columns
on
the
right
side
of
the
input
image.
The
same
logic
can
                                                                                
                                                                                
     
be
used
to
compute
the
output
height,
and
any
ignored
rows
at
the
bottom
of
the
image.
      </p>
     </li>
     <li class="itemize">
      <p class="noindent">
       With <span style="color:#054C5C;"><code class="verb">padding="same"</code></span>,
the
output
width
is
equal
to
ih
/
sh,
rounded
up.
To
make
this
possible,
the
appropriate
number
of
zero
columns
are
padded
to
the
left
and
right
of
the
input
image
(an
equal
number
if
possible,
or
just
one
more
on
the
right
                                                                                
                                                                                
     
side).
Assuming
the
output
width
is
ow,
then
the
number
of
padded
zero
columns
is
(ow
-
1)
Œ
sh
+
fh
-
ih.
Again,
the
same
logic
can
be
used
to
compute
the
output
height
and
the
number
of
padded
rows.
      </p>
     </li>
    </ul>
    <p class="noindent">
     Now let’s look at the layer’s weights (which were noted wu, v, k’, k and bk in Equation 14-1). Just like a Dense layer, a Conv2D
layer holds all the layer’s weights, including the kernels and biases. The kernels are initialized randomly, while the biases are
initialized to zero. These weights are accessible as TF variables via the weights attribute, or as NumPy arrays via the <span style="color:#054C5C;"><code class="verb">get_weights()</code></span>
     method:
    </p>
    <p class="noindent">
     The kernels array is 4D, and its shape is [ <span style="color:#054C5C;"><code class="verb">kernel_height</code></span>, <span style="color:#054C5C;"><code class="verb">kernel_width</code></span>, <span style="color:#054C5C;"><code class="verb">input_channels</code></span>, <span style="color:#054C5C;"><code class="verb">output_channels</code></span>
     ]. The
biases array is 1D, with shape [ <span style="color:#054C5C;"><code class="verb">output_channels</code></span>
     ]. The number of output channels is equal to the number of output
feature maps, which is also equal to the number of filters. Most importantly, note that the height and width of the
input images do not appear in the kernel’s shape: this is because all the neurons in the output feature maps
share the same weights, as explained earlier. This means that we can feed images of any size to this layer, as
long as they are at least as large as the kernels, and if they have the right number of channels (three in this
case).
    </p>
    <p class="noindent">
     Lastly, we will generally want to specify an activation function (such as ReLU) when creating a Conv2D layer, and
also specify the corresponding kernel initializer. This is for the same reason as for Dense layers: a convolutional
layer performs a linear operation, so if we stacked multiple convolutional layers without any activation functions
they would all be equivalent to a single convolutional layer, and they wouldn’t be able to learn anything really
complex.
    </p>
    <p class="noindent">
     As we can see, convolutional layers have quite a few hyperparameters: filters, <span style="color:#054C5C;"><code class="verb">kernel_size</code></span>, padding, strides, activation, <span style="color:#054C5C;"><code class="verb">kernel_initializer</code></span>, etc. As always, we can use cross-validation to find the right hyperparameter values, but this is very
time-consuming. We will discuss common
     <a class="glossary" href="DataScienceIILectureBookli2.html#glo:cnn">
      CNN
     </a>
     architectures later in this chapter, to give you some idea of which hyperparameter
values work best in practice.
    </p>
    <h3 class="subsectionHead">
     <a aria-label="Link to this section in PDF version" class="pdf-link" href="DataScienceIILectureBook.pdf#subsection.7.3.4" style="color: red;  position: absolute;  left: -30px;" target="_blank">
      <svg class="bi bi-file-pdf" fill="currentColor" height="20" viewbox="0 0 16 16" width="20" xmlns="http://www.w3.org/2000/svg">
       <path d="M4 0a2 2 0 0 0-2 2v12a2 2 0 0 0 2 2h8a2 2 0 0 0 2-2V2a2 2 0 0 0-2-2H4zm0 1h8a1 1 0 0 1 1 1v12a1 1 0 0 1-1 1H4a1 1 0 0 1-1-1V2a1 1 0 0 1 1-1z">
       </path>
       <path d="M4.603 12.087a.81.81 0 0 1-.438-.42c-.195-.388-.13-.776.08-1.102.198-.307.526-.568.897-.787a7.68 7.68 0 0 1 1.482-.645 19.701 19.701 0 0 0 1.062-2.227 7.269 7.269 0 0 1-.43-1.295c-.086-.4-.119-.796-.046-1.136.075-.354.274-.672.65-.823.192-.077.4-.12.602-.077a.7.7 0 0 1 .477.365c.088.164.12.356.127.538.007.187-.012.395-.047.614-.084.51-.27 1.134-.52 1.794a10.954 10.954 0 0 0 .98 1.686 5.753 5.753 0 0 1 1.334.05c.364.065.734.195.96.465.12.144.193.32.2.518.007.192-.047.382-.138.563a1.04 1.04 0 0 1-.354.416.856.856 0 0 1-.51.138c-.331-.014-.654-.196-.933-.417a5.716 5.716 0 0 1-.911-.95 11.642 11.642 0 0 0-1.997.406 11.311 11.311 0 0 1-1.021 1.51c-.29.35-.608.655-.926.787a.793.793 0 0 1-.58.029zm1.379-1.901c-.166.076-.32.156-.459.238-.328.194-.541.383-.647.547-.094.145-.096.25-.04.361.01.022.02.036.026.044a.27.27 0 0 0 .035-.012c.137-.056.355-.235.635-.572a8.18 8.18 0 0 0 .45-.606zm1.64-1.33a12.647 12.647 0 0 1 1.01-.193 11.666 11.666 0 0 1-.51-.858 20.741 20.741 0 0 1-.5 1.05zm2.446.45c.15.162.296.3.435.41.24.19.407.253.498.256a.107.107 0 0 0 .07-.015.307.307 0 0 0 .094-.125.436.436 0 0 0 .059-.2.095.095 0 0 0-.026-.063c-.052-.062-.2-.152-.518-.209a3.881 3.881 0 0 0-.612-.053zM8.078 5.8a6.7 6.7 0 0 0 .2-.828c.031-.188.043-.343.038-.465a.613.613 0 0 0-.032-.198.517.517 0 0 0-.145.04c-.087.035-.158.106-.196.283-.04.192-.03.469.046.822.024.111.054.227.09.346z">
       </path>
      </svg>
     </a>
     <small class="titlemark">
      7.3.4
     </small>
     <a id="x9-1080007.3.4">
     </a>
     Memory Requirements
    </h3>
    <p class="noindent">
     Another challenge with
     <a class="glossary" href="DataScienceIILectureBookli2.html#glo:cnn">
      CNN
     </a>
     s is that the convolutional layers require a huge amount of RAM. This is especially true during
training, because the reverse pass of back-propagation requires all the intermediate values computed during the forward
pass.
    </p>
    <p class="noindent">
     As
an
example,
consider
a
convolutional
layer
with
200
5-by-5
filters,
with
stride
1
and <span style="color:#054C5C;"><code class="verb">"same"</code></span>
     padding.
If
the
input
is
a
150-by-100
RGB
image,
then
the
number
of
parameters
is
(5-by-5-by-3+1)
Œ
200
=
15,200
     <alert style="color: #821131;">
      <span id="bold" style="font-weight:bold;">
       <sup class="textsuperscript">
        13
       </sup>
      </span>
     </alert>
     <span class="marginnote">
      <span style="color:#999999;">
      </span>
      <alert style="color: #821131;">
       <span id="bold" style="font-weight:bold;">
        <sup class="textsuperscript">
         13
        </sup>
       </span>
      </alert>
      <span style="color:#0063B2;">
       the
+
1
corresponds
to
the
bias
terms
      </span>
     </span>
     ,
which
is
fairly
small
compared
to
a
fully
connected
layer.
However,
each
of
the
200
feature
maps
                                                                                
                                                                                
contains
150
Œ
100
neurons,
and
each
of
these
neurons
needs
to
compute
a
weighted
sum
of
its
5-by-5-by-3
=
75
inputs:
that’s
a
total
of
225
million
float
multiplications.
    </p>
    
    <p class="noindent">
     Not
as
bad
as
a
fully
connected
layer,
but
still
quite
computationally
intensive.
Moreover,
if
the
feature
maps
are
represented
using
32-bit
floats,
then
the
convolutional
layer’s
output
will
occupy
     <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
      <mn>
       2
      </mn>
      <mn>
       0
      </mn>
      <mn>
       0
      </mn>
      <mo class="MathClass-bin" stretchy="false">
       ×
      </mo>
      <mn>
       1
      </mn>
      <mn>
       5
      </mn>
      <mn>
       0
      </mn>
      <mo class="MathClass-bin" stretchy="false">
       ×
      </mo>
      <mn>
       1
      </mn>
      <mn>
       0
      </mn>
      <mn>
       0
      </mn>
      <mo class="MathClass-bin" stretchy="false">
       ×
      </mo>
      <mn>
       3
      </mn>
      <mn>
       2
      </mn>
     </math>
     =
96
million
bits
(12
MB)
                                                                                
                                                                                
of
RAM.
8
And
that’s
just
for
one
instance-if
a
training
batch
contains
100
instances,
then
this
layer
will
use
up
1.2
GB
of
RAM.
    </p>
    <p class="noindent">
     During
inference
(i.e.,
when
making
a
prediction
for
a
new
instance)
the
RAM
occupied
by
one
layer
can
be
released
as
soon
as
the
next
layer
has
been
computed,
so
we
only
need
as
much
RAM
as
required
by
two
consecutive
layers.
                                                                                
                                                                                
But
during
training
everything
computed
during
the
forward
pass
needs
to
be
preserved
for
the
reverse
pass,
so
the
amount
of
RAM
needed
is
(at
least)
the
total
amount
of
RAM
required
by
all
layers.
    </p>
    <p class="noindent">
     Now
let’s
look
at
the
second
common
building
block
of
     <a class="glossary" href="DataScienceIILectureBookli2.html#glo:cnn">
      CNN
     </a>
     s:
the
     <alert style="color: #821131;">
      pooling
layer
     </alert>
     .
    </p>
    <p class="noindent">
    </p>
    <h2 class="sectionHead">
     <a aria-label="Link to this section in PDF version" class="pdf-link" href="DataScienceIILectureBook.pdf#section.7.4" style="color: red;  position: absolute;  left: -30px;" target="_blank">
      <svg class="bi bi-file-pdf" fill="currentColor" height="20" viewbox="0 0 16 16" width="20" xmlns="http://www.w3.org/2000/svg">
       <path d="M4 0a2 2 0 0 0-2 2v12a2 2 0 0 0 2 2h8a2 2 0 0 0 2-2V2a2 2 0 0 0-2-2H4zm0 1h8a1 1 0 0 1 1 1v12a1 1 0 0 1-1 1H4a1 1 0 0 1-1-1V2a1 1 0 0 1 1-1z">
       </path>
       <path d="M4.603 12.087a.81.81 0 0 1-.438-.42c-.195-.388-.13-.776.08-1.102.198-.307.526-.568.897-.787a7.68 7.68 0 0 1 1.482-.645 19.701 19.701 0 0 0 1.062-2.227 7.269 7.269 0 0 1-.43-1.295c-.086-.4-.119-.796-.046-1.136.075-.354.274-.672.65-.823.192-.077.4-.12.602-.077a.7.7 0 0 1 .477.365c.088.164.12.356.127.538.007.187-.012.395-.047.614-.084.51-.27 1.134-.52 1.794a10.954 10.954 0 0 0 .98 1.686 5.753 5.753 0 0 1 1.334.05c.364.065.734.195.96.465.12.144.193.32.2.518.007.192-.047.382-.138.563a1.04 1.04 0 0 1-.354.416.856.856 0 0 1-.51.138c-.331-.014-.654-.196-.933-.417a5.716 5.716 0 0 1-.911-.95 11.642 11.642 0 0 0-1.997.406 11.311 11.311 0 0 1-1.021 1.51c-.29.35-.608.655-.926.787a.793.793 0 0 1-.58.029zm1.379-1.901c-.166.076-.32.156-.459.238-.328.194-.541.383-.647.547-.094.145-.096.25-.04.361.01.022.02.036.026.044a.27.27 0 0 0 .035-.012c.137-.056.355-.235.635-.572a8.18 8.18 0 0 0 .45-.606zm1.64-1.33a12.647 12.647 0 0 1 1.01-.193 11.666 11.666 0 0 1-.51-.858 20.741 20.741 0 0 1-.5 1.05zm2.446.45c.15.162.296.3.435.41.24.19.407.253.498.256a.107.107 0 0 0 .07-.015.307.307 0 0 0 .094-.125.436.436 0 0 0 .059-.2.095.095 0 0 0-.026-.063c-.052-.062-.2-.152-.518-.209a3.881 3.881 0 0 0-.612-.053zM8.078 5.8a6.7 6.7 0 0 0 .2-.828c.031-.188.043-.343.038-.465a.613.613 0 0 0-.032-.198.517.517 0 0 0-.145.04c-.087.035-.158.106-.196.283-.04.192-.03.469.046.822.024.111.054.227.09.346z">
       </path>
      </svg>
     </a>
     <small class="titlemark">
      7.4
     </small>
     <a id="x9-1090007.4">
     </a>
     Pooling Layer
    </h2>
    <p class="noindent">
     The
goal
of
a
pooling
layer
is
to
subsample
(i.e.,
shrink)
the
input
image
to
reduce
the
computational
load,
the
memory
usage,
and
the
number
of
parameters
     <alert style="color: #821131;">
      <span id="bold" style="font-weight:bold;">
       <sup class="textsuperscript">
        14
       </sup>
      </span>
     </alert>
     <span class="marginnote">
      <span style="color:#999999;">
      </span>
      <alert style="color: #821131;">
       <span id="bold" style="font-weight:bold;">
        <sup class="textsuperscript">
         14
        </sup>
       </span>
      </alert>
      <span style="color:#0063B2;">
       This
limits
the
risk
of
overfitting
      </span>
     </span>
     .
    </p>
    
    <div class="figure">
     <p class="noindent">
      <img alt="PIC" height="" src="figures/Computer-Vision-using-Convolutional-Neural-Networks/raster/cnn-pooling-example.png" width="150%"/>
      <a id="x9-109001r9">
      </a>
     </p>
     <figcaption class="caption">
      <span class="id">
       Figure 7.9:
      </span>
      <span class="content">
      </span>
     </figcaption>
    </div>
    <p class="noindent">
     Just like in convolutional layers, each neuron in a pooling layer is connected to the outputs of a limited number of neurons in the
previous layer, located within a
     <alert style="color: #821131;">
      small rectangular receptive field
     </alert>
     . We must define its size, the stride, and the padding type,
just like before.
    </p>
    <p class="noindent">
     However, a pooling neuron has
     <span id="bold" style="font-weight:bold;">
      NO
     </span>
     weights. All it does is aggregate the inputs using an aggregation function such as the max or
mean.
    </p>
    <p class="noindent">
     <span id="bold" style="font-weight:bold;">
      Fig.
     </span>
     <a href="#x9-109001r9">
      7.9
     </a>
     shows a
     <span id="bold" style="font-weight:bold;">
      max pooling layer
     </span>
     , which is the most common type of pooling layer. In this example, we use a 2-by-2
pooling kernel, with a stride of 2 and no padding. Only the max input value in each receptive field makes it to the
next layer, while the other inputs are dropped. For example, in the lower-left receptive field in
     <span id="bold" style="font-weight:bold;">
      Fig.
     </span>
     <a href="#x9-109001r9">
      7.9
     </a>, the
input values are 1, 5, 3, 2, so only the max value, 5, is propagated to the next layer. Because of the stride of
2, the output image has half the height and half the width of the input image (rounded down since we use no
padding).
    </p>
    <div class="warning">
     <p class="noindent">
      A pooling layer typically works on every input channel independently, so the output depth (i.e., the number of channels) is the
same as the input depth.
     </p>
    </div>
    <p class="noindent">
     Other
than
reducing
computations,
memory
usage,
and
the
number
of
parameters,
a
max
pooling
layer
also
introduces
some
level
of
invariance
     <alert style="color: #821131;">
      <span id="bold" style="font-weight:bold;">
       <sup class="textsuperscript">
        15
       </sup>
      </span>
     </alert>
     <span class="marginnote">
      <span style="color:#999999;">
      </span>
      <alert style="color: #821131;">
       <span id="bold" style="font-weight:bold;">
        <sup class="textsuperscript">
         15
        </sup>
       </span>
      </alert>
      <span style="color:#0063B2;">
       In
the
context
of
computer
vision,
it
means
that
we
can
recognize
an
object
as
an
object,
even
when
its
appearance
varies
in
some
way.
      </span>
     </span>
     to
small
translations,
as
                                                                                
                                                                                
shown
in
     <span id="bold" style="font-weight:bold;">
      Fig.
     </span>
     <a href="#x9-109002r10">
      7.10
     </a>.
    </p>
    
    <div class="figure">
     <p class="noindent">
      <img alt="PIC" height="" src="figures/Computer-Vision-using-Convolutional-Neural-Networks/figures-3.svg" width="150%"/>
      <a id="x9-109002r10">
      </a>
     </p>
     <figcaption class="caption">
      <span class="id">
       Figure 7.10:
      </span>
      <span class="content">
       As can be seen from the image above, max-pooling allow a certain level of invariance when the object
          moves in small increments. This is an important property as it is favourable when small changes in
          movement don’t affect the overall recognition of the image.
      </span>
     </figcaption>
    </div>
    <p class="noindent">
     Here we assume that the bright pixels have a lower value than dark pixels, and we consider three images (A, B, C) going through
a max pooling layer with a 2-by-2 kernel and stride 2. Images B and C are the same as image A, but shifted by one and two
pixels to the right. As we can see, the outputs of the max pooling layer for images A and B are identical. This is what translation
invariance means. For image C, the output is different: it is shifted one pixel to the right (but there is still 50% invariance). By
inserting a max pooling layer every few layers in a
     <a class="glossary" href="DataScienceIILectureBookli2.html#glo:cnn">
      CNN
     </a>, it is possible to get some level of translation invariance at a larger
scale.
    </p>
    <div class="knowledge">
     <p class="noindent">
      Moreover, max pooling offers a small amount of rotational invariance and a slight scale invariance. Such invariance (even if it is
limited) can be useful in cases where the prediction should not depend on these details, such as in classification tasks.
     </p>
    </div>
    <p class="noindent">
     However,
max
pooling
has
some
downsides
too.
It’s
obviously
very
destructive:
even
with
a
tiny
2
Œ
2
kernel
and
a
stride
of
2,
the
output
will
be
two
times
smaller
in
both
directions
(so
its
area
will
be
four
times
smaller),
simply
dropping
75%
of
the
input
values.
And
in
some
applications,
invariance
is
                                                                                
                                                                                
not
desirable.
Take
semantic
segmentation
     <alert style="color: #821131;">
      <span id="bold" style="font-weight:bold;">
       <sup class="textsuperscript">
        16
       </sup>
      </span>
     </alert>
     <span class="marginnote">
      <span style="color:#999999;">
      </span>
      <alert style="color: #821131;">
       <span id="bold" style="font-weight:bold;">
        <sup class="textsuperscript">
         16
        </sup>
       </span>
      </alert>
      <span style="color:#0063B2;">
       The
task
of
classifying
each
pixel
in
an
image
according
to
the
object
that
pixel
belongs
to
      </span>
     </span>
     .
    </p>
    
    <p class="noindent">
     For
this
case,
if
the
input
image
is
translated
by
one
pixel
to
the
right,
the
output
should
also
be
translated
by
one
pixel
to
the
right.
The
goal
in
this
case
is
equivariance,
not
invariance:
a
small
change
to
the
inputs
should
lead
                                                                                
                                                                                
to
a
coresponding
small
change
in
the
output.
    </p>
    <p class="noindent">
    </p>
    <h2 class="sectionHead">
     <a aria-label="Link to this section in PDF version" class="pdf-link" href="DataScienceIILectureBook.pdf#section.7.5" style="color: red;  position: absolute;  left: -30px;" target="_blank">
      <svg class="bi bi-file-pdf" fill="currentColor" height="20" viewbox="0 0 16 16" width="20" xmlns="http://www.w3.org/2000/svg">
       <path d="M4 0a2 2 0 0 0-2 2v12a2 2 0 0 0 2 2h8a2 2 0 0 0 2-2V2a2 2 0 0 0-2-2H4zm0 1h8a1 1 0 0 1 1 1v12a1 1 0 0 1-1 1H4a1 1 0 0 1-1-1V2a1 1 0 0 1 1-1z">
       </path>
       <path d="M4.603 12.087a.81.81 0 0 1-.438-.42c-.195-.388-.13-.776.08-1.102.198-.307.526-.568.897-.787a7.68 7.68 0 0 1 1.482-.645 19.701 19.701 0 0 0 1.062-2.227 7.269 7.269 0 0 1-.43-1.295c-.086-.4-.119-.796-.046-1.136.075-.354.274-.672.65-.823.192-.077.4-.12.602-.077a.7.7 0 0 1 .477.365c.088.164.12.356.127.538.007.187-.012.395-.047.614-.084.51-.27 1.134-.52 1.794a10.954 10.954 0 0 0 .98 1.686 5.753 5.753 0 0 1 1.334.05c.364.065.734.195.96.465.12.144.193.32.2.518.007.192-.047.382-.138.563a1.04 1.04 0 0 1-.354.416.856.856 0 0 1-.51.138c-.331-.014-.654-.196-.933-.417a5.716 5.716 0 0 1-.911-.95 11.642 11.642 0 0 0-1.997.406 11.311 11.311 0 0 1-1.021 1.51c-.29.35-.608.655-.926.787a.793.793 0 0 1-.58.029zm1.379-1.901c-.166.076-.32.156-.459.238-.328.194-.541.383-.647.547-.094.145-.096.25-.04.361.01.022.02.036.026.044a.27.27 0 0 0 .035-.012c.137-.056.355-.235.635-.572a8.18 8.18 0 0 0 .45-.606zm1.64-1.33a12.647 12.647 0 0 1 1.01-.193 11.666 11.666 0 0 1-.51-.858 20.741 20.741 0 0 1-.5 1.05zm2.446.45c.15.162.296.3.435.41.24.19.407.253.498.256a.107.107 0 0 0 .07-.015.307.307 0 0 0 .094-.125.436.436 0 0 0 .059-.2.095.095 0 0 0-.026-.063c-.052-.062-.2-.152-.518-.209a3.881 3.881 0 0 0-.612-.053zM8.078 5.8a6.7 6.7 0 0 0 .2-.828c.031-.188.043-.343.038-.465a.613.613 0 0 0-.032-.198.517.517 0 0 0-.145.04c-.087.035-.158.106-.196.283-.04.192-.03.469.046.822.024.111.054.227.09.346z">
       </path>
      </svg>
     </a>
     <small class="titlemark">
      7.5
     </small>
     <a id="x9-1100007.5">
     </a>
     Implementing Pooling Layers with Keras
    </h2>
    <p class="noindent">
     The following code creates a MaxPooling2D layer, alias
     <span id="bold" style="font-weight:bold;">
      MaxPool2D
     </span>
     , using a 2-by-2 kernel. The strides default to the kernel
size, so this layer uses a stride of 2 (horizontally and vertically). By default, it uses
     <span id="bold" style="font-weight:bold;">
      "valid"
     </span>
     padding (i.e., no padding at
all):
    </p>
    <p class="noindent">
    </p>
    <pre><div id="fancyvrb168" style="padding:20px;border-radius: 3px;"><a id="x9-110002r86"></a>max_pool = tf.keras.layers.MaxPool2D(pool_size=<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">2</span></span>)</div></pre>
    <p class="noindent">
     To create an average pooling layer, just use AveragePooling2D, alias
     <span id="bold" style="font-weight:bold;">
      AvgPool2D
     </span>
     , instead of MaxPool2D. As we might expect, it
works exactly like a max pooling layer, except it computes the mean rather than the max. Average pooling layers used to be very
popular, but people mostly use max pooling layers now, as they generally perform better. This may seem surprising, since
computing the mean generally loses less information than computing the max. But on the other hand, max pooling preserves only
the strongest features, getting rid of all the meaningless ones, so the next layers get a cleaner signal to work with.
Moreover, max pooling offers stronger translation invariance than average pooling, and it requires slightly less
compute.
    </p>
    <p class="noindent">
     Note that max pooling and average pooling can be performed along the depth dimension instead of the spatial dimensions,
although it’s not as common. This can allow the
     <a class="glossary" href="DataScienceIILectureBookli2.html#glo:cnn">
      CNN
     </a>
     to learn to be invariant to various features. For example, it could
learn multiple filters, each detecting a different rotation of the same pattern (such as handwritten digits seen
in
     <span id="bold" style="font-weight:bold;">
      Fig.
     </span>
     <a href="#x9-110003r11">
      7.11
     </a>
     ), and the depthwise max pooling layer would ensure that the output is the same regardless of the
rotation. The
     <a class="glossary" href="DataScienceIILectureBookli2.html#glo:cnn">
      CNN
     </a>
     could similarly learn to be invariant to anything: thickness, brightness, skew, color, and so
on.
    </p>
    <div class="figure">
     <p class="noindent">
      <img alt="PIC" height="" src="figures/Computer-Vision-using-Convolutional-Neural-Networks/raster/cnn-max-pooling.png" width="150%"/>
      <a id="x9-110003r11">
      </a>
     </p>
     <figcaption class="caption">
      <span class="id">
       Figure 7.11:
      </span>
      <span class="content">
       Depthwise max pooling can help the
       <a class="glossary" href="DataScienceIILectureBookli2.html#glo:cnn">
        CNN
       </a>
       learn to be invariant (to rotation in this case).
      </span>
     </figcaption>
    </div>
    <p class="noindent">
     Keras does not include a depthwise max pooling layer, but it’s not too difficult to implement a custom layer for
that:
    </p>
    <pre><div id="fancyvrb169" style="padding:20px;border-radius: 3px;"><a id="x9-110005r91"></a><span style="color:#2B2BFF;">class</span><span style="color:#BABABA;"> </span>DepthPool(tf.keras.layers.Layer): 
<a id="x9-110007r92"></a>   <span style="color:#2B2BFF;">def</span><span style="color:#BABABA;"> </span><italic><span id="bold" style="font-weight:bold;">__init__</span></italic>(<span style="color:#2B2BFF;">self</span>, pool_size=<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">2</span></span>, **kwargs): 
<a id="x9-110009r93"></a>      <span style="color:#2B2BFF;">super</span>().<italic><span id="bold" style="font-weight:bold;">__init__</span></italic>(**kwargs) 
<a id="x9-110011r94"></a>      <span style="color:#2B2BFF;">self</span>.pool_size = pool_size 
<a id="x9-110013r95"></a> 
<a id="x9-110015r96"></a>   <span style="color:#2B2BFF;">def</span><span style="color:#BABABA;"> </span><italic><span id="bold" style="font-weight:bold;">call</span></italic>(<span style="color:#2B2BFF;">self</span>, inputs): 
<a id="x9-110017r97"></a>      shape = tf.shape(inputs)  <span style="color:#008700;"><italic># shape[-1] is the number of channels</italic></span> 
<a id="x9-110019r98"></a>      groups = shape[-<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">1</span></span>] // <span style="color:#2B2BFF;">self</span>.pool_size  <span style="color:#008700;"><italic># number of channel groups</italic></span> 
<a id="x9-110021r99"></a>      new_shape = tf.concat([shape[:-<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">1</span></span>], [groups, <span style="color:#2B2BFF;">self</span>.pool_size]], axis=<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">0</span></span>) 
<a id="x9-110023r100"></a>      <span style="color:#2B2BFF;">return</span> tf.reduce_max(tf.reshape(inputs, new_shape), axis=-<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">1</span></span>)</div></pre>
    <p class="noindent">
     This layer reshapes its inputs to split the channels into groups of the desired size <span style="color:#054C5C;">(<code class="verb">pool_size</code>)</span>, then it uses <span style="color:#054C5C;"><code class="verb">tf.reduce_max()</code></span>
     to
compute the max of each group. This implementation assumes that the stride is equal to the pool size, which is generally what
we want. Alternatively, we could use TensorFlow’s <span style="color:#054C5C;"><code class="verb">tf.nn.max_pool()</code></span>
     operation, and wrap in a Lambda layer to
use it inside a Keras model, but sadly this op does not implement depthwise pooling for the GPU, only for the
CPU.
    </p>
    <p class="noindent">
     One last type of pooling layer that we will often see in modern architectures is the global average pooling layer. It works very
differently: all it does is compute the mean of each entire feature map (it’s like an average pooling layer using a pooling kernel
with the same spatial dimensions as the inputs). This means that it just outputs a single number per feature map and per
instance. Although this is of course extremely destructive (most of the information in the feature map is lost), it can be useful
just before the output layer, as we will see later in this chapter. To create such a layer, simply use the GlobalAveragePooling2D
class, alias GlobalAvgPool2D:
    </p>
    <p class="noindent">
    </p>
    <h2 class="sectionHead">
     <a aria-label="Link to this section in PDF version" class="pdf-link" href="DataScienceIILectureBook.pdf#section.7.6" style="color: red;  position: absolute;  left: -30px;" target="_blank">
      <svg class="bi bi-file-pdf" fill="currentColor" height="20" viewbox="0 0 16 16" width="20" xmlns="http://www.w3.org/2000/svg">
       <path d="M4 0a2 2 0 0 0-2 2v12a2 2 0 0 0 2 2h8a2 2 0 0 0 2-2V2a2 2 0 0 0-2-2H4zm0 1h8a1 1 0 0 1 1 1v12a1 1 0 0 1-1 1H4a1 1 0 0 1-1-1V2a1 1 0 0 1 1-1z">
       </path>
       <path d="M4.603 12.087a.81.81 0 0 1-.438-.42c-.195-.388-.13-.776.08-1.102.198-.307.526-.568.897-.787a7.68 7.68 0 0 1 1.482-.645 19.701 19.701 0 0 0 1.062-2.227 7.269 7.269 0 0 1-.43-1.295c-.086-.4-.119-.796-.046-1.136.075-.354.274-.672.65-.823.192-.077.4-.12.602-.077a.7.7 0 0 1 .477.365c.088.164.12.356.127.538.007.187-.012.395-.047.614-.084.51-.27 1.134-.52 1.794a10.954 10.954 0 0 0 .98 1.686 5.753 5.753 0 0 1 1.334.05c.364.065.734.195.96.465.12.144.193.32.2.518.007.192-.047.382-.138.563a1.04 1.04 0 0 1-.354.416.856.856 0 0 1-.51.138c-.331-.014-.654-.196-.933-.417a5.716 5.716 0 0 1-.911-.95 11.642 11.642 0 0 0-1.997.406 11.311 11.311 0 0 1-1.021 1.51c-.29.35-.608.655-.926.787a.793.793 0 0 1-.58.029zm1.379-1.901c-.166.076-.32.156-.459.238-.328.194-.541.383-.647.547-.094.145-.096.25-.04.361.01.022.02.036.026.044a.27.27 0 0 0 .035-.012c.137-.056.355-.235.635-.572a8.18 8.18 0 0 0 .45-.606zm1.64-1.33a12.647 12.647 0 0 1 1.01-.193 11.666 11.666 0 0 1-.51-.858 20.741 20.741 0 0 1-.5 1.05zm2.446.45c.15.162.296.3.435.41.24.19.407.253.498.256a.107.107 0 0 0 .07-.015.307.307 0 0 0 .094-.125.436.436 0 0 0 .059-.2.095.095 0 0 0-.026-.063c-.052-.062-.2-.152-.518-.209a3.881 3.881 0 0 0-.612-.053zM8.078 5.8a6.7 6.7 0 0 0 .2-.828c.031-.188.043-.343.038-.465a.613.613 0 0 0-.032-.198.517.517 0 0 0-.145.04c-.087.035-.158.106-.196.283-.04.192-.03.469.046.822.024.111.054.227.09.346z">
       </path>
      </svg>
     </a>
     <small class="titlemark">
      7.6
     </small>
     <a id="x9-1110007.6">
     </a>
     CNN Architectures
    </h2>
    <p class="noindent">
     Typical
     <a class="glossary" href="DataScienceIILectureBookli2.html#glo:cnn">
      CNN
     </a>
     architectures stack a few convolutional layers (each one generally followed by a ReLU layer), then a pooling layer,
then another few convolutional layers (+ReLU), then another pooling layer, and so on. The image gets smaller and smaller as it
progresses through the network, but it also typically gets deeper and deeper (i.e., with more feature maps), thanks to the
convolutional layers (see
     <span id="bold" style="font-weight:bold;">
      Fig.
     </span>
     ??). At the top of the stack, a regular feedforward neural network is added, composed of a few
fully connected layers (+ReLUs), and the final layer outputs the prediction (e.g., a softmax layer that outputs estimated class
probabilities).
    </p>
    <div class="figure">
     <p class="noindent">
      <img alt="PIC" height="" src="figures/Computer-Vision-using-Convolutional-Neural-Networks/figures-4.svg" width="150%"/>
      <a id="x9-111001r12">
      </a>
     </p>
     <figcaption class="caption">
      <span class="id">
       Figure 7.12:
      </span>
      <span class="content">
       An example structure of a CNN network
      </span>
     </figcaption>
    </div>
    <div class="warning">
     <p class="noindent">
      A common mistake is to use convolution kernels that are too large. For example, instead of using a convolutional layer with a 5
Œ 5 kernel, stack two layers with 3 Œ 3 kernels: it will use fewer parameters and require fewer computations, and it will
usually perform better. One exception is for the first convolutional layer: it can typically have a large kernel (e.g.,
5 Œ 5), usually with a stride of 2 or more. This will reduce the spatial dimension of the image without losing
too much information, and since the input image only has three channels in general, it will not be too costly.
     </p>
    </div>
    <p class="noindent">
     Here is how we can implement a basic
     <a class="glossary" href="DataScienceIILectureBookli2.html#glo:cnn">
      CNN
     </a>
     to tackle the Fashion MNIST dataset.
    </p>
    <p class="noindent">
     Let’s first download and allocated the traning/testing.
    </p>
    <pre><div id="fancyvrb170" style="padding:20px;border-radius: 3px;"><a id="x9-111003r107"></a><span style="color:#2B2BFF;">import</span><span style="color:#BABABA;"> </span>numpy<span style="color:#BABABA;"> </span><span style="color:#2B2BFF;">as</span><span style="color:#BABABA;"> </span>np 
<a id="x9-111005r108"></a> 
<a id="x9-111007r109"></a>mnist = tf.keras.datasets.fashion_mnist.load_data() 
<a id="x9-111009r110"></a>(X_train_full, y_train_full), (X_test, y_test) = mnist 
<a id="x9-111011r111"></a>X_train_full = np.expand_dims(X_train_full, axis=-<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">1</span></span>).astype(np.float32) / <span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">255</span></span> 
<a id="x9-111013r112"></a>X_test = np.expand_dims(X_test.astype(np.float32), axis=-<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">1</span></span>) / <span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">255</span></span> 
<a id="x9-111015r113"></a>X_train, X_valid = X_train_full[:-<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">5000</span></span>], X_train_full[-<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">5000</span></span>:] 
<a id="x9-111017r114"></a>y_train, y_valid = y_train_full[:-<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">5000</span></span>], y_train_full[-<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">5000</span></span>:]</div></pre>
    <pre><div id="fancyvrb171" style="padding:20px;border-radius: 3px;"><a id="x9-111019r121"></a><span style="color:#2B2BFF;">from</span><span style="color:#BABABA;"> </span>functools<span style="color:#BABABA;"> </span><span style="color:#2B2BFF;">import</span> partial 
<a id="x9-111021r122"></a> 
<a id="x9-111023r123"></a>tf.random.set_seed(<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">42</span></span>)  <span style="color:#008700;"><italic># extra code  ensures reproducibility</italic></span> 
<a id="x9-111025r124"></a>DefaultConv2D = partial(tf.keras.layers.Conv2D, kernel_size=<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">3</span></span>, padding=<span style="color:#800080;">"same"</span>, 
<a id="x9-111027r125"></a>                 activation=<span style="color:#800080;">"relu"</span>, kernel_initializer=<span style="color:#800080;">"he_normal"</span>) 
<a id="x9-111029r126"></a>model = tf.keras.Sequential([ 
<a id="x9-111031r127"></a>   DefaultConv2D(filters=<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">64</span></span>, kernel_size=<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">7</span></span>, input_shape=[<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">28</span></span>, <span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">28</span></span>, <span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">1</span></span>]), 
<a id="x9-111033r128"></a>   tf.keras.layers.MaxPool2D(), 
<a id="x9-111035r129"></a>   DefaultConv2D(filters=<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">128</span></span>), 
<a id="x9-111037r130"></a>   DefaultConv2D(filters=<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">128</span></span>), 
<a id="x9-111039r131"></a>   tf.keras.layers.MaxPool2D(), 
<a id="x9-111041r132"></a>   DefaultConv2D(filters=<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">256</span></span>), 
<a id="x9-111043r133"></a>   DefaultConv2D(filters=<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">256</span></span>), 
<a id="x9-111045r134"></a>   tf.keras.layers.MaxPool2D(), 
<a id="x9-111047r135"></a>   tf.keras.layers.Flatten(), 
<a id="x9-111049r136"></a>   tf.keras.layers.Dense(units=<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">128</span></span>, activation=<span style="color:#800080;">"relu"</span>, 
<a id="x9-111051r137"></a>                  kernel_initializer=<span style="color:#800080;">"he_normal"</span>), 
<a id="x9-111053r138"></a>   tf.keras.layers.Dropout(<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">0.5</span></span>), 
<a id="x9-111055r139"></a>   tf.keras.layers.Dense(units=<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">64</span></span>, activation=<span style="color:#800080;">"relu"</span>, 
<a id="x9-111057r140"></a>                  kernel_initializer=<span style="color:#800080;">"he_normal"</span>), 
<a id="x9-111059r141"></a>   tf.keras.layers.Dropout(<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">0.5</span></span>), 
<a id="x9-111061r142"></a>   tf.keras.layers.Dense(units=<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">10</span></span>, activation=<span style="color:#800080;">"softmax"</span>) 
<a id="x9-111063r143"></a>])</div></pre>
    <p class="noindent">
     Let’s go through this code:
    </p>
    <dl class="enumerate-enumitem">
     <dt class="enumerate-enumitem">
      1.
     </dt>
     <dd class="enumerate-enumitem">
      We
use
the <span style="color:#054C5C;"><code class="verb">functools.partial()</code></span>
      function
to
define
DefaultConv2D,
which
acts
just
like
Conv2D
but
with
different
default
arguments:
a
small
kernel
size
of
3, <span style="color:#054C5C;"><code class="verb">"same"</code></span>
      padding,
the
ReLU
activation
function,
and
its
corresponding
      <italic>
       He
initializer
      </italic>
      .
     </dd>
     <dt class="enumerate-enumitem">
      2.
     </dt>
     <dd class="enumerate-enumitem">
      We
create
the
Sequential
model.
Its
first
layer
                                                                                
                                                                                
     
is
a
DefaultConv2D
with
64
fairly
large
filters
(7-by-7).
It
uses
the
default
stride
of
1
because
the
input
images
are
not
very
large.
It
also
sets <span style="color:#054C5C;"><code class="verb">input_shape=[28, 28, 1]</code></span>,
as
the
images
are
28-by-28
pixels,
with
a
single
color
channel
(i.e.,
grayscale).
     </dd>
     <dt class="enumerate-enumitem">
      3.
     </dt>
     <dd class="enumerate-enumitem">
      When
we
load
the
Fashion
MNIST
dataset,
we
need
to
make
sure
each
image
has
this
shape.
Therefore
we
may
require
to
                                                                                
                                                                                
     
use <span style="color:#054C5C;"><code class="verb">np.reshape()</code></span>
      or <span style="color:#054C5C;"><code class="verb">np.expanddims()</code></span>
      to
add
the
channels
dimension
or
we
could
use
a
Reshape
layer
as
the
first
layer
in
the
model.
     </dd>
     <dt class="enumerate-enumitem">
      4.
     </dt>
     <dd class="enumerate-enumitem">
      We
then
add
a
max
pooling
layer
that
uses
the
default
pool
size
of
2,
so
it
divides
each
spatial
dimension
by
a
factor
of
2.
     </dd>
     <dt class="enumerate-enumitem">
      5.
     </dt>
     <dd class="enumerate-enumitem">
      We
repeat
the
same
structure
twice:
two
convolutional
layers
followed
                                                                                
                                                                                
     
by
a
max
pooling
layer.
For
larger
images,
we
could
repeat
this
structure
several
more
times.
The
number
of
repetitions
is
a
hyperparameter
we
can
tune
     </dd>
     <dt class="enumerate-enumitem">
     </dt>
     <dd class="enumerate-enumitem">
      <div class="warning">
       <p class="noindent">
        Observe the number of filters doubles as we climb up the
        <a class="glossary" href="DataScienceIILectureBookli2.html#glo:cnn">
         CNN
        </a>
        toward the output layer (it is initially 64, then 128, then
256): it makes sense for it to grow, since the number of low-level features is often fairly low (e.g., small
circles, horizontal lines), but there are many different ways to combine them into higher-level features.
It is a common practice to double the number of filters after each pooling layer: since a pooling layer
divides each spatial dimension by a factor of 2, we can afford to double the number of feature maps in the
next layer without fear of exploding the number of parameters, memory usage, or computational load.
       </p>
      </div>
     </dd>
     <dt class="enumerate-enumitem">
      6.
     </dt>
     <dd class="enumerate-enumitem">
      Next is the fully connected network, composed of two hidden dense layers and a dense output layer. Since
it’s a classification task with 10 classes, the output layer has 10 units, and it uses the softmax activation
function. Note that we must flatten the inputs just before the first dense layer, since it expects a 1D array of
features for each instance. We also add two dropout layers, with a dropout rate of 50% each, to reduce
overfitting.
     </dd>
    </dl>
    <p class="noindent">
     If we compile this model using the <span style="color:#054C5C;"><code class="verb">"sparse_categorical_crossentropy"</code></span>
     loss and we fit the model to the Fashion MNIST training
set, it should reach over 92% accuracy on the test set.
    </p>
    <pre><div id="fancyvrb172" style="padding:20px;border-radius: 3px;"><a id="x9-111071r154"></a>model.compile(loss=<span style="color:#800080;">"sparse_categorical_crossentropy"</span>, optimizer=<span style="color:#800080;">"nadam"</span>, 
<a id="x9-111073r155"></a>          metrics=[<span style="color:#800080;">"accuracy"</span>]) 
<a id="x9-111075r156"></a>history = model.fit(X_train, y_train, epochs=<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">10</span></span>, 
<a id="x9-111077r157"></a>              validation_data=(X_valid, y_valid)) 
<a id="x9-111079r158"></a>score = model.evaluate(X_test, y_test) 
<a id="x9-111081r159"></a>X_new = X_test[:<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">10</span></span>]  <span style="color:#008700;"><italic># pretend we have new images</italic></span> 
<a id="x9-111083r160"></a>y_pred = model.predict(X_new)</div></pre>
    <p class="noindent">
     Over the years, variants of this fundamental architecture have been developed, leading to amazing advances in the field. A good
measure of this progress is the error rate in competitions such as the ILSVRC ImageNet challenge. In this competition, the
top-five error rate for image classification -that is, the number of test images for which the system’s top five predictions did
not include the correct answer-fell from over 26% to less than 2.3% in just six years. The images are fairly large
(e.g., 256 pixels high) and there are 1,000 classes, some of which are really subtle (try distinguishing 120 dog
breeds).
    </p>
    <p class="noindent">
     Looking at the evolution of the winning entries is a good way to understand how
     <a class="glossary" href="DataScienceIILectureBookli2.html#glo:cnn">
      CNN
     </a>
     s work, and how research in deep learning
progresses. We will first look at:
    </p>
    <ul class="itemize1">
     <li class="itemize">
      <p class="noindent">
       LeNet-5
architecture
(1998)
      </p>
     </li>
     <li class="itemize">
      <p class="noindent">
       AlexNet
(2012),
      </p>
     </li>
     <li class="itemize">
      <p class="noindent">
       GoogLeNet
(2014),
      </p>
     </li>
     <li class="itemize">
      <p class="noindent">
       ResNet
(2015),
      </p>
     </li>
     <li class="itemize">
      <p class="noindent">
       SENet
(2017).
      </p>
     </li>
    </ul>
    <p class="noindent">
     In addition, we will also briefly look at more architectures, including Xception, ResNeXt, DenseNet, MobileNet, CSPNet, and
EfficientNet.
    </p>
    <h3 class="subsectionHead">
     <a aria-label="Link to this section in PDF version" class="pdf-link" href="DataScienceIILectureBook.pdf#subsection.7.6.1" style="color: red;  position: absolute;  left: -30px;" target="_blank">
      <svg class="bi bi-file-pdf" fill="currentColor" height="20" viewbox="0 0 16 16" width="20" xmlns="http://www.w3.org/2000/svg">
       <path d="M4 0a2 2 0 0 0-2 2v12a2 2 0 0 0 2 2h8a2 2 0 0 0 2-2V2a2 2 0 0 0-2-2H4zm0 1h8a1 1 0 0 1 1 1v12a1 1 0 0 1-1 1H4a1 1 0 0 1-1-1V2a1 1 0 0 1 1-1z">
       </path>
       <path d="M4.603 12.087a.81.81 0 0 1-.438-.42c-.195-.388-.13-.776.08-1.102.198-.307.526-.568.897-.787a7.68 7.68 0 0 1 1.482-.645 19.701 19.701 0 0 0 1.062-2.227 7.269 7.269 0 0 1-.43-1.295c-.086-.4-.119-.796-.046-1.136.075-.354.274-.672.65-.823.192-.077.4-.12.602-.077a.7.7 0 0 1 .477.365c.088.164.12.356.127.538.007.187-.012.395-.047.614-.084.51-.27 1.134-.52 1.794a10.954 10.954 0 0 0 .98 1.686 5.753 5.753 0 0 1 1.334.05c.364.065.734.195.96.465.12.144.193.32.2.518.007.192-.047.382-.138.563a1.04 1.04 0 0 1-.354.416.856.856 0 0 1-.51.138c-.331-.014-.654-.196-.933-.417a5.716 5.716 0 0 1-.911-.95 11.642 11.642 0 0 0-1.997.406 11.311 11.311 0 0 1-1.021 1.51c-.29.35-.608.655-.926.787a.793.793 0 0 1-.58.029zm1.379-1.901c-.166.076-.32.156-.459.238-.328.194-.541.383-.647.547-.094.145-.096.25-.04.361.01.022.02.036.026.044a.27.27 0 0 0 .035-.012c.137-.056.355-.235.635-.572a8.18 8.18 0 0 0 .45-.606zm1.64-1.33a12.647 12.647 0 0 1 1.01-.193 11.666 11.666 0 0 1-.51-.858 20.741 20.741 0 0 1-.5 1.05zm2.446.45c.15.162.296.3.435.41.24.19.407.253.498.256a.107.107 0 0 0 .07-.015.307.307 0 0 0 .094-.125.436.436 0 0 0 .059-.2.095.095 0 0 0-.026-.063c-.052-.062-.2-.152-.518-.209a3.881 3.881 0 0 0-.612-.053zM8.078 5.8a6.7 6.7 0 0 0 .2-.828c.031-.188.043-.343.038-.465a.613.613 0 0 0-.032-.198.517.517 0 0 0-.145.04c-.087.035-.158.106-.196.283-.04.192-.03.469.046.822.024.111.054.227.09.346z">
       </path>
      </svg>
     </a>
     <small class="titlemark">
      7.6.1
     </small>
     <a id="x9-1120007.6.1">
     </a>
     LeNet-5
    </h3>
    <p class="noindent">
     The LeNet-5 architecture is perhaps the most widely known
     <a class="glossary" href="DataScienceIILectureBookli2.html#glo:cnn">
      CNN
     </a>
     architecture. As mentioned, it was created by Yann LeCun in
1998 and has been widely used for handwritten digit recognition (MNIST).
    </p>
    <p class="noindent">
    </p>
    <figure class="float">
     <table class="tabularray tblr" id="tbl-10">
      <tr id="row-10-1-">
       <td id="cell10-1-1" style="text-align:right;vertical-align:top;">
        <span id="bold" style="font-weight:bold;">
         Layer
        </span>
       </td>
       <td id="cell10-1-2" style="text-align:justify;vertical-align:top;">
        <span id="bold" style="font-weight:bold;">
         Type
        </span>
       </td>
       <td id="cell10-1-3" style="vertical-align:top;">
        <span id="bold" style="font-weight:bold;">
         Maps
        </span>
       </td>
       <td id="cell10-1-4" style="vertical-align:top;">
        <span id="bold" style="font-weight:bold;">
         Size
        </span>
       </td>
       <td id="cell10-1-5" style="vertical-align:top;">
        <span id="bold" style="font-weight:bold;">
         Kernel 
Size
        </span>
       </td>
      </tr>
      <tr id="row-10-2-">
       <td id="cell10-2-1" style="text-align:right;vertical-align:top;">
        Out
       </td>
       <td id="cell10-2-2" style="text-align:justify;vertical-align:top;">
        Fully Connected
       </td>
       <td id="cell10-2-3" style="vertical-align:top;">
        -
       </td>
       <td id="cell10-2-4" style="vertical-align:top;">
        10
       </td>
       <td id="cell10-2-5" style="vertical-align:top;">
        -
       </td>
      </tr>
      <tr id="row-10-3-">
       <td id="cell10-3-1" style="text-align:right;vertical-align:top;">
        F6
       </td>
       <td id="cell10-3-2" style="text-align:justify;vertical-align:top;">
        Fully connected
       </td>
       <td id="cell10-3-3" style="vertical-align:top;">
        -
       </td>
       <td id="cell10-3-4" style="vertical-align:top;">
        84
       </td>
       <td id="cell10-3-5" style="vertical-align:top;">
        -
       </td>
      </tr>
      <tr id="row-10-4-">
       <td id="cell10-4-1" style="text-align:right;vertical-align:top;">
        C5
       </td>
       <td id="cell10-4-2" style="text-align:justify;vertical-align:top;">
        Convolution
       </td>
       <td id="cell10-4-3" style="vertical-align:top;">
        120
       </td>
       <td id="cell10-4-4" style="vertical-align:top;">
        1-by-1
       </td>
       <td id="cell10-4-5" style="vertical-align:top;">
        5-by-5
       </td>
      </tr>
      <tr id="row-10-5-">
       <td id="cell10-5-1" style="text-align:right;vertical-align:top;">
        S4
       </td>
       <td id="cell10-5-2" style="text-align:justify;vertical-align:top;">
        Average Pooling
       </td>
       <td id="cell10-5-3" style="vertical-align:top;">
        16
       </td>
       <td id="cell10-5-4" style="vertical-align:top;">
        5-by-5
       </td>
       <td id="cell10-5-5" style="vertical-align:top;">
        2-by-2
       </td>
      </tr>
      <tr id="row-10-6-">
       <td id="cell10-6-1" style="text-align:right;vertical-align:top;">
        C3
       </td>
       <td id="cell10-6-2" style="text-align:justify;vertical-align:top;">
        Convolution
       </td>
       <td id="cell10-6-3" style="vertical-align:top;">
        16
       </td>
       <td id="cell10-6-4" style="vertical-align:top;">
        10-by-10
       </td>
       <td id="cell10-6-5" style="vertical-align:top;">
        5-by-5
       </td>
      </tr>
      <tr id="row-10-7-">
       <td id="cell10-7-1" style="text-align:right;vertical-align:top;">
        S2
       </td>
       <td id="cell10-7-2" style="text-align:justify;vertical-align:top;">
        Average Pooling
       </td>
       <td id="cell10-7-3" style="vertical-align:top;">
        6
       </td>
       <td id="cell10-7-4" style="vertical-align:top;">
        14-by-14
       </td>
       <td id="cell10-7-5" style="vertical-align:top;">
        2-by-2
       </td>
      </tr>
      <tr id="row-10-8-">
       <td id="cell10-8-1" style="text-align:right;vertical-align:top;">
        C1
       </td>
       <td id="cell10-8-2" style="text-align:justify;vertical-align:top;">
        Convolution
       </td>
       <td id="cell10-8-3" style="vertical-align:top;">
        6
       </td>
       <td id="cell10-8-4" style="vertical-align:top;">
        28-by-28
       </td>
       <td id="cell10-8-5" style="vertical-align:top;">
        5-by-5
       </td>
      </tr>
      <tr id="row-10-9-">
       <td id="cell10-9-1" style="text-align:right;vertical-align:top;">
        In
       </td>
       <td id="cell10-9-2" style="text-align:justify;vertical-align:top;">
        Input
       </td>
       <td id="cell10-9-3" style="vertical-align:top;">
        1
       </td>
       <td id="cell10-9-4" style="vertical-align:top;">
        32-by-32
       </td>
       <td id="cell10-9-5" style="vertical-align:top;">
        -
       </td>
      </tr>
     </table>
     <a id="x9-112001r1">
     </a>
     <figcaption class="caption">
      <span class="id">
       Table 7.1:
      </span>
      <span class="content">
       LeNet-5 Architecture.
      </span>
     </figcaption>
    </figure>
    <p class="noindent">
     As we can see, this looks pretty similar to our Fashion MNIST model: a stack of convolutional layers and pooling layers, followed
by a dense network. Perhaps the main difference with more modern classification
     <a class="glossary" href="DataScienceIILectureBookli2.html#glo:cnn">
      CNN
     </a>
     s is the activation functions: today, we
would use ReLU instead of tanh and softmax instead of RBF.
    </p>
    <h3 class="subsectionHead">
     <a aria-label="Link to this section in PDF version" class="pdf-link" href="DataScienceIILectureBook.pdf#subsection.7.6.2" style="color: red;  position: absolute;  left: -30px;" target="_blank">
      <svg class="bi bi-file-pdf" fill="currentColor" height="20" viewbox="0 0 16 16" width="20" xmlns="http://www.w3.org/2000/svg">
       <path d="M4 0a2 2 0 0 0-2 2v12a2 2 0 0 0 2 2h8a2 2 0 0 0 2-2V2a2 2 0 0 0-2-2H4zm0 1h8a1 1 0 0 1 1 1v12a1 1 0 0 1-1 1H4a1 1 0 0 1-1-1V2a1 1 0 0 1 1-1z">
       </path>
       <path d="M4.603 12.087a.81.81 0 0 1-.438-.42c-.195-.388-.13-.776.08-1.102.198-.307.526-.568.897-.787a7.68 7.68 0 0 1 1.482-.645 19.701 19.701 0 0 0 1.062-2.227 7.269 7.269 0 0 1-.43-1.295c-.086-.4-.119-.796-.046-1.136.075-.354.274-.672.65-.823.192-.077.4-.12.602-.077a.7.7 0 0 1 .477.365c.088.164.12.356.127.538.007.187-.012.395-.047.614-.084.51-.27 1.134-.52 1.794a10.954 10.954 0 0 0 .98 1.686 5.753 5.753 0 0 1 1.334.05c.364.065.734.195.96.465.12.144.193.32.2.518.007.192-.047.382-.138.563a1.04 1.04 0 0 1-.354.416.856.856 0 0 1-.51.138c-.331-.014-.654-.196-.933-.417a5.716 5.716 0 0 1-.911-.95 11.642 11.642 0 0 0-1.997.406 11.311 11.311 0 0 1-1.021 1.51c-.29.35-.608.655-.926.787a.793.793 0 0 1-.58.029zm1.379-1.901c-.166.076-.32.156-.459.238-.328.194-.541.383-.647.547-.094.145-.096.25-.04.361.01.022.02.036.026.044a.27.27 0 0 0 .035-.012c.137-.056.355-.235.635-.572a8.18 8.18 0 0 0 .45-.606zm1.64-1.33a12.647 12.647 0 0 1 1.01-.193 11.666 11.666 0 0 1-.51-.858 20.741 20.741 0 0 1-.5 1.05zm2.446.45c.15.162.296.3.435.41.24.19.407.253.498.256a.107.107 0 0 0 .07-.015.307.307 0 0 0 .094-.125.436.436 0 0 0 .059-.2.095.095 0 0 0-.026-.063c-.052-.062-.2-.152-.518-.209a3.881 3.881 0 0 0-.612-.053zM8.078 5.8a6.7 6.7 0 0 0 .2-.828c.031-.188.043-.343.038-.465a.613.613 0 0 0-.032-.198.517.517 0 0 0-.145.04c-.087.035-.158.106-.196.283-.04.192-.03.469.046.822.024.111.054.227.09.346z">
       </path>
      </svg>
     </a>
     <small class="titlemark">
      7.6.2
     </small>
     <a id="x9-1130007.6.2">
     </a>
     AlexNet
    </h3>
    <p class="noindent">
     The AlexNet
     <a class="glossary" href="DataScienceIILectureBookli2.html#glo:cnn">
      CNN
     </a>
     architecture won the 2012 ILSVRC challenge by a large margin: it achieved a top-five error rate of 17%, while
the second best competitor achieved only 26%! AlexaNet was developed by Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton.
It is similar to LeNet-5, only much larger and deeper, and it was the first to stack convolutional layers directly on
top of one another, instead of stacking a pooling layer on top of each convolutional layer. Table X presents this
architecture.
    </p>
    <p class="noindent">
     To reduce overfitting, the authors used two regularization techniques. First, they applied dropout with a 50%
dropout rate during training to the outputs of layers F9 and F10. Second, they performed data augmentation by
randomly shifting the training images by various offsets, flipping them horizontally, and changing the lighting
conditions.
    </p>
    <div class="informationblock" id="tcolobox-78">
     <div class="title">
      <p class="noindent">
       <span id="bold" style="font-weight:bold;">
        Information
       </span>
       : Data Augmentation
      </p>
     </div>
     <div class="box-content">
      <p class="noindent">
       It is the process of
       <alert style="color: #821131;">
        artificially increasing
       </alert>
       the size of the training set by generating many realistic
       <span id="bold" style="font-weight:bold;">
        variants
       </span>
       of
each training instance. This process aims to reduce over-fitting, making this a regularisation technique.
      </p>
      <p class="noindent">
       The generated instances should be as realistic as possible. This means, in an ideal scenario, a human should not
be able to tell whether it was augmented or not. In adition, it has to be something
       <alert style="color: #821131;">
        learnable
       </alert>
       ; Adding white
noise will not help as it is a random process and there is no pattern in the data for the algorithm to learn.
      </p>
      <p class="noindent">
       For example, we can slightly shift, rotate, and resize every picture in the training set by various amounts and
add the resulting pictures to the training set.
      </p>
      <div class="figure">
       <p class="noindent">
        <img alt="PIC" height="" src="figures/Computer-Vision-using-Convolutional-Neural-Networks/raster/cnn-data-augmentation.jpg" width="150%"/>
        <a id="x9-113001r13">
        </a>
       </p>
       <figcaption class="caption">
        <span class="id">
         Figure 7.13:
        </span>
        <span class="content">
         Data augmentation is the process of artificially generating new data from existing data. Here can
                    see the process where the original image is transformed (shear, rotation) and fed to the
         <a class="glossary" href="DataScienceIILectureBookli2.html#glo:ml">
          ML
         </a>
         to
                    train on this new data. This allows the reuse of the image without requiring to gather new data <a href="#X0-Alto2020">[66]</a>.
        </span>
       </figcaption>
      </div>
      <p class="noindent">
       To use this feature in code, use Keras’s data augmentation layers, (i.e., <span style="color:#054C5C;"><code class="verb">RandomCrop</code></span>, <span style="color:#054C5C;"><code class="verb">RandomRotation</code></span>, etc.). These layers
force the model to be more tolerant to variations in the position, orientation, and size of the objects in the pictures. To
produce a model that’s more tolerant of different lighting conditions, we can similarly generate many images with
       <alert style="color: #821131;">
        various contrasts
       </alert>
       . In general, we can also flip the pictures horizontally (except for text, and other asymmetrical
objects). By combining these transformations, we can greatly increase your training set size. Example of this operation
can be seen in
       <span id="bold" style="font-weight:bold;">
        Fig.
       </span>
       <a href="#x9-113001r13">
        7.13
       </a>.
      </p>
      <p class="noindent">
       Data augmentation is also useful when we have an unbalanced dataset: we can use it to generate more samples of the
less frequent classes. This is called the synthetic minority oversampling technique (SMOTE).
      </p>
     </div>
    </div>
    <p class="noindent">
     AlexNet also uses a competitive normalization step immediately after the ReLU step of layers C1 and C3, called
     <italic>
      local response
normalization
     </italic>
     (LRN):
    </p>
    <div class="quoteblock">
     <p class="noindent">
      The
most
strongly
activated
neurons
inhibit
other
neurons
                                                                                
                                                                                
     
located
at
the
same
position
in
neighboring
feature
maps.
     </p>
    </div>
    <p class="noindent">
     Such competitive activation has been observed in biological neurons <a id="x9-113003"></a><a href="#X0-deco2005neurodynamics">[67]</a>. This encourages different feature maps to specialize,
pushing them apart and forcing them to explore a wider range of features, ultimately improving generalization. The following
equation gives use a view on how to apply this method:
    </p>
    <table class="equation-star">
     <tr>
      <td>
       <math class="equation" display="block" xmlns="http://www.w3.org/1998/Math/MathML">
        <munder accent="true">
         <mrow>
          <mo class="MathClass-open" stretchy="false">
           [
          </mo>
         </mrow>
         <mo accent="true">
          ¯
         </mo>
        </munder>
        <mi>
         i
        </mi>
        <mo class="MathClass-close" stretchy="false">
         ]
        </mo>
        <mo class="MathClass-rel" stretchy="false">
         =
        </mo>
        <msubsup>
         <mrow>
          <mi>
           a
          </mi>
         </mrow>
         <mrow>
          <mi>
           i
          </mi>
         </mrow>
         <mrow>
         </mrow>
        </msubsup>
        <msup>
         <mrow>
          <mrow>
           <mo fence="true" form="prefix">
            (
           </mo>
           <mrow>
            <mi>
             k
            </mi>
            <mo class="MathClass-bin" stretchy="false">
             +
            </mo>
            <mi>
             α
            </mi>
            <munderover accent="false" accentunder="false">
             <mrow>
              <mo>
               ∑
              </mo>
             </mrow>
             <mrow>
              <mi>
               j
              </mi>
              <mo class="MathClass-rel" stretchy="false">
               =
              </mo>
              <msubsup>
               <mrow>
                <mi>
                 j
                </mi>
               </mrow>
               <mrow>
                <mi>
                 l
                </mi>
                <mi>
                 o
                </mi>
                <mi>
                 w
                </mi>
               </mrow>
               <mrow>
               </mrow>
              </msubsup>
             </mrow>
             <mrow>
              <msubsup>
               <mrow>
                <mi>
                 j
                </mi>
               </mrow>
               <mrow>
                <mi>
                 h
                </mi>
                <mi>
                 i
                </mi>
                <mi>
                 g
                </mi>
                <mi>
                 h
                </mi>
               </mrow>
               <mrow>
               </mrow>
              </msubsup>
             </mrow>
            </munderover>
            <msubsup>
             <mrow>
              <mi>
               a
              </mi>
             </mrow>
             <mrow>
              <mi>
               j
              </mi>
             </mrow>
             <mrow>
              <mn>
               2
              </mn>
             </mrow>
            </msubsup>
           </mrow>
           <mo fence="true" form="postfix">
            )
           </mo>
          </mrow>
         </mrow>
         <mrow>
          <mo class="MathClass-bin" stretchy="false">
           −
          </mo>
          <mi>
           β
          </mi>
         </mrow>
        </msup>
        <mspace class="qquad" width="2em">
        </mspace>
        <mstyle class="text">
         <mtext>
          with
         </mtext>
        </mstyle>
        <mspace class="qquad" width="2em">
        </mspace>
        <mrow class="cases">
         <mrow>
          <mo fence="true" form="prefix">
           {
          </mo>
          <mrow>
           <mtable align="axis" class="array" columnlines="none" displaystyle="true" equalcolumns="false" equalrows="false" style="">
            <mtr class="array-row">
             <mtd class="array-td" columnalign="left">
              <msubsup>
               <mrow>
                <mi>
                 j
                </mi>
               </mrow>
               <mrow>
                <mi>
                 h
                </mi>
                <mi>
                 i
                </mi>
                <mi>
                 g
                </mi>
                <mi>
                 h
                </mi>
               </mrow>
               <mrow>
               </mrow>
              </msubsup>
              <mo class="MathClass-rel" stretchy="false">
               =
              </mo>
              <mstyle class="text">
               <mtext>
                min
               </mtext>
              </mstyle>
              <msup>
               <mrow>
                <mrow>
                 <mo fence="true" form="prefix">
                  (
                 </mo>
                 <mrow>
                  <mi>
                   i
                  </mi>
                  <mo class="MathClass-bin" stretchy="false">
                   +
                  </mo>
                  <mfrac>
                   <mrow>
                    <mi>
                     r
                    </mi>
                   </mrow>
                   <mrow>
                    <mn>
                     2
                    </mn>
                   </mrow>
                  </mfrac>
                  <mo class="MathClass-punc" stretchy="false">
                   ,
                  </mo>
                  <mspace class="thinspace" width="0.17em">
                  </mspace>
                  <msubsup>
                   <mrow>
                    <mi>
                     f
                    </mi>
                   </mrow>
                   <mrow>
                    <mi>
                     n
                    </mi>
                   </mrow>
                   <mrow>
                   </mrow>
                  </msubsup>
                  <mo class="MathClass-bin" stretchy="false">
                   −
                  </mo>
                  <mn>
                   1
                  </mn>
                 </mrow>
                 <mo fence="true" form="postfix">
                  )
                 </mo>
                </mrow>
               </mrow>
               <mrow>
               </mrow>
              </msup>
              <mspace class="quad" width="1em">
              </mspace>
             </mtd>
            </mtr>
            <mtr class="array-row">
             <mtd class="array-td" columnalign="left">
              <msubsup>
               <mrow>
                <mi>
                 j
                </mi>
               </mrow>
               <mrow>
                <mi>
                 l
                </mi>
                <mi>
                 o
                </mi>
                <mi>
                 w
                </mi>
               </mrow>
               <mrow>
               </mrow>
              </msubsup>
              <mo class="MathClass-rel" stretchy="false">
               =
              </mo>
              <mstyle class="text">
               <mtext>
                max
               </mtext>
              </mstyle>
              <msup>
               <mrow>
                <mrow>
                 <mo fence="true" form="prefix">
                  (
                 </mo>
                 <mrow>
                  <mn>
                   0
                  </mn>
                  <mo class="MathClass-punc" stretchy="false">
                   ,
                  </mo>
                  <mspace class="thinspace" width="0.17em">
                  </mspace>
                  <mi>
                   i
                  </mi>
                  <mo class="MathClass-bin" stretchy="false">
                   −
                  </mo>
                  <mfrac>
                   <mrow>
                    <mi>
                     r
                    </mi>
                   </mrow>
                   <mrow>
                    <mn>
                     2
                    </mn>
                   </mrow>
                  </mfrac>
                 </mrow>
                 <mo fence="true" form="postfix">
                  )
                 </mo>
                </mrow>
               </mrow>
               <mrow>
               </mrow>
              </msup>
              <mspace class="quad" width="1em">
              </mspace>
             </mtd>
            </mtr>
           </mtable>
          </mrow>
          <mo fence="true" form="postfix">
          </mo>
         </mrow>
        </mrow>
       </math>
      </td>
     </tr>
    </table>
    <p class="noindent">
     Let’s discuss what each parameter means:
    </p>
    <ul class="itemize1">
     <li class="itemize">
      <p class="noindent">
       <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
        <munder accent="true">
         <mrow>
          <mo class="MathClass-open" stretchy="false">
           [
          </mo>
         </mrow>
         <mo accent="true">
          ¯
         </mo>
        </munder>
        <mi>
         i
        </mi>
        <mo class="MathClass-close" stretchy="false">
         ]
        </mo>
       </math>
       is
the
neuron’s
normalised
output
located
in
feature
map
       <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
        <mi>
         i
        </mi>
       </math>
       ,
at
some
row
       <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
        <mi>
         u
        </mi>
       </math>
       and
column
       <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
        <mi>
         v
        </mi>
       </math>
       <alert style="color: #821131;">
        <span id="bold" style="font-weight:bold;">
         <sup class="textsuperscript">
          17
         </sup>
        </span>
       </alert>
       <span class="marginnote">
        <span style="color:#999999;">
        </span>
        <alert style="color: #821131;">
         <span id="bold" style="font-weight:bold;">
          <sup class="textsuperscript">
           17
          </sup>
         </span>
        </alert>
        <span style="color:#0063B2;">
         In
this
equation
we
consider
only
neurons
located
at
this
row
and
column,
so
                                                                                
                                                                                
     
u
and
v
are
not
shown
        </span>
       </span>
       .
      </p>
      
     </li>
     <li class="itemize">
      <p class="noindent">
       <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
        <msubsup>
         <mrow>
          <mi>
           a
          </mi>
         </mrow>
         <mrow>
          <mi>
           i
          </mi>
         </mrow>
         <mrow>
         </mrow>
        </msubsup>
       </math>
       is
the
activation
of
that
neuron
       <alert style="color: #821131;">
        after
the
ReLU
       </alert>
       step,
but
       <alert style="color: #821131;">
        before
normalisation
       </alert>
       .
      </p>
     </li>
     <li class="itemize">
      <p class="noindent">
       <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
        <mi>
         k
        </mi>
        <mo class="MathClass-punc" stretchy="false">
         ,
        </mo>
        <mspace class="thinspace" width="0.17em">
        </mspace>
        <mi>
         α
        </mi>
        <mo class="MathClass-punc" stretchy="false">
         ,
        </mo>
        <mspace class="thinspace" width="0.17em">
        </mspace>
        <mi>
         β
        </mi>
       </math>
       and
       <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
        <mi>
         r
        </mi>
       </math>
       are
hyperparameters.
       <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
        <mi>
         k
        </mi>
       </math>
       is
called
the
       <alert style="color: #821131;">
        bias
       </alert>
       ,
and
       <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
        <mi>
         r
        </mi>
       </math>
       is
called
the
       <alert style="color: #821131;">
        depth
radius
       </alert>
       .
      </p>
     </li>
     <li class="itemize">
      <p class="noindent">
       <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
        <msubsup>
         <mrow>
          <mi>
           f
          </mi>
         </mrow>
         <mrow>
          <mi>
           n
          </mi>
         </mrow>
         <mrow>
         </mrow>
        </msubsup>
       </math>
       is
the
number
of
feature
maps.
      </p>
     </li>
    </ul>
    <p class="noindent">
     To give an example, if
     <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
      <mi>
       r
      </mi>
      <mo class="MathClass-rel" stretchy="false">
       =
      </mo>
      <mn>
       2
      </mn>
     </math>
     and a neuron has a
     <span id="bold" style="font-weight:bold;">
      strong activation
     </span>
     , it will inhibit the activation of the neurons located in the feature maps immediately
above and below its own.
    </p>
    <p class="noindent">
     In AlexNet, the hyperparameters are set as:
    </p>
    <div class="center">
     <p class="noindent">
     </p>
     <p class="noindent">
      <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
       <mi>
        r
       </mi>
       <mo class="MathClass-rel" stretchy="false">
        =
       </mo>
       <mn>
        5
       </mn>
       <mo class="MathClass-punc" stretchy="false">
        ,
       </mo>
       <mspace class="thinspace" width="0.17em">
       </mspace>
       <mi>
        α
       </mi>
       <mo class="MathClass-rel" stretchy="false">
        =
       </mo>
       <mn>
        0
       </mn>
       <mo class="MathClass-punc" stretchy="false">
        .
       </mo>
       <mn>
        0
       </mn>
       <mn>
        0
       </mn>
       <mn>
        0
       </mn>
       <mn>
        1
       </mn>
       <mo class="MathClass-punc" stretchy="false">
        ,
       </mo>
       <mspace class="thinspace" width="0.17em">
       </mspace>
       <mi>
        β
       </mi>
       <mo class="MathClass-rel" stretchy="false">
        =
       </mo>
       <mn>
        0
       </mn>
       <mo class="MathClass-punc" stretchy="false">
        .
       </mo>
       <mn>
        7
       </mn>
       <mn>
        5
       </mn>
       <mspace class="quad" width="1em">
       </mspace>
       <mstyle class="text">
        <mtext>
         and
        </mtext>
       </mstyle>
       <mspace class="quad" width="1em">
       </mspace>
       <mi>
        k
       </mi>
       <mo class="MathClass-rel" stretchy="false">
        =
       </mo>
       <mn>
        2
       </mn>
       <mo class="MathClass-punc" stretchy="false">
        .
       </mo>
      </math>
     </p>
    </div>
    <p class="noindent">
     We can implement this step by using the <span style="color:#054C5C;"><code class="verb">tf.nn.local_response_normalization()</code></span>
     function.
    </p>
    <div class="knowledge">
     <p class="noindent">
      A variant of AlexNet called ZF Net was developed by Matthew Zeiler and Rob Fergus and won the 2013 ILSVRC challenge <a id="x9-113004"></a><a href="#X0-zeiler2014visualizing">[68]</a>.
It is essentially AlexNet with a few tweaked hyperparameters (number of feature maps, kernel size, stride, etc.).
     </p>
    </div>
    <h3 class="subsectionHead">
     <a aria-label="Link to this section in PDF version" class="pdf-link" href="DataScienceIILectureBook.pdf#subsection.7.6.3" style="color: red;  position: absolute;  left: -30px;" target="_blank">
      <svg class="bi bi-file-pdf" fill="currentColor" height="20" viewbox="0 0 16 16" width="20" xmlns="http://www.w3.org/2000/svg">
       <path d="M4 0a2 2 0 0 0-2 2v12a2 2 0 0 0 2 2h8a2 2 0 0 0 2-2V2a2 2 0 0 0-2-2H4zm0 1h8a1 1 0 0 1 1 1v12a1 1 0 0 1-1 1H4a1 1 0 0 1-1-1V2a1 1 0 0 1 1-1z">
       </path>
       <path d="M4.603 12.087a.81.81 0 0 1-.438-.42c-.195-.388-.13-.776.08-1.102.198-.307.526-.568.897-.787a7.68 7.68 0 0 1 1.482-.645 19.701 19.701 0 0 0 1.062-2.227 7.269 7.269 0 0 1-.43-1.295c-.086-.4-.119-.796-.046-1.136.075-.354.274-.672.65-.823.192-.077.4-.12.602-.077a.7.7 0 0 1 .477.365c.088.164.12.356.127.538.007.187-.012.395-.047.614-.084.51-.27 1.134-.52 1.794a10.954 10.954 0 0 0 .98 1.686 5.753 5.753 0 0 1 1.334.05c.364.065.734.195.96.465.12.144.193.32.2.518.007.192-.047.382-.138.563a1.04 1.04 0 0 1-.354.416.856.856 0 0 1-.51.138c-.331-.014-.654-.196-.933-.417a5.716 5.716 0 0 1-.911-.95 11.642 11.642 0 0 0-1.997.406 11.311 11.311 0 0 1-1.021 1.51c-.29.35-.608.655-.926.787a.793.793 0 0 1-.58.029zm1.379-1.901c-.166.076-.32.156-.459.238-.328.194-.541.383-.647.547-.094.145-.096.25-.04.361.01.022.02.036.026.044a.27.27 0 0 0 .035-.012c.137-.056.355-.235.635-.572a8.18 8.18 0 0 0 .45-.606zm1.64-1.33a12.647 12.647 0 0 1 1.01-.193 11.666 11.666 0 0 1-.51-.858 20.741 20.741 0 0 1-.5 1.05zm2.446.45c.15.162.296.3.435.41.24.19.407.253.498.256a.107.107 0 0 0 .07-.015.307.307 0 0 0 .094-.125.436.436 0 0 0 .059-.2.095.095 0 0 0-.026-.063c-.052-.062-.2-.152-.518-.209a3.881 3.881 0 0 0-.612-.053zM8.078 5.8a6.7 6.7 0 0 0 .2-.828c.031-.188.043-.343.038-.465a.613.613 0 0 0-.032-.198.517.517 0 0 0-.145.04c-.087.035-.158.106-.196.283-.04.192-.03.469.046.822.024.111.054.227.09.346z">
       </path>
      </svg>
     </a>
     <small class="titlemark">
      7.6.3
     </small>
     <a id="x9-1140007.6.3">
     </a>
     GoogLeNet
    </h3>
    <p class="noindent">
     The GoogLeNet architecture was developed by Christian Szegedy et al. from Google Research, and won the ILSVRC 2014
challenge by pushing the top-five error rate below 7% <a id="x9-114001"></a><a href="#X0-szegedy2015going">[69]</a>.
    </p>
    <p class="noindent">
     This performance boost came in from the network being
     <alert style="color: #821131;">
      deeper
     </alert>
     than previous
     <a class="glossary" href="DataScienceIILectureBookli2.html#glo:cnn">
      CNN
     </a>
     s. This was made possible by
subnetworks called
     <alert style="color: #821131;">
      inception modules
     </alert>
     , which allow GoogLeNet to use parameters much more efficiently than previous
architectures:
    </p>
    <div class="knowledge">
     <p class="noindent">
      GoogLeNet actually has 10 times fewer parameters than AlexNet
      <alert style="color: #821131;">
       <span id="bold" style="font-weight:bold;">
        <sup class="textsuperscript">
         18
        </sup>
       </span>
      </alert>
      <span class="marginnote">
       <span style="color:#999999;">
       </span>
       <alert style="color: #821131;">
        <span id="bold" style="font-weight:bold;">
         <sup class="textsuperscript">
          18
         </sup>
        </span>
       </alert>
       <span style="color:#0063B2;">
        roughly 6 million instead of 60 million
       </span>
      </span>
      .
     </p>
     
    </div>
    <p class="noindent">
     Figure 14-14 shows the architecture of an inception module. The notation
"
     <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
      <mn>
       3
      </mn>
      <mo class="MathClass-bin" stretchy="false">
       ×
      </mo>
      <mn>
       3
      </mn>
      <mo class="MathClass-bin" stretchy="false">
       +
      </mo>
      <mn>
       1
      </mn>
      <msup>
       <mrow>
        <mrow>
         <mo fence="true" form="prefix">
          (
         </mo>
         <mrow>
          <mi>
           S
          </mi>
         </mrow>
         <mo fence="true" form="postfix">
          )
         </mo>
        </mrow>
       </mrow>
       <mrow>
       </mrow>
      </msup>
     </math>
     " means
the layer uses a 3-by-3 kernel, stride 1, and <span style="color:#054C5C;"><code class="verb">same</code></span>
     padding. The input signal is first fed to four <alert style="color: #821131;">(4)</alert> different layers in parallel. All
convolutional layers use the
     <alert style="color: #821131;">
      ReLU
     </alert>
     activation function.
    </p>
    <div class="warning">
     <p class="noindent">
      Top convolutional layers use different kernel sizes (1-by-1, 3-by-3, and 5-by-5), allowing them to capture patterns at different
scales.
     </p>
    </div>
    <p class="noindent">
     Also
note
that
every
single
layer
uses
a
stride
of
1
and
     <alert style="color: #821131;">
      "same"
     </alert>
     padding
     <alert style="color: #821131;">
      <span id="bold" style="font-weight:bold;">
       <sup class="textsuperscript">
        19
       </sup>
      </span>
     </alert>
     <span class="marginnote">
      <span style="color:#999999;">
      </span>
      <alert style="color: #821131;">
       <span id="bold" style="font-weight:bold;">
        <sup class="textsuperscript">
         19
        </sup>
       </span>
      </alert>
      <span style="color:#0063B2;">
       even
the
max
pooling
layer
      </span>
     </span>
     .
This
is
done
so
     <alert style="color: #821131;">
      outputs
all
have
the
same
height
and
width
as
their
inputs
     </alert>
     .
This
allows
concatenating
all
outputs
along
                                                                                
                                                                                
the
depth
dimension
in
the
final
depth
concatenation
layer
(i.e.,
to
stack
the
feature
maps
from
all
four
top
convolutional
layers).
    </p>
    
    <div class="knowledge">
     <p class="noindent">
      It can be implemented using Keras’s <span style="color:#054C5C;"><code class="verb">Concatenate</code></span>
      layer, using the default <span style="color:#054C5C;"><code class="verb">axis=-1</code></span>.
     </p>
    </div>
    <p class="noindent">
     You may wonder why inception modules have convolutional layers with 1-by-1 kernels.
    </p>
    <div class="quoteblock">
     <p class="noindent">
      Surely
these
layers
cannot
capture
any
features
because
they
look
at
only
one
pixel
at
a
time,
right?
     </p>
    </div>
    <p class="noindent">
     In fact, these layers serve three <alert style="color: #821131;">(3)</alert> purposes:
    </p>
    <dl class="enumerate-enumitem">
     <dt class="enumerate-enumitem">
      1.
     </dt>
     <dd class="enumerate-enumitem">
      Although
they
cannot
capture
spatial
patterns,
they
can
capture
patterns
along
the
depth
                                                                                
                                                                                
     
dimension
(i.e.,
across
channels).
     </dd>
     <dt class="enumerate-enumitem">
      2.
     </dt>
     <dd class="enumerate-enumitem">
      They
are
configured
to
output
fewer
feature
maps
compared
to
their
inputs,
so
they
serve
as
      <span id="bold" style="font-weight:bold;">
       bottleneck
layers
      </span>
      ,
meaning
they
reduce
dimensionality.
This
cuts
the
computational
cost
and
the
number
of
parameters,
speeding
up
training
and
improving
generalization.
     </dd>
     <dt class="enumerate-enumitem">
      3.
     </dt>
     <dd class="enumerate-enumitem">
      Each
pair
of
convolutional
layers
([1-by-1,
3-by-3]
and
[1-by-1,
5-by-5])
acts
like
a
single
powerful
convolutional
layer,
                                                                                
                                                                                
     
capable
of
capturing
more
complex
patterns.
A
convolutional
layer
is
equivalent
to
sweeping
a
dense
layer
across
the
image
(at
each
location,
it
only
looks
at
a
small
receptive
field),
and
these
pairs
of
convolutional
layers
are
equivalent
to
sweeping
two-layer
neural
networks
across
the
image.
     </dd>
    </dl>
    <p class="noindent">
     In short, we can think of the whole inception module as a convolutional layer on steroids, able to output feature maps that
capture complex patterns at various scales.
    </p>
    <div class="figure">
     <p class="noindent">
      <img alt="PIC" height="" src="figures/Computer-Vision-using-Convolutional-Neural-Networks/figures-5.svg" width="150%"/>
      <a id="x9-114005r14">
      </a>
     </p>
     <figcaption class="caption">
      <span class="id">
       Figure 7.14:
      </span>
      <span class="content">
       The GoogLeNet architecture. The nodes coloured in (
       <span style="color:#FCC055;">
        <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
         <mo class="MathClass-ord">
          ■
         </mo>
        </math>
       </span>
       )
          are called inception nodes.
      </span>
     </figcaption>
    </div>
    <p class="noindent">
     Now let’s look at the architecture of the GoogLeNet
     <a class="glossary" href="DataScienceIILectureBookli2.html#glo:cnn">
      CNN
     </a>
     shown in
     <span id="bold" style="font-weight:bold;">
      Fig.
     </span>
     <a href="#x9-114005r14">
      7.14
     </a>. The number of feature maps output by each
convolutional layer and each pooling layer is shown before the kernel size. The architecture is so deep that it has to be
represented in three <alert style="color: #821131;">(3)</alert> columns, but GoogLeNet is actually one tall stack, including nine <alert style="color: #821131;">(9)</alert> inception modules (nodes coloured
in
     <span style="color:#FCC055;">
      <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
       <mo class="MathClass-ord">
        ■
       </mo>
      </math>
     </span>
     ). The
six numbers in the inception modules represent the number of feature maps output by each convolutional layer in the
module.
    </p>
    <div class="knowledge">
     <p class="noindent">
      All the convolutional layers use the ReLU activation function.
     </p>
    </div>
    <p class="noindent">
     Let’s go through this network together:
    </p>
    <ul class="itemize1">
     <li class="itemize">
      <p class="noindent">
       The
first
two
<alert style="color: #821131;">(2)</alert>
layers
divide
the
image’s
height
and
width
by
4
(so
its
area
is
divided
by
16).
This
reduces
the
computational
load.
The
first
layer
uses
a
large
kernel
size,
7-by-7,
so
that
much
of
the
information
is
preserved.
      </p>
     </li>
     <li class="itemize">
      <p class="noindent">
       Then
the
local
response
normalization
layer
                                                                                
                                                                                
     
ensures
that
the
previous
layers
learn
a
wide
variety
of
features
.
      </p>
     </li>
    </ul>
    <div class="informationblock" id="tcolobox-79">
     <div class="title">
      <p class="noindent">
       <span id="bold" style="font-weight:bold;">
        Information
       </span>
       : Local Normalisation Layer
      </p>
     </div>
     <div class="box-content">
      <p class="noindent">
       This layer’s job is to create a sort of
       <alert style="color: #821131;">
        lateral inhibition
       </alert>
       . This refers to the capacity of an excited neuron to
subdue its neighbors. We basically want a significant peak so that we have a form of local maxima. This tends
to create a contrast in that area, hence increasing the sensory perception.
      </p>
     </div>
    </div>
    <ul class="itemize1">
     <li class="itemize">
      <p class="noindent">
       Two
convolutional
layers
follow,
where
the
first
acts
like
a
bottleneck
layer.
Think
of
this
pair
as
a
single
smarter
convolutional
layer.
      </p>
     </li>
     <li class="itemize">
      <p class="noindent">
       A
local
response
normalisation
layer
ensures
the
previous
layers
capture
a
wide
variety
of
patterns.
      </p>
     </li>
     <li class="itemize">
      <p class="noindent">
       Next,
a
max
pooling
layer
reduces
the
image
height
and
width
by
2,
again
to
speed
up
computations.
      </p>
     </li>
     <li class="itemize">
      <p class="noindent">
       Then
comes
the
       <a class="glossary" href="DataScienceIILectureBookli2.html#glo:cnn">
        CNN
       </a>
       ’s
backbone:
a
tall
stack
of
nine
<alert style="color: #821131;">(9)</alert>
inception
modules,
interleaved
with
a
couple
of
max
pooling
layers
to
reduce
dimensionality
and
speed
up
the
net.
      </p>
     </li>
     <li class="itemize">
      <p class="noindent">
       Next,
the
global
average
pooling
layer
outputs
the
mean
of
each
feature
                                                                                
                                                                                
     
map:
this
drops
any
remaining
spatial
information,
which
is
fine
because
there
is
not
much
spatial
information
left
at
that
point.
Indeed,
GoogLeNet
input
images
are
typically
expected
to
be
224
Œ
224
pixels,
so
after
5
max
pooling
layers,
each
dividing
the
height
and
width
by
2,
the
feature
maps
are
down
to
7
Œ
7.
      </p>
     </li>
    </ul>
    <div class="warning">
     <p class="noindent">
      This classification task, not localization, so it doesn’t matter where the object is.
     </p>
    </div>
    <ul class="itemize1">
     <li class="itemize">
      <p class="noindent">
       Thanks
to
                                                                                
                                                                                
     
the
dimensionality
reduction
brought
by
the
global
average
pool
layer,
there
is
no
need
to
have
several
fully
connected
layers
at
the
top
of
the
       <a class="glossary" href="DataScienceIILectureBookli2.html#glo:cnn">
        CNN
       </a>
       <alert style="color: #821131;">
        <span id="bold" style="font-weight:bold;">
         <sup class="textsuperscript">
          20
         </sup>
        </span>
       </alert>
       <span class="marginnote">
        <span style="color:#999999;">
        </span>
        <alert style="color: #821131;">
         <span id="bold" style="font-weight:bold;">
          <sup class="textsuperscript">
           20
          </sup>
         </span>
        </alert>
        <span style="color:#0063B2;">
         This
is
unlike
AlexNet.
        </span>
       </span>
       ,
and
this
considerably
reduces
the
number
of
parameters
in
the
network
and
limits
the
risk
of
overfitting.
      </p>
      
     </li>
     <li class="itemize">
      <p class="noindent">
       The
last
layers
are
self-explanatory:
dropout
for
regularization,
then
a
fully
connected
layer
with
1,000
                                                                                
                                                                                
     
units
(since
there
are
1,000
classes)
and
a
softmax
activation
function
to
output
estimated
class
probabilities.
      </p>
     </li>
    </ul>
    <p class="noindent">
     The original GoogLeNet architecture included two auxiliary classifiers plugged on top of the third and sixth inception modules.
They were both composed:
    </p>
    <ul class="itemize1">
     <li class="itemize">
      <p class="noindent">
       One
average
pooling
layer
      </p>
     </li>
     <li class="itemize">
      <p class="noindent">
       one
convolutional
layer
      </p>
     </li>
     <li class="itemize">
      <p class="noindent">
       two
fully
connected
layers
      </p>
     </li>
     <li class="itemize">
      <p class="noindent">
       a
softmax
activation
layer
      </p>
     </li>
    </ul>
    <p class="noindent">
     During training, their loss (scaled down by 70%) was added to the overall loss.
    </p>
    <div class="knowledge">
     <p class="noindent">
      The goal adding these auxiliary classifiers was to fight the vanishing gradients problem and regularize the network, but it was
later shown that their effect was relatively minor.
     </p>
    </div>
    <p class="noindent">
     Several variants of the GoogLeNet architecture were later proposed by Google researchers, including Inception-v3 <a id="x9-114006"></a><a href="#X0-xia2017inception">[70]</a> and
Inception-v4 <a id="x9-114007"></a><a href="#X0-szegedy2017inception">[71]</a>, using slightly different inception modules to reach even better performance.
    </p>
    <h3 class="subsectionHead">
     <a aria-label="Link to this section in PDF version" class="pdf-link" href="DataScienceIILectureBook.pdf#subsection.7.6.4" style="color: red;  position: absolute;  left: -30px;" target="_blank">
      <svg class="bi bi-file-pdf" fill="currentColor" height="20" viewbox="0 0 16 16" width="20" xmlns="http://www.w3.org/2000/svg">
       <path d="M4 0a2 2 0 0 0-2 2v12a2 2 0 0 0 2 2h8a2 2 0 0 0 2-2V2a2 2 0 0 0-2-2H4zm0 1h8a1 1 0 0 1 1 1v12a1 1 0 0 1-1 1H4a1 1 0 0 1-1-1V2a1 1 0 0 1 1-1z">
       </path>
       <path d="M4.603 12.087a.81.81 0 0 1-.438-.42c-.195-.388-.13-.776.08-1.102.198-.307.526-.568.897-.787a7.68 7.68 0 0 1 1.482-.645 19.701 19.701 0 0 0 1.062-2.227 7.269 7.269 0 0 1-.43-1.295c-.086-.4-.119-.796-.046-1.136.075-.354.274-.672.65-.823.192-.077.4-.12.602-.077a.7.7 0 0 1 .477.365c.088.164.12.356.127.538.007.187-.012.395-.047.614-.084.51-.27 1.134-.52 1.794a10.954 10.954 0 0 0 .98 1.686 5.753 5.753 0 0 1 1.334.05c.364.065.734.195.96.465.12.144.193.32.2.518.007.192-.047.382-.138.563a1.04 1.04 0 0 1-.354.416.856.856 0 0 1-.51.138c-.331-.014-.654-.196-.933-.417a5.716 5.716 0 0 1-.911-.95 11.642 11.642 0 0 0-1.997.406 11.311 11.311 0 0 1-1.021 1.51c-.29.35-.608.655-.926.787a.793.793 0 0 1-.58.029zm1.379-1.901c-.166.076-.32.156-.459.238-.328.194-.541.383-.647.547-.094.145-.096.25-.04.361.01.022.02.036.026.044a.27.27 0 0 0 .035-.012c.137-.056.355-.235.635-.572a8.18 8.18 0 0 0 .45-.606zm1.64-1.33a12.647 12.647 0 0 1 1.01-.193 11.666 11.666 0 0 1-.51-.858 20.741 20.741 0 0 1-.5 1.05zm2.446.45c.15.162.296.3.435.41.24.19.407.253.498.256a.107.107 0 0 0 .07-.015.307.307 0 0 0 .094-.125.436.436 0 0 0 .059-.2.095.095 0 0 0-.026-.063c-.052-.062-.2-.152-.518-.209a3.881 3.881 0 0 0-.612-.053zM8.078 5.8a6.7 6.7 0 0 0 .2-.828c.031-.188.043-.343.038-.465a.613.613 0 0 0-.032-.198.517.517 0 0 0-.145.04c-.087.035-.158.106-.196.283-.04.192-.03.469.046.822.024.111.054.227.09.346z">
       </path>
      </svg>
     </a>
     <small class="titlemark">
      7.6.4
     </small>
     <a id="x9-1150007.6.4">
     </a>
     VGGNet
    </h3>
    <p class="noindent">
     The runner-up in the ILSVRC 2014 challenge was VGGNet <a id="x9-115001"></a><a href="#X0-karpathy2014large">[72]</a>, Karen Simonyan and Andrew Zisserman, from the Visual
Geometry Group (VGG) research lab at Oxford University, developed a very simple and classical architecture; it had 2
or 3 convolutional layers and a pooling layer, then again 2 or 3 convolutional layers and a pooling layer, and
so on, reaching a total of 16 or 19 convolutional layers, depending on the VGG variant. To add to this stack a
final dense network with 2 hidden layers and the output layer. It used small 3-by-3 filters, but it had many of
them.
    </p>
    <h3 class="subsectionHead">
     <a aria-label="Link to this section in PDF version" class="pdf-link" href="DataScienceIILectureBook.pdf#subsection.7.6.5" style="color: red;  position: absolute;  left: -30px;" target="_blank">
      <svg class="bi bi-file-pdf" fill="currentColor" height="20" viewbox="0 0 16 16" width="20" xmlns="http://www.w3.org/2000/svg">
       <path d="M4 0a2 2 0 0 0-2 2v12a2 2 0 0 0 2 2h8a2 2 0 0 0 2-2V2a2 2 0 0 0-2-2H4zm0 1h8a1 1 0 0 1 1 1v12a1 1 0 0 1-1 1H4a1 1 0 0 1-1-1V2a1 1 0 0 1 1-1z">
       </path>
       <path d="M4.603 12.087a.81.81 0 0 1-.438-.42c-.195-.388-.13-.776.08-1.102.198-.307.526-.568.897-.787a7.68 7.68 0 0 1 1.482-.645 19.701 19.701 0 0 0 1.062-2.227 7.269 7.269 0 0 1-.43-1.295c-.086-.4-.119-.796-.046-1.136.075-.354.274-.672.65-.823.192-.077.4-.12.602-.077a.7.7 0 0 1 .477.365c.088.164.12.356.127.538.007.187-.012.395-.047.614-.084.51-.27 1.134-.52 1.794a10.954 10.954 0 0 0 .98 1.686 5.753 5.753 0 0 1 1.334.05c.364.065.734.195.96.465.12.144.193.32.2.518.007.192-.047.382-.138.563a1.04 1.04 0 0 1-.354.416.856.856 0 0 1-.51.138c-.331-.014-.654-.196-.933-.417a5.716 5.716 0 0 1-.911-.95 11.642 11.642 0 0 0-1.997.406 11.311 11.311 0 0 1-1.021 1.51c-.29.35-.608.655-.926.787a.793.793 0 0 1-.58.029zm1.379-1.901c-.166.076-.32.156-.459.238-.328.194-.541.383-.647.547-.094.145-.096.25-.04.361.01.022.02.036.026.044a.27.27 0 0 0 .035-.012c.137-.056.355-.235.635-.572a8.18 8.18 0 0 0 .45-.606zm1.64-1.33a12.647 12.647 0 0 1 1.01-.193 11.666 11.666 0 0 1-.51-.858 20.741 20.741 0 0 1-.5 1.05zm2.446.45c.15.162.296.3.435.41.24.19.407.253.498.256a.107.107 0 0 0 .07-.015.307.307 0 0 0 .094-.125.436.436 0 0 0 .059-.2.095.095 0 0 0-.026-.063c-.052-.062-.2-.152-.518-.209a3.881 3.881 0 0 0-.612-.053zM8.078 5.8a6.7 6.7 0 0 0 .2-.828c.031-.188.043-.343.038-.465a.613.613 0 0 0-.032-.198.517.517 0 0 0-.145.04c-.087.035-.158.106-.196.283-.04.192-.03.469.046.822.024.111.054.227.09.346z">
       </path>
      </svg>
     </a>
     <small class="titlemark">
      7.6.5
     </small>
     <a id="x9-1160007.6.5">
     </a>
     ResNet
    </h3>
    <p class="noindent">
     Kaiming He et al. won the ILSVRC 2015 challenge using a Residual Network (ResNet) that delivered an astounding top-five error
rate under 3.6%. The winning variant used an extremely deep
     <a class="glossary" href="DataScienceIILectureBookli2.html#glo:cnn">
      CNN
     </a>
     composed of 152 layers (other variants had 34, 50, and 101
layers) <a id="x9-116001"></a><a href="#X0-he2016deep">[73]</a>.
    </p>
    <div class="knowledge">
     <p class="noindent">
      Computer vision models are getting deeper and deeper, with fewer and fewer parameters.
     </p>
    </div>
    The
key idea for training such a deep network is to use skip connections
    <alert style="color: #821131;">
     <span id="bold" style="font-weight:bold;">
      <sup class="textsuperscript">
       21
      </sup>
     </span>
    </alert>
    <span class="marginnote">
     <span style="color:#999999;">
     </span>
     <alert style="color: #821131;">
      <span id="bold" style="font-weight:bold;">
       <sup class="textsuperscript">
        21
       </sup>
      </span>
     </alert>
     <span style="color:#0063B2;">
      This is also called shortcut connections.
     </span>
    </span>
    .
    <div class="quoteblock">
     <p class="noindent">
      The
signal
feeding
into
a
layer
is
also
added
to
the
output
of
a
layer
located
higher
up
the
stack.
     </p>
     
    </div>
    <p class="noindent">
     Let’s see why this is useful. When training a neural network, the goal is simple:
    </p>
    <div class="quoteblock">
     <p class="noindent">
      To
make
it
model
a
target
function
      <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
       <mi>
        h
       </mi>
       <msup>
        <mrow>
         <mrow>
          <mo fence="true" form="prefix">
           (
          </mo>
          <mrow>
           <mi>
            x
           </mi>
          </mrow>
          <mo fence="true" form="postfix">
           )
          </mo>
         </mrow>
        </mrow>
        <mrow>
        </mrow>
       </msup>
      </math>
      .
     </p>
    </div>
    <p class="noindent">
     If we add the input
     <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
      <mi>
       x
      </mi>
     </math>
     to the output of the network (i.e., we add a skip connection), then the network will be forced to model:
    </p>
    <table class="equation-star">
     <tr>
      <td>
       <math class="equation" display="block" xmlns="http://www.w3.org/1998/Math/MathML">
        <mi>
         f
        </mi>
        <msup>
         <mrow>
          <mrow>
           <mo fence="true" form="prefix">
            (
           </mo>
           <mrow>
            <mi>
             x
            </mi>
           </mrow>
           <mo fence="true" form="postfix">
            )
           </mo>
          </mrow>
         </mrow>
         <mrow>
         </mrow>
        </msup>
        <mo class="MathClass-rel" stretchy="false">
         =
        </mo>
        <mi>
         h
        </mi>
        <msup>
         <mrow>
          <mrow>
           <mo fence="true" form="prefix">
            (
           </mo>
           <mrow>
            <mi>
             x
            </mi>
           </mrow>
           <mo fence="true" form="postfix">
            )
           </mo>
          </mrow>
         </mrow>
         <mrow>
         </mrow>
        </msup>
        <mo class="MathClass-bin" stretchy="false">
         −
        </mo>
        <mi>
         x
        </mi>
        <mspace class="qquad" width="2em">
        </mspace>
        <mstyle class="text">
         <mtext>
          rather than
         </mtext>
        </mstyle>
        <mspace class="qquad" width="2em">
        </mspace>
        <mi>
         h
        </mi>
        <msup>
         <mrow>
          <mrow>
           <mo fence="true" form="prefix">
            (
           </mo>
           <mrow>
            <mi>
             x
            </mi>
           </mrow>
           <mo fence="true" form="postfix">
            )
           </mo>
          </mrow>
         </mrow>
         <mrow>
         </mrow>
        </msup>
       </math>
      </td>
     </tr>
    </table>
    <p class="noindent">
     This approach is called
     <alert style="color: #821131;">
      residual learning
     </alert> <a id="x9-116002"></a><a href="#X0-he2016deep">[73]</a>.
    </p>
    <div class="figure">
     <p class="noindent">
      <img alt="PIC" height="" src="figures/Computer-Vision-using-Convolutional-Neural-Networks/figures-6.svg" width="150%"/>
      <a id="x9-116003r15">
      </a>
     </p>
     <figcaption class="caption">
      <span class="id">
       Figure 7.15:
      </span>
      <span class="content">
      </span>
     </figcaption>
    </div>
    <p class="noindent">
     When
we
initialise
a
regular
neural
network,
its
weights
are
close
to
zero
     <alert style="color: #821131;">
      <span id="bold" style="font-weight:bold;">
       <sup class="textsuperscript">
        22
       </sup>
      </span>
     </alert>
     <span class="marginnote">
      <span style="color:#999999;">
      </span>
      <alert style="color: #821131;">
       <span id="bold" style="font-weight:bold;">
        <sup class="textsuperscript">
         22
        </sup>
       </span>
      </alert>
      <span style="color:#0063B2;">
       They
are
not
       <alert style="color: #821131;">
        exactly
       </alert>
       zero
but
randomly
assigned
and
are
close
to
zero
      </span>
     </span>
     ,
so
the
network
just
outputs
values
close
to
zero.
If
we
add
a
skip
connection,
the
resulting
network
just
outputs
a
copy
of
its
inputs.
In
other
words,
it
     <alert style="color: #821131;">
      initially
models
the
identity
function
     </alert>
     .
    </p>
    
    <div class="knowledge">
     <p class="noindent">
      If the target function is fairly close to the identity function (which is often the case), this will speed up training considerably.
     </p>
    </div>
    <div class="figure">
     <p class="noindent">
      <img alt="PIC" height="" src="figures/Computer-Vision-using-Convolutional-Neural-Networks/figures-7.svg" width="150%"/>
      <a id="x9-116004r16">
      </a>
     </p>
     <figcaption class="caption">
      <span class="id">
       Figure 7.16:
      </span>
      <span class="content">
      </span>
     </figcaption>
    </div>
    <p class="noindent">
     In addition to the previously mentioned positive aspects, if we add many skip connections, the network can start making progress
even if several layers have not started learning yet <a id="x9-116005"></a><a href="#X0-liu2021rethinking">[74]</a>, which we can see the diagram in
     <span id="bold" style="font-weight:bold;">
      Fig.
     </span>
     <a href="#x9-116003r15">
      7.15
     </a>.
    </p>
    <div class="warning">
     <p class="noindent">
      Skip connections allows the signal to easily make its way across the whole network.
     </p>
    </div>
    <p class="noindent">
     The deep residual network can be seen as a stack of residual units (RUs), where each residual unit is a small neural network with
a skip connection. Now let’s look at ResNet’s architecture (see Figure 14-18).
    </p>
    <div class="figure">
     <p class="noindent">
      <img alt="PIC" height="" src="figures/Computer-Vision-using-Convolutional-Neural-Networks/raster/resnet-architecture.png" width="150%"/>
      <a id="x9-116006r17">
      </a>
     </p>
     <figcaption class="caption">
      <span class="id">
       Figure 7.17:
      </span>
      <span class="content">
      </span>
     </figcaption>
    </div>
    <p class="noindent">
     The idea of ResNet is simple to describe. It starts and ends exactly like GoogLeNet (except without a dropout layer), and in between
is just a very deep stack of residual units. Each residual unit is composed of two <alert style="color: #821131;">(2)</alert> convolutional layers
     <alert style="color: #821131;">
      <span id="bold" style="font-weight:bold;">
       <sup class="textsuperscript">
        23
       </sup>
      </span>
     </alert>
     <span class="marginnote">
      <span style="color:#999999;">
      </span>
      <alert style="color: #821131;">
       <span id="bold" style="font-weight:bold;">
        <sup class="textsuperscript">
         23
        </sup>
       </span>
      </alert>
      <span style="color:#0063B2;">
       There are no pooling layers
      </span>
     </span>
     , with batch normalization (BN) and ReLU activation, using 3-by-3 kernels and preserving spatial
dimensions (stride of 1, <span style="color:#054C5C;"><code class="verb">"same"</code></span>
     padding).
    </p>
    
    <div class="warning">
     <p class="noindent">
      The number of feature maps is doubled every few residual units, at the same time as their height and width are halved (using a
convolutional layer with stride 2)
     </p>
    </div>
    When this happens, the inputs cannot be added directly to the outputs of the residual unit
because they
    <alert style="color: #821131;">
     don’t have the same shape
    </alert>
    (for example, this problem affects the skip connection represented by the dashed arrow
in Figure 14-18). To solve this problem, the inputs are passed through a 1-by-1 convolutional layer with stride 2 and the right
number of output feature maps (see Figure 14-19).
    <p class="noindent">
     There are different variations of this aforementioned architecture, each having different numbers of layers. ResNet-34, as the
name implies, is a ResNet with 34 layers (only counting the convolutional layers and the fully connected layer)
containing 3 RUs that output 64 feature maps, 4 RUs with 128 maps, 6 RUs with 256 maps, and 3 RUs with 512
maps.
    </p>
    <p class="noindent">
     ResNets deeper than that, such as ResNet-152, use slightly different residual units. Instead of two <alert style="color: #821131;">(2)</alert> 3-by-3 convolutional layers
with 256 feature maps, they use three <alert style="color: #821131;">(3)</alert> convolutional layers:
    </p>
    <dl class="enumerate-enumitem">
     <dt class="enumerate-enumitem">
      1.
     </dt>
     <dd class="enumerate-enumitem">
      a
1-by-1
convolutional
layer
with
just
64
feature
maps
(4x
less),
which
acts
as
a
bottleneck
layer
(as
discussed
already)
     </dd>
     <dt class="enumerate-enumitem">
      2.
     </dt>
     <dd class="enumerate-enumitem">
      a
3-by-3
layer
with
64
feature
maps
     </dd>
     <dt class="enumerate-enumitem">
      3.
     </dt>
     <dd class="enumerate-enumitem">
      another
1-by-1
convolutional
layer
with
256
feature
maps
(4
times
64)
that
restores
                                                                                
                                                                                
     
the
original
depth
     </dd>
    </dl>
    <p class="noindent">
     ResNet-152 contains 3 such RUs that output 256 maps, then 8 RUs with 512 maps, a whopping 36 RUs with 1,024 maps, and
finally 3 RUs with 2,048 maps.
    </p>
    <p class="noindent">
    </p>
    <h2 class="sectionHead">
     <a aria-label="Link to this section in PDF version" class="pdf-link" href="DataScienceIILectureBook.pdf#section.7.7" style="color: red;  position: absolute;  left: -30px;" target="_blank">
      <svg class="bi bi-file-pdf" fill="currentColor" height="20" viewbox="0 0 16 16" width="20" xmlns="http://www.w3.org/2000/svg">
       <path d="M4 0a2 2 0 0 0-2 2v12a2 2 0 0 0 2 2h8a2 2 0 0 0 2-2V2a2 2 0 0 0-2-2H4zm0 1h8a1 1 0 0 1 1 1v12a1 1 0 0 1-1 1H4a1 1 0 0 1-1-1V2a1 1 0 0 1 1-1z">
       </path>
       <path d="M4.603 12.087a.81.81 0 0 1-.438-.42c-.195-.388-.13-.776.08-1.102.198-.307.526-.568.897-.787a7.68 7.68 0 0 1 1.482-.645 19.701 19.701 0 0 0 1.062-2.227 7.269 7.269 0 0 1-.43-1.295c-.086-.4-.119-.796-.046-1.136.075-.354.274-.672.65-.823.192-.077.4-.12.602-.077a.7.7 0 0 1 .477.365c.088.164.12.356.127.538.007.187-.012.395-.047.614-.084.51-.27 1.134-.52 1.794a10.954 10.954 0 0 0 .98 1.686 5.753 5.753 0 0 1 1.334.05c.364.065.734.195.96.465.12.144.193.32.2.518.007.192-.047.382-.138.563a1.04 1.04 0 0 1-.354.416.856.856 0 0 1-.51.138c-.331-.014-.654-.196-.933-.417a5.716 5.716 0 0 1-.911-.95 11.642 11.642 0 0 0-1.997.406 11.311 11.311 0 0 1-1.021 1.51c-.29.35-.608.655-.926.787a.793.793 0 0 1-.58.029zm1.379-1.901c-.166.076-.32.156-.459.238-.328.194-.541.383-.647.547-.094.145-.096.25-.04.361.01.022.02.036.026.044a.27.27 0 0 0 .035-.012c.137-.056.355-.235.635-.572a8.18 8.18 0 0 0 .45-.606zm1.64-1.33a12.647 12.647 0 0 1 1.01-.193 11.666 11.666 0 0 1-.51-.858 20.741 20.741 0 0 1-.5 1.05zm2.446.45c.15.162.296.3.435.41.24.19.407.253.498.256a.107.107 0 0 0 .07-.015.307.307 0 0 0 .094-.125.436.436 0 0 0 .059-.2.095.095 0 0 0-.026-.063c-.052-.062-.2-.152-.518-.209a3.881 3.881 0 0 0-.612-.053zM8.078 5.8a6.7 6.7 0 0 0 .2-.828c.031-.188.043-.343.038-.465a.613.613 0 0 0-.032-.198.517.517 0 0 0-.145.04c-.087.035-.158.106-.196.283-.04.192-.03.469.046.822.024.111.054.227.09.346z">
       </path>
      </svg>
     </a>
     <small class="titlemark">
      7.7
     </small>
     <a id="x9-1170007.7">
     </a>
     Implementing a ResNet-34 CNN using Keras
    </h2>
    <p class="noindent">
     Most
     <a class="glossary" href="DataScienceIILectureBookli2.html#glo:cnn">
      CNN
     </a>
     architectures
described
so
far
can
be
implemented
easily
using
Keras
     <alert style="color: #821131;">
      <span id="bold" style="font-weight:bold;">
       <sup class="textsuperscript">
        24
       </sup>
      </span>
     </alert>
     <span class="marginnote">
      <span style="color:#999999;">
      </span>
      <alert style="color: #821131;">
       <span id="bold" style="font-weight:bold;">
        <sup class="textsuperscript">
         24
        </sup>
       </span>
      </alert>
      <span style="color:#0063B2;">
       although
generally
we
would
load
a
       <alert style="color: #821131;">
        pre-trained
       </alert>
       network
instead,
as
we
will
see
later.
      </span>
     </span>
     .
To
illustrate
the
process,
let’s
implement
a
ResNet-34
from
scratch
with
Keras.
    </p>
    
    <p class="noindent">
     First,
we’ll
create
a <span style="color:#054C5C;"><code class="verb">ResidualUnit</code></span>
     layer:
    </p>
    <p class="noindent">
    </p>
    <pre><div id="fancyvrb173" style="padding:20px;border-radius: 3px;"><a id="x9-117002r165"></a>DefaultConv2D = partial(tf.keras.layers.Conv2D, kernel_size=<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">3</span></span>, strides=<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">1</span></span>, 
<a id="x9-117004r166"></a>                 padding=<span style="color:#800080;">"same"</span>, kernel_initializer=<span style="color:#800080;">"he_normal"</span>, 
<a id="x9-117006r167"></a>                 use_bias=<span style="color:#2B2BFF;"><span id="bold" style="font-weight:bold;">False</span></span>) 
<a id="x9-117008r168"></a> 
<a id="x9-117010r169"></a><span style="color:#2B2BFF;">class</span><span style="color:#BABABA;"> </span>ResidualUnit(tf.keras.layers.Layer): 
<a id="x9-117012r170"></a>   <span style="color:#2B2BFF;">def</span><span style="color:#BABABA;"> </span><italic><span id="bold" style="font-weight:bold;">__init__</span></italic>(<span style="color:#2B2BFF;">self</span>, filters, strides=<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">1</span></span>, activation=<span style="color:#800080;">"relu"</span>, **kwargs): 
<a id="x9-117014r171"></a>      <span style="color:#2B2BFF;">super</span>().<italic><span id="bold" style="font-weight:bold;">__init__</span></italic>(**kwargs) 
<a id="x9-117016r172"></a>      <span style="color:#2B2BFF;">self</span>.activation = tf.keras.activations.get(activation) 
<a id="x9-117018r173"></a>      <span style="color:#2B2BFF;">self</span>.main_layers = [ 
<a id="x9-117020r174"></a>         DefaultConv2D(filters, strides=strides), 
<a id="x9-117022r175"></a>         tf.keras.layers.BatchNormalization(), 
<a id="x9-117024r176"></a>         <span style="color:#2B2BFF;">self</span>.activation, 
<a id="x9-117026r177"></a>         DefaultConv2D(filters), 
<a id="x9-117028r178"></a>         tf.keras.layers.BatchNormalization() 
<a id="x9-117030r179"></a>      ] 
<a id="x9-117032r180"></a>      <span style="color:#2B2BFF;">self</span>.skip_layers = [] 
<a id="x9-117034r181"></a>      <span style="color:#2B2BFF;">if</span> strides &gt; <span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">1</span></span>: 
<a id="x9-117036r182"></a>         <span style="color:#2B2BFF;">self</span>.skip_layers = [ 
<a id="x9-117038r183"></a>           DefaultConv2D(filters, kernel_size=<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">1</span></span>, strides=strides), 
<a id="x9-117040r184"></a>           tf.keras.layers.BatchNormalization() 
<a id="x9-117042r185"></a>         ] 
<a id="x9-117044r186"></a> 
<a id="x9-117046r187"></a>   <span style="color:#2B2BFF;">def</span><span style="color:#BABABA;"> </span><italic><span id="bold" style="font-weight:bold;">call</span></italic>(<span style="color:#2B2BFF;">self</span>, inputs): 
<a id="x9-117048r188"></a>      Z = inputs 
<a id="x9-117050r189"></a>      <span style="color:#2B2BFF;">for</span> layer in <span style="color:#2B2BFF;">self</span>.main_layers: 
<a id="x9-117052r190"></a>         Z = layer(Z) 
<a id="x9-117054r191"></a>      skip_Z = inputs 
<a id="x9-117056r192"></a>      <span style="color:#2B2BFF;">for</span> layer in <span style="color:#2B2BFF;">self</span>.skip_layers: 
<a id="x9-117058r193"></a>         skip_Z = layer(skip_Z) 
<a id="x9-117060r194"></a>      <span style="color:#2B2BFF;">return</span> <span style="color:#2B2BFF;">self</span>.activation(Z + skip_Z) 
<a id="x9-117062r195"></a></div></pre>
    <p class="noindent">
     As we can see, this code matches Figure 14-19 pretty closely. In the constructor, we create all the layers we will
need:
    </p>
    <div class="quoteblock">
     <p class="noindent">
      the
main
layers
                                                                                
                                                                                
     
are
the
ones
on
the
right
side
of
the
diagram,
and
the
skip
layers
are
the
ones
on
the
left
      <alert style="color: #821131;">
       <span id="bold" style="font-weight:bold;">
        <sup class="textsuperscript">
         25
        </sup>
       </span>
      </alert>
      <span class="marginnote">
       <span style="color:#999999;">
       </span>
       <alert style="color: #821131;">
        <span id="bold" style="font-weight:bold;">
         <sup class="textsuperscript">
          25
         </sup>
        </span>
       </alert>
       <span style="color:#0063B2;">
        only
needed
if
the
stride
is
greater
than
1.
       </span>
      </span>
      .
     </p>
     
    </div>
    <p class="noindent">
     Then in the <span style="color:#054C5C;"><code class="verb">call()</code></span>
     method, we make the inputs go through the main layers and the skip layers (
     <italic>
      if any
     </italic>
     ), and we add both outputs
and apply the activation function.
    </p>
    <p class="noindent">
     Now we can build a ResNet-34 using a
     <alert style="color: #821131;">
      Sequential model
     </alert>
     , as it’s really just a long sequence of layers. We can treat each residual
unit as a single layer now that we have the <span style="color:#054C5C;"><code class="verb">ResidualUnit</code></span>
     class.
    </p>
    <p class="noindent">
     The code closely matches:
    </p>
    <p class="noindent">
    </p>
    <pre><div id="fancyvrb174" style="padding:20px;border-radius: 3px;"><a id="x9-117064r200"></a>model = tf.keras.Sequential([ 
<a id="x9-117066r201"></a>   DefaultConv2D(<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">64</span></span>, kernel_size=<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">7</span></span>, strides=<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">2</span></span>, input_shape=[<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">224</span></span>, <span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">224</span></span>, <span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">3</span></span>]), 
<a id="x9-117068r202"></a>   tf.keras.layers.BatchNormalization(), 
<a id="x9-117070r203"></a>   tf.keras.layers.Activation(<span style="color:#800080;">"relu"</span>), 
<a id="x9-117072r204"></a>   tf.keras.layers.MaxPool2D(pool_size=<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">3</span></span>, strides=<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">2</span></span>, padding=<span style="color:#800080;">"same"</span>), 
<a id="x9-117074r205"></a>]) 
<a id="x9-117076r206"></a>prev_filters = <span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">64</span></span> 
<a id="x9-117078r207"></a><span style="color:#2B2BFF;">for</span> filters in [<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">64</span></span>] * <span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">3</span></span> + [<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">128</span></span>] * <span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">4</span></span> + [<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">256</span></span>] * <span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">6</span></span> + [<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">512</span></span>] * <span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">3</span></span>: 
<a id="x9-117080r208"></a>   strides = <span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">1</span></span> <span style="color:#2B2BFF;">if</span> filters == prev_filters <span style="color:#2B2BFF;">else</span> <span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">2</span></span> 
<a id="x9-117082r209"></a>   model.add(ResidualUnit(filters, strides=strides)) 
<a id="x9-117084r210"></a>   prev_filters = filters 
<a id="x9-117086r211"></a> 
<a id="x9-117088r212"></a>model.add(tf.keras.layers.GlobalAvgPool2D()) 
<a id="x9-117090r213"></a>model.add(tf.keras.layers.Flatten()) 
<a id="x9-117092r214"></a>model.add(tf.keras.layers.Dense(<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">10</span></span>, activation=<span style="color:#800080;">"softmax"</span>))</div></pre>
    <p class="noindent">
     The only tricky part in this code is the loop that adds the <span style="color:#054C5C;"><code class="verb">ResidualUnit</code></span>
     layers to the model. As explained earlier:
    </p>
    <ul class="itemize1">
     <li class="itemize">
      <p class="noindent">
       the
first
3
RUs
have
64
filters,
      </p>
     </li>
     <li class="itemize">
      <p class="noindent">
       the
next
4
RUs
have
128
filters.
      </p>
     </li>
    </ul>
    <p class="noindent">
     and so on. At each iteration, we must set the stride to 1 when the number of filters is the same as in the previous RU, or else we
set it to 2; then we add the ResidualUnit, and finally we update <span style="color:#054C5C;"><code class="verb">prev_filters</code></span>.
    </p>
    <p class="noindent">
     With just 40 lines of code, we can build the model that won the ILSVRC 2015 challenge. This demonstrates both the elegance of
the ResNet model and the expressiveness of the Keras API. Implementing the other
     <a class="glossary" href="DataScienceIILectureBookli2.html#glo:cnn">
      CNN
     </a>
     architectures is a bit longer, but not
much harder.
    </p>
    <p class="noindent">
     However, Keras comes with several of these architectures built in, so why not use them instead?
    </p>
    <p class="noindent">
    </p>
    <h2 class="sectionHead">
     <a aria-label="Link to this section in PDF version" class="pdf-link" href="DataScienceIILectureBook.pdf#section.7.8" style="color: red;  position: absolute;  left: -30px;" target="_blank">
      <svg class="bi bi-file-pdf" fill="currentColor" height="20" viewbox="0 0 16 16" width="20" xmlns="http://www.w3.org/2000/svg">
       <path d="M4 0a2 2 0 0 0-2 2v12a2 2 0 0 0 2 2h8a2 2 0 0 0 2-2V2a2 2 0 0 0-2-2H4zm0 1h8a1 1 0 0 1 1 1v12a1 1 0 0 1-1 1H4a1 1 0 0 1-1-1V2a1 1 0 0 1 1-1z">
       </path>
       <path d="M4.603 12.087a.81.81 0 0 1-.438-.42c-.195-.388-.13-.776.08-1.102.198-.307.526-.568.897-.787a7.68 7.68 0 0 1 1.482-.645 19.701 19.701 0 0 0 1.062-2.227 7.269 7.269 0 0 1-.43-1.295c-.086-.4-.119-.796-.046-1.136.075-.354.274-.672.65-.823.192-.077.4-.12.602-.077a.7.7 0 0 1 .477.365c.088.164.12.356.127.538.007.187-.012.395-.047.614-.084.51-.27 1.134-.52 1.794a10.954 10.954 0 0 0 .98 1.686 5.753 5.753 0 0 1 1.334.05c.364.065.734.195.96.465.12.144.193.32.2.518.007.192-.047.382-.138.563a1.04 1.04 0 0 1-.354.416.856.856 0 0 1-.51.138c-.331-.014-.654-.196-.933-.417a5.716 5.716 0 0 1-.911-.95 11.642 11.642 0 0 0-1.997.406 11.311 11.311 0 0 1-1.021 1.51c-.29.35-.608.655-.926.787a.793.793 0 0 1-.58.029zm1.379-1.901c-.166.076-.32.156-.459.238-.328.194-.541.383-.647.547-.094.145-.096.25-.04.361.01.022.02.036.026.044a.27.27 0 0 0 .035-.012c.137-.056.355-.235.635-.572a8.18 8.18 0 0 0 .45-.606zm1.64-1.33a12.647 12.647 0 0 1 1.01-.193 11.666 11.666 0 0 1-.51-.858 20.741 20.741 0 0 1-.5 1.05zm2.446.45c.15.162.296.3.435.41.24.19.407.253.498.256a.107.107 0 0 0 .07-.015.307.307 0 0 0 .094-.125.436.436 0 0 0 .059-.2.095.095 0 0 0-.026-.063c-.052-.062-.2-.152-.518-.209a3.881 3.881 0 0 0-.612-.053zM8.078 5.8a6.7 6.7 0 0 0 .2-.828c.031-.188.043-.343.038-.465a.613.613 0 0 0-.032-.198.517.517 0 0 0-.145.04c-.087.035-.158.106-.196.283-.04.192-.03.469.046.822.024.111.054.227.09.346z">
       </path>
      </svg>
     </a>
     <small class="titlemark">
      7.8
     </small>
     <a id="x9-1180007.8">
     </a>
     Using Pre-Trained Models from Keras
    </h2>
    <p class="noindent">
     In general, we don’t have to implement standard models like GoogLeNet or ResNet manually, as pre-trained networks are readily
available using the <span style="color:#054C5C;"><code class="verb">tf.keras.applications</code></span>
     package.
    </p>
    <p class="noindent">
     For example, we can load the ResNet-50 model, pre-trained on ImageNet, with the following line of code:
    </p>
    <p class="noindent">
    </p>
    <pre><div id="fancyvrb175" style="padding:20px;border-radius: 3px;"><a id="x9-118002r221"></a><span style="color:#2B2BFF;">import</span><span style="color:#BABABA;"> </span>tensorflow<span style="color:#BABABA;"> </span><span style="color:#2B2BFF;">as</span><span style="color:#BABABA;"> </span>tf 
<a id="x9-118004r222"></a> 
<a id="x9-118006r223"></a>model = tf.keras.applications.ResNet50(weights=<span style="color:#800080;">"imagenet"</span>)</div></pre>
    <p class="noindent">
     This was surprisingly simple. This will create a ResNet-50 model and download weights already trained
on the ImageNet dataset. To use it, we first need to ensure the images have the correct size. A
ResNet-50 model expects an image with the dimensions of 224-by-224-pixel
     <alert style="color: #821131;">
      <span id="bold" style="font-weight:bold;">
       <sup class="textsuperscript">
        26
       </sup>
      </span>
     </alert>
     <span class="marginnote">
      <span style="color:#999999;">
      </span>
      <alert style="color: #821131;">
       <span id="bold" style="font-weight:bold;">
        <sup class="textsuperscript">
         26
        </sup>
       </span>
      </alert>
      <span style="color:#0063B2;">
       other models may expect other sizes, such as 299-by-299.
      </span>
     </span>
     , so let’s use the
     <alert style="color: #821131;">
      Resizing
     </alert>
     layer, provided by Keras, to resize two <alert style="color: #821131;">(2)</alert>
sample images (after cropping them to the target aspect ratio):
    </p>
    
    <pre><div id="fancyvrb176" style="padding:20px;border-radius: 3px;"><a id="x9-118008r228"></a><span style="color:#2B2BFF;">from</span><span style="color:#BABABA;"> </span>sklearn.datasets<span style="color:#BABABA;"> </span><span style="color:#2B2BFF;">import</span> load_sample_images 
<a id="x9-118010r229"></a> 
<a id="x9-118012r230"></a>K = tf.keras.backend 
<a id="x9-118014r231"></a>images = K.constant(load_sample_images()[<span style="color:#800080;">"images"</span>]) 
<a id="x9-118016r232"></a>images_resized = tf.keras.layers.Resizing(height=<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">224</span></span>, width=<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">224</span></span>, 
<a id="x9-118018r233"></a>                              crop_to_aspect_ratio=<span style="color:#2B2BFF;"><span id="bold" style="font-weight:bold;">True</span></span>)(images)</div></pre>
    <p class="noindent">
     The pre-trained models assumes the images are
     <alert style="color: #821131;">
      pre-processed
     </alert>
     in a specific way. In some cases the models can expect the inputs
to be scaled from 0 to 1, or from -1 to 1, and so on. Each model provides a <span style="color:#054C5C;"><code class="verb">preprocess_input()</code></span>
     function we can use to
pre-process our images. These functions assume the original pixel values range from 0 to 255, which is the case
here:
    </p>
    <pre><div id="fancyvrb177" style="padding:20px;border-radius: 3px;"><a id="x9-118020r238"></a>inputs = tf.keras.applications.resnet50.preprocess_input(images_resized)</div></pre>
    <p class="noindent">
     Now we can use the pre-trained model to make predictions:
    </p>
    <pre><div id="fancyvrb178" style="padding:20px;border-radius: 3px;"><a id="x9-118022r243"></a>Y_proba = model.predict(inputs) 
<a id="x9-118024r244"></a><span style="color:#2B2BFF;">print</span>(Y_proba.shape)</div></pre>
    <div class="tcolorbox tcolorbox" id="tcolobox-80">
     <div class="tcolorbox-content">
      <pre><div id="fancyvrb179" style="padding:20px;border-radius: 3px;"><a id="x9-118026r1"></a>(2, 1000)</div></pre>
     </div>
    </div>
    As
usual, the output
    <span style="color:#054C5C;">
     <code class="verb">
      Y_proba
     </code>
    </span>
    is a matrix with
    <alert style="color: #821131;">
     one row per image
    </alert>
    and one column per class
    <alert style="color: #821131;">
     <span id="bold" style="font-weight:bold;">
      <sup class="textsuperscript">
       27
      </sup>
     </span>
    </alert>
    <span class="marginnote">
     <span style="color:#999999;">
     </span>
     <alert style="color: #821131;">
      <span id="bold" style="font-weight:bold;">
       <sup class="textsuperscript">
        27
       </sup>
      </span>
     </alert>
     <span style="color:#0063B2;">
      in this case, there are 1,000 classes
     </span>
    </span>
    . To display the top
    <alert style="color: #821131;">
     K
    </alert>
    predictions, including the class name and the estimated probability
of each predicted class, use the
    <span style="color:#054C5C;">
     <code class="verb">
      decode_predictions()
     </code>
    </span>
    function. For each image, it returns an array containing the top
    <span style="color:#054C5C;">
     <code class="verb">
      K
     </code>
    </span>
    predictions, where each prediction is represented as an array containing the class identifier, its name, and the corresponding
confidence score:
    <pre><div id="fancyvrb180" style="padding:20px;border-radius: 3px;"><a id="x9-118028r254"></a>top_K = tf.keras.applications.resnet50.decode_predictions(Y_proba, top=<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">3</span></span>) 
<a id="x9-118030r255"></a><span style="color:#2B2BFF;">for</span> image_index in <span style="color:#2B2BFF;">range</span>(<span style="color:#2B2BFF;">len</span>(images)): 
<a id="x9-118032r256"></a>   <span style="color:#2B2BFF;">print</span>(<span style="color:#800080;">f</span><span style="color:#800080;">"Image #</span><span style="color:#800080;">{</span>image_index<span style="color:#800080;">}</span><span style="color:#800080;">"</span>) 
<a id="x9-118034r257"></a>   <span style="color:#2B2BFF;">for</span> class_id, name, y_proba in top_K[image_index]: 
<a id="x9-118036r258"></a>      <span style="color:#2B2BFF;">print</span>(<span style="color:#800080;">f</span><span style="color:#800080;">"  </span><span style="color:#800080;">{</span>class_id<span style="color:#800080;">}</span><span style="color:#800080;"> - </span><span style="color:#800080;">{</span>name<span style="color:#800080;">:</span><span style="color:#800080;">12s</span><span style="color:#800080;">}</span><span style="color:#800080;"> </span><span style="color:#800080;">{</span>y_proba<span style="color:#800080;">:</span><span style="color:#800080;">.2%</span><span style="color:#800080;">}</span><span style="color:#800080;">"</span>)</div></pre>
    <p class="noindent">
     The
output
looks
like
this:
    </p>
    
    <div class="tcolorbox tcolorbox" id="tcolobox-81">
     <div class="tcolorbox-content">
      <pre><div id="fancyvrb181" style="padding:20px;border-radius: 3px;"><a id="x9-118038r1"></a>Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/imagenet_class_index.json 
<a id="x9-118040r2"></a>Image #0 
<a id="x9-118042r3"></a>  n03877845 - palace       54.69% 
<a id="x9-118044r4"></a>  n03781244 - monastery    24.71% 
<a id="x9-118046r5"></a>  n02825657 - bell_cote    18.55% 
<a id="x9-118048r6"></a>Image #1 
<a id="x9-118050r7"></a>  n04522168 - vase         32.67% 
<a id="x9-118052r8"></a>  n11939491 - daisy        17.82% 
<a id="x9-118054r9"></a>  n03530642 - honeycomb    12.04%</div></pre>
     </div>
    </div>
    <div class="figure">
     <p class="noindent">
      <img alt="PIC" height="" src="codes/images/Computer-Vision-using-Convolutional-Neural-Networks/image-net-.svg" width="150%"/>
      <a id="x9-118055r18">
      </a>
     </p>
     <figcaption class="caption">
      <span class="id">
       Figure 7.18:
      </span>
      <span class="content">
       The images used in testing the image recognition.
      </span>
     </figcaption>
    </div>
    <p class="noindent">
     The correct classes are
     <span id="bold" style="font-weight:bold;">
      palace
     </span>
     and
     <span id="bold" style="font-weight:bold;">
      dahlia
     </span>
     (which you can see in
     <span id="bold" style="font-weight:bold;">
      Fig.
     </span>
     <a href="#x9-118055r18">
      7.18
     </a>
     ), so the model is
     <span id="bold" style="font-weight:bold;">
      correct for the first image but
wrong for the second
     </span>
     .
    </p>
    <div class="warning">
     <p class="noindent">
      This
is
caused
by
dahlia
not
being
part
of
the
1,000
ImageNet
classes.
     </p>
    </div>
    Keeping
this
in
mind,
vase
is
a
reasonable
guess
    <alert style="color: #821131;">
     <span id="bold" style="font-weight:bold;">
      <sup class="textsuperscript">
       28
      </sup>
     </span>
    </alert>
    <span class="marginnote">
     <span style="color:#999999;">
     </span>
     <alert style="color: #821131;">
      <span id="bold" style="font-weight:bold;">
       <sup class="textsuperscript">
        28
       </sup>
      </span>
     </alert>
     <span style="color:#0063B2;">
      The
intricate
design
of
the
flower
might
be
reinterpreted
as
a
decoration
painted
on
a
vase
     </span>
    </span>
    ,
and
daisy
is
also
not
a
bad
choice
either,
are
dahlias
and
daisies
both
share
similar
features.
As
we
can
see,
it
is
very
                                                                                
                                                                                
easy
to
create
a
pretty
good
image
classifier
using
a
pre-trained
model.
    <p class="noindent">
     Many
vision
models
are
available
in <span style="color:#054C5C;"><code class="verb">tf.keras.applications</code></span>,
from
lightweight
and
fast
models
to
large
and
accurate
ones.
But
what
if
we
want
to
use
an
image
classifier
for
classes
of
images
that
are
not
part
of
ImageNet?
In
that
case,
we
may
still
benefit
from
the
pre-trained
models
by
using
them
to
perform
     <span id="bold" style="font-weight:bold;">
      transfer
learning
     </span>
     .
    </p>
    
    <p class="noindent">
    </p>
    <h2 class="sectionHead">
     <a aria-label="Link to this section in PDF version" class="pdf-link" href="DataScienceIILectureBook.pdf#section.7.9" style="color: red;  position: absolute;  left: -30px;" target="_blank">
      <svg class="bi bi-file-pdf" fill="currentColor" height="20" viewbox="0 0 16 16" width="20" xmlns="http://www.w3.org/2000/svg">
       <path d="M4 0a2 2 0 0 0-2 2v12a2 2 0 0 0 2 2h8a2 2 0 0 0 2-2V2a2 2 0 0 0-2-2H4zm0 1h8a1 1 0 0 1 1 1v12a1 1 0 0 1-1 1H4a1 1 0 0 1-1-1V2a1 1 0 0 1 1-1z">
       </path>
       <path d="M4.603 12.087a.81.81 0 0 1-.438-.42c-.195-.388-.13-.776.08-1.102.198-.307.526-.568.897-.787a7.68 7.68 0 0 1 1.482-.645 19.701 19.701 0 0 0 1.062-2.227 7.269 7.269 0 0 1-.43-1.295c-.086-.4-.119-.796-.046-1.136.075-.354.274-.672.65-.823.192-.077.4-.12.602-.077a.7.7 0 0 1 .477.365c.088.164.12.356.127.538.007.187-.012.395-.047.614-.084.51-.27 1.134-.52 1.794a10.954 10.954 0 0 0 .98 1.686 5.753 5.753 0 0 1 1.334.05c.364.065.734.195.96.465.12.144.193.32.2.518.007.192-.047.382-.138.563a1.04 1.04 0 0 1-.354.416.856.856 0 0 1-.51.138c-.331-.014-.654-.196-.933-.417a5.716 5.716 0 0 1-.911-.95 11.642 11.642 0 0 0-1.997.406 11.311 11.311 0 0 1-1.021 1.51c-.29.35-.608.655-.926.787a.793.793 0 0 1-.58.029zm1.379-1.901c-.166.076-.32.156-.459.238-.328.194-.541.383-.647.547-.094.145-.096.25-.04.361.01.022.02.036.026.044a.27.27 0 0 0 .035-.012c.137-.056.355-.235.635-.572a8.18 8.18 0 0 0 .45-.606zm1.64-1.33a12.647 12.647 0 0 1 1.01-.193 11.666 11.666 0 0 1-.51-.858 20.741 20.741 0 0 1-.5 1.05zm2.446.45c.15.162.296.3.435.41.24.19.407.253.498.256a.107.107 0 0 0 .07-.015.307.307 0 0 0 .094-.125.436.436 0 0 0 .059-.2.095.095 0 0 0-.026-.063c-.052-.062-.2-.152-.518-.209a3.881 3.881 0 0 0-.612-.053zM8.078 5.8a6.7 6.7 0 0 0 .2-.828c.031-.188.043-.343.038-.465a.613.613 0 0 0-.032-.198.517.517 0 0 0-.145.04c-.087.035-.158.106-.196.283-.04.192-.03.469.046.822.024.111.054.227.09.346z">
       </path>
      </svg>
     </a>
     <small class="titlemark">
      7.9
     </small>
     <a id="x9-1190007.9">
     </a>
     Pre-Trained Models for Transfer Learning
    </h2>
    <p class="noindent">
     If we want to build an
     <alert style="color: #821131;">
      image classifier
     </alert>
     but not have enough data to train it from scratch, it is often a good idea to reuse the
lower layers of a pre-trained model <a id="x9-119001"></a><a href="#X0-han2021pre">[75]</a>. For example, let’s train a model to classify pictures of
     <alert style="color: #821131;">
      flowers
     </alert>
     , reusing a pre-trained
Xception model.
    </p>
    <p class="noindent">
     First, we’ll load the flowers dataset using TensorFlow Datasets:
    </p>
    <pre><div id="fancyvrb182" style="padding:20px;border-radius: 3px;"><a id="x9-119003r289"></a><span style="color:#2B2BFF;">import</span><span style="color:#BABABA;"> </span>tensorflow_datasets<span style="color:#BABABA;"> </span><span style="color:#2B2BFF;">as</span><span style="color:#BABABA;"> </span>tfds 
<a id="x9-119005r290"></a> 
<a id="x9-119007r291"></a>dataset, info = tfds.load(<span style="color:#800080;">"tf_flowers"</span>, as_supervised=<span style="color:#2B2BFF;"><span id="bold" style="font-weight:bold;">True</span></span>, with_info=<span style="color:#2B2BFF;"><span id="bold" style="font-weight:bold;">True</span></span>) 
<a id="x9-119009r292"></a>dataset_size = info.splits[<span style="color:#800080;">"train"</span>].num_examples 
<a id="x9-119011r293"></a>class_names = info.features[<span style="color:#800080;">"label"</span>].names 
<a id="x9-119013r294"></a>n_classes = info.features[<span style="color:#800080;">"label"</span>].num_classes 
<a id="x9-119015r295"></a><span style="color:#2B2BFF;">print</span>(info)</div></pre>
    <div class="knowledge">
     <p class="noindent">
      We can get information about the dataset by setting <span style="color:#054C5C;"><code class="verb">with_info=True</code></span>.
     </p>
    </div>
    Here, we get the dataset size and the names of the classes.
Unfortunately, there is only a
    <span style="color:#054C5C;">
     <code class="verb">
      "train"
     </code>
    </span>
    dataset, no test set or validation set, so we need to split the training set. Let’s call
    <span style="color:#054C5C;">
     <code class="verb">
      tfds.load()
     </code>
    </span>
    again, but this time taking the first 10% of the dataset for testing, the next 15% for validation, and the remaining
75% for training:
    <p class="noindent">
    </p>
    <pre><div id="fancyvrb183" style="padding:20px;border-radius: 3px;"><a id="x9-119017r300"></a>test_set_raw, valid_set_raw, train_set_raw = tfds.load( 
<a id="x9-119019r301"></a>   <span style="color:#800080;">"tf_flowers"</span>, 
<a id="x9-119021r302"></a>   split=[<span style="color:#800080;">"train[:10%]"</span>, <span style="color:#800080;">"train[10%:25%]"</span>, <span style="color:#800080;">"train[25%:]"</span>], 
<a id="x9-119023r303"></a>   as_supervised=<span style="color:#2B2BFF;"><span id="bold" style="font-weight:bold;">True</span></span>)</div></pre>
    <p class="noindent">
     All three <alert style="color: #821131;">(3)</alert> datasets contain individual images. First let’s have a look at the data to get some idea on what we are working
with.
    </p>
    <div class="figure">
     <p class="noindent">
      <img alt="PIC" height="" src="codes/images/Computer-Vision-using-Convolutional-Neural-Networks/flower-set-.svg" width="150%"/>
      <a id="x9-119024r19">
      </a>
     </p>
     <figcaption class="caption">
      <span class="id">
       Figure 7.19:
      </span>
      <span class="content">
       Sample images present in the dataset dataset. As you can see the images are not all in the same shape
          which we need to work on.
      </span>
     </figcaption>
    </div>
    <p class="noindent">
     We need to batch them, but first we need to ensure they all have the same size, otherwise batching will fail. We can use a <span style="color:#054C5C;"><code class="verb">Resizing</code></span>
     layer for this. We must also call the <span style="color:#054C5C;"><code class="verb">tf.keras.applications</code></span>. <span style="color:#054C5C;"><code class="verb">xception.preprocess_input()</code></span>
     function to
preprocess the images appropriately for the Xception model. Lastly, we’ll also shuffle the training set and use
prefetching:
    </p>
    <pre><div id="fancyvrb184" style="padding:20px;border-radius: 3px;"><a id="x9-119026r322"></a>batch_size = <span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">32</span></span> 
<a id="x9-119028r323"></a>preprocess = tf.keras.Sequential([ 
<a id="x9-119030r324"></a>   tf.keras.layers.Resizing(height=<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">224</span></span>, width=<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">224</span></span>, crop_to_aspect_ratio=<span style="color:#2B2BFF;"><span id="bold" style="font-weight:bold;">True</span></span>), 
<a id="x9-119032r325"></a>   tf.keras.layers.Lambda(tf.keras.applications.xception.preprocess_input) 
<a id="x9-119034r326"></a>]) 
<a id="x9-119036r327"></a>train_set = train_set_raw.map(<span style="color:#2B2BFF;">lambda</span> X, y: (preprocess(X), y)) 
<a id="x9-119038r328"></a>train_set = train_set.shuffle(<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">1000</span></span>, seed=<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">42</span></span>).batch(batch_size).prefetch(<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">1</span></span>) 
<a id="x9-119040r329"></a>valid_set = valid_set_raw.map(<span style="color:#2B2BFF;">lambda</span> X, y: (preprocess(X), y)).batch(batch_size) 
<a id="x9-119042r330"></a>test_set = test_set_raw.map(<span style="color:#2B2BFF;">lambda</span> X, y: (preprocess(X), y)).batch(batch_size)</div></pre>
    <p class="noindent">
     Now each batch contains 32 images, all of them 224-by-224 pixels, with pixel values ranging from -1 to 1.
    </p>
    <div class="figure">
     <p class="noindent">
      <img alt="PIC" height="" src="codes/images/Computer-Vision-using-Convolutional-Neural-Networks/flower-set-adjusted-.svg" width="150%"/>
      <a id="x9-119043r20">
      </a>
     </p>
     <figcaption class="caption">
      <span class="id">
       Figure 7.20:
      </span>
      <span class="content">
       Sample images present in the dataset, normalised and all of them have the same dimensions.
      </span>
     </figcaption>
    </div>
    <p class="noindent">
     This is the ideal values for training such a network. As the dataset is not very large, a bit of data augmentation will certainly
help. Let’s create a data augmentation model that we will embed in our final model. During training, it will randomly flip the
images horizontally, rotate them a little bit, and tweak the contrast:
    </p>
    <pre><div id="fancyvrb185" style="padding:20px;border-radius: 3px;"><a id="x9-119045r348"></a>data_augmentation = tf.keras.Sequential([ 
<a id="x9-119047r349"></a>   tf.keras.layers.RandomFlip(mode=<span style="color:#800080;">"horizontal"</span>, seed=<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">42</span></span>), 
<a id="x9-119049r350"></a>   tf.keras.layers.RandomRotation(factor=<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">0.05</span></span>, seed=<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">42</span></span>), 
<a id="x9-119051r351"></a>   tf.keras.layers.RandomContrast(factor=<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">0.2</span></span>, seed=<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">42</span></span>) 
<a id="x9-119053r352"></a>])</div></pre>
    <p class="noindent">
     Next let’s load an Xception model, which is pre-trained on ImageNet. We exclude the top of the network by setting <span style="color:#054C5C;"><code class="verb">include_top=False</code></span>. This excludes the global average pooling layer and the dense output layer. We then add our own global
average pooling layer (feeding it the output of the base model), followed by a dense output layer with one unit per class, using
the softmax activation function. Finally, we wrap all this in a Keras Model:
    </p>
    <pre><div id="fancyvrb186" style="padding:20px;border-radius: 3px;"><a id="x9-119055r357"></a>tf.random.set_seed(<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">42</span></span>)  <span style="color:#008700;"><italic># extra code  ensures reproducibility</italic></span> 
<a id="x9-119057r358"></a>base_model = tf.keras.applications.xception.Xception(weights=<span style="color:#800080;">"imagenet"</span>, 
<a id="x9-119059r359"></a>                                      include_top=<span style="color:#2B2BFF;"><span id="bold" style="font-weight:bold;">False</span></span>) 
<a id="x9-119061r360"></a>avg = tf.keras.layers.GlobalAveragePooling2D()(base_model.output) 
<a id="x9-119063r361"></a>output = tf.keras.layers.Dense(n_classes, activation=<span style="color:#800080;">"softmax"</span>)(avg) 
<a id="x9-119065r362"></a>model = tf.keras.Model(inputs=base_model.input, outputs=output)</div></pre>
    <p class="noindent">
     It’s usually a good idea to freeze the weights of the pre-trained layers, at least at the beginning of training
     <alert style="color: #821131;">
      <span id="bold" style="font-weight:bold;">
       <sup class="textsuperscript">
        29
       </sup>
      </span>
     </alert>
     <span class="marginnote">
      <span style="color:#999999;">
      </span>
      <alert style="color: #821131;">
       <span id="bold" style="font-weight:bold;">
        <sup class="textsuperscript">
         29
        </sup>
       </span>
      </alert>
      <span style="color:#0063B2;">
       By not updating the weights of the frozen layers, we avoid tweaking features that are already well-established and generalize
well across different tasks.
      </span>
     </span>
     :
    </p>
    
    <pre><div id="fancyvrb187" style="padding:20px;border-radius: 3px;"><a id="x9-119067r367"></a><span style="color:#2B2BFF;">for</span> layer in base_model.layers: 
<a id="x9-119069r368"></a>   layer.trainable = <span style="color:#2B2BFF;"><span id="bold" style="font-weight:bold;">False</span></span></div></pre>
    <p class="noindent">
     Finally, we can compile the model and start training:
    </p>
    <pre><div id="fancyvrb188" style="padding:20px;border-radius: 3px;"><a id="x9-119071r373"></a>optimizer = tf.keras.optimizers.SGD(learning_rate=<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">0.1</span></span>, momentum=<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">0.9</span></span>) 
<a id="x9-119073r374"></a>model.compile(loss=<span style="color:#800080;">"sparse_categorical_crossentropy"</span>, optimizer=optimizer, 
<a id="x9-119075r375"></a>          metrics=[<span style="color:#800080;">"accuracy"</span>]) 
<a id="x9-119077r376"></a>history = model.fit(train_set, validation_data=valid_set, epochs=<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">3</span></span>)</div></pre>
    <p class="noindent">
     After
training
the
model
for
a
few
epochs,
its
validation
accuracy
should
reach
a
bit
over
80%
and
then
stop
improving.
This
means
that
the
top
layers
are
now
pretty
well
trained,
and
we
are
ready
to
unfreeze
some
of
the
                                                                                
                                                                                
base
model’s
top
layers,
then
continue
training.
    </p>
    <p class="noindent">
     For example, let’s unfreeze layers 56 and above (that’s the start of residual unit 7 out of 14, as you can see if you list the layer
names):
    </p>
    <pre><div id="fancyvrb189" style="padding:20px;border-radius: 3px;"><a id="x9-119079r381"></a><span style="color:#2B2BFF;">for</span> layer in base_model.layers[<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">56</span></span>:]: 
<a id="x9-119081r382"></a>   layer.trainable = <span style="color:#2B2BFF;"><span id="bold" style="font-weight:bold;">True</span></span></div></pre>
    <p class="noindent">
     Don’t forget to compile the model whenever you freeze or unfreeze layers. Also make sure to use a much lower learning rate to
avoid damaging the pre-trained weights:
    </p>
    <pre><div id="fancyvrb190" style="padding:20px;border-radius: 3px;"><a id="x9-119083r387"></a>optimizer = tf.keras.optimizers.SGD(learning_rate=<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">0.01</span></span>, momentum=<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">0.9</span></span>) 
<a id="x9-119085r388"></a>model.compile(loss=<span style="color:#800080;">"sparse_categorical_crossentropy"</span>, optimizer=optimizer, 
<a id="x9-119087r389"></a>          metrics=[<span style="color:#800080;">"accuracy"</span>]) 
<a id="x9-119089r390"></a>history = model.fit(train_set, validation_data=valid_set, epochs=<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">10</span></span>)</div></pre>
    <p class="noindent">
     This
model
should
reach
around
92%
accuracy
on
the
test
set,
in
just
a
few
minutes
of
training
(with
a
GPU).
If
you
tune
the
hyperparameters,
lower
the
learning
rate,
and
train
for
quite
a
bit
longer,
you
should
be
able
to
reach
95%
to
97%.
    </p>
    <p class="noindent">
     But
there’s
more
                                                                                
                                                                                
to
computer
vision
than
just
classification.
For
example,
what
if
you
also
want
to
know
where
the
flower
is
in
a
picture?
Let’s
look
at
this
now.
    </p>
    <pre><div id="fancyvrb191" style="padding:20px;border-radius: 3px;"><a id="x9-119091r395"></a>tf.random.set_seed(<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">42</span></span>)  <span style="color:#008700;"><italic># extra code  ensures reproducibility</italic></span> 
<a id="x9-119093r396"></a>base_model = tf.keras.applications.xception.Xception(weights=<span style="color:#800080;">"imagenet"</span>, 
<a id="x9-119095r397"></a>                                      include_top=<span style="color:#2B2BFF;"><span id="bold" style="font-weight:bold;">False</span></span>) 
<a id="x9-119097r398"></a>avg = tf.keras.layers.GlobalAveragePooling2D()(base_model.output) 
<a id="x9-119099r399"></a>class_output = tf.keras.layers.Dense(n_classes, activation=<span style="color:#800080;">"softmax"</span>)(avg) 
<a id="x9-119101r400"></a>loc_output = tf.keras.layers.Dense(<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">4</span></span>)(avg) 
<a id="x9-119103r401"></a>model = tf.keras.Model(inputs=base_model.input, 
<a id="x9-119105r402"></a>                outputs=[class_output, loc_output]) 
<a id="x9-119107r403"></a>optimizer = tf.keras.optimizers.SGD(learning_rate=<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">0.01</span></span>, momentum=<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">0.9</span></span>)  <span style="color:#008700;"><italic># added this line</italic></span> 
<a id="x9-119109r404"></a>model.compile(loss=[<span style="color:#800080;">"sparse_categorical_crossentropy"</span>, <span style="color:#800080;">"mse"</span>], 
<a id="x9-119111r405"></a>          loss_weights=[<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">0.8</span></span>, <span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">0.2</span></span>],  <span style="color:#008700;"><italic># depends on what you care most about</italic></span> 
<a id="x9-119113r406"></a>          optimizer=optimizer, metrics=[<span style="color:#800080;">"accuracy"</span>, <span style="color:#800080;">"mse"</span>])</div></pre>
   </article>
   <footer>
    <div class="next">
     <p class="noindent">
      <a href="DataScienceIILectureBookch6.html" style="float: left;">
       ← Previous Chapter
      </a>
      <a href="index.html">
       TOP
      </a>
     </p>
    </div>
   </footer>
  </div>
  <p class="noindent">
  </p>
  <p class="noindent">
   <a id="likesection.21">
   </a>
   <a id="x9-119114x7.9">
   </a>
   <a id="Q1-9-209">
   </a>
  </p>
  <p class="noindent">
   <a id="tailDataScienceIILectureBookch7.html">
   </a>
  </p>
 </body>
</html>
