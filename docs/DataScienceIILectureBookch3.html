<!DOCTYPE html>
<html lang="en-US" xml:lang="en-US">
 <head>
  <title>
   3 Ensemble Learning and Random Forests
  </title>
    <link rel='icon' type='image/x-icon' href='figures/logo.svg'></link>
  <meta charset="utf-8"/>
  <meta content="ParSnip" name="generator"/>
  <meta content="width=device-width,initial-scale=1" name="viewport"/>
  <link href="DataScienceIILectureBook.css" rel="stylesheet" type="text/css"/>
  <meta content="DataScienceIILectureBook.tex" name="src"/>
  <script async="async" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/mml-chtml.js" type="text/javascript">
  </script>
  <link href="flatweb.css" rel="stylesheet" type="text/css"/>
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.7.1/jquery.min.js">
  </script>
  <script src="flatjs.js">
  </script>
  <link href="http://fonts.googleapis.com/css?family=Roboto" rel="stylesheet" type="text/css"/>
  <script type="module">
   import pdfjsDist from 'https://cdn.jsdelivr.net/npm/pdfjs-dist@5.3.93/+esm'
  </script>
  <script>
   function moveTOC() { 
var htmlShow = document.getElementsByClassName("wide"); 
if (htmlShow[0].style.display === "none") { 
htmlShow[0].style.display = "block"; 
} else { 
htmlShow[0].style.display = "none"; 
} 
}
  </script>
  <link href="https://unpkg.com/nord-highlightjs@0.1.0/dist/nord.css" rel="stylesheet" type="text/css"/>
  <script src="https://unpkg.com/@highlightjs/cdn-assets@11.3.1/highlight.min.js">
  </script>
  <script src="https://unpkg.com/highlightjs-copy/dist/highlightjs-copy.min.js">
  </script>
  <link href="https://unpkg.com/highlightjs-copy/dist/highlightjs-copy.min.css" rel="stylesheet"/>
  <script src="flatjs.js">
  </script>
  <header>
  </header>
 </head>
 <body id="top">
  <nav class="wide">
   <div class="contents">
    <h3 style="font-weight:lighter; padding: 20px 0 20px 0;">
     3
   
     
   

   Ensemble Learning and Random Forests
    </h3>
    <h4 style="font-weight:lighter">
     Chapter Table of Contents
    </h4>
    <ul>
     <div class="chapterTOCS">
      <li>
       <span class="sectionToc">
        <small>
         3.1
        </small>
        <a href="#x5-250003.1">
         Introduction
        </a>
       </span>
      </li>
      <li>
       <span class="subsectionToc" style="font-size:12px;margin:0px;">
        <small>
         3.1.1
        </small>
        <a href="#x5-260003.1.1">
         Voting Classifiers
        </a>
       </span>
      </li>
      <li>
       <span class="sectionToc">
        <small>
         3.2
        </small>
        <a href="#x5-270003.2">
         Bagging and Pasting
        </a>
       </span>
      </li>
      <li>
       <span class="subsectionToc" style="font-size:12px;margin:0px;">
        <small>
         3.2.1
        </small>
        <a href="#x5-280003.2.1">
         Implementation
        </a>
       </span>
      </li>
      <li>
       <span class="subsectionToc" style="font-size:12px;margin:0px;">
        <small>
         3.2.2
        </small>
        <a href="#x5-290003.2.2">
         Out-of-Bag Evaluation
        </a>
       </span>
      </li>
      <li>
       <span class="subsectionToc" style="font-size:12px;margin:0px;">
        <small>
         3.2.3
        </small>
        <a href="#x5-300003.2.3">
         Random Patches and Random Subspaces
        </a>
       </span>
      </li>
      <li>
       <span class="sectionToc">
        <small>
         3.3
        </small>
        <a href="#x5-310003.3">
         Random Forests
        </a>
       </span>
      </li>
      <li>
       <span class="subsectionToc" style="font-size:12px;margin:0px;">
        <small>
         3.3.1
        </small>
        <a href="#x5-320003.3.1">
         Extra-Trees
        </a>
       </span>
      </li>
      <li>
       <span class="subsectionToc" style="font-size:12px;margin:0px;">
        <small>
         3.3.2
        </small>
        <a href="#x5-330003.3.2">
         Feature Importance
        </a>
       </span>
      </li>
      <li>
       <span class="sectionToc">
        <small>
         3.4
        </small>
        <a href="#x5-340003.4">
         Boosting
        </a>
       </span>
      </li>
      <li>
       <span class="subsectionToc" style="font-size:12px;margin:0px;">
        <small>
         3.4.1
        </small>
        <a href="#x5-350003.4.1">
         AdaBoost
        </a>
       </span>
      </li>
      <li>
       <span class="subsectionToc" style="font-size:12px;margin:0px;">
        <small>
         3.4.2
        </small>
        <a href="#x5-360003.4.2">
         Gradient Boosting
        </a>
       </span>
      </li>
      <li>
       <span class="subsectionToc" style="font-size:12px;margin:0px;">
        <small>
         3.4.3
        </small>
        <a href="#x5-370003.4.3">
         Histogram-Based Gradient Boosting
        </a>
       </span>
      </li>
      <li>
       <span class="sectionToc">
        <small>
         3.5
        </small>
        <a href="#x5-380003.5">
         Bagging v. Boosting
        </a>
       </span>
      </li>
      <li>
       <span class="sectionToc">
        <small>
         3.6
        </small>
        <a href="#x5-410003.6">
         Stacking
        </a>
       </span>
      </li>
     </div>
    </ul>
    <div class="prev-next" style="text-align: center">
     <p class="noindent">
      <a href="DataScienceIILectureBookch4.html" style="float:right; font-size:10px">
       NEXT →
      </a>
      <a href="DataScienceIILectureBookch2.html" style="float:left; font-size:10px">
       ← PREV
      </a>
      <a href="index.html">
       TOP
      </a>
     </p>
    </div>
    <div id="author-info" style="bottom: 0; padding-bottom: 10px;">
     <div id="author-logo">
      <img src="figures/logo.svg" style="margin: 10px 0;"/>
      <div>
       <div>
        <h4 style="color:#7aa0b8; font-weight:lighter;">
         WebBook | D. T. McGuiness, P.hD
        </h4>
       </div>
      </div>
      <div id="author-text" style="bottom: 0; padding-top: 50px;border-top: solid 1px #3b4b5e;font-size: 12px;text-align: right;">
       <p>
        <b>
         Authors Note
        </b>
        The website you are viewing is auto-generated
    using ParSnip and therefore subject to slight errors in
    typography and formatting. When in doubt, please consult the
    LectureBook.
       </p>
      </div>
     </div>
    </div>
   </div>
  </nav>
  <div class="page">
   <article class="chapter">
    <h1 class="chapterHead">
     <div class="number">
      3
     </div>
     <a id="x5-240003">
     </a>
     Ensemble Learning and Random Forests
    </h1><button id='toc-button' onclick='moveTOC()'>TOC</button>
    <div class="section-control" style="text-align: center">
     <a id="prev-section" onclick="goPrev()" style="float:left;">
      ← Previous Section
     </a>
     <a id="next-section" onclick="goNext()" style="float:right;">
      Next Section →
     </a>
    </div>
    <h2 class="sectionHead">
     <a aria-label="Link to this section in PDF version" class="pdf-link" href="DataScienceIILectureBook.pdf#section.3.1" style="color: red;  position: absolute;  left: -30px;" target="_blank">
      <svg class="bi bi-file-pdf" fill="currentColor" height="20" viewbox="0 0 16 16" width="20" xmlns="http://www.w3.org/2000/svg">
       <path d="M4 0a2 2 0 0 0-2 2v12a2 2 0 0 0 2 2h8a2 2 0 0 0 2-2V2a2 2 0 0 0-2-2H4zm0 1h8a1 1 0 0 1 1 1v12a1 1 0 0 1-1 1H4a1 1 0 0 1-1-1V2a1 1 0 0 1 1-1z">
       </path>
       <path d="M4.603 12.087a.81.81 0 0 1-.438-.42c-.195-.388-.13-.776.08-1.102.198-.307.526-.568.897-.787a7.68 7.68 0 0 1 1.482-.645 19.701 19.701 0 0 0 1.062-2.227 7.269 7.269 0 0 1-.43-1.295c-.086-.4-.119-.796-.046-1.136.075-.354.274-.672.65-.823.192-.077.4-.12.602-.077a.7.7 0 0 1 .477.365c.088.164.12.356.127.538.007.187-.012.395-.047.614-.084.51-.27 1.134-.52 1.794a10.954 10.954 0 0 0 .98 1.686 5.753 5.753 0 0 1 1.334.05c.364.065.734.195.96.465.12.144.193.32.2.518.007.192-.047.382-.138.563a1.04 1.04 0 0 1-.354.416.856.856 0 0 1-.51.138c-.331-.014-.654-.196-.933-.417a5.716 5.716 0 0 1-.911-.95 11.642 11.642 0 0 0-1.997.406 11.311 11.311 0 0 1-1.021 1.51c-.29.35-.608.655-.926.787a.793.793 0 0 1-.58.029zm1.379-1.901c-.166.076-.32.156-.459.238-.328.194-.541.383-.647.547-.094.145-.096.25-.04.361.01.022.02.036.026.044a.27.27 0 0 0 .035-.012c.137-.056.355-.235.635-.572a8.18 8.18 0 0 0 .45-.606zm1.64-1.33a12.647 12.647 0 0 1 1.01-.193 11.666 11.666 0 0 1-.51-.858 20.741 20.741 0 0 1-.5 1.05zm2.446.45c.15.162.296.3.435.41.24.19.407.253.498.256a.107.107 0 0 0 .07-.015.307.307 0 0 0 .094-.125.436.436 0 0 0 .059-.2.095.095 0 0 0-.026-.063c-.052-.062-.2-.152-.518-.209a3.881 3.881 0 0 0-.612-.053zM8.078 5.8a6.7 6.7 0 0 0 .2-.828c.031-.188.043-.343.038-.465a.613.613 0 0 0-.032-.198.517.517 0 0 0-.145.04c-.087.035-.158.106-.196.283-.04.192-.03.469.046.822.024.111.054.227.09.346z">
       </path>
      </svg>
     </a>
     <small class="titlemark">
      3.1
     </small>
     <a id="x5-250003.1">
     </a>
     Introduction
    </h2>
    <p class="noindent">
     Suppose you ask a complex question to millions of random people, then aggregate their
answers. In many cases you will find that this aggregated answer is better than an expert’s
answer.
    </p>
    <div class="knowledge">
     <p class="noindent">
      This is called the wisdom of the crowd.
      <alert style="color: #821131;">
       <span id="bold" style="font-weight:bold;">
        <sup class="textsuperscript">
         1
        </sup>
       </span>
      </alert>
      <span class="marginnote">
       <alert style="color: #821131;">
        <span id="bold" style="font-weight:bold;">
         <sup class="textsuperscript">
          1
         </sup>
        </span>
       </alert>
       <span style="color:#0063B2;">
        also known as “wisdom of the majority”, which expresses the notion that the
collective opinion of a diverse and independent group of individuals (rather than that of a single expert) produces the best
judgement <a id="x5-25001"></a><a href="#X0-millard2011wisdom">[1]</a>.
       </span>
      </span>
     </p>
    </div>
    <p class="noindent">
     In a similar fashion, aggregating the predictions of a group of predictors, such as classifiers or
regressors. We will often get better predictions than with the best individual predictor.
    </p>
    <div class="knowledge">
     <p class="noindent">
      A group of predictors is called an
      <span id="bold" style="font-weight:bold;">
       ensemble
      </span>
      and this technique,
      <span id="bold" style="font-weight:bold;">
       ensemble learning
      </span>
      , and the learning
method,
      <span id="bold" style="font-weight:bold;">
       ensemble method
      </span>
      .
     </p>
    </div>
    <p class="noindent">
     As an example of an ensemble method, we can train a group of
     <alert style="color: #821131;">
      decision tree classifiers
     </alert>
     , each on a
different random subset of the training set. We can then obtain the predictions of all the
individual trees, and the class that gets the most votes is the ensemble’s prediction. Such
an ensemble of decision trees is called a
     <a class="glossary" href="DataScienceIILectureBookli2.html#glo:rf">
      Random Forest (RF)
     </a>, and despite its simplicity,
this is one of the most powerful
     <a class="glossary" href="DataScienceIILectureBookli2.html#glo:ml">
      ML
     </a>
     algorithms available today.
     <alert style="color: #821131;">
      <span id="bold" style="font-weight:bold;">
       <sup class="textsuperscript">
        2
       </sup>
      </span>
     </alert>
     <span class="marginnote">
      <alert style="color: #821131;">
       <span id="bold" style="font-weight:bold;">
        <sup class="textsuperscript">
         2
        </sup>
       </span>
      </alert>
      <span style="color:#0063B2;">
       <span id="bold" style="font-weight:bold;">
        A Historical Overview
       </span>
       The general method of random decision forests was originally proposed by Salzberg and Heath in 1993 <a id="x5-25002"></a><a href="#X0-heath1993k">[2]</a>, with a method that used a
                                                                                
                                                                                
randomized decision tree algorithm to create multiple trees and then combine them using majority voting. This idea was developed
further by Ho in 1995 <a id="x5-25003"></a><a href="#X0-ho1995random">[3]</a>. Ho established that forests of trees splitting with oblique hyperplanes can gain accuracy as they grow
without suffering from overtraining, as long as the forests are randomly restricted to be sensitive to only selected feature dimensions.
A subsequent work along the same lines <a id="x5-25004"></a><a href="#X0-ho1998random">[4]</a> concluded that other splitting methods behave similarly, as long as they are
randomly forced to be insensitive to some feature dimensions. This observation that a more complex classifier (a larger forest)
gets more accurate nearly monotonically is in sharp contrast to the common belief that the complexity of a classifier can only
grow to a certain level of accuracy before being hurt by overfitting.
      </span>
     </span>
     In this chapter we will examine the most popular ensemble
methods, including:
    </p>
    <ul class="itemize1">
     <li class="itemize">
      <p class="noindent">
       voting
classifiers,
      </p>
     </li>
     <li class="itemize">
      <p class="noindent">
       bagging
and
pasting
ensembles,
      </p>
     </li>
     <li class="itemize">
      <p class="noindent">
       <a class="glossary" href="DataScienceIILectureBookli2.html#glo:rf">
        RF
       </a>
       s,
      </p>
     </li>
     <li class="itemize">
      <p class="noindent">
       boosting,
      </p>
     </li>
     <li class="itemize">
      <p class="noindent">
       and
stacking
ensembles.
      </p>
     </li>
    </ul>
    <h3 class="subsectionHead">
     <a aria-label="Link to this section in PDF version" class="pdf-link" href="DataScienceIILectureBook.pdf#subsection.3.1.1" style="color: red;  position: absolute;  left: -30px;" target="_blank">
      <svg class="bi bi-file-pdf" fill="currentColor" height="20" viewbox="0 0 16 16" width="20" xmlns="http://www.w3.org/2000/svg">
       <path d="M4 0a2 2 0 0 0-2 2v12a2 2 0 0 0 2 2h8a2 2 0 0 0 2-2V2a2 2 0 0 0-2-2H4zm0 1h8a1 1 0 0 1 1 1v12a1 1 0 0 1-1 1H4a1 1 0 0 1-1-1V2a1 1 0 0 1 1-1z">
       </path>
       <path d="M4.603 12.087a.81.81 0 0 1-.438-.42c-.195-.388-.13-.776.08-1.102.198-.307.526-.568.897-.787a7.68 7.68 0 0 1 1.482-.645 19.701 19.701 0 0 0 1.062-2.227 7.269 7.269 0 0 1-.43-1.295c-.086-.4-.119-.796-.046-1.136.075-.354.274-.672.65-.823.192-.077.4-.12.602-.077a.7.7 0 0 1 .477.365c.088.164.12.356.127.538.007.187-.012.395-.047.614-.084.51-.27 1.134-.52 1.794a10.954 10.954 0 0 0 .98 1.686 5.753 5.753 0 0 1 1.334.05c.364.065.734.195.96.465.12.144.193.32.2.518.007.192-.047.382-.138.563a1.04 1.04 0 0 1-.354.416.856.856 0 0 1-.51.138c-.331-.014-.654-.196-.933-.417a5.716 5.716 0 0 1-.911-.95 11.642 11.642 0 0 0-1.997.406 11.311 11.311 0 0 1-1.021 1.51c-.29.35-.608.655-.926.787a.793.793 0 0 1-.58.029zm1.379-1.901c-.166.076-.32.156-.459.238-.328.194-.541.383-.647.547-.094.145-.096.25-.04.361.01.022.02.036.026.044a.27.27 0 0 0 .035-.012c.137-.056.355-.235.635-.572a8.18 8.18 0 0 0 .45-.606zm1.64-1.33a12.647 12.647 0 0 1 1.01-.193 11.666 11.666 0 0 1-.51-.858 20.741 20.741 0 0 1-.5 1.05zm2.446.45c.15.162.296.3.435.41.24.19.407.253.498.256a.107.107 0 0 0 .07-.015.307.307 0 0 0 .094-.125.436.436 0 0 0 .059-.2.095.095 0 0 0-.026-.063c-.052-.062-.2-.152-.518-.209a3.881 3.881 0 0 0-.612-.053zM8.078 5.8a6.7 6.7 0 0 0 .2-.828c.031-.188.043-.343.038-.465a.613.613 0 0 0-.032-.198.517.517 0 0 0-.145.04c-.087.035-.158.106-.196.283-.04.192-.03.469.046.822.024.111.054.227.09.346z">
       </path>
      </svg>
     </a>
     <small class="titlemark">
      3.1.1
     </small>
     <a id="x5-260003.1.1">
     </a>
     Voting Classifiers
    </h3>
    <p class="noindent">
     Let’s assume that we have trained a few classifiers, with each one achieving about 80% accuracy.
    </p>
    <p class="noindent">
    </p>
    <div class="quoteblock">
     <p class="noindent">
      This
may
have
a
      <italic>
       logistic
regression
classifier
      </italic>
      ,
an
      <italic>
       SVM
classifier
      </italic>
      ,
a
      <a class="glossary" href="DataScienceIILectureBookli2.html#glo:rf">
       RF
      </a>
      <italic>
       classifier
      </italic>
      ,
                                                                                
                                                                                
     
a
      <italic>
       k-nearest
neighbour
classifier
      </italic>
      ,
and
perhaps
a
few
more.
     </p>
    </div>
    <p class="noindent">
     A simple way to create an even better classifier is to combine the predictions of each classifier where the class which gets the
most votes is the ensemble’s prediction.
    </p>
    <div class="knowledge">
     <p class="noindent">
      This majority-vote classifier is called a
      <span id="bold" style="font-weight:bold;">
       hard voting classifier
      </span>
      .
     </p>
    </div>
    <p class="noindent">
     Interestingly, this voting classifier often achieves a higher accuracy than the best classifier in the ensemble. In fact, even if each
classifier is a weak learner, meaning it does only slightly better than random guessing, the ensemble can still be a strong learner,
achieving high accuracy, provided there are a sufficient number of weak learners in the ensemble and they are
     <alert style="color: #821131;">
      sufficiently
diverse
     </alert>
     .
    </p>
    <p class="noindent">
     Let’s
discuss
how
this
all
works
with
an
example.
For
simplicity,
we
will
have
a
slightly
biased
coin
which
has
a
51%
chance
of
coming
up
heads
and
49%
chance
of
coming
up
tails.
     <alert style="color: #821131;">
      <span id="bold" style="font-weight:bold;">
       <sup class="textsuperscript">
        3
       </sup>
      </span>
     </alert>
     <span class="marginnote">
      <img alt="PIC" height="" src="figures/Ensemble-Learning-and-Random-Forests/figures-1.svg" width="100%"/>
      <alert style="color: #821131;">
       <span id="bold" style="font-weight:bold;">
        <sup class="textsuperscript">
         3
        </sup>
       </span>
      </alert>
      <span style="color:#0063B2;">
       A
biased
coin
such
as
this,
perhaps.
      </span>
     </span>
     If
you
toss
it
1,000
times,
                                                                                
                                                                                
we
will
generally
get
more
or
less
510
heads
and
490
tails,
and
therefore
will
result
in
a
majority
of
heads.
    </p>
    <p class="noindent">
     If
you
do
the
calculation,
you
will
find
that
the
probability
of
obtaining
a
majority
of
heads
after
1,000
tosses
is
close
to
75%.
The
more
you
toss
the
coin,
the
higher
the
probability
(e.g.,
with
10,000
tosses,
the
probability
climbs
over
97%).
This
is
due
                                                                                
                                                                                
to
the
     <span id="bold" style="font-weight:bold;">
      law
of
large
numbers
     </span>
     :
    </p>
    <p class="noindent">
    </p>
    <div class="quoteblock">
     <p class="noindent">
      as
we
keep
tossing
the
coin,
the
ratio
of
heads
gets
closer
and
closer
to
the
probability
of
heads
(51%).
     </p>
    </div>
    <div class="informationblock" id="tcolobox-13">
     <div class="title">
      <p class="noindent">
       <span id="bold" style="font-weight:bold;">
        Information
       </span>
       :
       <a class="glossary" href="DataScienceIILectureBookli2.html#glo:lln">
        LLN
       </a>
      </p>
     </div>
     <div class="box-content">
      <p class="noindent">
       A mathematical law states that the average of the results obtained from a large number of independent random
samples converges to the true value, if it exists. More formally, the
       <a class="glossary" href="DataScienceIILectureBookli2.html#glo:lln">
        LLN
       </a>
       states that given a sample of independent
and identically distributed values, the sample mean converges to the true mean. An example of this behaviour
is shown in
       <span id="bold" style="font-weight:bold;">
        Fig.
       </span>
       <a href="#x5-26001r1">
        3.1
       </a>.
      </p>
     </div>
    </div>
    <div class="figure">
     <p class="noindent">
      <img alt="PIC" height="" src="codes/images/Ensemble-Learning-and-Random-Forests/law-of-large-numbers-.svg" width="150%"/>
      <a id="x5-26001r1">
      </a>
     </p>
     <figcaption class="caption">
      <span class="id">
       Figure 3.1:
      </span>
      <span class="content">
       An example of a biased coin, where as the number of coin tosses increases the value of the will reach to
         51%.
      </span>
     </figcaption>
    </div>
    <p class="noindent">
     If
we
extend
this
analogy
to
our
previous
case,
Our
case
becomes
the
building
of
an
ensemble
containing
1,000
classifiers
which
are
individually
correct
only
51%
of
the
time.
     <alert style="color: #821131;">
      <span id="bold" style="font-weight:bold;">
       <sup class="textsuperscript">
        4
       </sup>
      </span>
     </alert>
     <span class="marginnote">
      <alert style="color: #821131;">
       <span id="bold" style="font-weight:bold;">
        <sup class="textsuperscript">
         4
        </sup>
       </span>
      </alert>
      <span style="color:#0063B2;">
       This
can
be
barely
considered
to
be
better
than
just
randomly
guessing.
      </span>
     </span>
    </p>
    <p class="noindent">
     In
this
scenario,
if
we
predict
the
majority
voted
class,
we
can
hope
for
up
to
75%
accuracy.
    </p>
    <p class="noindent">
     However,
this
is
only
true
if
                                                                                
                                                                                
all
classifiers
are
perfectly
independent,
making
uncorrelated
errors,
which
is
clearly
not
the
case
because
they
are
trained
on
the
same
data.
They
are
likely
to
make
the
same
types
of
errors,
so
there
will
be
many
majority
votes
for
the
wrong
class,
reducing
the
ensemble’s
accuracy.
    </p>
    <div class="knowledge">
     <p class="noindent">
      Ensemble methods work best when the predictors are as independent from one another as possible. One way to get diverse
classifiers is to train them using very different algorithms. This increases the chance that they will make very different types of
errors, improving the ensemble’s accuracy <a id="x5-26002"></a><a href="#X0-zhou2025ensemble">[5]</a>.
     </p>
    </div>
    <p class="noindent">
     To implement this behaviour, <span style="color:#054C5C;"><code class="verb">sklearn</code></span>
     provides a <span style="color:#054C5C;"><code class="verb">VotingClassifier</code></span>
     class which is intuitive as it just gives it a list of
name/predictor pairs, and use it like a normal classifier.
    </p>
    <p class="noindent">
     Let’s try it on the moons dataset we used in
     <a class="glossary" href="DataScienceIILectureBookli2.html#glo:svm">
      SVM
     </a>. We will load and split the moons dataset into a training set and a test set,
then we’ll create and train a voting classifier composed of three <alert style="color: #821131;">(3)</alert> diverse classifiers:
    </p>
    <pre><div id="fancyvrb20" style="padding:20px;border-radius: 3px;"><a id="x5-26004r50"></a><span style="color:#2B2BFF;">from</span><span style="color:#BABABA;"> </span>sklearn.datasets<span style="color:#BABABA;"> </span><span style="color:#2B2BFF;">import</span> make_moons 
<a id="x5-26006r51"></a><span style="color:#2B2BFF;">from</span><span style="color:#BABABA;"> </span>sklearn.ensemble<span style="color:#BABABA;"> </span><span style="color:#2B2BFF;">import</span> RandomForestClassifier, VotingClassifier 
<a id="x5-26008r52"></a><span style="color:#2B2BFF;">from</span><span style="color:#BABABA;"> </span>sklearn.linear_model<span style="color:#BABABA;"> </span><span style="color:#2B2BFF;">import</span> LogisticRegression 
<a id="x5-26010r53"></a><span style="color:#2B2BFF;">from</span><span style="color:#BABABA;"> </span>sklearn.model_selection<span style="color:#BABABA;"> </span><span style="color:#2B2BFF;">import</span> train_test_split 
<a id="x5-26012r54"></a><span style="color:#2B2BFF;">from</span><span style="color:#BABABA;"> </span>sklearn.svm<span style="color:#BABABA;"> </span><span style="color:#2B2BFF;">import</span> SVC 
<a id="x5-26014r55"></a>X, y = make_moons(n_samples=<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">500</span></span>, noise=<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">0.30</span></span>, random_state=<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">42</span></span>) 
<a id="x5-26016r56"></a>X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">42</span></span>) 
<a id="x5-26018r57"></a>voting_clf = VotingClassifier( 
<a id="x5-26020r58"></a>   estimators=[ 
<a id="x5-26022r59"></a>      (<span style="color:#800080;">'lr'</span>, LogisticRegression(random_state=<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">42</span></span>)), 
<a id="x5-26024r60"></a>      (<span style="color:#800080;">'rf'</span>, RandomForestClassifier(random_state=<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">42</span></span>)), 
<a id="x5-26026r61"></a>      (<span style="color:#800080;">'svc'</span>, SVC(random_state=<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">42</span></span>)) 
<a id="x5-26028r62"></a>   ] 
<a id="x5-26030r63"></a>) 
<a id="x5-26032r64"></a>voting_clf.fit(X_train, y_train)</div></pre>
    <p class="noindent">
     When we fit a <span style="color:#054C5C;"><code class="verb">VotingClassifier</code></span>, it clones every estimator and fits the clones. The original estimators are available via the <span style="color:#054C5C;"><code class="verb">estimators</code></span>
     attribute, whereas the fitted clones are available via the <span style="color:#054C5C;"><code class="verb">estimators_</code></span>
     attribute.
    </p>
    <div class="knowledge">
     <p class="noindent">
      If we prefer a <span style="color:#054C5C;"><code class="verb">dict</code></span>
      rather than a list, you can use <span style="color:#054C5C;"><code class="verb">named_estimators</code></span>
      or <span style="color:#054C5C;"><code class="verb">named_estimators_</code></span>
      instead.
     </p>
    </div>
    <p class="noindent">
     To begin, let’s look at each fitted classifier’s
     <span id="bold" style="font-weight:bold;">
      individual
     </span>
     accuracy on the test set:
    </p>
    <pre><div id="fancyvrb21" style="padding:20px;border-radius: 3px;"><a id="x5-26034r71"></a><span style="color:#2B2BFF;">for</span> name, clf in voting_clf.named_estimators_.items(): 
<a id="x5-26036r72"></a>   <span style="color:#2B2BFF;">print</span>(name, <span style="color:#800080;">"="</span>, clf.score(X_test, y_test))</div></pre>
    <div class="tcolorbox tcolorbox" id="tcolobox-14">
     <div class="tcolorbox-content">
      <pre><div id="fancyvrb22" style="padding:20px;border-radius: 3px;"><a id="x5-26038r1"></a>lr = 0.864 
<a id="x5-26040r2"></a>rf = 0.896 
<a id="x5-26042r3"></a>svc = 0.896</div></pre>
     </div>
    </div>
    <p class="noindent">
     When we call the voting classifier’s <span style="color:#054C5C;"><code class="verb">predict()</code></span>
     method, it performs hard voting.
    </p>
    <p class="noindent">
     For example, the voting classifier predicts
     <span id="bold" style="font-weight:bold;">
      class 1
     </span>
     for the first instance of the test set, because
     <span id="bold" style="font-weight:bold;">
      two out of three classifiers
     </span>
     predict that class:
    </p>
    <pre><div id="fancyvrb23" style="padding:20px;border-radius: 3px;"><a id="x5-26044r84"></a><span style="color:#2B2BFF;">print</span>(voting_clf.predict(X_test[:<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">1</span></span>])) 
<a id="x5-26046r85"></a><span style="color:#2B2BFF;">print</span>([clf.predict(X_test[:<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">1</span></span>]) <span style="color:#2B2BFF;">for</span> clf in voting_clf.estimators_]) 
<a id="x5-26048r86"></a><span style="color:#2B2BFF;">print</span>(voting_clf.score(X_test, y_test))</div></pre>
    <div class="tcolorbox tcolorbox" id="tcolobox-15">
     <div class="tcolorbox-content">
      <pre><div id="fancyvrb24" style="padding:20px;border-radius: 3px;"><a id="x5-26050r1"></a>[1] 
<a id="x5-26052r2"></a>[array([1]), array([1]), array([0])] 
<a id="x5-26054r3"></a>0.912</div></pre>
     </div>
    </div>
    <p class="noindent">
     And we can look at the performance of the voting classifier on the test set which is <span style="color:#054C5C;"><code class="verb">0.912</code></span>, which shows, the voting classifier
outperforms all the individual classifiers.
    </p>
    <p class="noindent">
     If
all
classifiers
are
able
to
estimate
class
probabilities,
     <alert style="color: #821131;">
      <span id="bold" style="font-weight:bold;">
       <sup class="textsuperscript">
        5
       </sup>
      </span>
     </alert>
     <span class="marginnote">
      <alert style="color: #821131;">
       <span id="bold" style="font-weight:bold;">
        <sup class="textsuperscript">
         5
        </sup>
       </span>
      </alert>
      <span style="color:#0063B2;">
       i.e.,
if
they
all
have
a <span style="color:#054C5C;"><code class="verb">predict_proba()</code></span>
       method.
      </span>
     </span>
     then
we
can
tell <span style="color:#054C5C;"><code class="verb">sklearn</code></span>
     to
predict
the
class
with
the
highest
class
probability,
averaged
over
all
the
individual
classifiers.
    </p>
    <p class="noindent">
     This
is
called
     <span id="bold" style="font-weight:bold;">
      soft
voting
     </span>
     ,
which
often
achieves
higher
performance
than
                                                                                
                                                                                
hard
voting
as
it
gives
more
weight
to
highly
confident
votes.
All
we
need
to
do
is
set
the
voting
classifier’s
voting
hyperparameter
to <span style="color:#054C5C;"><code class="verb">soft</code></span>,
and
ensure
that
all
classifiers
can
estimate
class
probabilities.
    </p>
    <p class="noindent">
     This
is
     <span id="bold" style="font-weight:bold;">
      NOT
     </span>
     the
case
for
the
SVC
class
by
default,
so
you
need
to
set
its
probability
hyperparameter
to <span style="color:#054C5C;"><code class="verb">True</code></span>
    </p>
    <div class="warning">
     <p class="noindent">
      This will make the SVC class use cross-validation to estimate class probabilities, slowing down training, and it will add a <span style="color:#054C5C;"><code class="verb">predict_proba()</code></span>
      method.
     </p>
    </div>
    <p class="noindent">
     Let’s try that:
    </p>
    <pre><div id="fancyvrb25" style="padding:20px;border-radius: 3px;"><a id="x5-26056r98"></a>voting_clf.voting = <span style="color:#800080;">"soft"</span> 
<a id="x5-26058r99"></a>voting_clf.named_estimators[<span style="color:#800080;">"svc"</span>].probability = <span style="color:#2B2BFF;"><span id="bold" style="font-weight:bold;">True</span></span> 
<a id="x5-26060r100"></a>voting_clf.fit(X_train, y_train) 
<a id="x5-26062r101"></a><span style="color:#2B2BFF;">print</span>(voting_clf.score(X_test, y_test))</div></pre>
    <div class="tcolorbox tcolorbox" id="tcolobox-16">
     <div class="tcolorbox-content">
      <pre><div id="fancyvrb26" style="padding:20px;border-radius: 3px;"><a id="x5-26064r1"></a>0.92</div></pre>
     </div>
    </div>
    <p class="noindent">
     We reach 92% accuracy simply by using soft voting.
    </p>
    <p class="noindent">
     To give a brief summary of what we have learned till now:
    </p>
    <dl class="description">
     <dt class="description">
      <a id="x5-26065x3.1.1">
      </a>
      Hard Voting
     </dt>
     <dd class="description">
      <p class="noindent">
       Takes
a
simple
majority
vote
to
decide
the
final
prediction,
based
on
the
most
frequent
class
predicted
by
individual
models.
      </p>
     </dd>
     <dt class="description">
      <a id="x5-26066x3.1.1">
      </a>
      Soft Voting
     </dt>
     <dd class="description">
      <p class="noindent">
       Considers
the
probability
scores
of
each
class
predicted
by
individual
models
and
averages
them
to
produce
a
more
refined
final
prediction.
      </p>
      <p class="noindent">
       When
dealing
with
imbalanced
datasets,
soft
voting
can
help
mitigate
the
bias
towards
                                                                                
                                                                                
     
the
majority
class
by
taking
into
account
the
probabilities
of
all
classes.
      </p>
     </dd>
    </dl>
    <p class="noindent">
    </p>
    <h2 class="sectionHead">
     <a aria-label="Link to this section in PDF version" class="pdf-link" href="DataScienceIILectureBook.pdf#section.3.2" style="color: red;  position: absolute;  left: -30px;" target="_blank">
      <svg class="bi bi-file-pdf" fill="currentColor" height="20" viewbox="0 0 16 16" width="20" xmlns="http://www.w3.org/2000/svg">
       <path d="M4 0a2 2 0 0 0-2 2v12a2 2 0 0 0 2 2h8a2 2 0 0 0 2-2V2a2 2 0 0 0-2-2H4zm0 1h8a1 1 0 0 1 1 1v12a1 1 0 0 1-1 1H4a1 1 0 0 1-1-1V2a1 1 0 0 1 1-1z">
       </path>
       <path d="M4.603 12.087a.81.81 0 0 1-.438-.42c-.195-.388-.13-.776.08-1.102.198-.307.526-.568.897-.787a7.68 7.68 0 0 1 1.482-.645 19.701 19.701 0 0 0 1.062-2.227 7.269 7.269 0 0 1-.43-1.295c-.086-.4-.119-.796-.046-1.136.075-.354.274-.672.65-.823.192-.077.4-.12.602-.077a.7.7 0 0 1 .477.365c.088.164.12.356.127.538.007.187-.012.395-.047.614-.084.51-.27 1.134-.52 1.794a10.954 10.954 0 0 0 .98 1.686 5.753 5.753 0 0 1 1.334.05c.364.065.734.195.96.465.12.144.193.32.2.518.007.192-.047.382-.138.563a1.04 1.04 0 0 1-.354.416.856.856 0 0 1-.51.138c-.331-.014-.654-.196-.933-.417a5.716 5.716 0 0 1-.911-.95 11.642 11.642 0 0 0-1.997.406 11.311 11.311 0 0 1-1.021 1.51c-.29.35-.608.655-.926.787a.793.793 0 0 1-.58.029zm1.379-1.901c-.166.076-.32.156-.459.238-.328.194-.541.383-.647.547-.094.145-.096.25-.04.361.01.022.02.036.026.044a.27.27 0 0 0 .035-.012c.137-.056.355-.235.635-.572a8.18 8.18 0 0 0 .45-.606zm1.64-1.33a12.647 12.647 0 0 1 1.01-.193 11.666 11.666 0 0 1-.51-.858 20.741 20.741 0 0 1-.5 1.05zm2.446.45c.15.162.296.3.435.41.24.19.407.253.498.256a.107.107 0 0 0 .07-.015.307.307 0 0 0 .094-.125.436.436 0 0 0 .059-.2.095.095 0 0 0-.026-.063c-.052-.062-.2-.152-.518-.209a3.881 3.881 0 0 0-.612-.053zM8.078 5.8a6.7 6.7 0 0 0 .2-.828c.031-.188.043-.343.038-.465a.613.613 0 0 0-.032-.198.517.517 0 0 0-.145.04c-.087.035-.158.106-.196.283-.04.192-.03.469.046.822.024.111.054.227.09.346z">
       </path>
      </svg>
     </a>
     <small class="titlemark">
      3.2
     </small>
     <a id="x5-270003.2">
     </a>
     Bagging and Pasting
    </h2>
    <p class="noindent">
     Now we got a general idea of
     <a class="glossary" href="DataScienceIILectureBookli2.html#glo:rf">
      RF
     </a>, let’s look at methods to improve it’s performance. A way to get a diverse set of classifiers is to
use diverse training algorithms. An alternative approach is to use the same training algorithm for every predictor but train them
on different random subsets of the training set.
    </p>
    <ul class="itemize1">
     <li class="itemize">
      <p class="noindent">
       When
sampling
is
performed
with
replacement,
this
method
is
called
       <span id="bold" style="font-weight:bold;">
        bagging
       </span>
       .
       <alert style="color: #821131;">
        <span id="bold" style="font-weight:bold;">
         <sup class="textsuperscript">
          6
         </sup>
        </span>
       </alert>
       <span class="marginnote">
        <alert style="color: #821131;">
         <span id="bold" style="font-weight:bold;">
          <sup class="textsuperscript">
           6
          </sup>
         </span>
        </alert>
        <span style="color:#0063B2;">
         This
is
short
for
bootstrap
aggregating.
        </span>
       </span>
      </p>
     </li>
     <li class="itemize">
      <p class="noindent">
       When
sampling
is
performed
without
replacement,
it
is
called
       <span id="bold" style="font-weight:bold;">
        pasting
       </span>
       .
      </p>
     </li>
    </ul>
    <div class="warning">
     <p class="noindent">
      Both bagging and pasting allow training instances to be sampled several times across multiple predictors, but only bagging allows
training instances to be sampled several times for the same predictor.
     </p>
    </div>
    <p class="noindent">
     Once
all
predictors
are
trained,
the
ensemble
can
make
a
prediction
for
a
new
                                                                                
                                                                                
instance
by
simply
aggregating
the
predictions
of
all
predictors.
The
aggregation
function
is
typically
the
statistical
mode
for
classification,
     <alert style="color: #821131;">
      <span id="bold" style="font-weight:bold;">
       <sup class="textsuperscript">
        7
       </sup>
      </span>
     </alert>
     <span class="marginnote">
      <alert style="color: #821131;">
       <span id="bold" style="font-weight:bold;">
        <sup class="textsuperscript">
         7
        </sup>
       </span>
      </alert>
      <span style="color:#0063B2;">
       i.e.,
the
most
frequent
prediction,
just
like
with
a
hard
voting
classifier.
      </span>
     </span>
     or
the
average
for
regression.
Each
individual
predictor
has
a
higher
bias
than
if
it
were
trained
on
the
original
training
set,
but
aggregation
reduces
both
bias
and
variance.
    </p>
    <p class="noindent">
     Generally,
the
net
result
is
that
                                                                                
                                                                                
the
ensemble
has
a
similar
bias
but
a
lower
variance
than
a
single
predictor
trained
on
the
original
training
set.
    </p>
    <div class="knowledge">
     <p class="noindent">
      Predictors can all be trained in parallel, via different
      <a class="glossary" href="DataScienceIILectureBookli2.html#glo:cpu">
       Central Processing Unit (CPU)
      </a>
      cores or even different servers. Similarly,
predictions can be made in parallel. This is one of the reasons bagging and pasting are such popular methods as they scale very
well.
     </p>
    </div>
    <h3 class="subsectionHead">
     <a aria-label="Link to this section in PDF version" class="pdf-link" href="DataScienceIILectureBook.pdf#subsection.3.2.1" style="color: red;  position: absolute;  left: -30px;" target="_blank">
      <svg class="bi bi-file-pdf" fill="currentColor" height="20" viewbox="0 0 16 16" width="20" xmlns="http://www.w3.org/2000/svg">
       <path d="M4 0a2 2 0 0 0-2 2v12a2 2 0 0 0 2 2h8a2 2 0 0 0 2-2V2a2 2 0 0 0-2-2H4zm0 1h8a1 1 0 0 1 1 1v12a1 1 0 0 1-1 1H4a1 1 0 0 1-1-1V2a1 1 0 0 1 1-1z">
       </path>
       <path d="M4.603 12.087a.81.81 0 0 1-.438-.42c-.195-.388-.13-.776.08-1.102.198-.307.526-.568.897-.787a7.68 7.68 0 0 1 1.482-.645 19.701 19.701 0 0 0 1.062-2.227 7.269 7.269 0 0 1-.43-1.295c-.086-.4-.119-.796-.046-1.136.075-.354.274-.672.65-.823.192-.077.4-.12.602-.077a.7.7 0 0 1 .477.365c.088.164.12.356.127.538.007.187-.012.395-.047.614-.084.51-.27 1.134-.52 1.794a10.954 10.954 0 0 0 .98 1.686 5.753 5.753 0 0 1 1.334.05c.364.065.734.195.96.465.12.144.193.32.2.518.007.192-.047.382-.138.563a1.04 1.04 0 0 1-.354.416.856.856 0 0 1-.51.138c-.331-.014-.654-.196-.933-.417a5.716 5.716 0 0 1-.911-.95 11.642 11.642 0 0 0-1.997.406 11.311 11.311 0 0 1-1.021 1.51c-.29.35-.608.655-.926.787a.793.793 0 0 1-.58.029zm1.379-1.901c-.166.076-.32.156-.459.238-.328.194-.541.383-.647.547-.094.145-.096.25-.04.361.01.022.02.036.026.044a.27.27 0 0 0 .035-.012c.137-.056.355-.235.635-.572a8.18 8.18 0 0 0 .45-.606zm1.64-1.33a12.647 12.647 0 0 1 1.01-.193 11.666 11.666 0 0 1-.51-.858 20.741 20.741 0 0 1-.5 1.05zm2.446.45c.15.162.296.3.435.41.24.19.407.253.498.256a.107.107 0 0 0 .07-.015.307.307 0 0 0 .094-.125.436.436 0 0 0 .059-.2.095.095 0 0 0-.026-.063c-.052-.062-.2-.152-.518-.209a3.881 3.881 0 0 0-.612-.053zM8.078 5.8a6.7 6.7 0 0 0 .2-.828c.031-.188.043-.343.038-.465a.613.613 0 0 0-.032-.198.517.517 0 0 0-.145.04c-.087.035-.158.106-.196.283-.04.192-.03.469.046.822.024.111.054.227.09.346z">
       </path>
      </svg>
     </a>
     <small class="titlemark">
      3.2.1
     </small>
     <a id="x5-280003.2.1">
     </a>
     Implementation
    </h3>
    <p class="noindent"> <span style="color:#054C5C;"><code class="verb">sklearn</code></span>
     offers simple classes for both bagging and pasting:
    </p>
    <p class="noindent">
    </p>
    <div class="quoteblock">
     <p class="noindent">
      the <span style="color:#054C5C;"><code class="verb">BaggingClassifier</code></span>
      class,
or <span style="color:#054C5C;"><code class="verb">BaggingRegressor</code></span>
      for
regression.
     </p>
    </div>
    <p class="noindent">
     The
code
below
trains
an
ensemble
of
500
decision
tree
classifiers
where
each
is
trained
on
100
training
instances
     <span id="bold" style="font-weight:bold;">
      randomly
sampled
     </span>
     from
                                                                                
                                                                                
the
training
set
with
replacement.
     <alert style="color: #821131;">
      <span id="bold" style="font-weight:bold;">
       <sup class="textsuperscript">
        8
       </sup>
      </span>
     </alert>
     <span class="marginnote">
      <alert style="color: #821131;">
       <span id="bold" style="font-weight:bold;">
        <sup class="textsuperscript">
         8
        </sup>
       </span>
      </alert>
      <span style="color:#0063B2;">
       This
is
an
example
of
bagging,
but
if
you
want
to
use
pasting
instead,
just
set <span style="color:#054C5C;"><code class="verb">bootstrap=False.</code></span>
      </span>
     </span>
     The <span style="color:#054C5C;"><code class="verb">n_jobs</code></span>
     parameter
tells <span style="color:#054C5C;"><code class="verb">sklearn</code></span>
     the
number
of
     <a class="glossary" href="DataScienceIILectureBookli2.html#glo:cpu">
      CPU
     </a>
     cores
to
use
for
training
and
predictions,
and <span style="color:#054C5C;"><code class="verb">-1</code></span>
     tells <span style="color:#054C5C;"><code class="verb">sklearn</code></span>
     to
use
all
available
cores:
    </p>
    <p class="noindent">
    </p>
    <pre><div id="fancyvrb27" style="padding:20px;border-radius: 3px;"><a id="x5-28002r111"></a><span style="color:#2B2BFF;">from</span><span style="color:#BABABA;"> </span>sklearn.ensemble<span style="color:#BABABA;"> </span><span style="color:#2B2BFF;">import</span> BaggingClassifier 
<a id="x5-28004r112"></a><span style="color:#2B2BFF;">from</span><span style="color:#BABABA;"> </span>sklearn.tree<span style="color:#BABABA;"> </span><span style="color:#2B2BFF;">import</span> DecisionTreeClassifier 
<a id="x5-28006r113"></a> 
<a id="x5-28008r114"></a>bag_clf = BaggingClassifier(DecisionTreeClassifier(), n_estimators=<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">500</span></span>, 
<a id="x5-28010r115"></a>                    max_samples=<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">100</span></span>, n_jobs=-<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">1</span></span>, random_state=<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">42</span></span>) 
<a id="x5-28012r116"></a>bag_clf.fit(X_train, y_train)</div></pre>
    <div class="knowledge">
     <p class="noindent">
      A <span style="color:#054C5C;"><code class="verb">BaggingClassifier</code></span>
      automatically performs soft voting instead of hard voting if the base classifier can estimate class
probabilities,
      <alert style="color: #821131;">
       <span id="bold" style="font-weight:bold;">
        <sup class="textsuperscript">
         9
        </sup>
       </span>
      </alert>
      <span class="marginnote">
       <alert style="color: #821131;">
        <span id="bold" style="font-weight:bold;">
         <sup class="textsuperscript">
          9
         </sup>
        </span>
       </alert>
       <span style="color:#0063B2;">
        i.e., if it has a <span style="color:#054C5C;"><code class="verb">predict_proba()</code></span>
        method
       </span>
      </span>
      which is the case with decision tree classifiers.
     </p>
    </div>
    <p class="noindent">
     Please have a look at
     <span id="bold" style="font-weight:bold;">
      Fig.
     </span>
     <a href="#x5-28013r2">
      3.2
     </a>, which compares the decision boundary of a single decision tree with the decision boundary of a
bagging ensemble of 500 trees, both trained on the moons dataset.
    </p>
    <div class="figure">
     <p class="noindent">
      <img alt="PIC" height="" src="codes/images/Ensemble-Learning-and-Random-Forests/decision_tree_without_and_with_bagging_plot-.svg" width="150%"/>
      <a id="x5-28013r2">
      </a>
     </p>
     <figcaption class="caption">
      <span class="id">
       Figure 3.2:
      </span>
      <span class="content">
       A single decision tree (left) versus a bagging ensemble of 500 trees (right)
      </span>
     </figcaption>
    </div>
    <p class="noindent">
     As can be seen, the ensemble’s predictions will likely generalize much better than the single decision tree’s predictions:
    </p>
    <div class="quoteblock">
     <p class="noindent">
      the
ensemble
has
a
comparable
bias
but
a
smaller
variance.
     </p>
    </div>
    <div class="warning">
     <p class="noindent">
      It makes roughly the same number of errors on the training set, but the decision boundary is less irregular.
     </p>
    </div>
    <p class="noindent">
     Bagging introduces a higher diversity in the subsets that each predictor is trained on, so bagging ends up with a slightly higher
bias than pasting; but the extra diversity also means that the predictors end up being less correlated, so the ensemble’s variance
is reduced.
    </p>
    <p class="noindent">
     Overall, bagging often results in better models, which explains why it’s generally preferred. However if we have spare time and
     <a class="glossary" href="DataScienceIILectureBookli2.html#glo:cpu">
      CPU
     </a>
     power, we can also use cross-validation to evaluate both bagging and pasting and select the one that works
best.
    </p>
    <h3 class="subsectionHead">
     <a aria-label="Link to this section in PDF version" class="pdf-link" href="DataScienceIILectureBook.pdf#subsection.3.2.2" style="color: red;  position: absolute;  left: -30px;" target="_blank">
      <svg class="bi bi-file-pdf" fill="currentColor" height="20" viewbox="0 0 16 16" width="20" xmlns="http://www.w3.org/2000/svg">
       <path d="M4 0a2 2 0 0 0-2 2v12a2 2 0 0 0 2 2h8a2 2 0 0 0 2-2V2a2 2 0 0 0-2-2H4zm0 1h8a1 1 0 0 1 1 1v12a1 1 0 0 1-1 1H4a1 1 0 0 1-1-1V2a1 1 0 0 1 1-1z">
       </path>
       <path d="M4.603 12.087a.81.81 0 0 1-.438-.42c-.195-.388-.13-.776.08-1.102.198-.307.526-.568.897-.787a7.68 7.68 0 0 1 1.482-.645 19.701 19.701 0 0 0 1.062-2.227 7.269 7.269 0 0 1-.43-1.295c-.086-.4-.119-.796-.046-1.136.075-.354.274-.672.65-.823.192-.077.4-.12.602-.077a.7.7 0 0 1 .477.365c.088.164.12.356.127.538.007.187-.012.395-.047.614-.084.51-.27 1.134-.52 1.794a10.954 10.954 0 0 0 .98 1.686 5.753 5.753 0 0 1 1.334.05c.364.065.734.195.96.465.12.144.193.32.2.518.007.192-.047.382-.138.563a1.04 1.04 0 0 1-.354.416.856.856 0 0 1-.51.138c-.331-.014-.654-.196-.933-.417a5.716 5.716 0 0 1-.911-.95 11.642 11.642 0 0 0-1.997.406 11.311 11.311 0 0 1-1.021 1.51c-.29.35-.608.655-.926.787a.793.793 0 0 1-.58.029zm1.379-1.901c-.166.076-.32.156-.459.238-.328.194-.541.383-.647.547-.094.145-.096.25-.04.361.01.022.02.036.026.044a.27.27 0 0 0 .035-.012c.137-.056.355-.235.635-.572a8.18 8.18 0 0 0 .45-.606zm1.64-1.33a12.647 12.647 0 0 1 1.01-.193 11.666 11.666 0 0 1-.51-.858 20.741 20.741 0 0 1-.5 1.05zm2.446.45c.15.162.296.3.435.41.24.19.407.253.498.256a.107.107 0 0 0 .07-.015.307.307 0 0 0 .094-.125.436.436 0 0 0 .059-.2.095.095 0 0 0-.026-.063c-.052-.062-.2-.152-.518-.209a3.881 3.881 0 0 0-.612-.053zM8.078 5.8a6.7 6.7 0 0 0 .2-.828c.031-.188.043-.343.038-.465a.613.613 0 0 0-.032-.198.517.517 0 0 0-.145.04c-.087.035-.158.106-.196.283-.04.192-.03.469.046.822.024.111.054.227.09.346z">
       </path>
      </svg>
     </a>
     <small class="titlemark">
      3.2.2
     </small>
     <a id="x5-290003.2.2">
     </a>
     Out-of-Bag Evaluation
    </h3>
    <p class="noindent">
     With bagging, some training instances may be sampled several times for any given
predictor, while others may not be sampled at all. By default a <span style="color:#054C5C;"><code class="verb">BaggingClassifier</code></span>
     samples
     <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
      <mi>
       m
      </mi>
     </math>
     training instances with replacement <span style="color:#054C5C;">(<code class="verb">bootstrap=True</code>)</span>, where
     <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
      <mi>
       m
      </mi>
     </math>
     is the size of the training set. With this process, it can be shown mathematically that only about 63% of the training instances
are sampled on average for each predictor.
    </p>
    <div class="informationblock" id="tcolobox-17">
     <div class="title">
      <p class="noindent">
       <span id="bold" style="font-weight:bold;">
        Information
       </span>
       : Limits of Bagging
      </p>
     </div>
     <div class="box-content">
      <p class="noindent">
       If      we      were      to      randomly      draw      one      instance      from      a      dataset      of      size
       <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
        <mi>
         m
        </mi>
       </math>
       ,
each instance in the dataset obviously has probability
       <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
        <mfrac>
         <mrow>
          <mn>
           1
          </mn>
         </mrow>
         <mrow>
          <mi>
           m
          </mi>
         </mrow>
        </mfrac>
       </math>
       of getting picked, and therefore it has a probability
       <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
        <mn>
         1
        </mn>
        <mo class="MathClass-bin" stretchy="false">
         −
        </mo>
        <mfrac>
         <mrow>
          <mn>
           1
          </mn>
         </mrow>
         <mrow>
          <mi>
           m
          </mi>
         </mrow>
        </mfrac>
       </math>
       of
       <span id="bold" style="font-weight:bold;">
        NOT
       </span>
       getting picked.
      </p>
      <p class="noindent">
       If you draw
       <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
        <mi>
         m
        </mi>
       </math>
       instances with
       <span id="bold" style="font-weight:bold;">
        replacement
       </span>
       , all draws are independent and therefore each instance has a probability
      </p>
      <table class="equation-star">
       <tr>
        <td>
         <math class="equation" display="block" xmlns="http://www.w3.org/1998/Math/MathML">
          <msup>
           <mrow>
            <mrow>
             <mo fence="true" form="prefix">
              (
             </mo>
             <mrow>
              <mn>
               1
              </mn>
              <mo class="MathClass-bin" stretchy="false">
               −
              </mo>
              <mfrac>
               <mrow>
                <mn>
                 1
                </mn>
               </mrow>
               <mrow>
                <mi>
                 m
                </mi>
               </mrow>
              </mfrac>
             </mrow>
             <mo fence="true" form="postfix">
              )
             </mo>
            </mrow>
           </mrow>
           <mrow>
            <mi>
             m
            </mi>
           </mrow>
          </msup>
         </math>
        </td>
       </tr>
      </table>
      <p class="noindent">
       of
       <span id="bold" style="font-weight:bold;">
        NOT
       </span>
       getting picked. Now let’s use the fact that
       <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
        <mi class="loglike">
         exp
        </mi>
        <mo>
         ⁡
        </mo>
        <mi>
         x
        </mi>
       </math>
       is equal to the limit of:
      </p>
      <table class="equation-star">
       <tr>
        <td>
         <math class="equation" display="block" xmlns="http://www.w3.org/1998/Math/MathML">
          <mi class="loglike">
           exp
          </mi>
          <mo>
           ⁡
          </mo>
          <mi>
           x
          </mi>
          <mo class="MathClass-rel" stretchy="false">
           =
          </mo>
          <munder class="msub">
           <mrow>
            <mi class="qopname">
             lim
            </mi>
            <mo>
             ⁡
            </mo>
           </mrow>
           <mrow>
            <mi>
             x
            </mi>
            <mo class="MathClass-rel" stretchy="false">
             →
            </mo>
            <mi>
             ∞
            </mi>
           </mrow>
          </munder>
          <msup>
           <mrow>
            <mrow>
             <mo fence="true" form="prefix">
              (
             </mo>
             <mrow>
              <mn>
               1
              </mn>
              <mo class="MathClass-bin" stretchy="false">
               +
              </mo>
              <mfrac>
               <mrow>
                <mi>
                 x
                </mi>
               </mrow>
               <mrow>
                <mi>
                 m
                </mi>
               </mrow>
              </mfrac>
             </mrow>
             <mo fence="true" form="postfix">
              )
             </mo>
            </mrow>
           </mrow>
           <mrow>
            <mi>
             m
            </mi>
           </mrow>
          </msup>
         </math>
        </td>
       </tr>
      </table>
      <p class="noindent">
       So if we to assume
       <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
        <mi>
         m
        </mi>
       </math>
       to be sufficiently large, the ratio of
       <a class="glossary" href="DataScienceIILectureBookli2.html#glo:oob">
        Out-of Bag (OOB)
       </a>
       instances will be about
       <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
        <mi class="loglike">
         exp
        </mi>
        <mo>
         ⁡
        </mo>
        <mo class="MathClass-bin" stretchy="false">
         −
        </mo>
        <mn>
         1
        </mn>
        <mo class="MathClass-rel" stretchy="false">
         ≃
        </mo>
        <mn>
         0
        </mn>
        <mo class="MathClass-punc" stretchy="false">
         .
        </mo>
        <mn>
         3
        </mn>
        <mn>
         7
        </mn>
       </math>
       . Therefore roughly
63% will be sampled
       <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
        <mspace class="quad" width="1em">
        </mspace>
        <mo class="MathClass-ord">
         ■
        </mo>
       </math>
      </p>
     </div>
    </div>
    <p class="noindent">
     The remaining 37% of the training instances that are
     <span id="bold" style="font-weight:bold;">
      NOT
     </span>
     sampled are called
     <a class="glossary" href="DataScienceIILectureBookli2.html#glo:oob">
      OOB
     </a>
     instances.
    </p>
    <div class="warning">
     <p class="noindent">
      Note that they are
      <span id="bold" style="font-weight:bold;">
       NOT
      </span>
      the same 37% for all predictors.
     </p>
    </div>
    <p class="noindent">
     A bagging ensemble can be evaluated using
     <a class="glossary" href="DataScienceIILectureBookli2.html#glo:oob">
      OOB
     </a>
     instances, without the need for a separate validation set as, if there are enough
estimators, then each instance in the training set will likely be an
     <a class="glossary" href="DataScienceIILectureBookli2.html#glo:oob">
      OOB
     </a>
     instance of several estimators, so these estimators can be
used to make a fair ensemble prediction for that instance.
    </p>
    <p class="noindent">
     Once
we
have
a
prediction
for
each
instance,
we
can
determine
the
ensemble’s
                                                                                
                                                                                
prediction
accuracy
     <alert style="color: #821131;">
      <span id="bold" style="font-weight:bold;">
       <sup class="textsuperscript">
        10
       </sup>
      </span>
     </alert>
     <span class="marginnote">
      <alert style="color: #821131;">
       <span id="bold" style="font-weight:bold;">
        <sup class="textsuperscript">
         10
        </sup>
       </span>
      </alert>
      <span style="color:#0063B2;">
       Or
any
other
metric
of
interest.
      </span>
     </span>
     In <span style="color:#054C5C;"><code class="verb">sklearn</code></span>,
we
can
set <span style="color:#054C5C;"><code class="verb">oob_score=True</code></span>
     when
creating
a <span style="color:#054C5C;"><code class="verb">BaggingClassifier</code></span>
     to
request
an
automatic
     <a class="glossary" href="DataScienceIILectureBookli2.html#glo:oob">
      OOB
     </a>
     evaluation
after
training.
    </p>
    <p class="noindent">
     The
following
code
demonstrates
this
effect:
    </p>
    <p class="noindent">
    </p>
    <pre><div id="fancyvrb28" style="padding:20px;border-radius: 3px;"><a id="x5-29002r160"></a>bag_clf = BaggingClassifier(DecisionTreeClassifier(), n_estimators=<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">500</span></span>, 
<a id="x5-29004r161"></a>                    oob_score=<span style="color:#2B2BFF;"><span id="bold" style="font-weight:bold;">True</span></span>, n_jobs=-<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">1</span></span>, random_state=<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">42</span></span>) 
<a id="x5-29006r162"></a>bag_clf.fit(X_train, y_train) 
<a id="x5-29008r163"></a><span style="color:#2B2BFF;">print</span>(bag_clf.oob_score_)</div></pre>
    <p class="noindent">
     The
resulting
evaluation
score
is
available
in
the <span style="color:#054C5C;"><code class="verb">oob_score_</code></span>
     attribute:
    </p>
    <div class="tcolorbox tcolorbox" id="tcolobox-18">
     <div class="tcolorbox-content">
      <pre><div id="fancyvrb29" style="padding:20px;border-radius: 3px;"><a id="x5-29010r1"></a>0.896</div></pre>
     </div>
    </div>
    <p class="noindent">
     According
to
this
     <a class="glossary" href="DataScienceIILectureBookli2.html#glo:oob">
      OOB
     </a>
     evaluation,
this <span style="color:#054C5C;"><code class="verb">BaggingClassifier</code></span>
     is
likely
to
achieve
about
                                                                                
                                                                                
89.6%
accuracy
on
the
test
set.
Let’s
verify
this:
    </p>
    <p class="noindent">
    </p>
    <pre><div id="fancyvrb30" style="padding:20px;border-radius: 3px;"><a id="x5-29012r173"></a><span style="color:#2B2BFF;">from</span><span style="color:#BABABA;"> </span>sklearn.metrics<span style="color:#BABABA;"> </span><span style="color:#2B2BFF;">import</span> accuracy_score 
<a id="x5-29014r174"></a> 
<a id="x5-29016r175"></a>y_pred = bag_clf.predict(X_test) 
<a id="x5-29018r176"></a><span style="color:#2B2BFF;">print</span>(accuracy_score(y_test, y_pred))</div></pre>
    <div class="tcolorbox tcolorbox" id="tcolobox-19">
     <div class="tcolorbox-content">
      <pre><div id="fancyvrb31" style="padding:20px;border-radius: 3px;"><a id="x5-29020r1"></a>0.912</div></pre>
     </div>
    </div>
    <p class="noindent">
     We
get
92%
accuracy
on
the
test.
The
     <a class="glossary" href="DataScienceIILectureBookli2.html#glo:oob">
      OOB
     </a>
     evaluation
was
a
bit
too
pessimistic,
just
over
2%
too
low.
The
     <a class="glossary" href="DataScienceIILectureBookli2.html#glo:oob">
      OOB
     </a>
     decision
function
for
each
training
instance
is
also
available
as
the <span style="color:#054C5C;"><code class="verb">oob_decision_function_</code></span>
     attribute.
As
the
base
estimator
has
a <span style="color:#054C5C;"><code class="verb">predict_proba()</code></span>
     method,
the
decision
function
returns
                                                                                
                                                                                
the
class
probabilities
for
each
training
instance.
    </p>
    <p class="noindent">
     For
example,
the
     <a class="glossary" href="DataScienceIILectureBookli2.html#glo:oob">
      OOB
     </a>
     evaluation
estimates
that
the
first
training
instance
has
a
67.6%
probability
of
belonging
to
the
positive
class
and
a
32.4%
probability
of
belonging
to
the
negative
class:
    </p>
    <p class="noindent">
    </p>
    <pre><div id="fancyvrb32" style="padding:20px;border-radius: 3px;"><a id="x5-29022r186"></a><span style="color:#2B2BFF;">print</span>(bag_clf.oob_decision_function_[:<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">3</span></span>])  <span style="color:#008700;"><italic># probas for the first 3 instances</italic></span></div></pre>
    <div class="tcolorbox tcolorbox" id="tcolobox-20">
     <div class="tcolorbox-content">
      <pre><div id="fancyvrb33" style="padding:20px;border-radius: 3px;"><a id="x5-29024r1"></a>[[0.32352941 0.67647059] 
<a id="x5-29026r2"></a> [0.3375     0.6625    ] 
<a id="x5-29028r3"></a> [1.         0.        ]]</div></pre>
     </div>
    </div>
    <h3 class="subsectionHead">
     <a aria-label="Link to this section in PDF version" class="pdf-link" href="DataScienceIILectureBook.pdf#subsection.3.2.3" style="color: red;  position: absolute;  left: -30px;" target="_blank">
      <svg class="bi bi-file-pdf" fill="currentColor" height="20" viewbox="0 0 16 16" width="20" xmlns="http://www.w3.org/2000/svg">
       <path d="M4 0a2 2 0 0 0-2 2v12a2 2 0 0 0 2 2h8a2 2 0 0 0 2-2V2a2 2 0 0 0-2-2H4zm0 1h8a1 1 0 0 1 1 1v12a1 1 0 0 1-1 1H4a1 1 0 0 1-1-1V2a1 1 0 0 1 1-1z">
       </path>
       <path d="M4.603 12.087a.81.81 0 0 1-.438-.42c-.195-.388-.13-.776.08-1.102.198-.307.526-.568.897-.787a7.68 7.68 0 0 1 1.482-.645 19.701 19.701 0 0 0 1.062-2.227 7.269 7.269 0 0 1-.43-1.295c-.086-.4-.119-.796-.046-1.136.075-.354.274-.672.65-.823.192-.077.4-.12.602-.077a.7.7 0 0 1 .477.365c.088.164.12.356.127.538.007.187-.012.395-.047.614-.084.51-.27 1.134-.52 1.794a10.954 10.954 0 0 0 .98 1.686 5.753 5.753 0 0 1 1.334.05c.364.065.734.195.96.465.12.144.193.32.2.518.007.192-.047.382-.138.563a1.04 1.04 0 0 1-.354.416.856.856 0 0 1-.51.138c-.331-.014-.654-.196-.933-.417a5.716 5.716 0 0 1-.911-.95 11.642 11.642 0 0 0-1.997.406 11.311 11.311 0 0 1-1.021 1.51c-.29.35-.608.655-.926.787a.793.793 0 0 1-.58.029zm1.379-1.901c-.166.076-.32.156-.459.238-.328.194-.541.383-.647.547-.094.145-.096.25-.04.361.01.022.02.036.026.044a.27.27 0 0 0 .035-.012c.137-.056.355-.235.635-.572a8.18 8.18 0 0 0 .45-.606zm1.64-1.33a12.647 12.647 0 0 1 1.01-.193 11.666 11.666 0 0 1-.51-.858 20.741 20.741 0 0 1-.5 1.05zm2.446.45c.15.162.296.3.435.41.24.19.407.253.498.256a.107.107 0 0 0 .07-.015.307.307 0 0 0 .094-.125.436.436 0 0 0 .059-.2.095.095 0 0 0-.026-.063c-.052-.062-.2-.152-.518-.209a3.881 3.881 0 0 0-.612-.053zM8.078 5.8a6.7 6.7 0 0 0 .2-.828c.031-.188.043-.343.038-.465a.613.613 0 0 0-.032-.198.517.517 0 0 0-.145.04c-.087.035-.158.106-.196.283-.04.192-.03.469.046.822.024.111.054.227.09.346z">
       </path>
      </svg>
     </a>
     <small class="titlemark">
      3.2.3
     </small>
     <a id="x5-300003.2.3">
     </a>
     Random Patches and Random Subspaces
    </h3>
    <p class="noindent">
     The <span style="color:#054C5C;"><code class="verb">BaggingClassifier</code></span>
     class supports sampling the features as well. Sampling is controlled by two <alert style="color: #821131;">(2)</alert> hyper-parameters:
    </p>
    <ul class="itemize1">
     <li class="itemize">
      <p class="noindent"> <span style="color:#054C5C;"><code class="verb">max_features</code></span>,
      </p>
     </li>
     <li class="itemize">
      <p class="noindent"> <span style="color:#054C5C;"><code class="verb">bootstrap_features</code></span>.
      </p>
     </li>
    </ul>
    <p class="noindent">
     They work the same way as <span style="color:#054C5C;"><code class="verb">max_samples</code></span>
     and bootstrap, but for feature sampling instead of instance sampling. Therefore, each
predictor will be trained on a
     <span id="bold" style="font-weight:bold;">
      random subset of the input features
     </span>
     .
    </p>
    <p class="noindent">
     This
technique
is
particularly
useful
when
you
are
dealing
with
high-dimensional
inputs,
     <alert style="color: #821131;">
      <span id="bold" style="font-weight:bold;">
       <sup class="textsuperscript">
        11
       </sup>
      </span>
     </alert>
     <span class="marginnote">
      <alert style="color: #821131;">
       <span id="bold" style="font-weight:bold;">
        <sup class="textsuperscript">
         11
        </sup>
       </span>
      </alert>
      <span style="color:#0063B2;">
       such
as
images.
      </span>
     </span>
     as
it
can
considerably
speed
up
training.
    </p>
    <div class="knowledge">
     <p class="noindent">
      Sampling both training instances and features is called the random patches method.
     </p>
    </div>
    <p class="noindent">
     Keeping all training instances (by setting <span style="color:#054C5C;"><code class="verb">bootstrap=False</code></span>
     and <span style="color:#054C5C;"><code class="verb">max_samples=1.0</code></span>
     ) but sampling features (by setting <span style="color:#054C5C;"><code class="verb">bootstrap_features</code></span>
     to <span style="color:#054C5C;"><code class="verb">True</code></span>
     and/or <span style="color:#054C5C;"><code class="verb">max_features</code></span>
     to a value smaller than 1.0) is called the random subspaces method <a id="x5-30001"></a><a href="#X0-ho1998random">[4]</a>.
    </p>
    <p class="noindent">
     Sampling features results in even more predictor diversity, trading a bit more bias for a lower variance.
    </p>
    <p class="noindent">
    </p>
    <h2 class="sectionHead">
     <a aria-label="Link to this section in PDF version" class="pdf-link" href="DataScienceIILectureBook.pdf#section.3.3" style="color: red;  position: absolute;  left: -30px;" target="_blank">
      <svg class="bi bi-file-pdf" fill="currentColor" height="20" viewbox="0 0 16 16" width="20" xmlns="http://www.w3.org/2000/svg">
       <path d="M4 0a2 2 0 0 0-2 2v12a2 2 0 0 0 2 2h8a2 2 0 0 0 2-2V2a2 2 0 0 0-2-2H4zm0 1h8a1 1 0 0 1 1 1v12a1 1 0 0 1-1 1H4a1 1 0 0 1-1-1V2a1 1 0 0 1 1-1z">
       </path>
       <path d="M4.603 12.087a.81.81 0 0 1-.438-.42c-.195-.388-.13-.776.08-1.102.198-.307.526-.568.897-.787a7.68 7.68 0 0 1 1.482-.645 19.701 19.701 0 0 0 1.062-2.227 7.269 7.269 0 0 1-.43-1.295c-.086-.4-.119-.796-.046-1.136.075-.354.274-.672.65-.823.192-.077.4-.12.602-.077a.7.7 0 0 1 .477.365c.088.164.12.356.127.538.007.187-.012.395-.047.614-.084.51-.27 1.134-.52 1.794a10.954 10.954 0 0 0 .98 1.686 5.753 5.753 0 0 1 1.334.05c.364.065.734.195.96.465.12.144.193.32.2.518.007.192-.047.382-.138.563a1.04 1.04 0 0 1-.354.416.856.856 0 0 1-.51.138c-.331-.014-.654-.196-.933-.417a5.716 5.716 0 0 1-.911-.95 11.642 11.642 0 0 0-1.997.406 11.311 11.311 0 0 1-1.021 1.51c-.29.35-.608.655-.926.787a.793.793 0 0 1-.58.029zm1.379-1.901c-.166.076-.32.156-.459.238-.328.194-.541.383-.647.547-.094.145-.096.25-.04.361.01.022.02.036.026.044a.27.27 0 0 0 .035-.012c.137-.056.355-.235.635-.572a8.18 8.18 0 0 0 .45-.606zm1.64-1.33a12.647 12.647 0 0 1 1.01-.193 11.666 11.666 0 0 1-.51-.858 20.741 20.741 0 0 1-.5 1.05zm2.446.45c.15.162.296.3.435.41.24.19.407.253.498.256a.107.107 0 0 0 .07-.015.307.307 0 0 0 .094-.125.436.436 0 0 0 .059-.2.095.095 0 0 0-.026-.063c-.052-.062-.2-.152-.518-.209a3.881 3.881 0 0 0-.612-.053zM8.078 5.8a6.7 6.7 0 0 0 .2-.828c.031-.188.043-.343.038-.465a.613.613 0 0 0-.032-.198.517.517 0 0 0-.145.04c-.087.035-.158.106-.196.283-.04.192-.03.469.046.822.024.111.054.227.09.346z">
       </path>
      </svg>
     </a>
     <small class="titlemark">
      3.3
     </small>
     <a id="x5-310003.3">
     </a>
     Random Forests
    </h2>
    <p class="noindent">
     As
we
have
discussed,
a
     <a class="glossary" href="DataScienceIILectureBookli2.html#glo:rf">
      RF
     </a>
     is
an
     <span id="bold" style="font-weight:bold;">
      ensemble
of
decision
trees
     </span>
     ,
generally
trained
via
the
bagging
method,
     <alert style="color: #821131;">
      <span id="bold" style="font-weight:bold;">
       <sup class="textsuperscript">
        12
       </sup>
      </span>
     </alert>
     <span class="marginnote">
      <alert style="color: #821131;">
       <span id="bold" style="font-weight:bold;">
        <sup class="textsuperscript">
         12
        </sup>
       </span>
      </alert>
      <span style="color:#0063B2;">
       or
sometimes
pasting.
      </span>
     </span>
     typically
with <span style="color:#054C5C;"><code class="verb">max_samples</code></span>
     set
to
the
size
of
the
training
set.
Instead
of
building
a <span style="color:#054C5C;"><code class="verb">BaggingClassifier</code></span>
     class
and
passing
it
a <span style="color:#054C5C;"><code class="verb">DecisionTreeClassifier</code></span>,
we
can
just
use
the <span style="color:#054C5C;"><code class="verb">RandomForestClassifier</code></span>
     class,
which
is
more
convenient
and
optimised
for
decision
                                                                                
                                                                                
trees.
     <alert style="color: #821131;">
      <span id="bold" style="font-weight:bold;">
       <sup class="textsuperscript">
        13
       </sup>
      </span>
     </alert>
     <span class="marginnote">
      <alert style="color: #821131;">
       <span id="bold" style="font-weight:bold;">
        <sup class="textsuperscript">
         13
        </sup>
       </span>
      </alert>
      <span style="color:#0063B2;">
       similarly,
there
is
a <span style="color:#054C5C;"><code class="verb">RandomForestRegressor</code></span>
       class
for
regression
tasks
      </span>
     </span>
     The
following
code
trains
a
     <a class="glossary" href="DataScienceIILectureBookli2.html#glo:rf">
      RF
     </a>
     classifier
with
500
trees,
each
limited
to
maximum
16
leaf
nodes,
using
all
available
     <a class="glossary" href="DataScienceIILectureBookli2.html#glo:cpu">
      CPU
     </a>
     cores:
    </p>
    <p class="noindent">
    </p>
    <pre><div id="fancyvrb34" style="padding:20px;border-radius: 3px;"><a id="x5-31002r198"></a><span style="color:#2B2BFF;">from</span><span style="color:#BABABA;"> </span>sklearn.ensemble<span style="color:#BABABA;"> </span><span style="color:#2B2BFF;">import</span> RandomForestClassifier 
<a id="x5-31004r199"></a> 
<a id="x5-31006r200"></a>rnd_clf = RandomForestClassifier(n_estimators=<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">500</span></span>, max_leaf_nodes=<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">16</span></span>, 
<a id="x5-31008r201"></a>                       n_jobs=-<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">1</span></span>, random_state=<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">42</span></span>) 
<a id="x5-31010r202"></a>rnd_clf.fit(X_train, y_train) 
<a id="x5-31012r203"></a>y_pred_rf = rnd_clf.predict(X_test)</div></pre>
    <p class="noindent">
     With
a
few
slight
exceptions,
a <span style="color:#054C5C;"><code class="verb">RandomForestClassifier</code></span>
     has
all
the
hyperparameters
of
the <span style="color:#054C5C;"><code class="verb">DecisionTreeClassifier</code></span>
     class,
which
allows
to
control
how
trees
are
grown,
in
addition
to
all
the
hyperparameters
of
                                                                                
                                                                                
a <span style="color:#054C5C;"><code class="verb">BaggingClassifier</code></span>
     to
control
the
ensemble
itself.
    </p>
    <p class="noindent">
     The
     <a class="glossary" href="DataScienceIILectureBookli2.html#glo:rf">
      RF
     </a>
     algorithm
introduces
     <span id="bold" style="font-weight:bold;">
      extra
randomness
     </span>
     when
growing
trees.
Instead
of
searching
for
the
very
best
feature
when
splitting
a
node,
it
searches
for
the
best
feature
among
a
random
subset
of
features.
    </p>
    <p class="noindent">
     By
default,
it
samples
     <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
      <mi>
       n
      </mi>
     </math>
     features,
where
     <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
      <mi>
       n
      </mi>
     </math>
     is
the
total
number
of
features.
The
algorithm
results
in
greater
tree
diversity,
which
trades
a
higher
bias
for
                                                                                
                                                                                
a
lower
variance,
generally
giving
an
overall
better
model.
    </p>
    <p class="noindent">
     So,
the
following <span style="color:#054C5C;"><code class="verb">BaggingClassifier</code></span>
     is
equivalent
to
the
previous <span style="color:#054C5C;"><code class="verb">RandomForestClassifier</code></span>
     :
    </p>
    <p class="noindent">
    </p>
    <pre><div id="fancyvrb35" style="padding:20px;border-radius: 3px;"><a id="x5-31014r210"></a>bag_clf = BaggingClassifier( 
<a id="x5-31016r211"></a>   DecisionTreeClassifier(max_features=<span style="color:#800080;">"sqrt"</span>, max_leaf_nodes=<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">16</span></span>), 
<a id="x5-31018r212"></a>   n_estimators=<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">500</span></span>, n_jobs=-<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">1</span></span>, random_state=<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">42</span></span>)</div></pre>
    <h3 class="subsectionHead">
     <a aria-label="Link to this section in PDF version" class="pdf-link" href="DataScienceIILectureBook.pdf#subsection.3.3.1" style="color: red;  position: absolute;  left: -30px;" target="_blank">
      <svg class="bi bi-file-pdf" fill="currentColor" height="20" viewbox="0 0 16 16" width="20" xmlns="http://www.w3.org/2000/svg">
       <path d="M4 0a2 2 0 0 0-2 2v12a2 2 0 0 0 2 2h8a2 2 0 0 0 2-2V2a2 2 0 0 0-2-2H4zm0 1h8a1 1 0 0 1 1 1v12a1 1 0 0 1-1 1H4a1 1 0 0 1-1-1V2a1 1 0 0 1 1-1z">
       </path>
       <path d="M4.603 12.087a.81.81 0 0 1-.438-.42c-.195-.388-.13-.776.08-1.102.198-.307.526-.568.897-.787a7.68 7.68 0 0 1 1.482-.645 19.701 19.701 0 0 0 1.062-2.227 7.269 7.269 0 0 1-.43-1.295c-.086-.4-.119-.796-.046-1.136.075-.354.274-.672.65-.823.192-.077.4-.12.602-.077a.7.7 0 0 1 .477.365c.088.164.12.356.127.538.007.187-.012.395-.047.614-.084.51-.27 1.134-.52 1.794a10.954 10.954 0 0 0 .98 1.686 5.753 5.753 0 0 1 1.334.05c.364.065.734.195.96.465.12.144.193.32.2.518.007.192-.047.382-.138.563a1.04 1.04 0 0 1-.354.416.856.856 0 0 1-.51.138c-.331-.014-.654-.196-.933-.417a5.716 5.716 0 0 1-.911-.95 11.642 11.642 0 0 0-1.997.406 11.311 11.311 0 0 1-1.021 1.51c-.29.35-.608.655-.926.787a.793.793 0 0 1-.58.029zm1.379-1.901c-.166.076-.32.156-.459.238-.328.194-.541.383-.647.547-.094.145-.096.25-.04.361.01.022.02.036.026.044a.27.27 0 0 0 .035-.012c.137-.056.355-.235.635-.572a8.18 8.18 0 0 0 .45-.606zm1.64-1.33a12.647 12.647 0 0 1 1.01-.193 11.666 11.666 0 0 1-.51-.858 20.741 20.741 0 0 1-.5 1.05zm2.446.45c.15.162.296.3.435.41.24.19.407.253.498.256a.107.107 0 0 0 .07-.015.307.307 0 0 0 .094-.125.436.436 0 0 0 .059-.2.095.095 0 0 0-.026-.063c-.052-.062-.2-.152-.518-.209a3.881 3.881 0 0 0-.612-.053zM8.078 5.8a6.7 6.7 0 0 0 .2-.828c.031-.188.043-.343.038-.465a.613.613 0 0 0-.032-.198.517.517 0 0 0-.145.04c-.087.035-.158.106-.196.283-.04.192-.03.469.046.822.024.111.054.227.09.346z">
       </path>
      </svg>
     </a>
     <small class="titlemark">
      3.3.1
     </small>
     <a id="x5-320003.3.1">
     </a>
     Extra-Trees
    </h3>
    <p class="noindent">
     When
we
are
growing
a
tree
in
a
     <a class="glossary" href="DataScienceIILectureBookli2.html#glo:rf">
      RF
     </a>,
at
each
node,
only
a
random
subset
of
the
features
is
considered
for
splitting.
     <alert style="color: #821131;">
      <span id="bold" style="font-weight:bold;">
       <sup class="textsuperscript">
        14
       </sup>
      </span>
     </alert>
     <span class="marginnote">
      <alert style="color: #821131;">
       <span id="bold" style="font-weight:bold;">
        <sup class="textsuperscript">
         14
        </sup>
       </span>
      </alert>
      <span style="color:#0063B2;">
       as
discussed
previously.
      </span>
     </span>
     It
is
possible
to
make
trees
even
more
random
by
also
                                                                                
                                                                                
using
random
thresholds
for
each
feature
rather
than
searching
for
the
best
possible
thresholds.
     <alert style="color: #821131;">
      <span id="bold" style="font-weight:bold;">
       <sup class="textsuperscript">
        15
       </sup>
      </span>
     </alert>
     <span class="marginnote">
      <alert style="color: #821131;">
       <span id="bold" style="font-weight:bold;">
        <sup class="textsuperscript">
         15
        </sup>
       </span>
      </alert>
      <span style="color:#0063B2;">
       Which
is
what
regular
decision
trees
implement.
      </span>
     </span>
     For
this,
we
can
simply
set <span style="color:#054C5C;"><code class="verb">splitter="random"</code></span>
     when
creating
a <span style="color:#054C5C;"><code class="verb">DecisionTreeClassifier</code></span>.
    </p>
    <p class="noindent">
     A
forest
of
such
extremely
random
trees
is
called
an
     <span id="bold" style="font-weight:bold;">
      extremely
randomised
trees
     </span>
     ,
or
     <span id="bold" style="font-weight:bold;">
      ExtraTrees
     </span>
     .
Here,
values
are
chosen
from
a
uniform
distribution
within
the
feature’s
empirical
range.
     <alert style="color: #821131;">
      <span id="bold" style="font-weight:bold;">
       <sup class="textsuperscript">
        16
       </sup>
      </span>
     </alert>
     <span class="marginnote">
      <alert style="color: #821131;">
       <span id="bold" style="font-weight:bold;">
        <sup class="textsuperscript">
         16
        </sup>
       </span>
      </alert>
      <span style="color:#0063B2;">
       in
the
tree’s
training
set.
      </span>
     </span>
     Then,
of
all
the
randomly
chosen
splits,
the
one
which
produces
the
highest
score
is
used
to
split
the
node <a id="x5-32001"></a><a href="#X0-geurts2006extremely">[6]</a>.
    </p>
    <div class="knowledge">
     <p class="noindent">
      As with previous methods, this technique trades more bias for a lower variance.
     </p>
    </div>
    <p class="noindent">
     It also makes extra-trees classifiers much faster to train than regular
     <a class="glossary" href="DataScienceIILectureBookli2.html#glo:rf">
      RF
     </a>
     s, as finding the best possible threshold for each feature
at every node is one of the most time-consuming tasks of growing a tree <a id="x5-32002"></a><a href="#X0-geurts2006extremely">[6]</a>.
    </p>
    <p class="noindent">
     We can create an extra-trees classifier using <span style="color:#054C5C;"><code class="verb">sklearn</code></span>
     ’s <span style="color:#054C5C;"><code class="verb">ExtraTreesClassifier</code></span>
     class. Its
     <a class="glossary" href="DataScienceIILectureBookli2.html#glo:api">
      Application-Programming
Interface (API)
     </a>
     is identical to the <span style="color:#054C5C;"><code class="verb">RandomForestClassifier</code></span>
     class, except bootstrap defaults to <span style="color:#054C5C;"><code class="verb">False</code></span>. Similarly, the <span style="color:#054C5C;"><code class="verb">ExtraTreesRegressor</code></span>
     class has the same
     <a class="glossary" href="DataScienceIILectureBookli2.html#glo:api">
      API
     </a>
     as the <span style="color:#054C5C;"><code class="verb">RandomForestRegressor</code></span>
     class, except bootstrap defaults to <span style="color:#054C5C;"><code class="verb">False</code></span>.
    </p>
    <div class="knowledge">
     <p class="noindent">
      It is hard to tell in advance whether a <span style="color:#054C5C;"><code class="verb">RandomForestClassifier</code></span>
      will perform better or worse than an <span style="color:#054C5C;"><code class="verb">ExtraTreesClassifier</code></span>.
Generally, the only way to know is to try both and compare them using cross-validation.
     </p>
    </div>
    <h3 class="subsectionHead">
     <a aria-label="Link to this section in PDF version" class="pdf-link" href="DataScienceIILectureBook.pdf#subsection.3.3.2" style="color: red;  position: absolute;  left: -30px;" target="_blank">
      <svg class="bi bi-file-pdf" fill="currentColor" height="20" viewbox="0 0 16 16" width="20" xmlns="http://www.w3.org/2000/svg">
       <path d="M4 0a2 2 0 0 0-2 2v12a2 2 0 0 0 2 2h8a2 2 0 0 0 2-2V2a2 2 0 0 0-2-2H4zm0 1h8a1 1 0 0 1 1 1v12a1 1 0 0 1-1 1H4a1 1 0 0 1-1-1V2a1 1 0 0 1 1-1z">
       </path>
       <path d="M4.603 12.087a.81.81 0 0 1-.438-.42c-.195-.388-.13-.776.08-1.102.198-.307.526-.568.897-.787a7.68 7.68 0 0 1 1.482-.645 19.701 19.701 0 0 0 1.062-2.227 7.269 7.269 0 0 1-.43-1.295c-.086-.4-.119-.796-.046-1.136.075-.354.274-.672.65-.823.192-.077.4-.12.602-.077a.7.7 0 0 1 .477.365c.088.164.12.356.127.538.007.187-.012.395-.047.614-.084.51-.27 1.134-.52 1.794a10.954 10.954 0 0 0 .98 1.686 5.753 5.753 0 0 1 1.334.05c.364.065.734.195.96.465.12.144.193.32.2.518.007.192-.047.382-.138.563a1.04 1.04 0 0 1-.354.416.856.856 0 0 1-.51.138c-.331-.014-.654-.196-.933-.417a5.716 5.716 0 0 1-.911-.95 11.642 11.642 0 0 0-1.997.406 11.311 11.311 0 0 1-1.021 1.51c-.29.35-.608.655-.926.787a.793.793 0 0 1-.58.029zm1.379-1.901c-.166.076-.32.156-.459.238-.328.194-.541.383-.647.547-.094.145-.096.25-.04.361.01.022.02.036.026.044a.27.27 0 0 0 .035-.012c.137-.056.355-.235.635-.572a8.18 8.18 0 0 0 .45-.606zm1.64-1.33a12.647 12.647 0 0 1 1.01-.193 11.666 11.666 0 0 1-.51-.858 20.741 20.741 0 0 1-.5 1.05zm2.446.45c.15.162.296.3.435.41.24.19.407.253.498.256a.107.107 0 0 0 .07-.015.307.307 0 0 0 .094-.125.436.436 0 0 0 .059-.2.095.095 0 0 0-.026-.063c-.052-.062-.2-.152-.518-.209a3.881 3.881 0 0 0-.612-.053zM8.078 5.8a6.7 6.7 0 0 0 .2-.828c.031-.188.043-.343.038-.465a.613.613 0 0 0-.032-.198.517.517 0 0 0-.145.04c-.087.035-.158.106-.196.283-.04.192-.03.469.046.822.024.111.054.227.09.346z">
       </path>
      </svg>
     </a>
     <small class="titlemark">
      3.3.2
     </small>
     <a id="x5-330003.3.2">
     </a>
     Feature Importance
    </h3>
    <p class="noindent">
     Another great quality of
     <a class="glossary" href="DataScienceIILectureBookli2.html#glo:rf">
      RF
     </a>
     s is that they make it easy to measure the relative importance of each feature. <span style="color:#054C5C;"><code class="verb">sklearn</code></span>
     measures a
feature’s importance by looking at how much the tree nodes that use that feature reduce impurity on average, across all trees in
the forest. More precisely, it is a weighted average, where each node’s weight is equal to the number of training samples that are
associated with it.
    </p>
    <p class="noindent"> <span style="color:#054C5C;"><code class="verb">sklearn</code></span>
     computes this score automatically for each feature after training, then it scales the results so that the sum of all
importances is equal to 1. You can access the result using the <span style="color:#054C5C;"><code class="verb">feature_importances_</code></span>
     variable. The following code trains a <span style="color:#054C5C;"><code class="verb">RandomForestClassifier</code></span>
     on the
     <italic>
      iris dataset
     </italic>
     and outputs each feature’s importance.
    </p>
    <p class="noindent">
    </p>
    <pre><div id="fancyvrb36" style="padding:20px;border-radius: 3px;"><a id="x5-33002r219"></a><span style="color:#2B2BFF;">from</span><span style="color:#BABABA;"> </span>sklearn.datasets<span style="color:#BABABA;"> </span><span style="color:#2B2BFF;">import</span> load_iris 
<a id="x5-33004r220"></a> 
<a id="x5-33006r221"></a>iris = load_iris(as_frame=<span style="color:#2B2BFF;"><span id="bold" style="font-weight:bold;">True</span></span>) 
<a id="x5-33008r222"></a>rnd_clf = RandomForestClassifier(n_estimators=<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">500</span></span>, random_state=<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">42</span></span>) 
<a id="x5-33010r223"></a>rnd_clf.fit(iris.data, iris.target) 
<a id="x5-33012r224"></a><span style="color:#2B2BFF;">for</span> score, name in <span style="color:#2B2BFF;">zip</span>(rnd_clf.feature_importances_, iris.data.columns): 
<a id="x5-33014r225"></a>   <span style="color:#2B2BFF;">print</span>(<span style="color:#2B2BFF;">round</span>(score, <span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">2</span></span>), name)</div></pre>
    <div class="tcolorbox tcolorbox" id="tcolobox-21">
     <div class="tcolorbox-content">
      <pre><div id="fancyvrb37" style="padding:20px;border-radius: 3px;"><a id="x5-33016r1"></a>0.11 sepal length (cm) 
<a id="x5-33018r2"></a>0.02 sepal width (cm) 
<a id="x5-33020r3"></a>0.44 petal length (cm) 
<a id="x5-33022r4"></a>0.42 petal width (cm)</div></pre>
     </div>
    </div>
    <p class="noindent">
     Based on the results, it seems the most important features are the petal length (44%) and width (42%), while sepal length and
width are rather unimportant in comparison which are 11% and 2%, respectively.
    </p>
    <p class="noindent">
     Similarly, if we were to train a
     <a class="glossary" href="DataScienceIILectureBookli2.html#glo:rf">
      RF
     </a>
     classifier on the
     <a class="glossary" href="DataScienceIILectureBookli2.html#glo:mnist">
      Modified National Institute of Standards and Technology (MNIST)
     </a>
     dataset
and plot each pixel’s importance, we get the image represented in
     <span id="bold" style="font-weight:bold;">
      Fig.
     </span>
     <a href="#x5-33023r3">
      3.3
     </a>.
    </p>
    <div class="figure">
     <p class="noindent">
      <img alt="PIC" height="" src="codes/images/Ensemble-Learning-and-Random-Forests/mnist_feature_importance_plot-.svg" width="150%"/>
      <a id="x5-33023r3">
      </a>
     </p>
     <figcaption class="caption">
      <span class="id">
       Figure 3.3:
      </span>
      <span class="content">
       MNIST pixel importance (according to a
       <a class="glossary" href="DataScienceIILectureBookli2.html#glo:rf">
        RF
       </a>
       classifier)
      </span>
     </figcaption>
    </div>
    <p class="noindent">
     <a class="glossary" href="DataScienceIILectureBookli2.html#glo:rf">
      RF
     </a>
     s are very handy to get a quick understanding of what features actually matter, in particular if you need to perform feature
selection.
    </p>
    <p class="noindent">
    </p>
    <h2 class="sectionHead">
     <a aria-label="Link to this section in PDF version" class="pdf-link" href="DataScienceIILectureBook.pdf#section.3.4" style="color: red;  position: absolute;  left: -30px;" target="_blank">
      <svg class="bi bi-file-pdf" fill="currentColor" height="20" viewbox="0 0 16 16" width="20" xmlns="http://www.w3.org/2000/svg">
       <path d="M4 0a2 2 0 0 0-2 2v12a2 2 0 0 0 2 2h8a2 2 0 0 0 2-2V2a2 2 0 0 0-2-2H4zm0 1h8a1 1 0 0 1 1 1v12a1 1 0 0 1-1 1H4a1 1 0 0 1-1-1V2a1 1 0 0 1 1-1z">
       </path>
       <path d="M4.603 12.087a.81.81 0 0 1-.438-.42c-.195-.388-.13-.776.08-1.102.198-.307.526-.568.897-.787a7.68 7.68 0 0 1 1.482-.645 19.701 19.701 0 0 0 1.062-2.227 7.269 7.269 0 0 1-.43-1.295c-.086-.4-.119-.796-.046-1.136.075-.354.274-.672.65-.823.192-.077.4-.12.602-.077a.7.7 0 0 1 .477.365c.088.164.12.356.127.538.007.187-.012.395-.047.614-.084.51-.27 1.134-.52 1.794a10.954 10.954 0 0 0 .98 1.686 5.753 5.753 0 0 1 1.334.05c.364.065.734.195.96.465.12.144.193.32.2.518.007.192-.047.382-.138.563a1.04 1.04 0 0 1-.354.416.856.856 0 0 1-.51.138c-.331-.014-.654-.196-.933-.417a5.716 5.716 0 0 1-.911-.95 11.642 11.642 0 0 0-1.997.406 11.311 11.311 0 0 1-1.021 1.51c-.29.35-.608.655-.926.787a.793.793 0 0 1-.58.029zm1.379-1.901c-.166.076-.32.156-.459.238-.328.194-.541.383-.647.547-.094.145-.096.25-.04.361.01.022.02.036.026.044a.27.27 0 0 0 .035-.012c.137-.056.355-.235.635-.572a8.18 8.18 0 0 0 .45-.606zm1.64-1.33a12.647 12.647 0 0 1 1.01-.193 11.666 11.666 0 0 1-.51-.858 20.741 20.741 0 0 1-.5 1.05zm2.446.45c.15.162.296.3.435.41.24.19.407.253.498.256a.107.107 0 0 0 .07-.015.307.307 0 0 0 .094-.125.436.436 0 0 0 .059-.2.095.095 0 0 0-.026-.063c-.052-.062-.2-.152-.518-.209a3.881 3.881 0 0 0-.612-.053zM8.078 5.8a6.7 6.7 0 0 0 .2-.828c.031-.188.043-.343.038-.465a.613.613 0 0 0-.032-.198.517.517 0 0 0-.145.04c-.087.035-.158.106-.196.283-.04.192-.03.469.046.822.024.111.054.227.09.346z">
       </path>
      </svg>
     </a>
     <small class="titlemark">
      3.4
     </small>
     <a id="x5-340003.4">
     </a>
     Boosting
    </h2>
    <p class="noindent">
     Boosting refers to any ensemble method that can combine
     <alert style="color: #821131;">
      several weak learners into a strong learner
     </alert>
     . The general idea of
most boosting methods is to
     <span id="bold" style="font-weight:bold;">
      train predictors sequentially
     </span>
     , each trying to correct its predecessor. The general structure of it is
as follows:
    </p>
    <ul class="itemize1">
     <li class="itemize">
      <p class="noindent">
       Initially,
a
model
is
built
using
the
training
data.
      </p>
     </li>
     <li class="itemize">
      <p class="noindent">
       Subsequent
models
are
then
trained
to
address
the
mistakes
of
their
predecessors.
      </p>
     </li>
     <li class="itemize">
      <p class="noindent">
       Boosting
assigns
weights
to
the
data
points
in
the
original
dataset.
      </p>
      <dl class="description">
       <dt class="description">
        <a id="x5-34001x3.4">
        </a>
        Higher weights
       </dt>
       <dd class="description">
        <p class="noindent">
         Instances
which
were
misclassified
by
the
previous
                                                                                
                                                                                
         
model
receive
higher
weights.
        </p>
       </dd>
       <dt class="description">
        <a id="x5-34002x3.4">
        </a>
        Lower weights
       </dt>
       <dd class="description">
        <p class="noindent">
         Instances
which
were
correctly
classified
receive
lower
weights.
        </p>
       </dd>
      </dl>
     </li>
     <li class="itemize">
      <p class="noindent">
       Training on weighted data: The subsequent model learns from the weighted dataset, focusing its attention on
harder-to-learn examples (those with higher weights).
      </p>
     </li>
     <li class="itemize">
      <p class="noindent">
       This iterative process continues until the entire training dataset is accurately predicted, or a predefined maximum number
of models is reached.
      </p>
     </li>
    </ul>
    <p class="noindent">
     There
are
many
boosting
methods
available,
but
by
far
the
most
popular
are
     <span id="bold" style="font-weight:bold;">
      AdaBoost
     </span>
     and
     <span id="bold" style="font-weight:bold;">
      gradient
boosting
     </span>
     .
Let’s
start
with
AdaBoost.
     <alert style="color: #821131;">
      <span id="bold" style="font-weight:bold;">
       <sup class="textsuperscript">
        17
       </sup>
      </span>
     </alert>
     <span class="marginnote">
      <alert style="color: #821131;">
       <span id="bold" style="font-weight:bold;">
        <sup class="textsuperscript">
         17
        </sup>
       </span>
      </alert>
      <span style="color:#0063B2;">
       which
is
short
for
adaptive
boosting.
      </span>
     </span>
    </p>
    <h3 class="subsectionHead">
     <a aria-label="Link to this section in PDF version" class="pdf-link" href="DataScienceIILectureBook.pdf#subsection.3.4.1" style="color: red;  position: absolute;  left: -30px;" target="_blank">
      <svg class="bi bi-file-pdf" fill="currentColor" height="20" viewbox="0 0 16 16" width="20" xmlns="http://www.w3.org/2000/svg">
       <path d="M4 0a2 2 0 0 0-2 2v12a2 2 0 0 0 2 2h8a2 2 0 0 0 2-2V2a2 2 0 0 0-2-2H4zm0 1h8a1 1 0 0 1 1 1v12a1 1 0 0 1-1 1H4a1 1 0 0 1-1-1V2a1 1 0 0 1 1-1z">
       </path>
       <path d="M4.603 12.087a.81.81 0 0 1-.438-.42c-.195-.388-.13-.776.08-1.102.198-.307.526-.568.897-.787a7.68 7.68 0 0 1 1.482-.645 19.701 19.701 0 0 0 1.062-2.227 7.269 7.269 0 0 1-.43-1.295c-.086-.4-.119-.796-.046-1.136.075-.354.274-.672.65-.823.192-.077.4-.12.602-.077a.7.7 0 0 1 .477.365c.088.164.12.356.127.538.007.187-.012.395-.047.614-.084.51-.27 1.134-.52 1.794a10.954 10.954 0 0 0 .98 1.686 5.753 5.753 0 0 1 1.334.05c.364.065.734.195.96.465.12.144.193.32.2.518.007.192-.047.382-.138.563a1.04 1.04 0 0 1-.354.416.856.856 0 0 1-.51.138c-.331-.014-.654-.196-.933-.417a5.716 5.716 0 0 1-.911-.95 11.642 11.642 0 0 0-1.997.406 11.311 11.311 0 0 1-1.021 1.51c-.29.35-.608.655-.926.787a.793.793 0 0 1-.58.029zm1.379-1.901c-.166.076-.32.156-.459.238-.328.194-.541.383-.647.547-.094.145-.096.25-.04.361.01.022.02.036.026.044a.27.27 0 0 0 .035-.012c.137-.056.355-.235.635-.572a8.18 8.18 0 0 0 .45-.606zm1.64-1.33a12.647 12.647 0 0 1 1.01-.193 11.666 11.666 0 0 1-.51-.858 20.741 20.741 0 0 1-.5 1.05zm2.446.45c.15.162.296.3.435.41.24.19.407.253.498.256a.107.107 0 0 0 .07-.015.307.307 0 0 0 .094-.125.436.436 0 0 0 .059-.2.095.095 0 0 0-.026-.063c-.052-.062-.2-.152-.518-.209a3.881 3.881 0 0 0-.612-.053zM8.078 5.8a6.7 6.7 0 0 0 .2-.828c.031-.188.043-.343.038-.465a.613.613 0 0 0-.032-.198.517.517 0 0 0-.145.04c-.087.035-.158.106-.196.283-.04.192-.03.469.046.822.024.111.054.227.09.346z">
       </path>
      </svg>
     </a>
     <small class="titlemark">
      3.4.1
     </small>
     <a id="x5-350003.4.1">
     </a>
     AdaBoost
    </h3>
    <p class="noindent">
     One way for a new predictor to correct its predecessor is to pay a bit more attention to the training instances which the
predecessor
     <span id="bold" style="font-weight:bold;">
      underfit
     </span>
     . This results in new predictors focusing more and more on the hard cases. This is the technique used by
AdaBoost.
    </p>
    <p class="noindent">
     For
example,
                                                                                
                                                                                
when
training
an
AdaBoost
classifier,
the
algorithm
first
trains
a
base
classifier
     <alert style="color: #821131;">
      <span id="bold" style="font-weight:bold;">
       <sup class="textsuperscript">
        18
       </sup>
      </span>
     </alert>
     <span class="marginnote">
      <alert style="color: #821131;">
       <span id="bold" style="font-weight:bold;">
        <sup class="textsuperscript">
         18
        </sup>
       </span>
      </alert>
      <span style="color:#0063B2;">
       i.e.,
a
decision
tree.
      </span>
     </span>
     and
uses
it
to
make
predictions
on
the
training
set.
The
algorithm
then
increases
the
relative
weight
of
misclassified
training
instances.
Following
this,
it
trains
a
second
classifier,
using
the
updated
weights,
and
again
makes
predictions
on
the
training
set,
updates
the
instance
weights,
and
so
on.
    </p>
    <div class="figure">
     <p class="noindent">
      <img alt="PIC" height="" src="codes/images/Ensemble-Learning-and-Random-Forests/boosting_plot-.svg" width="150%"/>
      <a id="x5-35001r4">
      </a>
     </p>
     <figcaption class="caption">
      <span class="id">
       Figure 3.4:
      </span>
      <span class="content">
       Decision boundaries of consecutive predictors.
      </span>
     </figcaption>
    </div>
    <p class="noindent">
     <span id="bold" style="font-weight:bold;">
      Fig.
     </span>
     <a href="#x5-35001r4">
      3.4
     </a>
     shows
the
decision
boundaries
of
five
<alert style="color: #821131;">(5)</alert> consecutive
predictors
on
the
moons
dataset.
     <alert style="color: #821131;">
      <span id="bold" style="font-weight:bold;">
       <sup class="textsuperscript">
        19
       </sup>
      </span>
     </alert>
     <span class="marginnote">
      <alert style="color: #821131;">
       <span id="bold" style="font-weight:bold;">
        <sup class="textsuperscript">
         19
        </sup>
       </span>
      </alert>
      <span style="color:#0063B2;">
       In
this
example,
each
predictor
is
a
highly
regularized
SVM
classifier
with
an
RBF
kernel.
      </span>
     </span>
     The
first
classifier
gets
many
instances
wrong,
so
their
weights
get
boosted.
    </p>
    <p class="noindent">
     The
second
classifier
therefore
does
a
better
job
on
these
instances,
and
so
on.
The
plot
on
the
right
in
     <span id="bold" style="font-weight:bold;">
      Fig.
     </span>
     <a href="#x5-35001r4">
      3.4
     </a>
     represents
the
same
sequence
                                                                                
                                                                                
of
predictors,
except
that
the
learning
rate
is
halved.
     <alert style="color: #821131;">
      <span id="bold" style="font-weight:bold;">
       <sup class="textsuperscript">
        20
       </sup>
      </span>
     </alert>
     <span class="marginnote">
      <alert style="color: #821131;">
       <span id="bold" style="font-weight:bold;">
        <sup class="textsuperscript">
         20
        </sup>
       </span>
      </alert>
      <span style="color:#0063B2;">
       i.e.,
the
misclassified
instance
weights
are
boosted
much
less
at
every
iteration.
      </span>
     </span>
     As
can
be
seen,
this
sequential
learning
technique
has
some
similarities
with
gradient
descent,
except
instead
of
tweaking
a
single
predictor’s
parameters
to
minimise
a
cost
function,
AdaBoost
adds
predictors
to
the
ensemble,
gradually
making
it
better.
    </p>
    <p class="noindent">
     Once
all
predictors
are
trained,
the
ensemble
makes
                                                                                
                                                                                
predictions
very
much
like
bagging
or
pasting,
except
that
predictors
have
different
weights
depending
on
their
overall
accuracy
on
the
weighted
training
set.
    </p>
    <div class="warning">
     <p class="noindent">
      There is one important drawback to this sequential learning technique. Training cannot be parallelized as each predictor can only
be trained after the previous predictor has been trained and evaluated. Therefore, it does
      <span id="bold" style="font-weight:bold;">
       NOT
      </span>
      scale as well as bagging or
pasting.
     </p>
    </div>
    <p class="noindent">
     Let’s take a closer look at the AdaBoost algorithm.
    </p>
    <p class="noindent">
     Each instance weight
     <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
      <msubsup>
       <mrow>
        <mi>
         w
        </mi>
       </mrow>
       <mrow>
       </mrow>
       <mrow>
        <mo class="MathClass-open" stretchy="false">
         (
        </mo>
        <mi>
         i
        </mi>
        <mo class="MathClass-close" stretchy="false">
         )
        </mo>
       </mrow>
      </msubsup>
     </math>
     is initially
set to
     <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
      <mn>
       1
      </mn>
      <mo class="MathClass-bin" stretchy="false">
       ∕
      </mo>
      <mi>
       m
      </mi>
     </math>
     . A first predictor is trained,
and its weighted error rate
     <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
      <msubsup>
       <mrow>
        <mi>
         r
        </mi>
       </mrow>
       <mrow>
        <mn>
         1
        </mn>
       </mrow>
       <mrow>
       </mrow>
      </msubsup>
     </math>
     is computed on the training set.
    </p>
    <table class="equation">
     <tr>
      <td>
       <math class="equation" display="block" xmlns="http://www.w3.org/1998/Math/MathML">
        <mstyle class="label" id="x5-35002r1">
        </mstyle>
        <msubsup>
         <mrow>
          <mi>
           r
          </mi>
         </mrow>
         <mrow>
          <mi>
           j
          </mi>
         </mrow>
         <mrow>
         </mrow>
        </msubsup>
        <mo class="MathClass-rel" stretchy="false">
         =
        </mo>
        <mstyle displaystyle="true">
         <mfrac>
          <mrow>
           <munderover accent="false" accentunder="false">
            <mrow>
             <mo>
              ∑
             </mo>
            </mrow>
            <mrow>
             <mtable class="subarray-c" columnalign="center" rowspacing="0">
              <mtr>
               <mtd>
                <mi>
                 i
                </mi>
                <mo class="MathClass-rel" stretchy="false">
                 =
                </mo>
                <mn>
                 1
                </mn>
               </mtd>
              </mtr>
              <mtr>
               <mtd>
                <msubsup>
                 <mrow>
                  <mi>
                   ŷ
                  </mi>
                 </mrow>
                 <mrow>
                  <mi>
                   j
                  </mi>
                 </mrow>
                 <mrow>
                  <mo class="MathClass-open" stretchy="false">
                   (
                  </mo>
                  <mi>
                   i
                  </mi>
                  <mo class="MathClass-close" stretchy="false">
                   )
                  </mo>
                 </mrow>
                </msubsup>
                <mo class="MathClass-rel" stretchy="false">
                 ≠
                </mo>
                <msubsup>
                 <mrow>
                  <mi>
                   y
                  </mi>
                 </mrow>
                 <mrow>
                 </mrow>
                 <mrow>
                  <mo class="MathClass-open" stretchy="false">
                   (
                  </mo>
                  <mi>
                   i
                  </mi>
                  <mo class="MathClass-close" stretchy="false">
                   )
                  </mo>
                 </mrow>
                </msubsup>
               </mtd>
              </mtr>
             </mtable>
            </mrow>
            <mrow>
             <mi>
              m
             </mi>
            </mrow>
           </munderover>
           <msubsup>
            <mrow>
             <mi>
              w
             </mi>
            </mrow>
            <mrow>
            </mrow>
            <mrow>
             <mo class="MathClass-open" stretchy="false">
              (
             </mo>
             <mi>
              i
             </mi>
             <mo class="MathClass-close" stretchy="false">
              )
             </mo>
            </mrow>
           </msubsup>
          </mrow>
          <mrow>
           <munderover accent="false" accentunder="false">
            <mrow>
             <mo>
              ∑
             </mo>
            </mrow>
            <mrow>
             <mi>
              i
             </mi>
             <mo class="MathClass-rel" stretchy="false">
              =
             </mo>
             <mn>
              1
             </mn>
            </mrow>
            <mrow>
             <mi>
              m
             </mi>
            </mrow>
           </munderover>
           <msubsup>
            <mrow>
             <mi>
              w
             </mi>
            </mrow>
            <mrow>
            </mrow>
            <mrow>
             <mo class="MathClass-open" stretchy="false">
              (
             </mo>
             <mi>
              i
             </mi>
             <mo class="MathClass-close" stretchy="false">
              )
             </mo>
            </mrow>
           </msubsup>
          </mrow>
         </mfrac>
        </mstyle>
       </math>
      </td>
      <td class="eq-no">
       (3.1)
      </td>
     </tr>
    </table>
    <p class="noindent">
    </p>
    <p class="noindent">
     where
     <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
      <msubsup>
       <mrow>
        <mi>
         ŷ
        </mi>
       </mrow>
       <mrow>
        <mi>
         j
        </mi>
       </mrow>
       <mrow>
        <mo class="MathClass-open" stretchy="false">
         (
        </mo>
        <mi>
         i
        </mi>
        <mo class="MathClass-close" stretchy="false">
         )
        </mo>
       </mrow>
      </msubsup>
     </math>
     is the
     <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
      <msup>
       <mrow>
        <mstyle class="text">
         <mtext>
          j
         </mtext>
        </mstyle>
       </mrow>
       <mrow>
        <mstyle class="text">
         <mtext>
          th
         </mtext>
        </mstyle>
       </mrow>
      </msup>
     </math>
     predictor’s prediction for the
     <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
      <msup>
       <mrow>
        <mstyle class="text">
         <mtext>
          i
         </mtext>
        </mstyle>
       </mrow>
       <mrow>
        <mstyle class="text">
         <mtext>
          th
         </mtext>
        </mstyle>
       </mrow>
      </msup>
     </math>
     instance. The predictor’s
weight (
     <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
      <msubsup>
       <mrow>
        <mi>
         α
        </mi>
       </mrow>
       <mrow>
        <mi>
         j
        </mi>
       </mrow>
       <mrow>
       </mrow>
      </msubsup>
     </math>
     ) is then determined
using Eq. (
     <a href="#x5-35003r2">
      3.2
     </a>
     ), where
     <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
      <mi>
       η
      </mi>
     </math>
     is the learning rate hyperparameter.
     <alert style="color: #821131;">
      <span id="bold" style="font-weight:bold;">
       <sup class="textsuperscript">
        21
       </sup>
      </span>
     </alert>
     <span class="marginnote">
      <alert style="color: #821131;">
       <span id="bold" style="font-weight:bold;">
        <sup class="textsuperscript">
         21
        </sup>
       </span>
      </alert>
      <span style="color:#0063B2;">
       which is 1 by default.
      </span>
     </span>
    </p>
    <table class="equation">
     <tr>
      <td>
       <math class="equation" display="block" xmlns="http://www.w3.org/1998/Math/MathML">
        <mstyle class="label" id="x5-35003r2">
        </mstyle>
        <msub>
         <mrow>
          <mi>
           α
          </mi>
         </mrow>
         <mrow>
          <mi>
           j
          </mi>
         </mrow>
        </msub>
        <mo class="MathClass-rel" stretchy="false">
         =
        </mo>
        <mi>
         η
        </mi>
        <mi class="loglike">
         log
        </mi>
        <mo>
         ⁡
        </mo>
        <mfrac>
         <mrow>
          <mn>
           1
          </mn>
          <mo class="MathClass-bin" stretchy="false">
           −
          </mo>
          <msubsup>
           <mrow>
            <mi>
             r
            </mi>
           </mrow>
           <mrow>
            <mi>
             j
            </mi>
           </mrow>
           <mrow>
           </mrow>
          </msubsup>
         </mrow>
         <mrow>
          <msubsup>
           <mrow>
            <mi>
             r
            </mi>
           </mrow>
           <mrow>
            <mi>
             j
            </mi>
           </mrow>
           <mrow>
           </mrow>
          </msubsup>
         </mrow>
        </mfrac>
       </math>
      </td>
      <td class="eq-no">
       (3.2)
      </td>
     </tr>
    </table>
    <p class="noindent">
     The
more
accurate
the
predictor
is,
the
higher
its
weight
will
be.
If
it
is
just
guessing
randomly,
then
its
weight
will
be
close
to
zero.
However,
if
it
is
most
often
wrong,
     <alert style="color: #821131;">
      <span id="bold" style="font-weight:bold;">
       <sup class="textsuperscript">
        22
       </sup>
      </span>
     </alert>
     <span class="marginnote">
      <alert style="color: #821131;">
       <span id="bold" style="font-weight:bold;">
        <sup class="textsuperscript">
         22
        </sup>
       </span>
      </alert>
      <span style="color:#0063B2;">
       i.e.,
less
accurate
than
random
guessing.
      </span>
     </span>
     then
its
weight
will
be
negative.
    </p>
    <p class="noindent">
     Next,
the
AdaBoost
algorithm
updates
the
instance
weights,
using
Eq.
(
     <a href="#x5-35004r3">
      3.3
     </a>
     )
,
which
boosts
the
                                                                                
                                                                                
weights
of
the
misclassified
instances.
    </p>
    <table class="equation">
     <tr>
      <td>
       <math class="equation" display="block" xmlns="http://www.w3.org/1998/Math/MathML">
        <mstyle class="label" id="x5-35004r3">
        </mstyle>
        <msubsup>
         <mrow>
          <mi>
           w
          </mi>
         </mrow>
         <mrow>
         </mrow>
         <mrow>
          <mo class="MathClass-open" stretchy="false">
           (
          </mo>
          <mi>
           i
          </mi>
          <mo class="MathClass-close" stretchy="false">
           )
          </mo>
         </mrow>
        </msubsup>
        <mo class="MathClass-rel" stretchy="false">
         =
        </mo>
        <mrow class="cases">
         <mrow>
          <mo fence="true" form="prefix">
           {
          </mo>
          <mrow>
           <mtable align="axis" class="array" columnlines="none" displaystyle="true" equalcolumns="false" equalrows="false" style="">
            <mtr class="array-row">
             <mtd class="array-td" columnalign="left">
              <msubsup>
               <mrow>
                <mi>
                 w
                </mi>
               </mrow>
               <mrow>
               </mrow>
               <mrow>
                <mo class="MathClass-open" stretchy="false">
                 (
                </mo>
                <mi>
                 i
                </mi>
                <mo class="MathClass-close" stretchy="false">
                 )
                </mo>
               </mrow>
              </msubsup>
              <mspace class="qquad" width="2em">
              </mspace>
              <mspace class="quad" width="1em">
              </mspace>
             </mtd>
             <mtd class="array-td" columnalign="left">
              <mstyle class="text">
               <mtext>
                if
               </mtext>
              </mstyle>
              <mspace class="quad" width="1em">
              </mspace>
              <msubsup>
               <mrow>
                <mi>
                 ŷ
                </mi>
               </mrow>
               <mrow>
                <mi>
                 j
                </mi>
               </mrow>
               <mrow>
                <mo class="MathClass-open" stretchy="false">
                 (
                </mo>
                <mi>
                 i
                </mi>
                <mo class="MathClass-close" stretchy="false">
                 )
                </mo>
               </mrow>
              </msubsup>
              <mo class="MathClass-rel" stretchy="false">
               =
              </mo>
              <msubsup>
               <mrow>
                <mi>
                 y
                </mi>
               </mrow>
               <mrow>
               </mrow>
               <mrow>
                <mo class="MathClass-open" stretchy="false">
                 (
                </mo>
                <mi>
                 i
                </mi>
                <mo class="MathClass-close" stretchy="false">
                 )
                </mo>
               </mrow>
              </msubsup>
              <mo class="MathClass-punc" stretchy="false">
               ,
              </mo>
             </mtd>
            </mtr>
            <mtr class="array-row">
             <mtd class="array-td" columnalign="left">
              <msubsup>
               <mrow>
                <mi>
                 w
                </mi>
               </mrow>
               <mrow>
               </mrow>
               <mrow>
                <mo class="MathClass-open" stretchy="false">
                 (
                </mo>
                <mi>
                 i
                </mi>
                <mo class="MathClass-close" stretchy="false">
                 )
                </mo>
               </mrow>
              </msubsup>
              <mi class="loglike">
               exp
              </mi>
              <mo>
               ⁡
              </mo>
              <msub>
               <mrow>
                <mi>
                 α
                </mi>
               </mrow>
               <mrow>
                <mi>
                 j
                </mi>
               </mrow>
              </msub>
              <mspace class="qquad" width="2em">
              </mspace>
              <mspace class="quad" width="1em">
              </mspace>
             </mtd>
             <mtd class="array-td" columnalign="left">
              <mstyle class="text">
               <mtext>
                if
               </mtext>
              </mstyle>
              <mspace class="quad" width="1em">
              </mspace>
              <msubsup>
               <mrow>
                <mi>
                 ŷ
                </mi>
               </mrow>
               <mrow>
                <mi>
                 j
                </mi>
               </mrow>
               <mrow>
                <mo class="MathClass-open" stretchy="false">
                 (
                </mo>
                <mi>
                 i
                </mi>
                <mo class="MathClass-close" stretchy="false">
                 )
                </mo>
               </mrow>
              </msubsup>
              <mo class="MathClass-rel" stretchy="false">
               ≠
              </mo>
              <msubsup>
               <mrow>
                <mi>
                 y
                </mi>
               </mrow>
               <mrow>
               </mrow>
               <mrow>
                <mo class="MathClass-open" stretchy="false">
                 (
                </mo>
                <mi>
                 i
                </mi>
                <mo class="MathClass-close" stretchy="false">
                 )
                </mo>
               </mrow>
              </msubsup>
              <mo class="MathClass-punc" stretchy="false">
               .
              </mo>
             </mtd>
            </mtr>
           </mtable>
          </mrow>
          <mo fence="true" form="postfix">
          </mo>
         </mrow>
        </mrow>
       </math>
      </td>
      <td class="eq-no">
       (3.3)
      </td>
     </tr>
    </table>
    <p class="noindent">
    </p>
    <p class="noindent">
     Then all the instance weights are
     <alert style="color: #821131;">
      normalised
     </alert>
     by dividing it with
     <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
      <msubsup>
       <mrow>
        <mi class="MathClass-op">
         ∑
        </mi>
        <mo>
         ⁡
        </mo>
       </mrow>
       <mrow>
        <mi>
         i
        </mi>
        <mo class="MathClass-rel" stretchy="false">
         =
        </mo>
        <mn>
         1
        </mn>
       </mrow>
       <mrow>
        <mi>
         m
        </mi>
       </mrow>
      </msubsup>
      <msubsup>
       <mrow>
        <mi>
         w
        </mi>
       </mrow>
       <mrow>
       </mrow>
       <mrow>
        <mo class="MathClass-open" stretchy="false">
         (
        </mo>
        <mi>
         i
        </mi>
        <mo class="MathClass-close" stretchy="false">
         )
        </mo>
       </mrow>
      </msubsup>
     </math>
     .
    </p>
    <p class="noindent">
     Finally, a new predictor is trained using the updated weights, and the whole process is repeated:
    </p>
    <div class="quoteblock">
     <p class="noindent">
      the
new
predictor’s
weight
is
computed,
the
instance
weights
are
updated,
then
another
predictor
is
trained,
and
so
on.
     </p>
    </div>
    <p class="noindent">
     The algorithm stops when the desired number of predictors is reached, or when a perfect predictor is found.
    </p>
    <p class="noindent">
     To make predictions, AdaBoost simply computes the predictions of all the predictors and weighs them using the predictor weights
     <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
      <msubsup>
       <mrow>
        <mi>
         α
        </mi>
       </mrow>
       <mrow>
        <mi>
         j
        </mi>
       </mrow>
       <mrow>
       </mrow>
      </msubsup>
     </math>
     . The
predicted class is the one that receives the majority of weighted votes.
    </p>
    <p class="noindent"> <span style="color:#054C5C;"><code class="verb">sklearn</code></span>
     uses
a
multi-class
version
of
AdaBoost
called
     <a class="glossary" href="DataScienceIILectureBookli2.html#glo:samme">
      Stagewise
Additive
Modeling
using
a
                                                                                
                                                                                
Multiclass
Exponential
loss
function
(SAMME)
     </a>.
When
there
are
just
two
<alert style="color: #821131;">(2)</alert> classes,
     <a class="glossary" href="DataScienceIILectureBookli2.html#glo:samme">
      SAMME
     </a>
     is
equivalent
to
AdaBoost.
If
the
predictors
can
estimate
class
probabilities,
     <alert style="color: #821131;">
      <span id="bold" style="font-weight:bold;">
       <sup class="textsuperscript">
        23
       </sup>
      </span>
     </alert>
     <span class="marginnote">
      <alert style="color: #821131;">
       <span id="bold" style="font-weight:bold;">
        <sup class="textsuperscript">
         23
        </sup>
       </span>
      </alert>
      <span style="color:#0063B2;">
       i.e.,
if
they
have
a <span style="color:#054C5C;"><code class="verb">predict_proba()</code></span>
       method.
      </span>
     </span> <span style="color:#054C5C;"><code class="verb">sklearn</code></span>
     can
use
a
variant
of
SAMME
called
SAMME.R
(the
R
stands
for
“Real”),
which
relies
on
class
probabilities
rather
than
predictions
and
generally
performs
better.
    </p>
    <p class="noindent">
     The
following
code
trains
an
AdaBoost
classifier
based
on
30
                                                                                
                                                                                
decision
stumps
using <span style="color:#054C5C;"><code class="verb">sklearn</code></span>
     ’s
AdaBoostClassifier
class.
     <alert style="color: #821131;">
      <span id="bold" style="font-weight:bold;">
       <sup class="textsuperscript">
        24
       </sup>
      </span>
     </alert>
     <span class="marginnote">
      <alert style="color: #821131;">
       <span id="bold" style="font-weight:bold;">
        <sup class="textsuperscript">
         24
        </sup>
       </span>
      </alert>
      <span style="color:#0063B2;">
       As
you
might
expect,
there
is
also
an
AdaBoostRegressor
class.
      </span>
     </span>
     A
decision
stump
is
a
decision
tree
with <span style="color:#054C5C;"><code class="verb">max_depth=1</code></span>,
which
in
other
words,
a
tree
composed
of
a
single
decision
node
plus
two
leaf
nodes.
This
is
the
default
base
estimator
for
the <span style="color:#054C5C;"><code class="verb">AdaBoostClassifier</code></span>
     class:
    </p>
    <pre><div id="fancyvrb38" style="padding:20px;border-radius: 3px;"><a id="x5-35006r293"></a><span style="color:#2B2BFF;">from</span><span style="color:#BABABA;"> </span>sklearn.ensemble<span style="color:#BABABA;"> </span><span style="color:#2B2BFF;">import</span> AdaBoostClassifier 
<a id="x5-35008r294"></a> 
<a id="x5-35010r295"></a>ada_clf = AdaBoostClassifier( 
<a id="x5-35012r296"></a>   DecisionTreeClassifier(max_depth=<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">1</span></span>), n_estimators=<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">30</span></span>, 
<a id="x5-35014r297"></a>   learning_rate=<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">0.5</span></span>, random_state=<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">42</span></span>) 
<a id="x5-35016r298"></a>ada_clf.fit(X_train, y_train)</div></pre>
    <p class="noindent">
    </p>
    <figure class="float">
     <table class="tabularray tblr" id="tbl-3">
      <tr id="row-3-1-">
       <th id="cell3-1-1" style="text-align:justify;vertical-align:top;background-color:#F2F2F2;">
        <span id="bold" style="font-weight:bold;">
         Advantages
        </span>
       </th>

       <th id="cell3-1-2" style="text-align:justify;vertical-align:top;background-color:#F2F2F2;">
        <span id="bold" style="font-weight:bold;">
         Disadvantages
        </span>
       </th>
      </tr>
      <tr id="row-3-2-">
       <td id="cell3-2-1" style="text-align:justify;vertical-align:top;">
        Can                                         effectively 
combine multiple weak classifiers to create a 
strong classifier with high accuracy
       </td>
       <td id="cell3-2-2" style="text-align:justify;vertical-align:top;">
        Can be sensitive to outliers and noisy data
       </td>
      </tr>
      <tr id="row-3-3-">
       <td id="cell3-3-1" style="text-align:justify;vertical-align:top;">
        Can  handle  complex  datasets  and  capture 
intricate  patters  by  iteratively  adapting  to 
difficult examples
       </td>
       <td id="cell3-3-2" style="text-align:justify;vertical-align:top;">
        Training  process  can  be  computationally 
expensive,   especially   dealing   with   large 
datasets.
       </td>
      </tr>
      <tr id="row-3-4-">
       <td id="cell3-4-1" style="text-align:justify;vertical-align:top;">
        By  focusing  on  misclassified  examples  and 
adjusting     sample     weights,     AdaBoost 
mitigates the risk of overfitting
       </td>
       <td id="cell3-4-2" style="text-align:justify;vertical-align:top;">
        Appropriate selection of weak classifiers and 
the number of hyperparameters are crucial for 
performance.
       </td>
      </tr>
      <tr id="row-3-5-">
       <td id="cell3-5-1" style="text-align:justify;vertical-align:top;">
        A  versatile  algorithm  which  can  work  with 
different types of base classifiers
       </td>
       <td id="cell3-5-2" style="text-align:justify;vertical-align:top;">
        Can struggle with imbalanced datasets.
       </td>
      </tr>
     </table>
     <a id="x5-35017r1">
     </a>
     <figcaption class="caption">
      <span class="id">
       Table 3.1:
      </span>
      <span class="content">
       The advantages and disadvantages of AdaBoost.
      </span>
     </figcaption>
    </figure>
    <div class="knowledge">
     <p class="noindent">
      If AdaBoost ensemble is overfitting the training set, we can try reducing the number of estimators or more strongly regularizing
the base estimator.
     </p>
     
    </div>
    <h3 class="subsectionHead">
     <a aria-label="Link to this section in PDF version" class="pdf-link" href="DataScienceIILectureBook.pdf#subsection.3.4.2" style="color: red;  position: absolute;  left: -30px;" target="_blank">
      <svg class="bi bi-file-pdf" fill="currentColor" height="20" viewbox="0 0 16 16" width="20" xmlns="http://www.w3.org/2000/svg">
       <path d="M4 0a2 2 0 0 0-2 2v12a2 2 0 0 0 2 2h8a2 2 0 0 0 2-2V2a2 2 0 0 0-2-2H4zm0 1h8a1 1 0 0 1 1 1v12a1 1 0 0 1-1 1H4a1 1 0 0 1-1-1V2a1 1 0 0 1 1-1z">
       </path>
       <path d="M4.603 12.087a.81.81 0 0 1-.438-.42c-.195-.388-.13-.776.08-1.102.198-.307.526-.568.897-.787a7.68 7.68 0 0 1 1.482-.645 19.701 19.701 0 0 0 1.062-2.227 7.269 7.269 0 0 1-.43-1.295c-.086-.4-.119-.796-.046-1.136.075-.354.274-.672.65-.823.192-.077.4-.12.602-.077a.7.7 0 0 1 .477.365c.088.164.12.356.127.538.007.187-.012.395-.047.614-.084.51-.27 1.134-.52 1.794a10.954 10.954 0 0 0 .98 1.686 5.753 5.753 0 0 1 1.334.05c.364.065.734.195.96.465.12.144.193.32.2.518.007.192-.047.382-.138.563a1.04 1.04 0 0 1-.354.416.856.856 0 0 1-.51.138c-.331-.014-.654-.196-.933-.417a5.716 5.716 0 0 1-.911-.95 11.642 11.642 0 0 0-1.997.406 11.311 11.311 0 0 1-1.021 1.51c-.29.35-.608.655-.926.787a.793.793 0 0 1-.58.029zm1.379-1.901c-.166.076-.32.156-.459.238-.328.194-.541.383-.647.547-.094.145-.096.25-.04.361.01.022.02.036.026.044a.27.27 0 0 0 .035-.012c.137-.056.355-.235.635-.572a8.18 8.18 0 0 0 .45-.606zm1.64-1.33a12.647 12.647 0 0 1 1.01-.193 11.666 11.666 0 0 1-.51-.858 20.741 20.741 0 0 1-.5 1.05zm2.446.45c.15.162.296.3.435.41.24.19.407.253.498.256a.107.107 0 0 0 .07-.015.307.307 0 0 0 .094-.125.436.436 0 0 0 .059-.2.095.095 0 0 0-.026-.063c-.052-.062-.2-.152-.518-.209a3.881 3.881 0 0 0-.612-.053zM8.078 5.8a6.7 6.7 0 0 0 .2-.828c.031-.188.043-.343.038-.465a.613.613 0 0 0-.032-.198.517.517 0 0 0-.145.04c-.087.035-.158.106-.196.283-.04.192-.03.469.046.822.024.111.054.227.09.346z">
       </path>
      </svg>
     </a>
     <small class="titlemark">
      3.4.2
     </small>
     <a id="x5-360003.4.2">
     </a>
     Gradient Boosting
    </h3>
    <p class="noindent">
     Another very popular boosting algorithm is gradient boosting. Just like AdaBoost, gradient boosting works by sequentially
adding predictors to an ensemble, each one correcting its predecessor. However, instead of tweaking the instance weights at every
iteration like AdaBoost does, this method tries to fit the new predictor to the residual errors made by the previous
predictor.
    </p>
    <p class="noindent">
     Let’s go through a simple regression example, using decision trees as the base predictors. This is called gradient tree boosting, or
gradient boosted regression trees (GBRT). First, let’s generate a noisy quadratic dataset and fit a <span style="color:#054C5C;"><code class="verb">DecisionTreeRegressor</code></span>
     to
it:
    </p>
    <p class="noindent">
    </p>
    <pre><div id="fancyvrb39" style="padding:20px;border-radius: 3px;"><a id="x5-36002r307"></a><span style="color:#2B2BFF;">import</span><span style="color:#BABABA;"> </span>numpy<span style="color:#BABABA;"> </span><span style="color:#2B2BFF;">as</span><span style="color:#BABABA;"> </span>np 
<a id="x5-36004r308"></a><span style="color:#2B2BFF;">from</span><span style="color:#BABABA;"> </span>sklearn.tree<span style="color:#BABABA;"> </span><span style="color:#2B2BFF;">import</span> DecisionTreeRegressor 
<a id="x5-36006r309"></a> 
<a id="x5-36008r310"></a>np.random.seed(<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">42</span></span>) 
<a id="x5-36010r311"></a>X = np.random.rand(<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">100</span></span>, <span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">1</span></span>) - <span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">0.5</span></span> 
<a id="x5-36012r312"></a>y = <span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">3</span></span> * X[:, <span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">0</span></span>] ** <span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">2</span></span> + <span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">0.05</span></span> * np.random.randn(<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">100</span></span>)  <span style="color:#008700;"><italic># y = 3xš + Gaussian noise</italic></span> 
<a id="x5-36014r313"></a> 
<a id="x5-36016r314"></a>tree_reg1 = DecisionTreeRegressor(max_depth=<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">2</span></span>, random_state=<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">42</span></span>) 
<a id="x5-36018r315"></a>tree_reg1.fit(X, y)</div></pre>
    <p class="noindent">
     Next, we’ll train a second <span style="color:#054C5C;"><code class="verb">DecisionTreeRegressor</code></span>
     on the residual errors made by the first predictor:
    </p>
    <p class="noindent">
    </p>
    <pre><div id="fancyvrb40" style="padding:20px;border-radius: 3px;"><a id="x5-36020r320"></a>y2 = y - tree_reg1.predict(X) 
<a id="x5-36022r321"></a>tree_reg2 = DecisionTreeRegressor(max_depth=<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">2</span></span>, random_state=<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">43</span></span>) 
<a id="x5-36024r322"></a>tree_reg2.fit(X, y2)</div></pre>
    <p class="noindent">
     And then we’ll train a third regressor on the residual errors made by the second predictor:
    </p>
    <p class="noindent">
    </p>
    <pre><div id="fancyvrb41" style="padding:20px;border-radius: 3px;"><a id="x5-36026r327"></a>y3 = y2 - tree_reg2.predict(X) 
<a id="x5-36028r328"></a>tree_reg3 = DecisionTreeRegressor(max_depth=<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">2</span></span>, random_state=<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">44</span></span>) 
<a id="x5-36030r329"></a>tree_reg3.fit(X, y3)</div></pre>
    <p class="noindent">
     Now we have an ensemble containing three <alert style="color: #821131;">(3)</alert> trees. It can make predictions on a new instance simply by adding up the
predictions of all the trees:
    </p>
    <p class="noindent">
    </p>
    <pre><div id="fancyvrb42" style="padding:20px;border-radius: 3px;"><a id="x5-36032r334"></a>X_new = np.array([[-<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">0.4</span></span>], [<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">0.</span></span>], [<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">0.5</span></span>]]) 
<a id="x5-36034r335"></a><span style="color:#2B2BFF;">print</span>(<span style="color:#2B2BFF;">sum</span>(tree.predict(X_new) <span style="color:#2B2BFF;">for</span> tree in (tree_reg1, tree_reg2, tree_reg3)))</div></pre>
    <div class="tcolorbox tcolorbox" id="tcolobox-22">
     <div class="tcolorbox-content">
      <pre><div id="fancyvrb43" style="padding:20px;border-radius: 3px;"><a id="x5-36036r1"></a>[0.49484029 0.04021166 0.75026781]</div></pre>
     </div>
    </div>
    <div class="figure">
     <p class="noindent">
      <img alt="PIC" height="" src="codes/images/Ensemble-Learning-and-Random-Forests/gradient_boosting_plot-.svg" width="150%"/>
      <a id="x5-36037r5">
      </a>
     </p>
     <figcaption class="caption">
      <span class="id">
       Figure 3.5:
      </span>
      <span class="content">
       In this depiction of gradient boosting, the first predictor (top left) is trained normally, then each consecutive
         predictor (middle left and lower left) is trained on the previous predictor’s residuals; the right column
         shows the resulting ensemble’s predictions
      </span>
     </figcaption>
    </div>
    <p class="noindent">
     <span id="bold" style="font-weight:bold;">
      Fig.
     </span>
     <a href="#x5-36037r5">
      3.5
     </a>
     represents the predictions of these three trees in the left column, and the ensemble’s predictions in the right column. In
the first row, the ensemble has just one tree, so its predictions are exactly the same as the first tree’s predictions. In the second
row, a new tree is trained on the residual errors of the first tree. On the right we can see that the ensemble’s predictions are equal
to the sum of the predictions of the first two trees. Similarly, in the third row another tree is trained on the residual errors
of the second tree. You can see that the ensemble’s predictions gradually get better as trees are added to the
ensemble.
    </p>
    <p class="noindent">
     We
can
use <span style="color:#054C5C;"><code class="verb">sklearn</code></span>
     ’s <span style="color:#054C5C;"><code class="verb">GradientBoostingRegressor</code></span>
     class
to
train
GBRT
ensembles
more
easily.
As
a
reminder,
there’s
also
a <span style="color:#054C5C;"><code class="verb">GradientBoostingClassifier</code></span>
     class
for
classification.
Much
like
the <span style="color:#054C5C;"><code class="verb">RandomForestRegressor</code></span>
     class,
it
has
hyperparameters
to
control
the
growth
of
decision
trees,
such
as <span style="color:#054C5C;"><code class="verb">max_depth</code></span>, <span style="color:#054C5C;"><code class="verb">min_samples_leaf</code></span>,
as
well
as
hyperparameters
to
control
the
ensemble
training,
such
as
the
number
of
trees.
     <alert style="color: #821131;">
      <span id="bold" style="font-weight:bold;">
       <sup class="textsuperscript">
        25
       </sup>
      </span>
     </alert>
     <span class="marginnote">
      <alert style="color: #821131;">
       <span id="bold" style="font-weight:bold;">
        <sup class="textsuperscript">
         25
        </sup>
       </span>
      </alert>
      <span style="color:#0063B2;">
       i.e., <span style="color:#054C5C;"><code class="verb">n_estimators</code></span>.
      </span>
     </span>
    </p>
    <p class="noindent">
     The
                                                                                
                                                                                
following
code
creates
the
same
ensemble
as
the
previous
one:
    </p>
    <pre><div id="fancyvrb44" style="padding:20px;border-radius: 3px;"><a id="x5-36039r397"></a><span style="color:#2B2BFF;">from</span><span style="color:#BABABA;"> </span>sklearn.ensemble<span style="color:#BABABA;"> </span><span style="color:#2B2BFF;">import</span> GradientBoostingRegressor 
<a id="x5-36041r398"></a> 
<a id="x5-36043r399"></a>gbrt = GradientBoostingRegressor(max_depth=<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">2</span></span>, n_estimators=<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">3</span></span>, 
<a id="x5-36045r400"></a>                       learning_rate=<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">1.0</span></span>, random_state=<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">42</span></span>) 
<a id="x5-36047r401"></a>gbrt.fit(X, y)</div></pre>
    <p class="noindent">
     The <span style="color:#054C5C;"><code class="verb">learning_rate</code></span>
     hyperparameter
scales
the
contribution
of
each
tree.
If
you
set
it
to
a
low
value,
such
as
0.05,
you
will
need
more
trees
in
the
ensemble
to
fit
the
training
set,
but
the
predictions
will
usually
generalize
better.
This
is
a
regularization
technique
called
     <span id="bold" style="font-weight:bold;">
      shrinkage
     </span>
     where
     <span id="bold" style="font-weight:bold;">
      Fig.
     </span>
     <a href="#x5-36048r6">
      3.6
     </a>
     shows
two
GBRT
ensembles
                                                                                
                                                                                
trained
with
different
hyperparameters:
the
one
on
the
left
does
not
have
enough
trees
to
fit
the
training
set,
while
the
one
on
the
right
has
about
the
right
amount.
If
we
added
more
trees,
the
GBRT
would
start
to
overfit
the
training
set.
    </p>
    <div class="figure">
     <p class="noindent">
      <img alt="PIC" height="" src="codes/images/Ensemble-Learning-and-Random-Forests/gbrt_learning_rate_plot-.svg" width="150%"/>
      <a id="x5-36048r6">
      </a>
     </p>
     <figcaption class="caption">
      <span class="id">
       Figure 3.6:
      </span>
      <span class="content">
       GBRT ensembles with not enough predictors (left) and just enough (right).
      </span>
     </figcaption>
    </div>
    <p class="noindent">
     To find the optimal number of trees, we could perform cross-validation using <span style="color:#054C5C;"><code class="verb">GridSearchCV</code></span>
     or <span style="color:#054C5C;"><code class="verb">RandomizedSearchCV</code></span>, as usual, but
there’s a simpler way:
    </p>
    <div class="quoteblock">
     <p class="noindent">
      if
we
set
the <span style="color:#054C5C;"><code class="verb">n_iter_no_change</code></span>
      hyperparameter
to
an
integer
value,
say
10,
then
the <span style="color:#054C5C;"><code class="verb">GradientBoostingRegressor</code></span>
      will
automatically
stop
adding
more
trees
during
training
if
it
sees
that
the
last
10
trees
didn’t
help.
     </p>
    </div>
    <p class="noindent">
     This is simply early stopping, but with a little bit of patience. It tolerates having no progress for a few iterations before it stops.
Let’s train the ensemble using early stopping:
    </p>
    <pre><div id="fancyvrb45" style="padding:20px;border-radius: 3px;"><a id="x5-36050r410"></a>gbrt_best = GradientBoostingRegressor( 
<a id="x5-36052r411"></a>   max_depth=<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">2</span></span>, learning_rate=<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">0.05</span></span>, n_estimators=<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">500</span></span>, 
<a id="x5-36054r412"></a>   n_iter_no_change=<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">10</span></span>, random_state=<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">42</span></span>) 
<a id="x5-36056r413"></a>gbrt_best.fit(X, y) 
<a id="x5-36058r414"></a></div></pre>
    <p class="noindent">
     If you set <span style="color:#054C5C;"><code class="verb">n_iter_no_change</code></span>
     too low, training may stop too early and the model will underfit. But if you set it too high, it will
overfit instead. We also set a fairly small learning rate and a high number of estimators, but the actual number of estimators in
the trained ensemble is much lower, thanks to early stopping:
    </p>
    <pre><div id="fancyvrb46" style="padding:20px;border-radius: 3px;"><a id="x5-36060r445"></a><span style="color:#2B2BFF;">print</span>(gbrt_best.n_estimators_)</div></pre>
    <div class="tcolorbox tcolorbox" id="tcolobox-23">
     <div class="tcolorbox-content">
      <pre><div id="fancyvrb47" style="padding:20px;border-radius: 3px;"><a id="x5-36062r1"></a>92</div></pre>
     </div>
    </div>
    <p class="noindent">
     When <span style="color:#054C5C;"><code class="verb">n_iter_no_change</code></span>
     is set, the <span style="color:#054C5C;"><code class="verb">fit()</code></span>
     method automatically splits the training set into a smaller training set and a validation
set: this allows it to evaluate the model’s performance each time it adds a new tree. The size of the validation set is controlled by
the <span style="color:#054C5C;"><code class="verb">validation_fraction</code></span>
     hyperparameter, which is 10% by default. The <span style="color:#054C5C;"><code class="verb">tol</code></span>
     hyperparameter determines the maximum
performance improvement that still counts as negligible. It defaults to 0.0001. The <span style="color:#054C5C;"><code class="verb">GradientBoostingRegressor</code></span>
     class also
supports a subsample hyperparameter, which specifies the fraction of training instances to be used for training each
tree.
    </p>
    <p class="noindent">
     For example, if <span style="color:#054C5C;"><code class="verb">subsample=0.25</code></span>, then each tree is trained on 25% of the training instances, selected randomly. As you can
probably guess by now, this technique trades a higher bias for a lower variance. It also speeds up training considerably. This is
called stochastic gradient boosting.
    </p>
    <h3 class="subsectionHead">
     <a aria-label="Link to this section in PDF version" class="pdf-link" href="DataScienceIILectureBook.pdf#subsection.3.4.3" style="color: red;  position: absolute;  left: -30px;" target="_blank">
      <svg class="bi bi-file-pdf" fill="currentColor" height="20" viewbox="0 0 16 16" width="20" xmlns="http://www.w3.org/2000/svg">
       <path d="M4 0a2 2 0 0 0-2 2v12a2 2 0 0 0 2 2h8a2 2 0 0 0 2-2V2a2 2 0 0 0-2-2H4zm0 1h8a1 1 0 0 1 1 1v12a1 1 0 0 1-1 1H4a1 1 0 0 1-1-1V2a1 1 0 0 1 1-1z">
       </path>
       <path d="M4.603 12.087a.81.81 0 0 1-.438-.42c-.195-.388-.13-.776.08-1.102.198-.307.526-.568.897-.787a7.68 7.68 0 0 1 1.482-.645 19.701 19.701 0 0 0 1.062-2.227 7.269 7.269 0 0 1-.43-1.295c-.086-.4-.119-.796-.046-1.136.075-.354.274-.672.65-.823.192-.077.4-.12.602-.077a.7.7 0 0 1 .477.365c.088.164.12.356.127.538.007.187-.012.395-.047.614-.084.51-.27 1.134-.52 1.794a10.954 10.954 0 0 0 .98 1.686 5.753 5.753 0 0 1 1.334.05c.364.065.734.195.96.465.12.144.193.32.2.518.007.192-.047.382-.138.563a1.04 1.04 0 0 1-.354.416.856.856 0 0 1-.51.138c-.331-.014-.654-.196-.933-.417a5.716 5.716 0 0 1-.911-.95 11.642 11.642 0 0 0-1.997.406 11.311 11.311 0 0 1-1.021 1.51c-.29.35-.608.655-.926.787a.793.793 0 0 1-.58.029zm1.379-1.901c-.166.076-.32.156-.459.238-.328.194-.541.383-.647.547-.094.145-.096.25-.04.361.01.022.02.036.026.044a.27.27 0 0 0 .035-.012c.137-.056.355-.235.635-.572a8.18 8.18 0 0 0 .45-.606zm1.64-1.33a12.647 12.647 0 0 1 1.01-.193 11.666 11.666 0 0 1-.51-.858 20.741 20.741 0 0 1-.5 1.05zm2.446.45c.15.162.296.3.435.41.24.19.407.253.498.256a.107.107 0 0 0 .07-.015.307.307 0 0 0 .094-.125.436.436 0 0 0 .059-.2.095.095 0 0 0-.026-.063c-.052-.062-.2-.152-.518-.209a3.881 3.881 0 0 0-.612-.053zM8.078 5.8a6.7 6.7 0 0 0 .2-.828c.031-.188.043-.343.038-.465a.613.613 0 0 0-.032-.198.517.517 0 0 0-.145.04c-.087.035-.158.106-.196.283-.04.192-.03.469.046.822.024.111.054.227.09.346z">
       </path>
      </svg>
     </a>
     <small class="titlemark">
      3.4.3
     </small>
     <a id="x5-370003.4.3">
     </a>
     Histogram-Based Gradient Boosting
    </h3>
    <p class="noindent"> <span style="color:#054C5C;"><code class="verb">sklearn</code></span>
     provides another GBRT implementation, optimised for large datasets: histogram based gradient boosting (HGB). It
works by binning the input features, replacing them with integers. The number of bins is controlled by the <span style="color:#054C5C;"><code class="verb">max_bins</code></span>
     hyperparameter, which defaults to 255 and cannot be set any higher than this. Binning can greatly reduce the number of possible
thresholds that the training algorithm needs to evaluate. Moreover, working with integers makes it possible to use faster and
more memory efficient data structures. And the way the bins are built removes the need for sorting the features when training
each tree.
    </p>
    <p class="noindent">
     As a result, this implementation has a computational complexity of
     <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
      <mstyle mathvariant="script">
       <mi>
        O
       </mi>
      </mstyle>
      <msup>
       <mrow>
        <mrow>
         <mo fence="true" form="prefix">
          (
         </mo>
         <mrow>
          <munder accent="true">
           <mrow>
            <mo class="MathClass-bin" stretchy="false">
             ×
            </mo>
           </mrow>
           <mo accent="true">
            ¯
           </mo>
          </munder>
          <mi>
           m
          </mi>
         </mrow>
         <mo fence="true" form="postfix">
          )
         </mo>
        </mrow>
       </mrow>
       <mrow>
       </mrow>
      </msup>
     </math>
     instead
of
     <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
      <mstyle mathvariant="script">
       <mi>
        O
       </mi>
      </mstyle>
      <msup>
       <mrow>
        <mrow>
         <mo fence="true" form="prefix">
          (
         </mo>
         <mrow>
          <mi>
           n
          </mi>
          <mo class="MathClass-bin" stretchy="false">
           ×
          </mo>
          <mi>
           m
          </mi>
          <mi class="loglike">
           log
          </mi>
          <mo>
           ⁡
          </mo>
          <mi>
           m
          </mi>
         </mrow>
         <mo fence="true" form="postfix">
          )
         </mo>
        </mrow>
       </mrow>
       <mrow>
       </mrow>
      </msup>
     </math>
     , where
b is the number of bins, m is the number of training instances, and n is the number of features. In practice, this means that HGB
can train hundreds of times faster than regular GBRT on large datasets. However, binning causes a precision
loss, which acts as a regularizer: depending on the dataset, this may help reduce overfitting, or it may cause
underfitting.
    </p>
    <p class="noindent"> <span style="color:#054C5C;"><code class="verb">sklearn</code></span>
     provides two <alert style="color: #821131;">(2)</alert> classes for HGB:
    </p>
    <dl class="enumerate-enumitem">
     <dt class="enumerate-enumitem">
      1.
     </dt>
     <dd class="enumerate-enumitem"> <span style="color:#054C5C;"><code class="verb">HistGradientBoostingRegressor</code></span>
     </dd>
     <dt class="enumerate-enumitem">
      2.
     </dt>
     <dd class="enumerate-enumitem"> <span style="color:#054C5C;"><code class="verb">HistGradientBoostingClassifier</code></span>
     </dd>
    </dl>
    <p class="noindent">
     They’re similar to <span style="color:#054C5C;"><code class="verb">GradientBoostingRegressor</code></span>
     and <span style="color:#054C5C;"><code class="verb">GradientBoostingClassifier</code></span>, with a few notable differences:
    </p>
    <ul class="itemize1">
     <li class="itemize">
      <p class="noindent">
       Early
stopping
is
automatically
activated
if
the
number
of
instances
is
greater
than
10,000.
You
can
turn
early
stopping
always
on
or
always
off
by
setting
the <span style="color:#054C5C;"><code class="verb">early_stopping</code></span>
       hyperparameter
to
True
or
False.
      </p>
     </li>
     <li class="itemize">
      <p class="noindent">
       Subsampling
is
not
supported.
      </p>
     </li>
     <li class="itemize">
      <p class="noindent"> <span style="color:#054C5C;"><code class="verb">n_estimators</code></span>
       is
renamed
to <span style="color:#054C5C;"><code class="verb">max_iter</code></span>.
      </p>
     </li>
     <li class="itemize">
      <p class="noindent">
       The only decision tree hyperparameters that can be tweaked are:
      </p>
      <ul class="itemize2">
       <li class="itemize">
        <p class="noindent"> <span style="color:#054C5C;"><code class="verb">max_leaf_nodes</code></span>,
        </p>
       </li>
       <li class="itemize">
        <p class="noindent"> <span style="color:#054C5C;"><code class="verb">min_samples_leaf</code></span>,
        </p>
       </li>
       <li class="itemize">
        <p class="noindent">
         and <span style="color:#054C5C;"><code class="verb">max_depth</code></span>.
        </p>
       </li>
      </ul>
     </li>
    </ul>
    <p class="noindent">
     The HGB classes also have two <alert style="color: #821131;">(2)</alert> nice features:
    </p>
    <p class="noindent">
    </p>
    <div class="quoteblock">
     <p class="noindent">
      They
support
both
categorical
features
and
missing
values.
This
simplifies
pre-processing
quite
a
bit.
     </p>
    </div>
    <p class="noindent">
     However, the categorical features must be represented as integers ranging from 0 to a number lower than <span style="color:#054C5C;"><code class="verb">max_bins</code></span>. You can use
an <span style="color:#054C5C;"><code class="verb">OrdinalEncoder</code></span>
     for this.
    </p>
    <p class="noindent">
     For example, here’s how to build and train a complete pipeline for the California housing dataset:
    </p>
    <p class="noindent">
    </p>
    <pre><div id="fancyvrb48" style="padding:20px;border-radius: 3px;"><a id="x5-37004r455"></a><span style="color:#2B2BFF;">from</span><span style="color:#BABABA;"> </span>sklearn.pipeline<span style="color:#BABABA;"> </span><span style="color:#2B2BFF;">import</span> make_pipeline 
<a id="x5-37006r456"></a><span style="color:#2B2BFF;">from</span><span style="color:#BABABA;"> </span>sklearn.compose<span style="color:#BABABA;"> </span><span style="color:#2B2BFF;">import</span> make_column_transformer 
<a id="x5-37008r457"></a><span style="color:#2B2BFF;">from</span><span style="color:#BABABA;"> </span>sklearn.ensemble<span style="color:#BABABA;"> </span><span style="color:#2B2BFF;">import</span> HistGradientBoostingRegressor 
<a id="x5-37010r458"></a><span style="color:#2B2BFF;">from</span><span style="color:#BABABA;"> </span>sklearn.preprocessing<span style="color:#BABABA;"> </span><span style="color:#2B2BFF;">import</span> OrdinalEncoder 
<a id="x5-37012r459"></a>hgb_reg = make_pipeline( 
<a id="x5-37014r460"></a>make_column_transformer((OrdinalEncoder(), [<span style="color:#800080;">"ocean_proximity"</span>]), 
<a id="x5-37016r461"></a>remainder=<span style="color:#800080;">"passthrough"</span>), 
<a id="x5-37018r462"></a>HistGradientBoostingRegressor(categorical_features=[<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">0</span></span>], random_state=<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">42</span></span>) 
<a id="x5-37020r463"></a>) 
<a id="x5-37022r464"></a>hgb_reg.fit(housing, housing_labels)</div></pre>
    <p class="noindent">
     The whole pipeline is just as short as the imports! No need for an imputer, scaler, or a one-hot encoder, so it’s really convenient.
Note that <span style="color:#054C5C;"><code class="verb">categorical_features</code></span>
     must be set to the categorical column indices (or a Boolean array). Without any hyperparameter
tuning, this model yields an RMSE of about 47,600, which is not too bad.
    </p>
    <div class="informationblock" id="tcolobox-24">
     <div class="title">
      <p class="noindent">
       <span id="bold" style="font-weight:bold;">
        Information
       </span>
       : Implementations
      </p>
     </div>
     <div class="box-content">
      <p class="noindent">
       Several other optimised implementations of gradient boosting are available in the Python ML ecosystem: in
particular, XGBoost, CatBoost, and LightGBM. These libraries have been around for several years. They are
all specialized for gradient boosting, their APIs are very similar to <span style="color:#054C5C;"><code class="verb">sklearn</code></span>
       ’s, and they provide many additional
features, including GPU acceleration; you should definitely check them out! Moreover, the TensorFlow
       <a class="glossary" href="DataScienceIILectureBookli2.html#glo:rf">
        RF
       </a>
       s library
provides optimised implementations of a variety of
       <a class="glossary" href="DataScienceIILectureBookli2.html#glo:rf">
        RF
       </a>
       algorithms, including plain
       <a class="glossary" href="DataScienceIILectureBookli2.html#glo:rf">
        RF
       </a>
       s, extra-trees, GBRT, and
several more.
      </p>
     </div>
    </div>
    <p class="noindent">
    </p>
    <h2 class="sectionHead">
     <a aria-label="Link to this section in PDF version" class="pdf-link" href="DataScienceIILectureBook.pdf#section.3.5" style="color: red;  position: absolute;  left: -30px;" target="_blank">
      <svg class="bi bi-file-pdf" fill="currentColor" height="20" viewbox="0 0 16 16" width="20" xmlns="http://www.w3.org/2000/svg">
       <path d="M4 0a2 2 0 0 0-2 2v12a2 2 0 0 0 2 2h8a2 2 0 0 0 2-2V2a2 2 0 0 0-2-2H4zm0 1h8a1 1 0 0 1 1 1v12a1 1 0 0 1-1 1H4a1 1 0 0 1-1-1V2a1 1 0 0 1 1-1z">
       </path>
       <path d="M4.603 12.087a.81.81 0 0 1-.438-.42c-.195-.388-.13-.776.08-1.102.198-.307.526-.568.897-.787a7.68 7.68 0 0 1 1.482-.645 19.701 19.701 0 0 0 1.062-2.227 7.269 7.269 0 0 1-.43-1.295c-.086-.4-.119-.796-.046-1.136.075-.354.274-.672.65-.823.192-.077.4-.12.602-.077a.7.7 0 0 1 .477.365c.088.164.12.356.127.538.007.187-.012.395-.047.614-.084.51-.27 1.134-.52 1.794a10.954 10.954 0 0 0 .98 1.686 5.753 5.753 0 0 1 1.334.05c.364.065.734.195.96.465.12.144.193.32.2.518.007.192-.047.382-.138.563a1.04 1.04 0 0 1-.354.416.856.856 0 0 1-.51.138c-.331-.014-.654-.196-.933-.417a5.716 5.716 0 0 1-.911-.95 11.642 11.642 0 0 0-1.997.406 11.311 11.311 0 0 1-1.021 1.51c-.29.35-.608.655-.926.787a.793.793 0 0 1-.58.029zm1.379-1.901c-.166.076-.32.156-.459.238-.328.194-.541.383-.647.547-.094.145-.096.25-.04.361.01.022.02.036.026.044a.27.27 0 0 0 .035-.012c.137-.056.355-.235.635-.572a8.18 8.18 0 0 0 .45-.606zm1.64-1.33a12.647 12.647 0 0 1 1.01-.193 11.666 11.666 0 0 1-.51-.858 20.741 20.741 0 0 1-.5 1.05zm2.446.45c.15.162.296.3.435.41.24.19.407.253.498.256a.107.107 0 0 0 .07-.015.307.307 0 0 0 .094-.125.436.436 0 0 0 .059-.2.095.095 0 0 0-.026-.063c-.052-.062-.2-.152-.518-.209a3.881 3.881 0 0 0-.612-.053zM8.078 5.8a6.7 6.7 0 0 0 .2-.828c.031-.188.043-.343.038-.465a.613.613 0 0 0-.032-.198.517.517 0 0 0-.145.04c-.087.035-.158.106-.196.283-.04.192-.03.469.046.822.024.111.054.227.09.346z">
       </path>
      </svg>
     </a>
     <small class="titlemark">
      3.5
     </small>
     <a id="x5-380003.5">
     </a>
     Bagging v. Boosting
    </h2>
    <a id="subsubsection*.2">
    </a>
    <h5 class="subsubsectionHead">
     <a id="x5-39000">
     </a>
     Similarities
    </h5>
    <p class="noindent">
     Bagging and Boosting, both being the commonly used methods, have a universal similarity of being classified as ensemble
methods. To summarise, let’s look at the similarities between them.
    </p>
    <ul class="itemize1">
     <li class="itemize">
      <p class="noindent">
       Both
are
ensemble
methods
to
get
N
learners
from
1
learner.
      </p>
     </li>
     <li class="itemize">
      <p class="noindent">
       Both
generate
several
training
data
sets
by
random
sampling.
      </p>
     </li>
     <li class="itemize">
      <p class="noindent">
       Both
make
the
final
decision
by
averaging
the
N
learners
(or
taking
the
majority
of
them
i.e
Majority
Voting).
      </p>
     </li>
     <li class="itemize">
      <p class="noindent">
       Both
are
good
at
reducing
variance
and
provide
higher
stability.
      </p>
     </li>
    </ul>
    <a id="subsubsection*.3">
    </a>
    <h5 class="subsubsectionHead">
     <a id="x5-40000">
     </a>
     Differences
    </h5>
    <p class="noindent">
    </p>
    <figure class="float">
     <table class="tabularray tblr" id="tbl-4">
      <tr id="row-4-1-">
       <th id="cell4-1-1" style="text-align:justify;vertical-align:top;background-color:#F2F2F2;">
        <span id="bold" style="font-weight:bold;">
         Bagging
        </span>
       </th>

       <th id="cell4-1-2" style="text-align:justify;vertical-align:top;background-color:#F2F2F2;">
        <span id="bold" style="font-weight:bold;">
         Boosting
        </span>
       </th>
      </tr>
      <tr id="row-4-2-">
       <td id="cell4-2-1" style="text-align:justify;vertical-align:top;">
        The  simplest  way  of  combining  predictions 
that belong to the same type. classifiers to 
create a strong classifier with high accuracy
       </td>
       <td id="cell4-2-2" style="text-align:justify;vertical-align:top;">
        A way of combining predictions that belong 
to the different types.
       </td>
      </tr>
      <tr id="row-4-3-">
       <td id="cell4-3-1" style="text-align:justify;vertical-align:top;">
        Aim to decrease variance, not bias
       </td>
       <td id="cell4-3-2" style="text-align:justify;vertical-align:top;">
        Aim to decrease bias, not variance.
       </td>
      </tr>
      <tr id="row-4-4-">
       <td id="cell4-4-1" style="text-align:justify;vertical-align:top;">
        Each model receives equal weight.
       </td>
       <td id="cell4-4-2" style="text-align:justify;vertical-align:top;">
        Models   are   weighted   according   to   their 
performance.
       </td>
      </tr>
      <tr id="row-4-5-">
       <td id="cell4-5-1" style="text-align:justify;vertical-align:top;">
        Each model is built independently.
       </td>
       <td id="cell4-5-2" style="text-align:justify;vertical-align:top;">
        New    models    are    influenced    by    the 
performance of previously built models.
       </td>
      </tr>
      <tr id="row-4-6-">
       <td id="cell4-6-1" style="text-align:justify;vertical-align:top;">
        Different  training  data  subsets  are  selected 
using  row  sampling  with  replacement  and 
random  sampling  methods  from  the  entire 
training dataset.
       </td>
       <td id="cell4-6-2" style="text-align:justify;vertical-align:top;">
        Iteratively   train   models,   with   each   new 
model   focusing   on   correcting   the   errors 
(misclassifications  or  high  residuals)  of  the 
previous models
       </td>
      </tr>
      <tr id="row-4-7-">
       <td id="cell4-7-1" style="text-align:justify;vertical-align:top;">
        Bagging   tries   to   solve   the   over-fitting 
problem.
       </td>
       <td id="cell4-7-2" style="text-align:justify;vertical-align:top;">
        Boosting tries to reduce bias.
       </td>
      </tr>
      <tr id="row-4-8-">
       <td id="cell4-8-1" style="text-align:justify;vertical-align:top;">
        In this base classifiers are trained in parallel.
       </td>
       <td id="cell4-8-2" style="text-align:justify;vertical-align:top;">
        In this base classifiers are trained sequentially.
       </td>
      </tr>
     </table>
     <a id="x5-40001r2">
     </a>
     <figcaption class="caption">
      <span class="id">
       Table 3.2:
      </span>
      <span class="content">
       The advantages and disadvantages of AdaBoost.
      </span>
     </figcaption>
    </figure>
    <p class="noindent">
    </p>
    
    <h2 class="sectionHead">
     <a aria-label="Link to this section in PDF version" class="pdf-link" href="DataScienceIILectureBook.pdf#section.3.6" style="color: red;  position: absolute;  left: -30px;" target="_blank">
      <svg class="bi bi-file-pdf" fill="currentColor" height="20" viewbox="0 0 16 16" width="20" xmlns="http://www.w3.org/2000/svg">
       <path d="M4 0a2 2 0 0 0-2 2v12a2 2 0 0 0 2 2h8a2 2 0 0 0 2-2V2a2 2 0 0 0-2-2H4zm0 1h8a1 1 0 0 1 1 1v12a1 1 0 0 1-1 1H4a1 1 0 0 1-1-1V2a1 1 0 0 1 1-1z">
       </path>
       <path d="M4.603 12.087a.81.81 0 0 1-.438-.42c-.195-.388-.13-.776.08-1.102.198-.307.526-.568.897-.787a7.68 7.68 0 0 1 1.482-.645 19.701 19.701 0 0 0 1.062-2.227 7.269 7.269 0 0 1-.43-1.295c-.086-.4-.119-.796-.046-1.136.075-.354.274-.672.65-.823.192-.077.4-.12.602-.077a.7.7 0 0 1 .477.365c.088.164.12.356.127.538.007.187-.012.395-.047.614-.084.51-.27 1.134-.52 1.794a10.954 10.954 0 0 0 .98 1.686 5.753 5.753 0 0 1 1.334.05c.364.065.734.195.96.465.12.144.193.32.2.518.007.192-.047.382-.138.563a1.04 1.04 0 0 1-.354.416.856.856 0 0 1-.51.138c-.331-.014-.654-.196-.933-.417a5.716 5.716 0 0 1-.911-.95 11.642 11.642 0 0 0-1.997.406 11.311 11.311 0 0 1-1.021 1.51c-.29.35-.608.655-.926.787a.793.793 0 0 1-.58.029zm1.379-1.901c-.166.076-.32.156-.459.238-.328.194-.541.383-.647.547-.094.145-.096.25-.04.361.01.022.02.036.026.044a.27.27 0 0 0 .035-.012c.137-.056.355-.235.635-.572a8.18 8.18 0 0 0 .45-.606zm1.64-1.33a12.647 12.647 0 0 1 1.01-.193 11.666 11.666 0 0 1-.51-.858 20.741 20.741 0 0 1-.5 1.05zm2.446.45c.15.162.296.3.435.41.24.19.407.253.498.256a.107.107 0 0 0 .07-.015.307.307 0 0 0 .094-.125.436.436 0 0 0 .059-.2.095.095 0 0 0-.026-.063c-.052-.062-.2-.152-.518-.209a3.881 3.881 0 0 0-.612-.053zM8.078 5.8a6.7 6.7 0 0 0 .2-.828c.031-.188.043-.343.038-.465a.613.613 0 0 0-.032-.198.517.517 0 0 0-.145.04c-.087.035-.158.106-.196.283-.04.192-.03.469.046.822.024.111.054.227.09.346z">
       </path>
      </svg>
     </a>
     <small class="titlemark">
      3.6
     </small>
     <a id="x5-410003.6">
     </a>
     Stacking
    </h2>
    <p class="noindent">
     The
last
ensemble
method
we
will
discuss,
is
called
stacking.
     <alert style="color: #821131;">
      <span id="bold" style="font-weight:bold;">
       <sup class="textsuperscript">
        26
       </sup>
      </span>
     </alert>
     <span class="marginnote">
      <alert style="color: #821131;">
       <span id="bold" style="font-weight:bold;">
        <sup class="textsuperscript">
         26
        </sup>
       </span>
      </alert>
      <span style="color:#0063B2;">
       short
for
stacked
generalization.
      </span>
     </span>
     It
is
based
on
a
simple
idea:
instead
of
using
trivial
functions
(such
as
hard
voting)
to
aggregate
the
predictions
of
all
predictors
in
an
ensemble,
why
don’t
we
train
a
model
to
perform
this
aggregation?
    </p>
    <p class="noindent">
     To
train
the
blender,
you
first
need
to
                                                                                
                                                                                
build
the
blending
training
set.
You
can
use <span style="color:#054C5C;"><code class="verb">cross_val_predict()</code></span>
     on
every
predictor
in
the
ensemble
to
get
out-of-sample
predictions
for
each
instance
in
the
original
training
set,
and
use
these
can
be
used
as
the
input
features
to
train
the
blender;
and
the
targets
can
simply
be
copied
from
the
original
training
set.
Note
that
regardless
of
the
number
of
features
in
the
original
training
set
(just
                                                                                
                                                                                
one
in
this
example),
the
blending
training
set
will
contain
one
input
feature
per
predictor
(three
in
this
example).
Once
the
blender
is
trained,
the
base
predictors
are
retrained
one
last
time
on
the
full
original
training
set.
    </p>
    <p class="noindent">
     It
is
actually
possible
to
train
several
different
blenders
this
way
(e.g.,
one
using
linear
regression,
another
using
     <a class="glossary" href="DataScienceIILectureBookli2.html#glo:rf">
      RF
     </a>
     regression)
to
get
a
whole
layer
of
blenders,
and
then
                                                                                
                                                                                
add
another
blender
on
top
of
that
to
produce
the
final
prediction.
You
may
be
able
to
squeeze
out
a
few
more
drops
of
performance
by
doing
this,
but
it
will
cost
you
in
both
training
time
and
system
complexity.
    </p>
    <p class="noindent"> <span style="color:#054C5C;"><code class="verb">sklearn</code></span>
     provides
two
classes
for
stacking
ensembles:
StackingClassifier
and
StackingRegressor.
For
example,
we
can
replace
the
VotingClassifier
we
used
at
the
beginning
of
this
chapter
on
the
                                                                                
                                                                                
moons
dataset
with
a <span style="color:#054C5C;"><code class="verb">StackingClassifier</code></span>
     :
    </p>
    <p class="noindent">
    </p>
    <pre><div id="fancyvrb49" style="padding:20px;border-radius: 3px;"><a id="x5-41002r471"></a><span style="color:#2B2BFF;">from</span><span style="color:#BABABA;"> </span>sklearn.ensemble<span style="color:#BABABA;"> </span><span style="color:#2B2BFF;">import</span> StackingClassifier 
<a id="x5-41004r472"></a>stacking_clf = StackingClassifier( 
<a id="x5-41006r473"></a>   estimators=[ 
<a id="x5-41008r474"></a>      (<span style="color:#800080;">'lr'</span>, LogisticRegression(random_state=<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">42</span></span>)), 
<a id="x5-41010r475"></a>      (<span style="color:#800080;">'rf'</span>, RandomForestClassifier(random_state=<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">42</span></span>)), 
<a id="x5-41012r476"></a>      (<span style="color:#800080;">'svc'</span>, SVC(probability=<span style="color:#2B2BFF;"><span id="bold" style="font-weight:bold;">True</span></span>, random_state=<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">42</span></span>)) 
<a id="x5-41014r477"></a>   ], 
<a id="x5-41016r478"></a>   final_estimator=RandomForestClassifier(random_state=<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">43</span></span>), 
<a id="x5-41018r479"></a> 
<a id="x5-41020r480"></a>cv=<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">5</span></span> <span style="color:#008700;"><italic># number of cross-validation folds</italic></span> 
<a id="x5-41022r481"></a>) 
<a id="x5-41024r482"></a>stacking_clf.fit(X_train, y_train)</div></pre>
    <p class="noindent">
     For
each
predictor,
the
stacking
classifier
will
call <span style="color:#054C5C;"><code class="verb">predict_proba()</code></span>
     if
available;
if
not
it
will
fall
back
to <span style="color:#054C5C;"><code class="verb">decision_function()</code></span>
     or,
as
a
last
resort,
call <span style="color:#054C5C;"><code class="verb">predict()</code></span>.
If
you
don’t
provide
a
final
estimator,
StackingClassifier
will
use <span style="color:#054C5C;"><code class="verb">LogisticRegression</code></span>
     and
StackingRegressor
will
use
RidgeCV.
If
you
evaluate
this
stacking
model
on
the
test
set,
you
will
find
92.8%
accuracy,
which
                                                                                
                                                                                
is
a
bit
better
than
the
voting
classifier
using
soft
voting,
which
got
92%.
    </p>
    <p class="noindent">
     In
conclusion,
ensemble
methods
are
versatile,
powerful,
and
fairly
simple
to
use.
     <a class="glossary" href="DataScienceIILectureBookli2.html#glo:rf">
      RF
     </a>
     s,
AdaBoost,
and
GBRT
are
among
the
first
models
you
should
test
for
most
     <a class="glossary" href="DataScienceIILectureBookli2.html#glo:ml">
      ML
     </a>
     tasks,
and
they
particularly
shine
with
heterogeneous
tabular
data.
Moreover,
as
they
require
very
little
preprocessing,
they’re
great
for
getting
a
prototype
up
and
running
quickly.
                                                                                
                                                                                
Lastly,
ensemble
methods
like
voting
classifiers
and
stacking
classifiers
can
help
push
your
system’s
performance
to
its
limits.
    </p>
   </article>
   <footer>
    <div class="next">
     <p class="noindent">
      <a href="DataScienceIILectureBookch4.html" style="float: right;">
       Next Chapter →
      </a>
      <a href="DataScienceIILectureBookch2.html" style="float: left;">
       ← Previous Chapter
      </a>
      <a href="index.html">
       TOP
      </a>
     </p>
    </div>
   </footer>
  </div>
  <p class="noindent">
   <a id="tailDataScienceIILectureBookch3.html">
   </a>
  </p>
 </body>
</html>
