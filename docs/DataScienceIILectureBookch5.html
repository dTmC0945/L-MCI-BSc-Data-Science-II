<!DOCTYPE html>
<html lang="en-US" xml:lang="en-US">
 <head>
  <title>
   5 Unsupervised Learning
  </title>
    <link rel='icon' type='image/x-icon' href='figures/logo.svg'></link>
  <meta charset="utf-8"/>
  <meta content="ParSnip" name="generator"/>
  <meta content="width=device-width,initial-scale=1" name="viewport"/>
  <link href="DataScienceIILectureBook.css" rel="stylesheet" type="text/css"/>
  <meta content="DataScienceIILectureBook.tex" name="src"/>
  <script async="async" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/mml-chtml.js" type="text/javascript">
  </script>
  <link href="flatweb.css" rel="stylesheet" type="text/css"/>
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.7.1/jquery.min.js">
  </script>
  <script src="flatjs.js">
  </script>
  <link href="http://fonts.googleapis.com/css?family=Roboto" rel="stylesheet" type="text/css"/>
  <script type="module">
   import pdfjsDist from 'https://cdn.jsdelivr.net/npm/pdfjs-dist@5.3.93/+esm'
  </script>
  <script>
   function moveTOC() { 
var htmlShow = document.getElementsByClassName("wide"); 
if (htmlShow[0].style.display === "none") { 
htmlShow[0].style.display = "block"; 
} else { 
htmlShow[0].style.display = "none"; 
} 
}
  </script>
  <link href="https://unpkg.com/nord-highlightjs@0.1.0/dist/nord.css" rel="stylesheet" type="text/css"/>
  <script src="https://unpkg.com/@highlightjs/cdn-assets@11.3.1/highlight.min.js">
  </script>
  <script src="https://unpkg.com/highlightjs-copy/dist/highlightjs-copy.min.js">
  </script>
  <link href="https://unpkg.com/highlightjs-copy/dist/highlightjs-copy.min.css" rel="stylesheet"/>
  <script src="flatjs.js">
  </script>
  <header>
  </header>
 </head>
 <body id="top">
  <nav class="wide">
   <div class="contents">
    <h3 style="font-weight:lighter; padding: 20px 0 20px 0;">
     5
   
     
   

   Unsupervised Learning
    </h3>
    <h4 style="font-weight:lighter">
     Chapter Table of Contents
    </h4>
    <ul>
     <div class="chapterTOCS">
      <li>
       <span class="sectionToc">
        <small>
         5.1
        </small>
        <a href="#x7-620005.1">
         Introduction
        </a>
       </span>
      </li>
      <li>
       <span class="sectionToc">
        <small>
         5.2
        </small>
        <a href="#x7-630005.2">
         Clustering Algorithms
        </a>
       </span>
      </li>
      <li>
       <span class="subsectionToc" style="font-size:12px;margin:0px;">
        <small>
         5.2.1
        </small>
        <a href="#x7-650005.2.1">
         k-means
        </a>
       </span>
      </li>
      <li>
       <span class="subsectionToc" style="font-size:12px;margin:0px;">
        <small>
         5.2.2
        </small>
        <a href="#x7-700005.2.2">
         Limits of K-Means
        </a>
       </span>
      </li>
      <li>
       <span class="subsectionToc" style="font-size:12px;margin:0px;">
        <small>
         5.2.3
        </small>
        <a href="#x7-710005.2.3">
         Using Clustering for Image Segmentation
        </a>
       </span>
      </li>
      <li>
       <span class="subsectionToc" style="font-size:12px;margin:0px;">
        <small>
         5.2.4
        </small>
        <a href="#x7-720005.2.4">
         Using Clustering for Semi-Supervised Learning
        </a>
       </span>
      </li>
      <li>
       <span class="subsectionToc" style="font-size:12px;margin:0px;">
        <small>
         5.2.5
        </small>
        <a href="#x7-730005.2.5">
         DBSCAN
        </a>
       </span>
      </li>
      <li>
       <span class="sectionToc">
        <small>
         5.3
        </small>
        <a href="#x7-750005.3">
         Gaussian Mixtures
        </a>
       </span>
      </li>
      <li>
       <span class="subsectionToc" style="font-size:12px;margin:0px;">
        <small>
         5.3.1
        </small>
        <a href="#x7-760005.3.1">
         Using Gaussian Mixtures for Anomaly Detection
        </a>
       </span>
      </li>
      <li>
       <span class="subsectionToc" style="font-size:12px;margin:0px;">
        <small>
         5.3.2
        </small>
        <a href="#x7-770005.3.2">
         Selecting the Number of Clusters
        </a>
       </span>
      </li>
      <li>
       <span class="subsectionToc" style="font-size:12px;margin:0px;">
        <small>
         5.3.3
        </small>
        <a href="#x7-780005.3.3">
         Bayesian Gaussian Mixture Models
        </a>
       </span>
      </li>
      <li>
       <span class="subsectionToc" style="font-size:12px;margin:0px;">
        <small>
         5.3.4
        </small>
        <a href="#x7-790005.3.4">
         Other Algorithms for Anomaly and Novelty Detection
        </a>
       </span>
      </li>
     </div>
    </ul>
    <div class="prev-next" style="text-align: center">
     <p class="noindent">
      <a href="DataScienceIILectureBookch6.html" style="float:right; font-size:10px">
       NEXT →
      </a>
      <a href="DataScienceIILectureBookch4.html" style="float:left; font-size:10px">
       ← PREV
      </a>
      <a href="index.html">
       TOP
      </a>
     </p>
    </div>
    <div id="author-info" style="bottom: 0; padding-bottom: 10px;">
     <div id="author-logo">
      <img src="figures/logo.svg" style="margin: 10px 0;"/>
      <div>
       <div>
        <h4 style="color:#7aa0b8; font-weight:lighter;">
         WebBook | D. T. McGuiness, P.hD
        </h4>
       </div>
      </div>
      <div id="author-text" style="bottom: 0; padding-top: 50px;border-top: solid 1px #3b4b5e;font-size: 12px;text-align: right;">
       <p>
        <b>
         Authors Note
        </b>
        The website you are viewing is auto-generated
    using ParSnip and therefore subject to slight errors in
    typography and formatting. When in doubt, please consult the
    LectureBook.
       </p>
      </div>
     </div>
    </div>
   </div>
  </nav>
  <div class="page">
   <article class="chapter">
    <h1 class="chapterHead">
     <div class="number">
      5
     </div>
     <a id="x7-610005">
     </a>
     Unsupervised Learning
    </h1><button id='toc-button' onclick='moveTOC()'>TOC</button>
    <div class="section-control" style="text-align: center">
     <a id="prev-section" onclick="goPrev()" style="float:left;">
      ← Previous Section
     </a>
     <a id="next-section" onclick="goNext()" style="float:right;">
      Next Section →
     </a>
    </div>
    <h2 class="sectionHead">
     <a aria-label="Link to this section in PDF version" class="pdf-link" href="DataScienceIILectureBook.pdf#section.5.1" style="color: red;  position: absolute;  left: -30px;" target="_blank">
      <svg class="bi bi-file-pdf" fill="currentColor" height="20" viewbox="0 0 16 16" width="20" xmlns="http://www.w3.org/2000/svg">
       <path d="M4 0a2 2 0 0 0-2 2v12a2 2 0 0 0 2 2h8a2 2 0 0 0 2-2V2a2 2 0 0 0-2-2H4zm0 1h8a1 1 0 0 1 1 1v12a1 1 0 0 1-1 1H4a1 1 0 0 1-1-1V2a1 1 0 0 1 1-1z">
       </path>
       <path d="M4.603 12.087a.81.81 0 0 1-.438-.42c-.195-.388-.13-.776.08-1.102.198-.307.526-.568.897-.787a7.68 7.68 0 0 1 1.482-.645 19.701 19.701 0 0 0 1.062-2.227 7.269 7.269 0 0 1-.43-1.295c-.086-.4-.119-.796-.046-1.136.075-.354.274-.672.65-.823.192-.077.4-.12.602-.077a.7.7 0 0 1 .477.365c.088.164.12.356.127.538.007.187-.012.395-.047.614-.084.51-.27 1.134-.52 1.794a10.954 10.954 0 0 0 .98 1.686 5.753 5.753 0 0 1 1.334.05c.364.065.734.195.96.465.12.144.193.32.2.518.007.192-.047.382-.138.563a1.04 1.04 0 0 1-.354.416.856.856 0 0 1-.51.138c-.331-.014-.654-.196-.933-.417a5.716 5.716 0 0 1-.911-.95 11.642 11.642 0 0 0-1.997.406 11.311 11.311 0 0 1-1.021 1.51c-.29.35-.608.655-.926.787a.793.793 0 0 1-.58.029zm1.379-1.901c-.166.076-.32.156-.459.238-.328.194-.541.383-.647.547-.094.145-.096.25-.04.361.01.022.02.036.026.044a.27.27 0 0 0 .035-.012c.137-.056.355-.235.635-.572a8.18 8.18 0 0 0 .45-.606zm1.64-1.33a12.647 12.647 0 0 1 1.01-.193 11.666 11.666 0 0 1-.51-.858 20.741 20.741 0 0 1-.5 1.05zm2.446.45c.15.162.296.3.435.41.24.19.407.253.498.256a.107.107 0 0 0 .07-.015.307.307 0 0 0 .094-.125.436.436 0 0 0 .059-.2.095.095 0 0 0-.026-.063c-.052-.062-.2-.152-.518-.209a3.881 3.881 0 0 0-.612-.053zM8.078 5.8a6.7 6.7 0 0 0 .2-.828c.031-.188.043-.343.038-.465a.613.613 0 0 0-.032-.198.517.517 0 0 0-.145.04c-.087.035-.158.106-.196.283-.04.192-.03.469.046.822.024.111.054.227.09.346z">
       </path>
      </svg>
     </a>
     <small class="titlemark">
      5.1
     </small>
     <a id="x7-620005.1">
     </a>
     Introduction
    </h2>
    <p class="noindent">
     While
most
     <a class="glossary" href="DataScienceIILectureBookli2.html#glo:ml">
      ML
     </a>
     application
nowadays
are
based
on
supervised
learning
     <alert style="color: #821131;">
      <span id="bold" style="font-weight:bold;">
       <sup class="textsuperscript">
        1
       </sup>
      </span>
     </alert>
     <span class="marginnote">
      <alert style="color: #821131;">
       <span id="bold" style="font-weight:bold;">
        <sup class="textsuperscript">
         1
        </sup>
       </span>
      </alert>
      <span style="color:#0063B2;">
       This
is
where
companies
spend
most
of
their
moneys
on
      </span>
     </span>
     ,
the
vast
majority
of
the
available
data
is
     <alert style="color: #821131;">
      unlabelled
     </alert>
     :
    </p>
    <p class="noindent">
    </p>
    <div class="quoteblock">
     <p class="noindent">
      This
means
we
have
the
input
features
(
      <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
       <mi>
        X
       </mi>
      </math>
      ),
but
do
not
have
the
labels
(
      <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
       <mi>
        y
       </mi>
      </math>
      ).
For
example
we
can
have
the
login
information
of
users
to
a
website
but
have
no
idea
of
their
name,
sex,
occupation,
etc.
     </p>
    </div>
    <p class="noindent">
     There
is
a
good
quote
by
computer
scientist
     <italic>
      Yann
LeCun
     </italic>
     (Former
Facebook
AI
Chief)
given
in
NIPS
     <alert style="color: #821131;">
      <span id="bold" style="font-weight:bold;">
       <sup class="textsuperscript">
        2
       </sup>
      </span>
     </alert>
     <span class="marginnote">
      <alert style="color: #821131;">
       <span id="bold" style="font-weight:bold;">
        <sup class="textsuperscript">
         2
        </sup>
       </span>
      </alert>
      <span style="color:#0063B2;">
       Neural
Information
Processing
Systems
      </span>
     </span>
     2016
which
gives
a
good
idea
on
the
types
of
learning
in
     <a class="glossary" href="DataScienceIILectureBookli2.html#glo:ml">
      ML
     </a> <a id="x7-62001"></a><a href="#X0-zhiqiang2017review">[11]</a>:
    </p>
    <p class="noindent">
    </p>
    <blockquote class="quotation">
     <p class="indent">
      If
intelligence
was
a
cake,
unsupervised
learning
would
be
the
cake,
supervised
learning
would
be
the
icing
on
the
cake,
and
reinforcement
learning
would
be
the
cherry
on
the
cake.
     </p>
    </blockquote>
    <p class="noindent">
     In other words, there is a huge potential in unsupervised learning that we have only barely started to sink our teeth
into.
    </p>
    <p class="noindent">
     To get a better picture, let’s image a scenario. Let’s say we want to create a system which will take a few pictures of each item on
a manufacturing production line and detect which items are
     <alert style="color: #821131;">
      defective
     </alert>
     . We can easily create a system which will take pictures
automatically, and this might give you thousands of pictures every day. We can then build a reasonably large dataset in just a
few weeks.
    </p>
    <p class="noindent">
     However we will hit a road block as there are
     <span id="bold" style="font-weight:bold;">
      no labels
     </span>
     .
    </p>
    <p class="noindent">
     To train a regular binary classifier to predict whether an item is defective or not, will need to label every single picture either as
     <alert style="color: #821131;">
      defective
     </alert>
     or
     <alert style="color: #821131;">
      normal
     </alert>
     . This will generally require human experts to sit down and manually go through all the pictures. As we can
imagine, this is rather a long, costly, and tedious task, so it will usually only be done on a small subset of the available
pictures. This in turn will make the labelled dataset quite small, and the classifier’s performance will be less than
optimal.
    </p>
    <div class="knowledge">
     <p class="noindent">
      In addition, every time the company makes any change to its products, the whole process will need to be started over from
scratch.
     </p>
    </div>
    <p class="noindent">
     These restrictions can make
     <a class="glossary" href="DataScienceIILectureBookli2.html#glo:ml">
      ML
     </a>
     either a tedious task at best or a massive time sink at worst. Wouldn’t it be great if the
algorithm could just exploit the unlabelled data without needing humans to label every picture?
    </p>
    <div class="warning">
     <p class="noindent">
      This is where unsupervised learning shows its performance.
     </p>
    </div>
    <p class="noindent">
     In the previous chapter, we looked at the most common unsupervised learning task,
     <alert style="color: #821131;">
      dimensionality reduction
     </alert>
     and in this chapter
we will look at a few more unsupervised tasks:
    </p>
    <dl class="description">
     <dt class="description">
      <a id="x7-62002x5.1">
      </a>
      Clustering
     </dt>
     <dd class="description">
      <p class="noindent">
       The
goal
is
to
group
similar
instances
together
into
clusters.
Clustering
is
a
great
tool
for
data
analysis,
customer
segmentation,
recommender
systems,
search
engines,
image
segmentation,
semi-supervised
learning,
dimensionality
reduction,
and
much
more.
      </p>
     </dd>
     <dt class="description">
      <a id="x7-62003x5.1">
      </a>
      Anomaly Detection
     </dt>
     <dd class="description">
      <p class="noindent">
       The
goal
is
to
learn
what
“normal”
data
looks
like,
and
then
use
that
to
detect
abnormal
                                                                                
                                                                                
     
instances.
These
instances
are
called
anomalies,
or
outliers,
while
the
normal
instances
are
called
       <alert style="color: #821131;">
        inliers
       </alert>
       .
Anomaly
detection
is
useful
in
a
wide
variety
of
applications,
such
as
fraud
detection,
detecting
defective
products
in
manufacturing,
identifying
new
trends
in
time
series,
or
removing
outliers
from
a
dataset
before
training
another
model,
which
can
significantly
improve
the
performance
of
the
resulting
model.
       <alert style="color: #821131;">
        <span id="bold" style="font-weight:bold;">
         <sup class="textsuperscript">
          3
         </sup>
        </span>
       </alert>
       <span class="marginnote">
        <alert style="color: #821131;">
         <span id="bold" style="font-weight:bold;">
          <sup class="textsuperscript">
           3
          </sup>
         </span>
        </alert>
        <span style="color:#0063B2;">
         Anomaly
detection
is
also
known
as
                                                                                
                                                                                
     
outlier
detection.
        </span>
       </span>
      </p>
     </dd>
     <dt class="description">
      <a id="x7-62004x5.1">
      </a>
      Density Estimation
     </dt>
     <dd class="description">
      <p class="noindent">
       This
is
the
task
of
estimating
the
PDF
of
the
random
process
that
generated
the
dataset.
Density
estimation
is
commonly
used
for
anomaly
detection
whereby
instances
located
in
very
low-density
regions
are
likely
to
be
anomalies.
It
is
also
useful
for
data
analysis
and
visualisation.
      </p>
     </dd>
    </dl>
    <p class="noindent">
    </p>
    <h2 class="sectionHead">
     <a aria-label="Link to this section in PDF version" class="pdf-link" href="DataScienceIILectureBook.pdf#section.5.2" style="color: red;  position: absolute;  left: -30px;" target="_blank">
      <svg class="bi bi-file-pdf" fill="currentColor" height="20" viewbox="0 0 16 16" width="20" xmlns="http://www.w3.org/2000/svg">
       <path d="M4 0a2 2 0 0 0-2 2v12a2 2 0 0 0 2 2h8a2 2 0 0 0 2-2V2a2 2 0 0 0-2-2H4zm0 1h8a1 1 0 0 1 1 1v12a1 1 0 0 1-1 1H4a1 1 0 0 1-1-1V2a1 1 0 0 1 1-1z">
       </path>
       <path d="M4.603 12.087a.81.81 0 0 1-.438-.42c-.195-.388-.13-.776.08-1.102.198-.307.526-.568.897-.787a7.68 7.68 0 0 1 1.482-.645 19.701 19.701 0 0 0 1.062-2.227 7.269 7.269 0 0 1-.43-1.295c-.086-.4-.119-.796-.046-1.136.075-.354.274-.672.65-.823.192-.077.4-.12.602-.077a.7.7 0 0 1 .477.365c.088.164.12.356.127.538.007.187-.012.395-.047.614-.084.51-.27 1.134-.52 1.794a10.954 10.954 0 0 0 .98 1.686 5.753 5.753 0 0 1 1.334.05c.364.065.734.195.96.465.12.144.193.32.2.518.007.192-.047.382-.138.563a1.04 1.04 0 0 1-.354.416.856.856 0 0 1-.51.138c-.331-.014-.654-.196-.933-.417a5.716 5.716 0 0 1-.911-.95 11.642 11.642 0 0 0-1.997.406 11.311 11.311 0 0 1-1.021 1.51c-.29.35-.608.655-.926.787a.793.793 0 0 1-.58.029zm1.379-1.901c-.166.076-.32.156-.459.238-.328.194-.541.383-.647.547-.094.145-.096.25-.04.361.01.022.02.036.026.044a.27.27 0 0 0 .035-.012c.137-.056.355-.235.635-.572a8.18 8.18 0 0 0 .45-.606zm1.64-1.33a12.647 12.647 0 0 1 1.01-.193 11.666 11.666 0 0 1-.51-.858 20.741 20.741 0 0 1-.5 1.05zm2.446.45c.15.162.296.3.435.41.24.19.407.253.498.256a.107.107 0 0 0 .07-.015.307.307 0 0 0 .094-.125.436.436 0 0 0 .059-.2.095.095 0 0 0-.026-.063c-.052-.062-.2-.152-.518-.209a3.881 3.881 0 0 0-.612-.053zM8.078 5.8a6.7 6.7 0 0 0 .2-.828c.031-.188.043-.343.038-.465a.613.613 0 0 0-.032-.198.517.517 0 0 0-.145.04c-.087.035-.158.106-.196.283-.04.192-.03.469.046.822.024.111.054.227.09.346z">
       </path>
      </svg>
     </a>
     <small class="titlemark">
      5.2
     </small>
     <a id="x7-630005.2">
     </a>
     Clustering Algorithms
    </h2>
    <div class="figure">
     <p class="noindent">
      <img alt="PIC" height="" src="figures/Unsupervised-Learning/raster/nordkette.jpg" width="150%"/>
      <a id="x7-63001r1">
      </a>
     </p>
     <figcaption class="caption">
      <span class="id">
       Figure 5.1:
      </span>
      <span class="content">
       Of course, a mountain-range like this one needs no introduction in Tirol. However, for looking at flowers
         perhaps a snowy day is not the best.
      </span>
     </figcaption>
    </div>
    <p class="noindent">
     As
with
all
previous
examples,
let’s
use
our
imagination
and
assume
we
are
enjoying
our
hike
through
the
mountains
of
Tirol,
perhaps
somewhere
in
Nordkette
(
     <span id="bold" style="font-weight:bold;">
      Fig.
     </span>
     <a href="#x7-63001r1">
      5.1
     </a>
     ),
and
we
stumble
upon
a
plant
we
have
never
seen
before.
It
could
be
Alpen-Edelweiß
     <alert style="color: #821131;">
      <span id="bold" style="font-weight:bold;">
       <sup class="textsuperscript">
        4
       </sup>
      </span>
     </alert>
     <span class="marginnote">
      <img alt="PIC" height="" src="figures/Unsupervised-Learning/raster/edelweiss.jpg" width="100%"/>
      <alert style="color: #821131;">
       <span id="bold" style="font-weight:bold;">
        <sup class="textsuperscript">
         4
        </sup>
       </span>
      </alert>
      <span style="color:#0063B2;">
       An
illustration
of
Edelweiß.
      </span>
     </span>
     or
maybe
something
else.
    </p>
    <div class="quoteblock">
     <p class="noindent">
      We
can’t
tell.
     </p>
    </div>
    <p class="noindent">
     We look around and we notice a few more. They are not identical, however they are
     <span id="bold" style="font-weight:bold;">
      sufficiently similar
     </span>
     for we to know that
they most likely belong to the same species. We may need a botanist, or a local, to tell you what species that is, but we certainly
don’t need an expert to identify groups of similar-looking objects.
    </p>
    <div class="figure">
     <p class="noindent">
      <img alt="PIC" height="" src="codes/images/Unsupervised-Learning/classification-vs-clustering-plot-.svg" width="150%"/>
      <a id="x7-63002r2">
      </a>
     </p>
     <figcaption class="caption">
      <span class="id">
       Figure 5.2:
      </span>
      <span class="content">
       lassification (left) versus clustering (right).
      </span>
     </figcaption>
    </div>
    <p class="noindent">
     This is called
     <alert style="color: #821131;">
      clustering
     </alert>
     :
    </p>
    <div class="warning">
     <p class="noindent">
      It is the task of identifying similar instances and assigning them to clusters, or groups of similar instances without knowing what
the instance really is.
     </p>
    </div>
    <p class="noindent">
     Similar
to
classification,
each
instance
gets
assigned
to
a
     <span id="bold" style="font-weight:bold;">
      group
     </span>
     .
However,
unlike
classification,
clustering
is
an
     <alert style="color: #821131;">
      unsupervised
task
     </alert>
     .
Consider
     <span id="bold" style="font-weight:bold;">
      Fig.
     </span>
     <a href="#x7-63002r2">
      5.2
     </a>
     :
on
the
left
is
the
     <italic>
      iris
dataset
     </italic>
     ,
where
each
instance’s
species
     <alert style="color: #821131;">
      <span id="bold" style="font-weight:bold;">
       <sup class="textsuperscript">
        5
       </sup>
      </span>
     </alert>
     <span class="marginnote">
      <alert style="color: #821131;">
       <span id="bold" style="font-weight:bold;">
        <sup class="textsuperscript">
         5
        </sup>
       </span>
      </alert>
      <span style="color:#0063B2;">
       i.e.,
its
class
      </span>
     </span>
     is
represented
with
a
different
marker.
It
is
a
labelled
dataset,
for
which
classification
algorithms
such
as
logistic
regression,
SVMs,
or
random
forest
classifiers
are
well
                                                                                
                                                                                
suited.
    </p>
    <p class="noindent">
     On the right is the same dataset, but without the labels, so we cannot use a classification algorithm anymore. This is where
clustering algorithms step in as many of them can easily detect the lower-left cluster. It is also quite easy to see with our own
eyes, but it is not so obvious that the upper-right cluster is composed of two <alert style="color: #821131;">(2)</alert> distinct subclusters. That said, the dataset has
two <alert style="color: #821131;">(2)</alert> additional features:
    </p>
    <ul class="itemize1">
     <li class="itemize">
      <p class="noindent">
       sepal
length,
and
      </p>
     </li>
     <li class="itemize">
      <p class="noindent">
       sepal
width,
      </p>
     </li>
    </ul>
    <p class="noindent">
     which
are
not
represented
here,
and
clustering
algorithms
can
make
good
use
of
all
features,
so
in
fact
they
identify
the
three
clusters
fairly
well.
     <alert style="color: #821131;">
      <span id="bold" style="font-weight:bold;">
       <sup class="textsuperscript">
        6
       </sup>
      </span>
     </alert>
     <span class="marginnote">
      <alert style="color: #821131;">
       <span id="bold" style="font-weight:bold;">
        <sup class="textsuperscript">
         6
        </sup>
       </span>
      </alert>
      <span style="color:#0063B2;">
       e.g.,
using
a
Gaussian
mixture
model,
only
5
instances
out
of
150
are
assigned
to
the
wrong
cluster.
      </span>
     </span>
     <a id="paragraph*.7">
     </a>
    </p>
    <p class="noindent">
     <span class="paragraphHead">
      <a id="x7-64000">
      </a>
      Applications
     </span>
     Clustering is used in a wide variety of applications, including:
    </p>
    <dl class="description">
     <dt class="description">
      <a id="x7-64001x">
      </a>
      Customer Segmentation
     </dt>
     <dd class="description">
      <p class="noindent">
       We can cluster our customers based on their purchases and their activity on our website. This is useful to understand who
our customers are and what they need, so we can adapt our products and marketing campaigns to each segment <a id="x7-64002"></a><a href="#X0-kansal2018customer">[12]</a>.
      </p>
      <div class="knowledge">
       <p class="noindent">
        Customer segmentation can be useful in recommender systems to suggest content that other users in the same cluster
enjoyed.
       </p>
      </div>
     </dd>
     <dt class="description">
      <a id="x7-64003x">
      </a>
      Data analysis
     </dt>
     <dd class="description">
      <p class="noindent">
       When we analyze a new dataset, it can be helpful to run a clustering algorithm, and then analyze each cluster
separately.
      </p>
     </dd>
     <dt class="description">
      <a id="x7-64004x">
      </a>
      Dimensionality reduction
     </dt>
     <dd class="description">
      <p class="noindent">
       Once a dataset has been clustered, it is usually possible to measure each instance’s
       <alert style="color: #821131;">
        affinity
       </alert>
       with each cluster.
Here, affinity is any measure of how well an instance fits into a cluster. Each instance’s feature vector
       <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
        <mover accent="true">
         <mrow>
          <mi>
           x
          </mi>
         </mrow>
         <mo accent="true">
          →
         </mo>
        </mover>
       </math>
       can then be replaced with the vector of its cluster affinities. If there are
       <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
        <mi>
         k
        </mi>
       </math>
       clusters, then this vector is k-dimensional.
      </p>
      <div class="knowledge">
       <p class="noindent">
        The new vector is typically much lower-dimensional than the original feature vector, but it can preserve enough
information for further processing.
       </p>
      </div>
     </dd>
     <dt class="description">
      <a id="x7-64005x">
      </a>
      Feature engineering
     </dt>
     <dd class="description">
      <p class="noindent">
       The cluster affinities can often be useful as extra features. For example, we used k-means before to add geographic cluster
affinity features to the California housing dataset, and they helped us get better performance.
      </p>
     </dd>
     <dt class="description">
      <a id="x7-64006x">
      </a>
      Anomaly detection
     </dt>
     <dd class="description">
      <p class="noindent">
       Any instance that has a low affinity to all the clusters is likely to be an anomaly. For example, if we have clustered the
users of our website based on their behavior, we can detect users with unusual behavior, such as an unusual number of
requests per second <a id="x7-64007"></a><a href="#X0-kumari2016anomaly">[13]</a>.
      </p>
     </dd>
     <dt class="description">
      <a id="x7-64008x">
      </a>
      Semi-supervised learning
     </dt>
     <dd class="description">
      <p class="noindent">
       If we only have a few labels, we could perform clustering and propagate the labels to all the instances in the same cluster.
This technique can greatly increase the number of labels available for a subsequent supervised learning algorithm, and
thus improve its performance <a id="x7-64009"></a><a href="#X0-bair2013semi">[14]</a>.
      </p>
     </dd>
     <dt class="description">
      <a id="x7-64010x">
      </a>
      Search engines
     </dt>
     <dd class="description">
      <p class="noindent">
       Some search engines let you search for images that are similar to a reference image. To build such a system, we first
apply a clustering algorithm to all the images in our database. This allows similar images to end up in
the same cluster. Then when a user provides a reference image, all we’d need to do is use the trained
clustering model to find this image’s cluster, and we could then simply return all the images from this cluster <a id="x7-64011"></a><a href="#X0-zamir1999clustering">[15]</a>.
      </p>
     </dd>
     <dt class="description">
      <a id="x7-64012x">
      </a>
      Image segmentation
     </dt>
     <dd class="description">
      <p class="noindent">
       By clustering pixels according to their color, then replacing each pixel’s color with the mean color of its cluster, it is
possible to considerably reduce the number of different colors in an image. Image segmentation is used in
many object detection and tracking systems, as it makes it easier to detect the contour of each object <a id="x7-64013"></a><a href="#X0-burney2014k">[16]</a>.
      </p>
     </dd>
    </dl>
    <p class="noindent">
     There is
     <span id="bold" style="font-weight:bold;">
      no universal definition of what a cluster is
     </span>
     as it really depends on the context, and different algorithms will capture
different kinds of clusters. Some algorithms, for example, look for instances centered around a particular point, called a
     <span id="bold" style="font-weight:bold;">
      centroid
     </span>
     .
Others look for continuous regions of densely packed instances: these clusters can take on any shape. Some algorithms are
hierarchical, looking for clusters of clusters. And the list goes on.
    </p>
    <p class="noindent">
     In this section of our chapter, we will look at two <alert style="color: #821131;">(2)</alert> popular clustering algorithms:
    </p>
    <ul class="itemize1">
     <li class="itemize">
      <p class="noindent">
       k-means,
and
      </p>
     </li>
     <li class="itemize">
      <p class="noindent">
       DBSCAN,
      </p>
     </li>
    </ul>
    <p class="noindent">
     and explore some of their applications, such as non-linear dimensionality reduction, semi-supervised learning, and anomaly
detection.
    </p>
    <h3 class="subsectionHead">
     <a aria-label="Link to this section in PDF version" class="pdf-link" href="DataScienceIILectureBook.pdf#subsection.5.2.1" style="color: red;  position: absolute;  left: -30px;" target="_blank">
      <svg class="bi bi-file-pdf" fill="currentColor" height="20" viewbox="0 0 16 16" width="20" xmlns="http://www.w3.org/2000/svg">
       <path d="M4 0a2 2 0 0 0-2 2v12a2 2 0 0 0 2 2h8a2 2 0 0 0 2-2V2a2 2 0 0 0-2-2H4zm0 1h8a1 1 0 0 1 1 1v12a1 1 0 0 1-1 1H4a1 1 0 0 1-1-1V2a1 1 0 0 1 1-1z">
       </path>
       <path d="M4.603 12.087a.81.81 0 0 1-.438-.42c-.195-.388-.13-.776.08-1.102.198-.307.526-.568.897-.787a7.68 7.68 0 0 1 1.482-.645 19.701 19.701 0 0 0 1.062-2.227 7.269 7.269 0 0 1-.43-1.295c-.086-.4-.119-.796-.046-1.136.075-.354.274-.672.65-.823.192-.077.4-.12.602-.077a.7.7 0 0 1 .477.365c.088.164.12.356.127.538.007.187-.012.395-.047.614-.084.51-.27 1.134-.52 1.794a10.954 10.954 0 0 0 .98 1.686 5.753 5.753 0 0 1 1.334.05c.364.065.734.195.96.465.12.144.193.32.2.518.007.192-.047.382-.138.563a1.04 1.04 0 0 1-.354.416.856.856 0 0 1-.51.138c-.331-.014-.654-.196-.933-.417a5.716 5.716 0 0 1-.911-.95 11.642 11.642 0 0 0-1.997.406 11.311 11.311 0 0 1-1.021 1.51c-.29.35-.608.655-.926.787a.793.793 0 0 1-.58.029zm1.379-1.901c-.166.076-.32.156-.459.238-.328.194-.541.383-.647.547-.094.145-.096.25-.04.361.01.022.02.036.026.044a.27.27 0 0 0 .035-.012c.137-.056.355-.235.635-.572a8.18 8.18 0 0 0 .45-.606zm1.64-1.33a12.647 12.647 0 0 1 1.01-.193 11.666 11.666 0 0 1-.51-.858 20.741 20.741 0 0 1-.5 1.05zm2.446.45c.15.162.296.3.435.41.24.19.407.253.498.256a.107.107 0 0 0 .07-.015.307.307 0 0 0 .094-.125.436.436 0 0 0 .059-.2.095.095 0 0 0-.026-.063c-.052-.062-.2-.152-.518-.209a3.881 3.881 0 0 0-.612-.053zM8.078 5.8a6.7 6.7 0 0 0 .2-.828c.031-.188.043-.343.038-.465a.613.613 0 0 0-.032-.198.517.517 0 0 0-.145.04c-.087.035-.158.106-.196.283-.04.192-.03.469.046.822.024.111.054.227.09.346z">
       </path>
      </svg>
     </a>
     <small class="titlemark">
      5.2.1
     </small>
     <a id="x7-650005.2.1">
     </a>
     k-means
    </h3>
    <p class="noindent">
     Consider the unlabelled dataset represented in
     <span id="bold" style="font-weight:bold;">
      Fig.
     </span>
     <a href="#x7-65003r3">
      5.3
     </a>. It is clear to us to say we can clearly see five <alert style="color: #821131;">(5)</alert> blobs of instances. The
     <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
      <mi>
       k
      </mi>
     </math>
     -means
algorithm is a simple algorithm capable of clustering this kind of dataset very quickly and efficiently, often in just a few
iterations. It was proposed by
     <italic>
      Stuart Lloyd
     </italic>
     at Bell Labs in 1957 as a technique for
     <a class="glossary" href="DataScienceIILectureBookli2.html#glo:pcm">
      Pulse Code Modulation (PCM)
     </a>, but it was
only published outside of the company in 1982 <a id="x7-65001"></a><a href="#X0-lloyd1982least">[17]</a>. In 1965,
     <italic>
      Edward W. Forgy
     </italic>
     had published virtually the same algorithm <a id="x7-65002"></a><a href="#X0-forgy1965cluster">[18]</a>,
so
     <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
      <mi>
       k
      </mi>
     </math>
     -means
is sometimes referred to as the Lloyd-Forgy algorithm.
    </p>
    <div class="figure">
     <p class="noindent">
      <img alt="PIC" height="" src="codes/images/Unsupervised-Learning/blobs-plot-.svg" width="150%"/>
      <a id="x7-65003r3">
      </a>
     </p>
     <figcaption class="caption">
      <span class="id">
       Figure 5.3:
      </span>
      <span class="content">
       An unlabelled dataset composed of five blobs of instances.
      </span>
     </figcaption>
    </div>
    <p class="noindent">
     Let’s train a
     <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
      <mi>
       k
      </mi>
     </math>
     -means
clusterer on this dataset. It will try to find each blob’s center and assign each instance to the closest blob:
    </p>
    <pre><div id="fancyvrb67" style="padding:20px;border-radius: 3px;"><a id="x7-65005r181"></a><span style="color:#2B2BFF;">from</span><span style="color:#BABABA;"> </span>sklearn.cluster<span style="color:#BABABA;"> </span><span style="color:#2B2BFF;">import</span> KMeans 
<a id="x7-65007r182"></a><span style="color:#2B2BFF;">from</span><span style="color:#BABABA;"> </span>sklearn.datasets<span style="color:#BABABA;"> </span><span style="color:#2B2BFF;">import</span> make_blobs 
<a id="x7-65009r183"></a> 
<a id="x7-65011r184"></a>blob_centers = np.array([[ <span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">0.2</span></span>,  <span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">2.3</span></span>], [-<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">1.5</span></span> ,  <span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">2.3</span></span>], [-<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">2.8</span></span>,  <span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">1.8</span></span>], 
<a id="x7-65013r185"></a>                  [-<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">2.8</span></span>,  <span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">2.8</span></span>], [-<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">2.8</span></span>,  <span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">1.3</span></span>]]) 
<a id="x7-65015r186"></a>blob_std = np.array([<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">0.4</span></span>, <span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">0.3</span></span>, <span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">0.1</span></span>, <span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">0.1</span></span>, <span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">0.1</span></span>]) 
<a id="x7-65017r187"></a>X, y = make_blobs(n_samples=<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">2000</span></span>, centers=blob_centers, cluster_std=blob_std, 
<a id="x7-65019r188"></a>             random_state=<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">42</span></span>) 
<a id="x7-65021r189"></a> 
<a id="x7-65023r190"></a>k = <span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">5</span></span> 
<a id="x7-65025r191"></a>kmeans = KMeans(n_clusters=k, n_init=<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">10</span></span>, random_state=<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">42</span></span>) 
<a id="x7-65027r192"></a>y_pred = kmeans.fit_predict(X)</div></pre>
    <div class="warning">
     <p class="noindent">
      For the method to work, we have to specify the number of clusters
(
      <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
       <mi>
        k
       </mi>
      </math>
      ) which
the algorithm must find.
     </p>
    </div>
    <p class="noindent">
     In this example, as said previously, it is obvious
     <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
      <mi>
       k
      </mi>
     </math>
     should be set to five <alert style="color: #821131;">(5)</alert>, but in general it is not that easy. Each instance will be assigned to one of the five <alert style="color: #821131;">(5)</alert> clusters.
In the context of clustering, an instance’s label is the index of the cluster to which the algorithm assigns this
instance.
    </p>
    <p class="noindent">
     This
should
not
to
be
confused
with
the
class
labels
in
classification,
which
are
used
as
targets.
     <alert style="color: #821131;">
      <span id="bold" style="font-weight:bold;">
       <sup class="textsuperscript">
        7
       </sup>
      </span>
     </alert>
     <span class="marginnote">
      <alert style="color: #821131;">
       <span id="bold" style="font-weight:bold;">
        <sup class="textsuperscript">
         7
        </sup>
       </span>
      </alert>
      <span style="color:#0063B2;">
       remember,
clustering
is
an
       <alert style="color: #821131;">
        unsupervised
       </alert>
       learning
task
      </span>
     </span>
    </p>
    <p class="noindent">
     The <span style="color:#054C5C;"><code class="verb">KMeans</code></span>
     instance
preserves
the
predicted
labels
of
the
instances
it
was
trained
on,
available
via
the <span style="color:#054C5C;"><code class="verb">labels_</code></span>
     instance
variable:
    </p>
    <pre><div id="fancyvrb68" style="padding:20px;border-radius: 3px;"><a id="x7-65029r223"></a><span style="color:#2B2BFF;">print</span>(y_pred)</div></pre>
    <div class="tcolorbox tcolorbox" id="tcolobox-34">
     <div class="tcolorbox-content">
      <pre><div id="fancyvrb69" style="padding:20px;border-radius: 3px;"><a id="x7-65031r1"></a>[2 1 3 ... 1 2 0]</div></pre>
     </div>
    </div>
    <pre><div id="fancyvrb70" style="padding:20px;border-radius: 3px;"><a id="x7-65033r233"></a>y_pred is kmeans.labels_</div></pre>
    <div class="tcolorbox tcolorbox" id="tcolobox-35">
     <div class="tcolorbox-content">
      <pre><div id="fancyvrb71" style="padding:20px;border-radius: 3px;"><a id="x7-65035r1"></a>True</div></pre>
     </div>
    </div>
    <p class="noindent">
     We
can
also
take
a
look
at
the
five
<alert style="color: #821131;">(5)</alert> centroids
that
the
algorithm
found:
    </p>
    <pre><div id="fancyvrb72" style="padding:20px;border-radius: 3px;"><a id="x7-65037r245"></a><span style="color:#2B2BFF;">print</span>(kmeans.cluster_centers_)</div></pre>
    <div class="tcolorbox tcolorbox" id="tcolobox-36">
     <div class="tcolorbox-content">
      <pre><div id="fancyvrb73" style="padding:20px;border-radius: 3px;"><a id="x7-65039r1"></a>[[-2.80372723  1.80873739] 
<a id="x7-65041r2"></a> [ 0.20925539  2.30351618] 
<a id="x7-65043r3"></a> [-2.79846237  2.80004584] 
<a id="x7-65045r4"></a> [-1.4453407   2.32051326] 
<a id="x7-65047r5"></a> [-2.79244799  1.2973862 ]]</div></pre>
     </div>
    </div>
    <p class="noindent">
     We
can
easily
assign
new
instances
to
the
cluster
whose
centroid
is
closest:
    </p>
    <pre><div id="fancyvrb74" style="padding:20px;border-radius: 3px;"><a id="x7-65049r271"></a><span style="color:#2B2BFF;">import</span><span style="color:#BABABA;"> </span>numpy<span style="color:#BABABA;"> </span><span style="color:#2B2BFF;">as</span><span style="color:#BABABA;"> </span>np 
<a id="x7-65051r272"></a> 
<a id="x7-65053r273"></a>X_new = np.array([[<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">0</span></span>, <span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">2</span></span>], [<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">3</span></span>, <span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">2</span></span>], [-<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">3</span></span>, <span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">3</span></span>], [-<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">3</span></span>, <span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">2.5</span></span>]]) 
<a id="x7-65055r274"></a><span style="color:#2B2BFF;">print</span>(kmeans.predict(X_new))</div></pre>
    <div class="tcolorbox tcolorbox" id="tcolobox-37">
     <div class="tcolorbox-content">
      <pre><div id="fancyvrb75" style="padding:20px;border-radius: 3px;"><a id="x7-65057r1"></a>[1 1 2 2]</div></pre>
     </div>
    </div>
    <p class="noindent">
     If
we
plot
the
cluster’s
decision
boundaries,
we
get
a
                                                                                
                                                                                
Voronoi
tessellation
     <alert style="color: #821131;">
      <span id="bold" style="font-weight:bold;">
       <sup class="textsuperscript">
        8
       </sup>
      </span>
     </alert>
     <span class="marginnote">
      <alert style="color: #821131;">
       <span id="bold" style="font-weight:bold;">
        <sup class="textsuperscript">
         8
        </sup>
       </span>
      </alert>
      <span style="color:#0063B2;">
       a
type
of
tessellation
pattern
in
which
a
number
of
points
scattered
on
a
plane
subdivides
in
exactly
n
cells
enclosing
a
portion
of
the
plane
that
is
closest
to
each
point.
      </span>
     </span>
     which
can
be
seen
in
     <span id="bold" style="font-weight:bold;">
      Fig.
     </span>
     <a href="#x7-65058r4">
      5.4
     </a>,
where
each
centroid
is
represented
with
an
     <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
      <mi>
       X
      </mi>
     </math>
     .
    </p>
    <div class="figure">
     <p class="noindent">
      <img alt="PIC" height="" src="codes/images/Unsupervised-Learning/voronoi-plot-.svg" width="150%"/>
      <a id="x7-65058r4">
      </a>
     </p>
     <figcaption class="caption">
      <span class="id">
       Figure 5.4:
      </span>
      <span class="content">
       <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
        <mi>
         k
        </mi>
       </math>
       -means
         decision boundaries (Voronoi tessellation)
      </span>
     </figcaption>
    </div>
    <p class="noindent">
     The vast majority of the instances were clearly assigned to the appropriate cluster, but a good part of the instances were mislabelled,
Such as parts in the lower left where there is obviously two <alert style="color: #821131;">(2)</alert> centre points, but the algorithm decided there is only one. Indeed, the
     <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
      <mi>
       k
      </mi>
     </math>
     -means
algorithm does not behave very well when the blobs have very different diameters because all it cares about when assigning an
instance to a cluster is the distance to the
     <span id="bold" style="font-weight:bold;">
      centroid
     </span>
     .
    </p>
    <p class="noindent">
     Instead
of
assigning
each
instance
to
a
single
cluster,
which
is
called
     <span id="bold" style="font-weight:bold;">
      hard
clustering
     </span>
     ,
it
can
be
useful
to
give
each
instance
     <alert style="color: #821131;">
      a
score
per
cluster
     </alert>
     ,
which
is
called
soft
clustering.
     <alert style="color: #821131;">
      <span id="bold" style="font-weight:bold;">
       <sup class="textsuperscript">
        9
       </sup>
      </span>
     </alert>
     <span class="marginnote">
      <alert style="color: #821131;">
       <span id="bold" style="font-weight:bold;">
        <sup class="textsuperscript">
         9
        </sup>
       </span>
      </alert>
      <span style="color:#0063B2;">
       This
is
a
similar
behaviour
to
hard
v. soft
voting
we
encountered
in
Random
Forest.
      </span>
     </span>
     The
score
can
be
the
distance
between
the
instance
and
the
centroid
or
a
similarity
                                                                                
                                                                                
score,
such
as
the
Gaussian
     <a class="glossary" href="DataScienceIILectureBookli2.html#glo:rbf">
      Radial
Basis
Function
(RBF)
     </a>.
In
the <span style="color:#054C5C;"><code class="verb">KMeans</code></span>
     class,
the <span style="color:#054C5C;"><code class="verb">transform()</code></span>
     method
measures
the
distance
from
each
instance
to
every
centroid:
    </p>
    <pre><div id="fancyvrb76" style="padding:20px;border-radius: 3px;"><a id="x7-65060r350"></a><span style="color:#2B2BFF;">print</span>(kmeans.transform(X_new).round(<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">2</span></span>))</div></pre>
    <div class="tcolorbox tcolorbox" id="tcolobox-38">
     <div class="tcolorbox-content">
      <pre><div id="fancyvrb77" style="padding:20px;border-radius: 3px;"><a id="x7-65062r1"></a>[[2.81 0.37 2.91 1.48 2.88] 
<a id="x7-65064r2"></a> [5.81 2.81 5.85 4.46 5.83] 
<a id="x7-65066r3"></a> [1.21 3.28 0.28 1.7  1.72] 
<a id="x7-65068r4"></a> [0.72 3.22 0.36 1.56 1.22]]</div></pre>
     </div>
    </div>
    <p class="noindent">
     In
the
aforementioned
example,
the
first
instance
in <span style="color:#054C5C;"><code class="verb">X_new</code></span>
     is
located
at
a
distance
of
about
2.84
from
the
     <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
      <msup>
       <mrow>
        <mstyle class="text">
         <mtext>
          1
         </mtext>
        </mstyle>
       </mrow>
       <mrow>
        <mstyle class="text">
         <mtext>
          st
         </mtext>
        </mstyle>
       </mrow>
      </msup>
     </math>
     centroid,
0.59
from
the
     <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
      <msup>
       <mrow>
        <mstyle class="text">
         <mtext>
          2
         </mtext>
        </mstyle>
       </mrow>
       <mrow>
        <mstyle class="text">
         <mtext>
          nd
         </mtext>
        </mstyle>
       </mrow>
      </msup>
     </math>
     centroid,
1.5
from
the
     <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
      <msup>
       <mrow>
        <mstyle class="text">
         <mtext>
          3
         </mtext>
        </mstyle>
       </mrow>
       <mrow>
        <mstyle class="text">
         <mtext>
          rd
         </mtext>
        </mstyle>
       </mrow>
      </msup>
     </math>
     centroid,
                                                                                
                                                                                
2.9
from
the
     <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
      <msup>
       <mrow>
        <mstyle class="text">
         <mtext>
          4
         </mtext>
        </mstyle>
       </mrow>
       <mrow>
        <mstyle class="text">
         <mtext>
          th
         </mtext>
        </mstyle>
       </mrow>
      </msup>
     </math>
     centroid,
and
0.31
from
the
     <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
      <msup>
       <mrow>
        <mstyle class="text">
         <mtext>
          5
         </mtext>
        </mstyle>
       </mrow>
       <mrow>
        <mstyle class="text">
         <mtext>
          th
         </mtext>
        </mstyle>
       </mrow>
      </msup>
     </math>
     centroid.
    </p>
    <p class="noindent">
     If
we
have
a
high-dimensional
dataset
and
we
transform
it
this
way,
we
end
up
with
a
k-dimensional
dataset.
This
transformation
can
be
a
very
efficient
non-linear
dimensionality
reduction
technique.
Alternatively,
we
can
use
these
distances
as
extra
features
to
train
another
model.
     <a id="subsubsection*.8">
     </a>
    </p>
    <h5 class="subsubsectionHead">
     <a id="x7-66000">
     </a>
     The Operation Principle
    </h5>
    <p class="noindent">
     Let’s try to understand
     <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
      <mi>
       k
      </mi>
     </math>
     -means
via an example. Suppose we were given the centroids. We could easily label all the instances in the dataset by assigning each of
them to the cluster whose centroid is closest. Or, if we were given all the instance labels, we could easily locate each cluster’s
centroid by computing the mean of the instances in that cluster.
    </p>
    <p class="noindent">
    </p>
    <div class="quoteblock">
     <p class="noindent">
      But
here
are
given
neither
the
labels
nor
the
centroids,
so
how
can
we
proceed?
     </p>
    </div>
    <p class="noindent">
     We
start
by
placing
the
centroids
randomly.
     <alert style="color: #821131;">
      <span id="bold" style="font-weight:bold;">
       <sup class="textsuperscript">
        10
       </sup>
      </span>
     </alert>
     <span class="marginnote">
      <alert style="color: #821131;">
       <span id="bold" style="font-weight:bold;">
        <sup class="textsuperscript">
         10
        </sup>
       </span>
      </alert>
      <span style="color:#0063B2;">
       e.g.,
by
picking
       <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
        <mi>
         k
        </mi>
       </math>
       instances
at
random
from
the
dataset
and
using
their
locations
as
centroids.
      </span>
     </span>
     Then
label
the
instances,
update
the
centroids,
label
the
instances,
update
the
centroids,
and
so
on
until
the
centroids
stop
moving.
    </p>
    <div class="warning">
     <p class="noindent">
      The algorithm is
      <span id="bold" style="font-weight:bold;">
       guaranteed
      </span>
      to converge in a finite number of steps.
     </p>
    </div>
    <p class="noindent">
     That’s because the mean squared distance between the instances and their closest centroids can only go down at
                                                                                
                                                                                
each step, and since it cannot be negative, it’s guaranteed to converge. We can see the algorithm in action in
     <span id="bold" style="font-weight:bold;">
      Fig.
     </span>
     <a href="#x7-66001r5">
      5.5
     </a>
     :
    </p>
    <div class="figure">
     <p class="noindent">
      <img alt="PIC" height="" src="codes/images/Unsupervised-Learning/kmeans-algorithm-plot-.svg" width="150%"/>
      <a id="x7-66001r5">
      </a>
     </p>
     <figcaption class="caption">
      <span class="id">
       Figure 5.5:
      </span>
      <span class="content">
       The
       <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
        <mi>
         k
        </mi>
       </math>
       -means
         algorithm.
      </span>
     </figcaption>
    </div>
    <p class="noindent">
     Let’s try to explain the behaviour of the figure.
    </p>
    <dl class="enumerate-enumitem">
     <dt class="enumerate-enumitem">
      1.
     </dt>
     <dd class="enumerate-enumitem">
      the
centroids
are
initialised
randomly
(top
left)
     </dd>
     <dt class="enumerate-enumitem">
      2.
     </dt>
     <dd class="enumerate-enumitem">
      then
the
instances
are
labelled
(top
right),
     </dd>
     <dt class="enumerate-enumitem">
      3.
     </dt>
     <dd class="enumerate-enumitem">
      the
centroids
are
updated
(center
left),
     </dd>
     <dt class="enumerate-enumitem">
      4.
     </dt>
     <dd class="enumerate-enumitem">
      the
instances
are
relabelled
(center
right),
     </dd>
    </dl>
    <p class="noindent">
     and so on. As we can see, in just three <alert style="color: #821131;">(3)</alert> iterations the algorithm has reached a clustering that seems close to
optimal.
    </p>
    <div class="informationblock" id="tcolobox-39">
     <div class="title">
      <p class="noindent">
       <span id="bold" style="font-weight:bold;">
        Information
       </span>
       : Computational Complexity
      </p>
     </div>
     <div class="box-content">
      <p class="noindent">
       The computational complexity of the algorithm is
       <alert style="color: #821131;">
        generally linear
       </alert>
       with regards to the number of instances
(
       <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
        <mi>
         m
        </mi>
       </math>
       ),
the number of clusters (
       <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
        <mi>
         k
        </mi>
       </math>
       ),
and the number of dimensions (
       <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
        <mi>
         n
        </mi>
       </math>
       ).
However, this is only true when the data has a clustering structure. If it does not, then in the worst-case
scenario the complexity can increase
       <alert style="color: #821131;">
        exponentially
       </alert>
       with the number of instances.
      </p>
      <p class="noindent">
       In practice, this rarely happens, and
       <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
        <mi>
         k
        </mi>
       </math>
       -means
is generally one of the fastest clustering algorithms.
      </p>
     </div>
    </div>
    <p class="noindent">
     Although
the
algorithm
is
guaranteed
to
converge,
it
may
not
converge
to
the
right
solution:
     <alert style="color: #821131;">
      <span id="bold" style="font-weight:bold;">
       <sup class="textsuperscript">
        11
       </sup>
      </span>
     </alert>
     <span class="marginnote">
      <alert style="color: #821131;">
       <span id="bold" style="font-weight:bold;">
        <sup class="textsuperscript">
         11
        </sup>
       </span>
      </alert>
      <span style="color:#0063B2;">
       i.e.,
it
may
converge
to
a
local
optimum,
instead
of
the
global.
      </span>
     </span>
     whether
it
does
or
not
depends
on
the
centroid
initialisation.
     <span id="bold" style="font-weight:bold;">
      Fig.
     </span>
     <a href="#x7-66006r6">
      5.6
     </a>
     shows
two
<alert style="color: #821131;">(2)</alert> sub-optimal
solutions
that
the
algorithm
can
converge
to
if
we
are
not
lucky
with
the
random
initialisation
                                                                                
                                                                                
step.
    </p>
    <div class="figure">
     <p class="noindent">
      <img alt="PIC" height="" src="codes/images/Unsupervised-Learning/kmeans-algorithm-2-plot-.svg" width="150%"/>
      <a id="x7-66006r6">
      </a>
     </p>
     <figcaption class="caption">
      <span class="id">
       Figure 5.6:
      </span>
      <span class="content">
       Suboptimal solutions due to unlucky centroid initialisation.
      </span>
     </figcaption>
    </div>
    <p class="noindent">
     Let’s take a look at a few ways we can mitigate this risk by improving the centroid initialisation.
     <a id="subsubsection*.9">
     </a>
    </p>
    <h5 class="subsubsectionHead">
     <a id="x7-67000">
     </a>
     Centroid initialisation methods
    </h5>
    <p class="noindent">
     If
we
happen
to
know
approximately
where
the
centroids
should
be,
     <alert style="color: #821131;">
      <span id="bold" style="font-weight:bold;">
       <sup class="textsuperscript">
        12
       </sup>
      </span>
     </alert>
     <span class="marginnote">
      <alert style="color: #821131;">
       <span id="bold" style="font-weight:bold;">
        <sup class="textsuperscript">
         12
        </sup>
       </span>
      </alert>
      <span style="color:#0063B2;">
       i.e.,
if
we
ran
another
clustering
algorithm
earlier.
      </span>
     </span>
     then
we
can
set
the <span style="color:#054C5C;"><code class="verb">init</code></span>
     hyperparameter
to
a
     <span id="bold" style="font-weight:bold;">
      numpy
     </span>
     array
containing
the
list
of
centroids,
and
set <span style="color:#054C5C;"><code class="verb">n_init</code></span>
     to <span style="color:#054C5C;"><code class="verb">1</code></span>
     :
    </p>
    <p class="noindent">
    </p>
    <pre><div id="fancyvrb78" style="padding:20px;border-radius: 3px;"><a id="x7-67002r483"></a>good_init = np.array([[-<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">3</span></span>, <span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">3</span></span>], [-<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">3</span></span>, <span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">2</span></span>], [-<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">3</span></span>, <span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">1</span></span>], [-<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">1</span></span>, <span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">2</span></span>], [<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">0</span></span>, <span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">2</span></span>]]) 
<a id="x7-67004r484"></a>kmeans = KMeans(n_clusters=<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">5</span></span>, 
<a id="x7-67006r485"></a>           init=good_init, 
<a id="x7-67008r486"></a>           n_init=<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">1</span></span>, 
<a id="x7-67010r487"></a>           random_state=<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">42</span></span>) 
<a id="x7-67012r488"></a>kmeans.fit(X)</div></pre>
    <p class="noindent">
     Another
solution
is
to
run
the
algorithm
multiple
times
with
     <alert style="color: #821131;">
      different
random
initialisations
     </alert>
     and
keep
the
best
solution.
The
number
of
random
initialisation
is
controlled
by
the <span style="color:#054C5C;"><code class="verb">n_init</code></span>
     hyperparameter:
    </p>
    <p class="noindent">
    </p>
    <div class="quoteblock">
     <p class="noindent">
      by
default
it
is
equal
to
10,
which
means
that
the
whole
algorithm
described
earlier
runs
10
times
when
we
call <span style="color:#054C5C;"><code class="verb">fit()</code></span>,
and <span style="color:#054C5C;"><code class="verb">sklearn</code></span>
      keeps
the
best
solution.
     </p>
    </div>
    <p class="noindent">
     But how exactly does it know which solution is the best? Well, it uses a performance metric. That metric is called the
     <span id="bold" style="font-weight:bold;">
      model’s inertia
     </span>
     , which is the sum of the squared distances between the instances and their closest centroids.
    </p>
    <table class="equation-star">
     <tr>
      <td>
       <math class="equation" display="block" xmlns="http://www.w3.org/1998/Math/MathML">
        <munderover accent="false" accentunder="false">
         <mrow>
          <mo>
           ∑
          </mo>
         </mrow>
         <mrow>
          <mi>
           i
          </mi>
          <mo class="MathClass-rel" stretchy="false">
           =
          </mo>
          <mn>
           1
          </mn>
         </mrow>
         <mrow>
          <mi>
           N
          </mi>
         </mrow>
        </munderover>
        <msup>
         <mrow>
          <mrow>
           <mo fence="true" form="prefix">
            (
           </mo>
           <mrow>
            <msubsup>
             <mrow>
              <mi>
               x
              </mi>
             </mrow>
             <mrow>
              <mi>
               i
              </mi>
             </mrow>
             <mrow>
             </mrow>
            </msubsup>
            <mo class="MathClass-bin" stretchy="false">
             −
            </mo>
            <msubsup>
             <mrow>
              <mi>
               C
              </mi>
             </mrow>
             <mrow>
              <mi>
               k
              </mi>
             </mrow>
             <mrow>
             </mrow>
            </msubsup>
           </mrow>
           <mo fence="true" form="postfix">
            )
           </mo>
          </mrow>
         </mrow>
         <mrow>
          <mn>
           2
          </mn>
         </mrow>
        </msup>
       </math>
      </td>
     </tr>
    </table>
    <p class="noindent">
     where
     <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
      <mi>
       N
      </mi>
     </math>
     is the number
of samples,
     <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
      <mi>
       x
      </mi>
     </math>
     is the
value of a sample,
     <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
      <mi>
       C
      </mi>
     </math>
     is the centre of the cluster centroid. For our example, this value is roughly equal to:
    </p>
    <ul class="itemize1">
     <li class="itemize">
      <p class="noindent">
       219.4
for
the
model
on
the
left
in
       <span id="bold" style="font-weight:bold;">
        Fig.
       </span>
       <a href="#x7-70001r12">
        5.12
       </a>,
      </p>
     </li>
     <li class="itemize">
      <p class="noindent">
       258.6
for
the
model
on
the
right
in
       <span id="bold" style="font-weight:bold;">
        Fig.
       </span>
       <a href="#x7-70001r12">
        5.12
       </a>,
      </p>
     </li>
     <li class="itemize">
      <p class="noindent">
       211.6
for
the
model
in
       <span id="bold" style="font-weight:bold;">
        Fig.
       </span>
       <a href="#x7-65058r4">
        5.4
       </a>.
      </p>
     </li>
    </ul>
    <p class="noindent">
     The <span style="color:#054C5C;"><code class="verb">KMeans</code></span>
     class runs the algorithm <span style="color:#054C5C;"><code class="verb">n_init</code></span>
     times and keeps the model with the
     <alert style="color: #821131;">
      lowest inertia
     </alert>
     .
    </p>
    <p class="noindent">
     In
this
example,
the
model
in
     <span id="bold" style="font-weight:bold;">
      Fig.
     </span>
     <a href="#x7-65058r4">
      5.4
     </a>
     will
be
selected.
     <alert style="color: #821131;">
      <span id="bold" style="font-weight:bold;">
       <sup class="textsuperscript">
        13
       </sup>
      </span>
     </alert>
     <span class="marginnote">
      <alert style="color: #821131;">
       <span id="bold" style="font-weight:bold;">
        <sup class="textsuperscript">
         13
        </sup>
       </span>
      </alert>
      <span style="color:#0063B2;">
       unless
we
are
very
unlucky
                                                                                
                                                                                
with <span style="color:#054C5C;"><code class="verb">n_init</code></span>
       consecutive
random
initialisation
      </span>
     </span>
     For
the
curious,
a
model’s
inertia
is
accessible
via
the <span style="color:#054C5C;"><code class="verb">inertia_</code></span>
     instance
variable:
    </p>
    <p class="noindent">
    </p>
    <pre><div id="fancyvrb79" style="padding:20px;border-radius: 3px;"><a id="x7-67014r495"></a>kmeans.inertia_</div></pre>
    <div class="tcolorbox tcolorbox" id="tcolobox-40">
     <div class="tcolorbox-content">
      <pre><div id="fancyvrb80" style="padding:20px;border-radius: 3px;"><a id="x7-67016r1"></a>211.59853725816836</div></pre>
     </div>
    </div>
    <p class="noindent">
     The <span style="color:#054C5C;"><code class="verb">score()</code></span>
     method
returns
the
negative
inertia:
     <alert style="color: #821131;">
      <span id="bold" style="font-weight:bold;">
       <sup class="textsuperscript">
        14
       </sup>
      </span>
     </alert>
     <span class="marginnote">
      <alert style="color: #821131;">
       <span id="bold" style="font-weight:bold;">
        <sup class="textsuperscript">
         14
        </sup>
       </span>
      </alert>
      <span style="color:#0063B2;">
       it’s
negative
because
a
predictor’s
score()
method
must
always
respect <span style="color:#054C5C;"><code class="verb">sklearn</code></span>
       ’s
“greater
is
better”
rule:
if
a
predictor
is
better
than
another,
its <span style="color:#054C5C;"><code class="verb">score()</code></span>
       method
should
return
a
greater
score
      </span>
     </span>
    </p>
    <p class="noindent">
    </p>
    <pre><div id="fancyvrb81" style="padding:20px;border-radius: 3px;"><a id="x7-67018r505"></a>kmeans.score(X)</div></pre>
    <div class="tcolorbox tcolorbox" id="tcolobox-41">
     <div class="tcolorbox-content">
      <pre><div id="fancyvrb82" style="padding:20px;border-radius: 3px;"><a id="x7-67020r1"></a>-211.5985372581684</div></pre>
     </div>
    </div>
    <p class="noindent">
     An
important
improvement
to
the
     <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
      <mi>
       k
      </mi>
     </math>
     -means
algorithm,
     <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
      <mi>
       k
      </mi>
     </math>
     -means++,
was
proposed
in
a
2006
paper
by
David
Arthur
and
Sergei
Vassilvitskii <a id="x7-67021"></a><a href="#X0-arthur2006k">[19]</a>.
They
introduced
a
smarter
initialisation
step
that
tends
to
select
centroids
that
are
distant
from
one
another,
and
this
improvement
makes
the
     <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
      <mi>
       k
      </mi>
     </math>
     -means
algorithm
much
less
likely
to
converge
to
a
sub-optimal
solution.
    </p>
    <p class="noindent">
     The
paper
showed,
                                                                                
                                                                                
the
additional
computation
required
for
the
smarter
initialisation
step
is
well
worth
it
because
it
makes
it
possible
to
drastically
reduce
the
number
of
times
the
algorithm
needs
to
be
run
to
find
the
optimal
solution.
    </p>
    <p class="noindent">
     The
     <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
      <mi>
       k
      </mi>
     </math>
     -means++
initialisation works as follows:
    </p>
    <dl class="enumerate-enumitem">
     <dt class="enumerate-enumitem">
      1.
     </dt>
     <dd class="enumerate-enumitem">
      Take
one
centroid
      <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
       <msubsup>
        <mrow>
         <mover accent="true">
          <mrow>
           <mi>
            c
           </mi>
          </mrow>
          <mo accent="true">
           →
          </mo>
         </mover>
        </mrow>
        <mrow>
        </mrow>
        <mrow>
         <mo class="MathClass-open" stretchy="false">
          (
         </mo>
         <mn>
          1
         </mn>
         <mo class="MathClass-close" stretchy="false">
          )
         </mo>
        </mrow>
       </msubsup>
      </math>
      ,
chosen
uniformly
at
random
from
the
dataset,
     </dd>
     <dt class="enumerate-enumitem">
      2.
     </dt>
     <dd class="enumerate-enumitem">
      Take the new centroid
      <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
       <msubsup>
        <mrow>
         <mover accent="true">
          <mrow>
           <mi>
            c
           </mi>
          </mrow>
          <mo accent="true">
           →
          </mo>
         </mover>
        </mrow>
        <mrow>
        </mrow>
        <mrow>
         <mo class="MathClass-open" stretchy="false">
          (
         </mo>
         <mi>
          i
         </mi>
         <mo class="MathClass-close" stretchy="false">
          )
         </mo>
        </mrow>
       </msubsup>
      </math>
      ,
choosing an instance
      <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
       <msubsup>
        <mrow>
         <mover accent="true">
          <mrow>
           <mi>
            x
           </mi>
          </mrow>
          <mo accent="true">
           →
          </mo>
         </mover>
        </mrow>
        <mrow>
        </mrow>
        <mrow>
         <mo class="MathClass-open" stretchy="false">
          (
         </mo>
         <mi>
          i
         </mi>
         <mo class="MathClass-close" stretchy="false">
          )
         </mo>
        </mrow>
       </msubsup>
      </math>
      with probability:
      <table class="equation-star">
       <tr>
        <td>
         <math class="equation" display="block" xmlns="http://www.w3.org/1998/Math/MathML">
          <mi>
           p
          </mi>
          <msup>
           <mrow>
            <mrow>
             <mo fence="true" form="prefix">
              (
             </mo>
             <mrow>
              <msubsup>
               <mrow>
                <mover accent="true">
                 <mrow>
                  <mi>
                   x
                  </mi>
                 </mrow>
                 <mo accent="true">
                  →
                 </mo>
                </mover>
               </mrow>
               <mrow>
               </mrow>
               <mrow>
                <mo class="MathClass-open" stretchy="false">
                 (
                </mo>
                <mi>
                 i
                </mi>
                <mo class="MathClass-close" stretchy="false">
                 )
                </mo>
               </mrow>
              </msubsup>
             </mrow>
             <mo fence="true" form="postfix">
              )
             </mo>
            </mrow>
           </mrow>
           <mrow>
           </mrow>
          </msup>
          <mo class="MathClass-rel" stretchy="false">
           =
          </mo>
          <mfrac>
           <mrow>
            <mi>
             D
            </mi>
            <msup>
             <mrow>
              <mrow>
               <mo fence="true" form="prefix">
                (
               </mo>
               <mrow>
                <msubsup>
                 <mrow>
                  <mover accent="true">
                   <mrow>
                    <mi>
                     x
                    </mi>
                   </mrow>
                   <mo accent="true">
                    →
                   </mo>
                  </mover>
                 </mrow>
                 <mrow>
                 </mrow>
                 <mrow>
                  <mo class="MathClass-open" stretchy="false">
                   (
                  </mo>
                  <mi>
                   i
                  </mi>
                  <mo class="MathClass-close" stretchy="false">
                   )
                  </mo>
                 </mrow>
                </msubsup>
               </mrow>
               <mo fence="true" form="postfix">
                )
               </mo>
              </mrow>
             </mrow>
             <mrow>
              <mn>
               2
              </mn>
             </mrow>
            </msup>
           </mrow>
           <mrow>
            <munderover accent="false" accentunder="false">
             <mrow>
              <mo>
               ∑
              </mo>
             </mrow>
             <mrow>
              <mi>
               j
              </mi>
              <mo class="MathClass-rel" stretchy="false">
               =
              </mo>
              <mn>
               1
              </mn>
             </mrow>
             <mrow>
              <mi>
               m
              </mi>
             </mrow>
            </munderover>
            <mi>
             D
            </mi>
            <msup>
             <mrow>
              <mrow>
               <mo fence="true" form="prefix">
                (
               </mo>
               <mrow>
                <msubsup>
                 <mrow>
                  <mover accent="true">
                   <mrow>
                    <mi>
                     x
                    </mi>
                   </mrow>
                   <mo accent="true">
                    →
                   </mo>
                  </mover>
                 </mrow>
                 <mrow>
                 </mrow>
                 <mrow>
                  <mo class="MathClass-open" stretchy="false">
                   (
                  </mo>
                  <mi>
                   i
                  </mi>
                  <mo class="MathClass-close" stretchy="false">
                   )
                  </mo>
                 </mrow>
                </msubsup>
               </mrow>
               <mo fence="true" form="postfix">
                )
               </mo>
              </mrow>
             </mrow>
             <mrow>
              <mn>
               2
              </mn>
             </mrow>
            </msup>
           </mrow>
          </mfrac>
         </math>
        </td>
       </tr>
      </table>
     </dd>
     <dt class="enumerate-enumitem">
     </dt>
     <dd class="enumerate-enumitem">
      where
      <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
       <mi>
        D
       </mi>
       <msup>
        <mrow>
         <mrow>
          <mo fence="true" form="prefix">
           (
          </mo>
          <mrow>
           <msubsup>
            <mrow>
             <mover accent="true">
              <mrow>
               <mi>
                x
               </mi>
              </mrow>
              <mo accent="true">
               →
              </mo>
             </mover>
            </mrow>
            <mrow>
            </mrow>
            <mrow>
             <mo class="MathClass-open" stretchy="false">
              (
             </mo>
             <mi>
              i
             </mi>
             <mo class="MathClass-close" stretchy="false">
              )
             </mo>
            </mrow>
           </msubsup>
          </mrow>
          <mo fence="true" form="postfix">
           )
          </mo>
         </mrow>
        </mrow>
        <mrow>
         <mn>
          2
         </mn>
        </mrow>
       </msup>
      </math>
      is the distance
between the instance
      <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
       <msubsup>
        <mrow>
         <mover accent="true">
          <mrow>
           <mi>
            x
           </mi>
          </mrow>
          <mo accent="true">
           →
          </mo>
         </mover>
        </mrow>
        <mrow>
        </mrow>
        <mrow>
         <mo class="MathClass-open" stretchy="false">
          (
         </mo>
         <mi>
          i
         </mi>
         <mo class="MathClass-close" stretchy="false">
          )
         </mo>
        </mrow>
       </msubsup>
      </math>
      and the closest centroid that was already chosen.
      <div class="warning">
       <p class="noindent">
        This probability distribution ensures that instances farther away from already chosen centroids are much more likely to be
selected as centroids.
       </p>
      </div>
     </dd>
     <dt class="enumerate-enumitem">
      3.
     </dt>
     <dd class="enumerate-enumitem">
      Repeat the previous step until all
      <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
       <mi>
        k
       </mi>
      </math>
      centroids have been chosen.
     </dd>
    </dl>
    <p class="noindent">
     The <span style="color:#054C5C;"><code class="verb">KMeans</code></span>
     class uses this initialisation method by
     <alert style="color: #821131;">
      default
     </alert>
     . To force it to use the original method (i.e., picking
     <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
      <mi>
       k
      </mi>
     </math>
     instances
randomly to define the initial centroids), then we can set the init hyperparameter to <span style="color:#054C5C;"><code class="verb">"random"</code></span>.
    </p>
    <div class="knowledge">
     <p class="noindent">
      We will rarely need to do this.
     </p>
    </div>
    <a id="subsubsection*.10">
    </a>
    <h5 class="subsubsectionHead">
     <a id="x7-68000">
     </a>
     Accelerated and mini-batch
    </h5>
    <p class="noindent">
     Another
improvement
to
the
     <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
      <mi>
       k
      </mi>
     </math>
     -means
algorithm
                                                                                
                                                                                
was
proposed
in
a
2003
paper
by
     <italic>
      Charles
Elkan
     </italic> <a id="x7-68001"></a><a href="#X0-elkan2003using">[20]</a>.
On
some
large
datasets
with
many
clusters,
the
algorithm
can
be
accelerated
by
avoiding
many
unnecessary
distance
calculations.
Elkan
achieved
this
by
exploiting
the
triangle
inequality
     <alert style="color: #821131;">
      <span id="bold" style="font-weight:bold;">
       <sup class="textsuperscript">
        15
       </sup>
      </span>
     </alert>
     <span class="marginnote">
      <alert style="color: #821131;">
       <span id="bold" style="font-weight:bold;">
        <sup class="textsuperscript">
         15
        </sup>
       </span>
      </alert>
      <span style="color:#0063B2;">
       The
triangle
inequality
is
AC
       <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
        <mo class="MathClass-rel" stretchy="false">
         ≤
        </mo>
       </math>
       AB
+
BC,
where
A,
B
and
C
are
three
points
and
AB,
AC,
and
BC
are
the
distances
between
these
points.
      </span>
     </span>
     and
by
                                                                                
                                                                                
keeping
track
of
lower
and
upper
bounds
for
distances
between
instances
and
centroids.
However,
Elkan’s
algorithm
does
not
always
accelerate
training,
and
sometimes
it
can
even
slow
down
training
significantly
as
it
     <alert style="color: #821131;">
      depends
on
the
dataset
     </alert>
     .
    </p>
    <div class="knowledge">
     <p class="noindent">
      To give it a try, set <span style="color:#054C5C;"><code class="verb">algorithm="elkan"</code></span>.
     </p>
    </div>
    <p class="noindent">
     Yet
another
important
variant
of
the
     <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
      <mi>
       k
      </mi>
     </math>
     -means
algorithm
was
proposed
in
a
2010
paper
by
David
Sculley <a id="x7-68002"></a><a href="#X0-sculley2010web">[21]</a>.
Instead
of
using
the
full
dataset
at
each
iteration,
the
algorithm
                                                                                
                                                                                
is
capable
of
using
     <alert style="color: #821131;">
      mini-batches
     </alert>
     ,
moving
the
centroids
just
slightly
at
each
iteration.
This
speeds
up
the
algorithm
     <alert style="color: #821131;">
      <span id="bold" style="font-weight:bold;">
       <sup class="textsuperscript">
        16
       </sup>
      </span>
     </alert>
     <span class="marginnote">
      <alert style="color: #821131;">
       <span id="bold" style="font-weight:bold;">
        <sup class="textsuperscript">
         16
        </sup>
       </span>
      </alert>
      <span style="color:#0063B2;">
       typically
by
a
factor
of
three
to
four
      </span>
     </span>
     and
makes
it
possible
to
cluster
huge
datasets
that
do
not
fit
in
memory. <span style="color:#054C5C;"><code class="verb">sklearn</code></span>
     implements
this
algorithm
in
the <span style="color:#054C5C;"><code class="verb">MiniBatchKMeans</code></span>
     class,
which
we
can
use
just
like
the <span style="color:#054C5C;"><code class="verb">KMeans</code></span>
     class:
    </p>
    <p class="noindent">
    </p>
    <pre><div id="fancyvrb83" style="padding:20px;border-radius: 3px;"><a id="x7-68004r579"></a><span style="color:#2B2BFF;">from</span><span style="color:#BABABA;"> </span>sklearn.cluster<span style="color:#BABABA;"> </span><span style="color:#2B2BFF;">import</span> MiniBatchKMeans 
<a id="x7-68006r580"></a> 
<a id="x7-68008r581"></a>minibatch_kmeans = MiniBatchKMeans(n_clusters=<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">10</span></span>, batch_size=<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">10</span></span>,                                  n_init=<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">3</span></span>, random_state=<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">42</span></span>) 
<a id="x7-68010r582"></a>minibatch_kmeans.fit(X_memmap)</div></pre>
    <p class="noindent">
     If
the
dataset
does
not
fit
                                                                                
                                                                                
in
memory,
the
simplest
option
is
to
use
the <span style="color:#054C5C;"><code class="verb">memmap</code></span>
     class.
Alternatively,
we
can
pass
one
mini-batch
at
a
time
to
the <span style="color:#054C5C;"><code class="verb">partial_fit()</code></span>
     method,
but
this
will
require
much
more
work,
as
we
will
need
to
perform
multiple
initialisations
and
select
the
best
one
ourselves.
    </p>
    <p class="noindent">
     Although
the
mini-batch
     <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
      <mi>
       k
      </mi>
     </math>
     -means
algorithm
is
much
faster
than
the
regular
     <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
      <mi>
       k
      </mi>
     </math>
     -means
algorithm,
its
inertia
is
generally
slightly
worse.
We
can
see
                                                                                
                                                                                
this
in
     <span id="bold" style="font-weight:bold;">
      Fig.
     </span>
     <a href="#x7-68011r7">
      5.7
     </a>.
The
plot
on
the
left
compares
the
inertiae
of
mini-batch
     <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
      <mi>
       k
      </mi>
     </math>
     -means
and
regular
     <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
      <mi>
       k
      </mi>
     </math>
     -means
models
trained
on
the
previous
five-blobs
dataset
using
various
numbers
of
clusters
     <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
      <mi>
       k
      </mi>
     </math>
     .
The
difference
between
the
two
curves
is
small,
but
visible.
In
the
plot
on
the
right,
we
can
see
that
mini-batch
     <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
      <mi>
       k
      </mi>
     </math>
     -means
is
roughly
1.5
- 2
times
faster
than
regular
     <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
      <mi>
       k
      </mi>
     </math>
     -means
on
this
dataset.
    </p>
    <div class="figure">
     <p class="noindent">
      <img alt="PIC" height="" src="codes/images/Unsupervised-Learning/minibatch-kmeans-vs-kmeans-plot-.svg" width="150%"/>
      <a id="x7-68011r7">
      </a>
     </p>
     <figcaption class="caption">
      <span class="id">
       Figure 5.7:
      </span>
      <span class="content">
       Mini-batch
       <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
        <mi>
         k
        </mi>
       </math>
       -means
         has a higher inertia than
       <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
        <mi>
         k
        </mi>
       </math>
       -means
         (left) but it is much faster (right), especially as
       <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
        <mi>
         k
        </mi>
       </math>
       increases.
      </span>
     </figcaption>
    </div>
    <a id="subsubsection*.11">
    </a>
    <h5 class="subsubsectionHead">
     <a id="x7-69000">
     </a>
     Finding the optimal number of clusters
    </h5>
    <p class="noindent">
     So far, we’ve set the number of clusters
     <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
      <mi>
       k
      </mi>
     </math>
     to five <alert style="color: #821131;">(5)</alert> as it was obvious by looking at the data that this was the correct number of clusters. But in general, it won’t be so easy to
know how to set
     <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
      <mi>
       k
      </mi>
     </math>
     ,
and the result might be quite bad if we set it to the wrong value. As we can see in
     <span id="bold" style="font-weight:bold;">
      Fig.
     </span>
     <a href="#x7-69001r8">
      5.8
     </a>, for this dataset setting
     <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
      <mi>
       k
      </mi>
     </math>
     to 3 or 8
results in fairly bad models.
    </p>
    <div class="figure">
     <p class="noindent">
      <img alt="PIC" height="" src="codes/images/Unsupervised-Learning/bad-cluster-plot-.svg" width="150%"/>
      <a id="x7-69001r8">
      </a>
     </p>
     <figcaption class="caption">
      <span class="id">
       Figure 5.8:
      </span>
      <span class="content">
       Bad choices for the number of clusters: when k is too small, separate clusters get merged (left), and when
         k is too large, some clusters get chopped into multiple pieces (right)
      </span>
     </figcaption>
    </div>
    <p class="noindent">
     We might be thinking that we could just pick the model with the lowest inertia. Unfortunately, it is not that simple. The inertia for
     <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
      <mi>
       k
      </mi>
      <mo class="MathClass-rel" stretchy="false">
       =
      </mo>
      <mn>
       3
      </mn>
     </math>
     is about 653.2, which is
much higher than for
     <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
      <mi>
       k
      </mi>
      <mo class="MathClass-rel" stretchy="false">
       =
      </mo>
      <mn>
       5
      </mn>
     </math>
     with a
value of 211.6. But with
     <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
      <mi>
       k
      </mi>
      <mo class="MathClass-rel" stretchy="false">
       =
      </mo>
      <mn>
       8
      </mn>
     </math>
     ,
the inertia is just 119.1.
    </p>
    <div class="warning">
     <p class="noindent">
      The inertia is not a good performance metric when trying to choose
      <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
       <mi>
        k
       </mi>
      </math>
      as it keeps getting
lower as we increase
      <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
       <mi>
        k
       </mi>
      </math>
      .
     </p>
    </div>
    Indeed, the more clusters there are, the closer each instance will be to its closest centroid, and
therefore the lower the inertia will be. To see this visually Let’s plot the inertia as a function of
    <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
     <mi>
      k
     </mi>
    </math>
    . When
we do this, the curve often contains an inflexion point called the elbow (see
    <span id="bold" style="font-weight:bold;">
     Fig.
    </span>
    <a href="#x7-69002r9">
     5.9
    </a>
    ).
    <div class="figure">
     <p class="noindent">
      <img alt="PIC" height="" src="codes/images/Unsupervised-Learning/inertia-vs-k-plot-.svg" width="150%"/>
      <a id="x7-69002r9">
      </a>
     </p>
     <figcaption class="caption">
      <span class="id">
       Figure 5.9:
      </span>
      <span class="content">
       Plotting the inertia as a function of the number of clusters k
      </span>
     </figcaption>
    </div>
    <p class="noindent">
     As we can see, the inertia drops very quickly as we increase
     <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
      <mi>
       k
      </mi>
     </math>
     up to 4,
but then it decreases much more slowly as we keep increasing k. This curve has roughly the shape of an arm, and there is an
elbow at
     <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
      <mi>
       k
      </mi>
     </math>
     = 4. So, if we did not know better, we might think 4 was a good choice as any lower value would be dramatic, while any
higher value would not help much, and we might just be splitting perfectly good clusters in half for no good
reason.
    </p>
    <p class="noindent">
     This technique for choosing the best value for the number of clusters is rather coarse. A more precise
     <alert style="color: #821131;">
      <span id="bold" style="font-weight:bold;">
       <sup class="textsuperscript">
        17
       </sup>
      </span>
     </alert>
     <span class="marginnote">
      <alert style="color: #821131;">
       <span id="bold" style="font-weight:bold;">
        <sup class="textsuperscript">
         17
        </sup>
       </span>
      </alert>
      <span style="color:#0063B2;">
       but also more
computationally expensive
      </span>
     </span>
     approach is to use the
     <alert style="color: #821131;">
      silhouette score
     </alert>
     , which is the mean silhouette coefficient over all the
instances. An instance’s silhouette coefficient is equal to:
    </p>
    <table class="equation-star">
     <tr>
      <td>
       <math class="equation" display="block" xmlns="http://www.w3.org/1998/Math/MathML">
        <mfrac>
         <mrow>
          <mi>
           b
          </mi>
          <mo class="MathClass-bin" stretchy="false">
           −
          </mo>
          <mi>
           a
          </mi>
         </mrow>
         <mrow>
          <mi class="qopname">
           max
          </mi>
          <mo>
           ⁡
          </mo>
          <msup>
           <mrow>
            <mrow>
             <mo fence="true" form="prefix">
              (
             </mo>
             <mrow>
              <mi>
               a
              </mi>
              <mo class="MathClass-punc" stretchy="false">
               ,
              </mo>
              <mi>
               b
              </mi>
             </mrow>
             <mo fence="true" form="postfix">
              )
             </mo>
            </mrow>
           </mrow>
           <mrow>
           </mrow>
          </msup>
         </mrow>
        </mfrac>
       </math>
      </td>
     </tr>
    </table>
    <p class="noindent">
     where
     <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
      <mi>
       a
      </mi>
     </math>
     is
the
mean
distance
to
the
other
instances
in
the
same
cluster
(i.e.,
the
mean
intra-cluster
distance)
and
     <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
      <mi>
       b
      </mi>
     </math>
     is
the
mean
nearest-cluster
distance
     <alert style="color: #821131;">
      <span id="bold" style="font-weight:bold;">
       <sup class="textsuperscript">
        18
       </sup>
      </span>
     </alert>
     <span class="marginnote">
      <alert style="color: #821131;">
       <span id="bold" style="font-weight:bold;">
        <sup class="textsuperscript">
         18
        </sup>
       </span>
      </alert>
      <span style="color:#0063B2;">
       the
mean
distance
to
the
instances
of
the
next
closest
cluster,
defined
as
                                                                                
                                                                                
the
one
that
minimizes
       <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
        <mi>
         b
        </mi>
       </math>
       ,
excluding
the
instance’s
own
cluster
      </span>
     </span>
     .
The
silhouette
coefficient
can
vary
between
-1
and
+1.
A
coefficient
close
to
+1
means
that
the
instance
is
well
inside
its
own
cluster
and
far
from
other
clusters,
while
a
coefficient
close
to
0
means
that
it
is
close
to
a
cluster
boundary
and
finally,
a
coefficient
close
to
-1
means
that
the
instance
may
have
                                                                                
                                                                                
been
assigned
to
the
wrong
cluster.
    </p>
    <p class="noindent">
     To
compute
the
silhouette
score,
we
can
use <span style="color:#054C5C;"><code class="verb">sklearn</code></span>
     ’s <span style="color:#054C5C;"><code class="verb">silhouette_score()</code></span>
     function,
giving
it
all
the
instances
in
the
dataset
and
the
labels
they
were
assigned:
    </p>
    <pre><div id="fancyvrb84" style="padding:20px;border-radius: 3px;"><a id="x7-69004r753"></a><span style="color:#2B2BFF;">from</span><span style="color:#BABABA;"> </span>sklearn.metrics<span style="color:#BABABA;"> </span><span style="color:#2B2BFF;">import</span> silhouette_score 
<a id="x7-69006r754"></a> 
<a id="x7-69008r755"></a>silhouette_score(X, kmeans.labels_)</div></pre>
    <div class="tcolorbox tcolorbox" id="tcolobox-42">
     <div class="tcolorbox-content">
      <pre><div id="fancyvrb85" style="padding:20px;border-radius: 3px;"><a id="x7-69010r1"></a>0.655517642572828</div></pre>
     </div>
    </div>
    <p class="noindent">
     Let’s
compare
the
silhouette
scores
for
different
numbers
of
clusters
(see
     <span id="bold" style="font-weight:bold;">
      Fig.
     </span>
     <a href="#x7-69011r10">
      5.10
     </a>
     ).
    </p>
    <div class="figure">
     <p class="noindent">
      <img alt="PIC" height="" src="codes/images/Unsupervised-Learning/silhouette-score-vs-k-plot-.svg" width="150%"/>
      <a id="x7-69011r10">
      </a>
     </p>
     <figcaption class="caption">
      <span class="id">
       Figure 5.10:
      </span>
      <span class="content">
       Selecting the number of clusters k using the silhouette score.
      </span>
     </figcaption>
    </div>
    <p class="noindent">
     As we can see, this visualisation gives more information compared to the previous one:
    </p>
    <div class="quoteblock">
     <p class="noindent">
      although
it
confirms
that
      <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
       <mi>
        k
       </mi>
       <mo class="MathClass-rel" stretchy="false">
        =
       </mo>
       <mn>
        4
       </mn>
      </math>
      is
a
very
good
choice,
it
also
highlights
the
fact
that
      <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
       <mi>
        k
       </mi>
       <mo class="MathClass-rel" stretchy="false">
        =
       </mo>
       <mn>
        5
       </mn>
      </math>
      is
quite
good
as
well,
at
least
much
better
than
      <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
       <mi>
        k
       </mi>
       <mo class="MathClass-rel" stretchy="false">
        =
       </mo>
       <mn>
        6
       </mn>
      </math>
      or
7.
     </p>
    </div>
    <div class="knowledge">
     <p class="noindent">
      This was not visible when comparing inertiae.
     </p>
    </div>
    <p class="noindent">
     An
even
more
informative
visualisation
is
obtained
when
we
plot
every
instance’s
silhouette
coefficient,
sorted
by
the
clusters
they
are
assigned
to
and
by
the
value
of
                                                                                
                                                                                
the
coefficient.
This
is
called
a
silhouette
diagram
(see
     <span id="bold" style="font-weight:bold;">
      Fig.
     </span>
     <a href="#x7-69012r11">
      5.11
     </a>
     ).
Each
diagram
contains
one
knife
shape
per
cluster.
The
shape’s
height
indicates
the
number
of
instances
in
the
cluster,
and
its
width
represents
the
sorted
silhouette
coefficients
of
the
instances
in
the
cluster
     <alert style="color: #821131;">
      <span id="bold" style="font-weight:bold;">
       <sup class="textsuperscript">
        19
       </sup>
      </span>
     </alert>
     <span class="marginnote">
      <alert style="color: #821131;">
       <span id="bold" style="font-weight:bold;">
        <sup class="textsuperscript">
         19
        </sup>
       </span>
      </alert>
      <span style="color:#0063B2;">
       wider
is
better.
      </span>
     </span>
     .
    </p>
    <p class="noindent">
     The
vertical
dashed
lines
represent
the
     <alert style="color: #821131;">
      mean
silhouette
score
     </alert>
     for
each
number
of
clusters.
When
most
instances
in
a
cluster
                                                                                
                                                                                
have
a
lower
coefficient
than
this
score
(i.e.,
if
many
of
the
instances
stop
short
of
the
dashed
line,
ending
to
the
left
of
it),
then
the
cluster
is
rather
bad
since
this
means
its
instances
are
much
too
close
to
other
clusters.
Here
we
can
see
that
when
k
=
3
or
6,
we
get
bad
clusters.
But
when
k
=
4
or
5,
the
clusters
                                                                                
                                                                                
look
pretty
good:
most
instances
extend
beyond
the
dashed
line,
to
the
right
and
closer
to
1.0.
    </p>
    <p class="noindent">
     When
k
=
4,
the
cluster
at
index
2
(the
second
from
the
top)
is
rather
big.
When
k
=
5,
all
clusters
have
similar
sizes.
So,
even
though
the
overall
silhouette
score
from
k
=
4
is
slightly
greater
than
for
k
=
5,
it
seems
like
a
good
                                                                                
                                                                                
idea
to
use
k
=
5
to
get
clusters
of
similar
sizes.
    </p>
    <div class="figure">
     <p class="noindent">
      <img alt="PIC" height="" src="codes/images/Unsupervised-Learning/silhouette-analysis-plot-.svg" width="150%"/>
      <a id="x7-69012r11">
      </a>
     </p>
     <figcaption class="caption">
      <span class="id">
       Figure 5.11:
      </span>
      <span class="content">
       Analyzing the silhouette diagrams for various values of k.
      </span>
     </figcaption>
    </div>
    <h3 class="subsectionHead">
     <a aria-label="Link to this section in PDF version" class="pdf-link" href="DataScienceIILectureBook.pdf#subsection.5.2.2" style="color: red;  position: absolute;  left: -30px;" target="_blank">
      <svg class="bi bi-file-pdf" fill="currentColor" height="20" viewbox="0 0 16 16" width="20" xmlns="http://www.w3.org/2000/svg">
       <path d="M4 0a2 2 0 0 0-2 2v12a2 2 0 0 0 2 2h8a2 2 0 0 0 2-2V2a2 2 0 0 0-2-2H4zm0 1h8a1 1 0 0 1 1 1v12a1 1 0 0 1-1 1H4a1 1 0 0 1-1-1V2a1 1 0 0 1 1-1z">
       </path>
       <path d="M4.603 12.087a.81.81 0 0 1-.438-.42c-.195-.388-.13-.776.08-1.102.198-.307.526-.568.897-.787a7.68 7.68 0 0 1 1.482-.645 19.701 19.701 0 0 0 1.062-2.227 7.269 7.269 0 0 1-.43-1.295c-.086-.4-.119-.796-.046-1.136.075-.354.274-.672.65-.823.192-.077.4-.12.602-.077a.7.7 0 0 1 .477.365c.088.164.12.356.127.538.007.187-.012.395-.047.614-.084.51-.27 1.134-.52 1.794a10.954 10.954 0 0 0 .98 1.686 5.753 5.753 0 0 1 1.334.05c.364.065.734.195.96.465.12.144.193.32.2.518.007.192-.047.382-.138.563a1.04 1.04 0 0 1-.354.416.856.856 0 0 1-.51.138c-.331-.014-.654-.196-.933-.417a5.716 5.716 0 0 1-.911-.95 11.642 11.642 0 0 0-1.997.406 11.311 11.311 0 0 1-1.021 1.51c-.29.35-.608.655-.926.787a.793.793 0 0 1-.58.029zm1.379-1.901c-.166.076-.32.156-.459.238-.328.194-.541.383-.647.547-.094.145-.096.25-.04.361.01.022.02.036.026.044a.27.27 0 0 0 .035-.012c.137-.056.355-.235.635-.572a8.18 8.18 0 0 0 .45-.606zm1.64-1.33a12.647 12.647 0 0 1 1.01-.193 11.666 11.666 0 0 1-.51-.858 20.741 20.741 0 0 1-.5 1.05zm2.446.45c.15.162.296.3.435.41.24.19.407.253.498.256a.107.107 0 0 0 .07-.015.307.307 0 0 0 .094-.125.436.436 0 0 0 .059-.2.095.095 0 0 0-.026-.063c-.052-.062-.2-.152-.518-.209a3.881 3.881 0 0 0-.612-.053zM8.078 5.8a6.7 6.7 0 0 0 .2-.828c.031-.188.043-.343.038-.465a.613.613 0 0 0-.032-.198.517.517 0 0 0-.145.04c-.087.035-.158.106-.196.283-.04.192-.03.469.046.822.024.111.054.227.09.346z">
       </path>
      </svg>
     </a>
     <small class="titlemark">
      5.2.2
     </small>
     <a id="x7-700005.2.2">
     </a>
     Limits of K-Means
    </h3>
    <p class="noindent">
     Despite its many merits, most notably being fast and scalable,
     <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
      <mi>
       k
      </mi>
     </math>
     -means
is not perfect. As we saw, it is necessary to run the algorithm several times to avoid sub-optimal
solutions, plus we need to specify the number of clusters, which can be quite a hassle. Moreover,
     <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
      <mi>
       k
      </mi>
     </math>
     -means does
not behave very well when the clusters have varying sizes, different densities, or non-spherical shapes. For example,
     <span id="bold" style="font-weight:bold;">
      Fig.
     </span>
     <a href="#x7-70001r12">
      5.12
     </a>
     shows
how
     <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
      <mi>
       k
      </mi>
     </math>
     -means
clusters a dataset containing three <alert style="color: #821131;">(3)</alert> ellipsoidal clusters of different dimensions, densities, and orientations.
    </p>
    <div class="figure">
     <p class="noindent">
      <img alt="PIC" height="" src="codes/images/Unsupervised-Learning/bad-kmeans-plot-.svg" width="150%"/>
      <a id="x7-70001r12">
      </a>
     </p>
     <figcaption class="caption">
      <span class="id">
       Figure 5.12:
      </span>
      <span class="content">
       <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
        <mi>
         k
        </mi>
       </math>
       -means
          fails to cluster these ellipsoidal blobs properly.
      </span>
     </figcaption>
    </div>
    <p class="noindent">
     As can be seen, neither of these solutions is any good. The solution on the left is better, but it still chops off 25% of the middle
cluster and assigns it to the cluster on the right. The solution on the right is just terrible, even though its inertia is lower. So,
depending on the data, different clustering algorithms may perform better. On these types of elliptical clusters,
     <span id="bold" style="font-weight:bold;">
      Gaussian
mixture models
     </span>
     work great.
    </p>
    <div class="knowledge">
     <p class="noindent">
      It is important to scale the input features before we run
      <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
       <mi>
        k
       </mi>
      </math>
      -means, or the clusters may
be very stretched and
      <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
       <mi>
        k
       </mi>
      </math>
      -means
will perform poorly. Scaling the features does not guarantee that all the clusters will be nice and spherical, but it generally helps
      <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
       <mi>
        k
       </mi>
      </math>
      -means.
     </p>
    </div>
    <p class="noindent">
     Now let’s look at a few ways we can benefit from clustering and for these will use
     <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
      <mi>
       k
      </mi>
     </math>
     -means
    </p>
    <h3 class="subsectionHead">
     <a aria-label="Link to this section in PDF version" class="pdf-link" href="DataScienceIILectureBook.pdf#subsection.5.2.3" style="color: red;  position: absolute;  left: -30px;" target="_blank">
      <svg class="bi bi-file-pdf" fill="currentColor" height="20" viewbox="0 0 16 16" width="20" xmlns="http://www.w3.org/2000/svg">
       <path d="M4 0a2 2 0 0 0-2 2v12a2 2 0 0 0 2 2h8a2 2 0 0 0 2-2V2a2 2 0 0 0-2-2H4zm0 1h8a1 1 0 0 1 1 1v12a1 1 0 0 1-1 1H4a1 1 0 0 1-1-1V2a1 1 0 0 1 1-1z">
       </path>
       <path d="M4.603 12.087a.81.81 0 0 1-.438-.42c-.195-.388-.13-.776.08-1.102.198-.307.526-.568.897-.787a7.68 7.68 0 0 1 1.482-.645 19.701 19.701 0 0 0 1.062-2.227 7.269 7.269 0 0 1-.43-1.295c-.086-.4-.119-.796-.046-1.136.075-.354.274-.672.65-.823.192-.077.4-.12.602-.077a.7.7 0 0 1 .477.365c.088.164.12.356.127.538.007.187-.012.395-.047.614-.084.51-.27 1.134-.52 1.794a10.954 10.954 0 0 0 .98 1.686 5.753 5.753 0 0 1 1.334.05c.364.065.734.195.96.465.12.144.193.32.2.518.007.192-.047.382-.138.563a1.04 1.04 0 0 1-.354.416.856.856 0 0 1-.51.138c-.331-.014-.654-.196-.933-.417a5.716 5.716 0 0 1-.911-.95 11.642 11.642 0 0 0-1.997.406 11.311 11.311 0 0 1-1.021 1.51c-.29.35-.608.655-.926.787a.793.793 0 0 1-.58.029zm1.379-1.901c-.166.076-.32.156-.459.238-.328.194-.541.383-.647.547-.094.145-.096.25-.04.361.01.022.02.036.026.044a.27.27 0 0 0 .035-.012c.137-.056.355-.235.635-.572a8.18 8.18 0 0 0 .45-.606zm1.64-1.33a12.647 12.647 0 0 1 1.01-.193 11.666 11.666 0 0 1-.51-.858 20.741 20.741 0 0 1-.5 1.05zm2.446.45c.15.162.296.3.435.41.24.19.407.253.498.256a.107.107 0 0 0 .07-.015.307.307 0 0 0 .094-.125.436.436 0 0 0 .059-.2.095.095 0 0 0-.026-.063c-.052-.062-.2-.152-.518-.209a3.881 3.881 0 0 0-.612-.053zM8.078 5.8a6.7 6.7 0 0 0 .2-.828c.031-.188.043-.343.038-.465a.613.613 0 0 0-.032-.198.517.517 0 0 0-.145.04c-.087.035-.158.106-.196.283-.04.192-.03.469.046.822.024.111.054.227.09.346z">
       </path>
      </svg>
     </a>
     <small class="titlemark">
      5.2.3
     </small>
     <a id="x7-710005.2.3">
     </a>
     Using Clustering for Image Segmentation
    </h3>
    <p class="noindent">
     Image segmentation is the task of partitioning an image into
     <alert style="color: #821131;">
      multiple segments
     </alert>
     . There are several variants: In color
segmentation,
    </p>
    <dl class="description">
     <dt class="description">
      <a id="x7-71001x5.2.3">
      </a>
      Colour Segmentation
     </dt>
     <dd class="description">
      <p class="noindent">
       pixels with a similar color get assigned to the same segment. This is sufficient in many applications.
      </p>
      <div class="knowledge">
       <p class="noindent">
        For example, if we want to analyze satellite images to measure how much total forest area there is in a region, color
segmentation may be just fine.
       </p>
      </div>
     </dd>
     <dt class="description">
      <a id="x7-71002x5.2.3">
      </a>
      Semantic Segmentation
     </dt>
     <dd class="description">
      <p class="noindent">
       all pixels that are part of the same object type get assigned to the same segment.
      </p>
      <div class="knowledge">
       <p class="noindent">
        For example, in a self-driving car’s vision system, all pixels that are part of a pedestrian’s image might be assigned to the
        <alert style="color: #821131;">
         pedestrian
        </alert>
        segment.
       </p>
      </div>
     </dd>
     <dt class="description">
      <a id="x7-71003x5.2.3">
      </a>
      Instance Segmentation
     </dt>
     <dd class="description">
      <p class="noindent">
       all pixels that are part of the same individual object are assigned to the same segment. In this case there would be a
different segment for each pedestrian.
      </p>
     </dd>
    </dl>
    <p class="noindent">
     The
state
of
the
art
in
semantic
or
instance
segmentation
today
is
achieved
using
complex
architectures
based
on
     <a class="glossary" href="DataScienceIILectureBookli2.html#glo:cnn">
      Convolutional
Neural
Networks
(CNN)
     </a>
     <alert style="color: #821131;">
      <span id="bold" style="font-weight:bold;">
       <sup class="textsuperscript">
        20
       </sup>
      </span>
     </alert>
     <span class="marginnote">
      <alert style="color: #821131;">
       <span id="bold" style="font-weight:bold;">
        <sup class="textsuperscript">
         20
        </sup>
       </span>
      </alert>
      <span style="color:#0063B2;">
       Which
                                                                                
                                                                                
is
taught
in
       <span id="bold" style="font-weight:bold;">
        B.Sc.
Image
processing
       </span>
       .
      </span>
     </span>
     .
Here
we
are
going
to
focus
on
the
colour
segmentation
task,
using
     <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
      <mi>
       k
      </mi>
     </math>
     -means.
We’ll
start
by
importing
the
Pillow
package,
which
we’ll
then
use
to
load
the <span style="color:#054C5C;"><code class="verb">Fruit.png</code></span>
     image
(see
the
upper-left
image
in
     <span id="bold" style="font-weight:bold;">
      Fig.
     </span>
     <a href="#x7-71040r13">
      5.13
     </a>
     ),
assuming
it’s
located
at
filepath:
    </p>
    <p class="noindent">
    </p>
    <pre><div id="fancyvrb86" style="padding:20px;border-radius: 3px;"><a id="x7-71005r912"></a><span style="color:#2B2BFF;">import</span><span style="color:#BABABA;"> </span>PIL 
<a id="x7-71007r913"></a> 
<a id="x7-71009r914"></a>image = np.asarray(PIL.Image.open(filepath)) 
<a id="x7-71011r915"></a><span style="color:#2B2BFF;">print</span>(image.shape)</div></pre>
    <div class="tcolorbox tcolorbox" id="tcolobox-43">
     <div class="tcolorbox-content">
      <pre><div id="fancyvrb87" style="padding:20px;border-radius: 3px;"><a id="x7-71013r1"></a>(680, 680, 3)</div></pre>
     </div>
    </div>
    <p class="noindent">
     The
image
is
represented
as
a
3D
array.
The
first
                                                                                
                                                                                
dimension’s
size
is
the
height,
the
second
is
the
width,
and
the
third
is
the
number
of
color
channels,
in
this
case
red,
green,
and
blue
(RGB).
In
other
words,
for
each
pixel
there
is
a
3D
vector
containing
the
intensities
of
red,
green,
and
blue
as
unsigned
8-
bit
integers
between
0
and
255.
Some
images
may
have
fewer
channels
     <alert style="color: #821131;">
      <span id="bold" style="font-weight:bold;">
       <sup class="textsuperscript">
        21
       </sup>
      </span>
     </alert>
     <span class="marginnote">
      <alert style="color: #821131;">
       <span id="bold" style="font-weight:bold;">
        <sup class="textsuperscript">
         21
        </sup>
       </span>
      </alert>
      <span style="color:#0063B2;">
       such
as
grayscale
images,
which
                                                                                
                                                                                
only
have
one.
      </span>
     </span>
     ,
and
some
images
may
have
more
channels
(such
as
images
with
an
additional
alpha
channel
for
transparency,
or
satellite
images,
which
often
contain
channels
for
additional
light
frequencies
(like
infrared).
The
following
code
reshapes
the
array
to
get
a
long
list
of
RGB
colors,
then
it
clusters
these
colours
using
     <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
      <mi>
       k
      </mi>
     </math>
     -means
with
eight
clusters.
It
creates
a <span style="color:#054C5C;"><code class="verb">segmented_img</code></span>
     array
containing
the
nearest
cluster
centre
                                                                                
                                                                                
for
each
pixel
(i.e.,
the
mean
colour
of
each
pixel’s
cluster),
and
lastly
it
reshapes
this
array
to
the
original
image
shape.
The
third
line
uses
advanced
NumPy
indexing;
for
example,
if
the
first
10
labels
in <span style="color:#054C5C;"><code class="verb">kmeans_.labels_</code></span>
     are
equal
to
1,
then
the
first
10
colors
in <span style="color:#054C5C;"><code class="verb">segmented_img</code></span>
     are
equal
to <span style="color:#054C5C;"><code class="verb">kmeans.cluster_centers_[1]:</code></span>
    </p>
    <p class="noindent">
    </p>
    <pre><div id="fancyvrb88" style="padding:20px;border-radius: 3px;"><a id="x7-71015r929"></a>X = image.reshape(-<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">1</span></span>, <span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">3</span></span>) 
<a id="x7-71017r930"></a>kmeans = KMeans(n_clusters=<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">8</span></span>, n_init=<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">10</span></span>, random_state=<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">42</span></span>).fit(X) 
<a id="x7-71019r931"></a>segmented_img = kmeans.cluster_centers_[kmeans.labels_] 
<a id="x7-71021r932"></a>segmented_img = segmented_img.reshape(image.shape) 
<a id="x7-71023r933"></a> 
<a id="x7-71025r934"></a>segmented_imgs = [] 
<a id="x7-71027r935"></a>n_colors = (<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">10</span></span>, <span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">8</span></span>, <span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">6</span></span>, <span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">4</span></span>, <span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">2</span></span>) 
<a id="x7-71029r936"></a> 
<a id="x7-71031r937"></a><span style="color:#2B2BFF;">for</span> n_clusters in n_colors: 
<a id="x7-71033r938"></a>   kmeans = KMeans(n_clusters=n_clusters, n_init=<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">10</span></span>, random_state=<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">42</span></span>).fit(X) 
<a id="x7-71035r939"></a>   segmented_img = kmeans.cluster_centers_[kmeans.labels_] 
<a id="x7-71037r940"></a>   segmented_imgs.append(segmented_img.reshape(image.shape)) 
<a id="x7-71039r941"></a></div></pre>
    <p class="noindent">
     This
outputs
the
image
shown
in
the
upper
right
of
     <span id="bold" style="font-weight:bold;">
      Fig.
     </span>
     <a href="#x7-71040r13">
      5.13
     </a>.
We
can
experiment
with
various
numbers
of
clusters,
as
shown
in
the
figure.
When
we
use
fewer
than
eight
clusters,
notice
that
the
ladybug’s
flashy
red
color
fails
to
get
a
cluster
of
its
own:
it
gets
merged
with
colors
from
the
environment.
This
is
because
     <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
      <mi>
       k
      </mi>
     </math>
     -means
prefers
clusters
of
similar
sizes.
The
ladybug
is
small-much
smaller
than
the
rest
of
the
image-so
even
though
its
                                                                                
                                                                                
colour
is
flashy,
     <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
      <mi>
       k
      </mi>
     </math>
     -means
fails
to
dedicate
a
cluster
to
it.
    </p>
    <div class="figure">
     <p class="noindent">
      <img alt="PIC" height="" src="codes/images/Unsupervised-Learning/image-segmentation-plot-.svg" width="150%"/>
      <a id="x7-71040r13">
      </a>
     </p>
     <figcaption class="caption">
      <span class="id">
       Figure 5.13:
      </span>
      <span class="content">
       Image segmentation using
       <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
        <mi>
         k
        </mi>
       </math>
       -means
          with various numbers of color clusters
      </span>
     </figcaption>
    </div>
    <p class="noindent">
     Now this looks very pretty. Now it is time to look at another application of clustering.
    </p>
    <h3 class="subsectionHead">
     <a aria-label="Link to this section in PDF version" class="pdf-link" href="DataScienceIILectureBook.pdf#subsection.5.2.4" style="color: red;  position: absolute;  left: -30px;" target="_blank">
      <svg class="bi bi-file-pdf" fill="currentColor" height="20" viewbox="0 0 16 16" width="20" xmlns="http://www.w3.org/2000/svg">
       <path d="M4 0a2 2 0 0 0-2 2v12a2 2 0 0 0 2 2h8a2 2 0 0 0 2-2V2a2 2 0 0 0-2-2H4zm0 1h8a1 1 0 0 1 1 1v12a1 1 0 0 1-1 1H4a1 1 0 0 1-1-1V2a1 1 0 0 1 1-1z">
       </path>
       <path d="M4.603 12.087a.81.81 0 0 1-.438-.42c-.195-.388-.13-.776.08-1.102.198-.307.526-.568.897-.787a7.68 7.68 0 0 1 1.482-.645 19.701 19.701 0 0 0 1.062-2.227 7.269 7.269 0 0 1-.43-1.295c-.086-.4-.119-.796-.046-1.136.075-.354.274-.672.65-.823.192-.077.4-.12.602-.077a.7.7 0 0 1 .477.365c.088.164.12.356.127.538.007.187-.012.395-.047.614-.084.51-.27 1.134-.52 1.794a10.954 10.954 0 0 0 .98 1.686 5.753 5.753 0 0 1 1.334.05c.364.065.734.195.96.465.12.144.193.32.2.518.007.192-.047.382-.138.563a1.04 1.04 0 0 1-.354.416.856.856 0 0 1-.51.138c-.331-.014-.654-.196-.933-.417a5.716 5.716 0 0 1-.911-.95 11.642 11.642 0 0 0-1.997.406 11.311 11.311 0 0 1-1.021 1.51c-.29.35-.608.655-.926.787a.793.793 0 0 1-.58.029zm1.379-1.901c-.166.076-.32.156-.459.238-.328.194-.541.383-.647.547-.094.145-.096.25-.04.361.01.022.02.036.026.044a.27.27 0 0 0 .035-.012c.137-.056.355-.235.635-.572a8.18 8.18 0 0 0 .45-.606zm1.64-1.33a12.647 12.647 0 0 1 1.01-.193 11.666 11.666 0 0 1-.51-.858 20.741 20.741 0 0 1-.5 1.05zm2.446.45c.15.162.296.3.435.41.24.19.407.253.498.256a.107.107 0 0 0 .07-.015.307.307 0 0 0 .094-.125.436.436 0 0 0 .059-.2.095.095 0 0 0-.026-.063c-.052-.062-.2-.152-.518-.209a3.881 3.881 0 0 0-.612-.053zM8.078 5.8a6.7 6.7 0 0 0 .2-.828c.031-.188.043-.343.038-.465a.613.613 0 0 0-.032-.198.517.517 0 0 0-.145.04c-.087.035-.158.106-.196.283-.04.192-.03.469.046.822.024.111.054.227.09.346z">
       </path>
      </svg>
     </a>
     <small class="titlemark">
      5.2.4
     </small>
     <a id="x7-720005.2.4">
     </a>
     Using Clustering for Semi-Supervised Learning
    </h3>
    <p class="noindent">
     Another use case for clustering is in
     <alert style="color: #821131;">
      semi-supervised learning
     </alert>
     , when we have plenty of unlabelled instances and very few
labelled instances. In this section, we’ll use the digits dataset, which is a simple
     <a class="glossary" href="DataScienceIILectureBookli2.html#glo:mnist">
      MNIST
     </a>
     -like dataset containing
1,797 grayscale 8-by-8 images representing the digits 0 to 9. First, let’s load and split the dataset (it’s already
shuffled):
    </p>
    <p class="noindent">
    </p>
    <pre><div id="fancyvrb89" style="padding:20px;border-radius: 3px;"><a id="x7-72002r982"></a><span style="color:#2B2BFF;">from</span><span style="color:#BABABA;"> </span>sklearn.datasets<span style="color:#BABABA;"> </span><span style="color:#2B2BFF;">import</span> load_digits 
<a id="x7-72004r983"></a> 
<a id="x7-72006r984"></a>X_digits, y_digits = load_digits(return_X_y=<span style="color:#2B2BFF;"><span id="bold" style="font-weight:bold;">True</span></span>) 
<a id="x7-72008r985"></a>X_train, y_train = X_digits[:<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">1400</span></span>], y_digits[:<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">1400</span></span>] 
<a id="x7-72010r986"></a>X_test, y_test = X_digits[<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">1400</span></span>:], y_digits[<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">1400</span></span>:]</div></pre>
    <p class="noindent">
     We will pretend we only have labels for 50 instances. To get a baseline performance, let’s train a logistic regression model on
these 50 labelled instances:
    </p>
    <p class="noindent">
    </p>
    <pre><div id="fancyvrb90" style="padding:20px;border-radius: 3px;"><a id="x7-72012r993"></a><span style="color:#2B2BFF;">from</span><span style="color:#BABABA;"> </span>sklearn.linear_model<span style="color:#BABABA;"> </span><span style="color:#2B2BFF;">import</span> LogisticRegression 
<a id="x7-72014r994"></a> 
<a id="x7-72016r995"></a>n_labeled = <span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">50</span></span> 
<a id="x7-72018r996"></a>log_reg = LogisticRegression(max_iter=<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">10_000</span></span>) 
<a id="x7-72020r997"></a>log_reg.fit(X_train[:n_labeled], y_train[:n_labeled])</div></pre>
    <p class="noindent">
     We can then measure the accuracy of this model on the test set:
    </p>
    <div class="warning">
     <p class="noindent">
      The test set must be labelled:
     </p>
    </div>
    <p class="noindent">
    </p>
    <pre><div id="fancyvrb91" style="padding:20px;border-radius: 3px;"><a id="x7-72022r1011"></a>log_reg.score(X_test, y_test)</div></pre>
    <div class="tcolorbox tcolorbox" id="tcolobox-44">
     <div class="tcolorbox-content">
      <pre><div id="fancyvrb92" style="padding:20px;border-radius: 3px;"><a id="x7-72024r1"></a>0.7581863979848866</div></pre>
     </div>
    </div>
    <p class="noindent">
     The model’s accuracy is just 75.8%. That’s not great: indeed, if we try training the model on the full training set, we will find
that it will reach about 90.9% accuracy. Let’s see how we can do better. First, let’s cluster the training set into 50 clusters. Then,
for each cluster, we’ll find the image closest to the centroid. We’ll call these images the representative images where we can see 50
of them in
     <span id="bold" style="font-weight:bold;">
      Fig.
     </span>
     <a href="#x7-72035r14">
      5.14
     </a>
    </p>
    <p class="noindent">
    </p>
    <pre><div id="fancyvrb93" style="padding:20px;border-radius: 3px;"><a id="x7-72026r1038"></a>k = <span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">50</span></span> 
<a id="x7-72028r1039"></a>kmeans = KMeans(n_clusters=k, n_init=<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">10</span></span>, random_state=<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">42</span></span>) 
<a id="x7-72030r1040"></a>X_digits_dist = kmeans.fit_transform(X_train) 
<a id="x7-72032r1041"></a>representative_digit_idx = X_digits_dist.argmin(axis=<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">0</span></span>) 
<a id="x7-72034r1042"></a>X_representative_digits = X_train[representative_digit_idx]</div></pre>
    <div class="figure">
     <p class="noindent">
      <img alt="PIC" height="" src="codes/images/Unsupervised-Learning/representative-images-plot-.svg" width="150%"/>
      <a id="x7-72035r14">
      </a>
     </p>
     <figcaption class="caption">
      <span class="id">
       Figure 5.14:
      </span>
      <span class="content">
       Fifty representative digit images (one per cluster).
      </span>
     </figcaption>
    </div>
    <p class="noindent">
     Let’s look at each image and manually label them:
    </p>
    <pre><div id="fancyvrb94" style="padding:20px;border-radius: 3px;"><a id="x7-72037r1063"></a>y_representative_digits = np.array([ 
<a id="x7-72039r1064"></a>   <span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">8</span></span>, <span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">4</span></span>, <span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">9</span></span>, <span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">6</span></span>, <span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">7</span></span>, <span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">5</span></span>, <span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">3</span></span>, <span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">0</span></span>, <span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">1</span></span>, <span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">2</span></span>, 
<a id="x7-72041r1065"></a>   <span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">3</span></span>, <span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">3</span></span>, <span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">4</span></span>, <span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">7</span></span>, <span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">2</span></span>, <span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">1</span></span>, <span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">5</span></span>, <span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">1</span></span>, <span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">6</span></span>, <span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">4</span></span>, 
<a id="x7-72043r1066"></a>   <span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">5</span></span>, <span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">6</span></span>, <span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">5</span></span>, <span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">7</span></span>, <span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">3</span></span>, <span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">1</span></span>, <span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">0</span></span>, <span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">8</span></span>, <span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">4</span></span>, <span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">7</span></span>, 
<a id="x7-72045r1067"></a>   <span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">1</span></span>, <span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">1</span></span>, <span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">8</span></span>, <span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">2</span></span>, <span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">9</span></span>, <span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">9</span></span>, <span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">5</span></span>, <span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">9</span></span>, <span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">7</span></span>, <span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">4</span></span>, 
<a id="x7-72047r1068"></a>   <span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">4</span></span>, <span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">9</span></span>, <span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">7</span></span>, <span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">8</span></span>, <span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">2</span></span>, <span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">6</span></span>, <span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">6</span></span>, <span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">3</span></span>, <span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">2</span></span>, <span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">8</span></span> 
<a id="x7-72049r1069"></a>])</div></pre>
    <p class="noindent">
     Now we have a dataset with just 50 labelled instances, but instead of being random instances, each of them is a representative
image of its cluster. Let’s see if the performance is any better:
    </p>
    <pre><div id="fancyvrb95" style="padding:20px;border-radius: 3px;"><a id="x7-72051r1077"></a>log_reg = LogisticRegression(max_iter=<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">10_000</span></span>) 
<a id="x7-72053r1078"></a>log_reg.fit(X_representative_digits, y_representative_digits) 
<a id="x7-72055r1079"></a>log_reg.score(X_test, y_test)</div></pre>
    <div class="tcolorbox tcolorbox" id="tcolobox-45">
     <div class="tcolorbox-content">
      <pre><div id="fancyvrb96" style="padding:20px;border-radius: 3px;"><a id="x7-72057r1"></a>0.8387909319899244</div></pre>
     </div>
    </div>
    <p class="noindent">
     Wow! We jumped from 75.8% accuracy to 83.8%, although we are still only training the model on 50 instances.
Since it is often costly and painful to label instances, especially when it has to be done manually by experts, it
is a good idea to label representative instances rather than just random instances. But perhaps we can go one
step further: what if we propagated the labels to all the other instances in the same cluster? This is called
     <alert style="color: #821131;">
      label
propagation
     </alert>
     :
    </p>
    <pre><div id="fancyvrb97" style="padding:20px;border-radius: 3px;"><a id="x7-72059r1095"></a>y_train_propagated = np.empty(<span style="color:#2B2BFF;">len</span>(X_train), dtype=np.int64) 
<a id="x7-72061r1096"></a><span style="color:#2B2BFF;">for</span> i in <span style="color:#2B2BFF;">range</span>(k): 
<a id="x7-72063r1097"></a>   y_train_propagated[kmeans.labels_ == i] = y_representative_digits[i]</div></pre>
    <p class="noindent">
     Now let’s train the model again and look at its performance:
    </p>
    <pre><div id="fancyvrb98" style="padding:20px;border-radius: 3px;"><a id="x7-72065r1102"></a>log_reg = LogisticRegression(max_iter=<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">10_000</span></span>) 
<a id="x7-72067r1103"></a>log_reg.fit(X_train, y_train_propagated)</div></pre>
    <pre><div id="fancyvrb99" style="padding:20px;border-radius: 3px;"><a id="x7-72069r1115"></a>log_reg.score(X_test, y_test)</div></pre>
    <div class="tcolorbox tcolorbox" id="tcolobox-46">
     <div class="tcolorbox-content">
      <pre><div id="fancyvrb100" style="padding:20px;border-radius: 3px;"><a id="x7-72071r1"></a>0.8589420654911839</div></pre>
     </div>
    </div>
    <p class="noindent">
     We got another significant accuracy boost.
    </p>
    <p class="noindent">
     <span id="bold" style="font-weight:bold;">
      Active Learning
     </span>
    </p>
    <div class="informationblock" id="tcolobox-47">
     <div class="title">
      <p class="noindent">
       <span id="bold" style="font-weight:bold;">
        Information
       </span>
       : Active Learning
      </p>
     </div>
     <div class="box-content">
      <p class="noindent">
       To continue improving our model and our training set, the next step could be to do a few rounds of active learning,
which is when a human expert interacts with the learning algorithm, providing labels for specific instances when the
algorithm requests them. There are many different strategies for active learning, but one of the most common ones is
called
       <alert style="color: #821131;">
        uncertainty sampling
       </alert>
       . Here is how it works:
      </p>
      <dl class="enumerate-enumitem">
       <dt class="enumerate-enumitem">
        1.
       </dt>
       <dd class="enumerate-enumitem">
        The model is trained on the labelled instances gathered so far, and this model is used to make predictions 
on all the unlabeled instances.
       </dd>
       <dt class="enumerate-enumitem">
        2.
       </dt>
       <dd class="enumerate-enumitem">
        The instances for which the model is most uncertain( i.e.,where its estimated probability is lowest) are 
given to the expert for labelling.
       </dd>
       <dt class="enumerate-enumitem">
        3.
       </dt>
       <dd class="enumerate-enumitem">
        We iterate this process until the performance improvement stops being worth the labeling effort.
       </dd>
      </dl>
      <p class="noindent">
       Other active learning strategies include labeling the instances that would result in the largest model change or the
largest drop in the model’s validation error, or the instances that different models disagree on (e.g., an
       <a class="glossary" href="DataScienceIILectureBookli2.html#glo:svm">
        SVM
       </a>
       and a
random forest).
      </p>
     </div>
    </div>
    <p class="noindent">
     Before we move on to Gaussian mixture models, let’s take a look at
     <a class="glossary" href="DataScienceIILectureBookli2.html#glo:dbscan">
      Density-Based Spatial Clustering of Applications with Noise
(DBSCAN)
     </a>, another popular clustering algorithm that illustrates a very different approach based on local density estimation.
This approach allows the algorithm to identify clusters of arbitrary shapes.
    </p>
    <h3 class="subsectionHead">
     <a aria-label="Link to this section in PDF version" class="pdf-link" href="DataScienceIILectureBook.pdf#subsection.5.2.5" style="color: red;  position: absolute;  left: -30px;" target="_blank">
      <svg class="bi bi-file-pdf" fill="currentColor" height="20" viewbox="0 0 16 16" width="20" xmlns="http://www.w3.org/2000/svg">
       <path d="M4 0a2 2 0 0 0-2 2v12a2 2 0 0 0 2 2h8a2 2 0 0 0 2-2V2a2 2 0 0 0-2-2H4zm0 1h8a1 1 0 0 1 1 1v12a1 1 0 0 1-1 1H4a1 1 0 0 1-1-1V2a1 1 0 0 1 1-1z">
       </path>
       <path d="M4.603 12.087a.81.81 0 0 1-.438-.42c-.195-.388-.13-.776.08-1.102.198-.307.526-.568.897-.787a7.68 7.68 0 0 1 1.482-.645 19.701 19.701 0 0 0 1.062-2.227 7.269 7.269 0 0 1-.43-1.295c-.086-.4-.119-.796-.046-1.136.075-.354.274-.672.65-.823.192-.077.4-.12.602-.077a.7.7 0 0 1 .477.365c.088.164.12.356.127.538.007.187-.012.395-.047.614-.084.51-.27 1.134-.52 1.794a10.954 10.954 0 0 0 .98 1.686 5.753 5.753 0 0 1 1.334.05c.364.065.734.195.96.465.12.144.193.32.2.518.007.192-.047.382-.138.563a1.04 1.04 0 0 1-.354.416.856.856 0 0 1-.51.138c-.331-.014-.654-.196-.933-.417a5.716 5.716 0 0 1-.911-.95 11.642 11.642 0 0 0-1.997.406 11.311 11.311 0 0 1-1.021 1.51c-.29.35-.608.655-.926.787a.793.793 0 0 1-.58.029zm1.379-1.901c-.166.076-.32.156-.459.238-.328.194-.541.383-.647.547-.094.145-.096.25-.04.361.01.022.02.036.026.044a.27.27 0 0 0 .035-.012c.137-.056.355-.235.635-.572a8.18 8.18 0 0 0 .45-.606zm1.64-1.33a12.647 12.647 0 0 1 1.01-.193 11.666 11.666 0 0 1-.51-.858 20.741 20.741 0 0 1-.5 1.05zm2.446.45c.15.162.296.3.435.41.24.19.407.253.498.256a.107.107 0 0 0 .07-.015.307.307 0 0 0 .094-.125.436.436 0 0 0 .059-.2.095.095 0 0 0-.026-.063c-.052-.062-.2-.152-.518-.209a3.881 3.881 0 0 0-.612-.053zM8.078 5.8a6.7 6.7 0 0 0 .2-.828c.031-.188.043-.343.038-.465a.613.613 0 0 0-.032-.198.517.517 0 0 0-.145.04c-.087.035-.158.106-.196.283-.04.192-.03.469.046.822.024.111.054.227.09.346z">
       </path>
      </svg>
     </a>
     <small class="titlemark">
      5.2.5
     </small>
     <a id="x7-730005.2.5">
     </a>
     DBSCAN
    </h3>
    <p class="noindent">
     The
     <a class="glossary" href="DataScienceIILectureBookli2.html#glo:dbscan">
      DBSCAN
     </a>
     algorithm defines clusters as continuous regions of high density. Here is how it works:
    </p>
    <dl class="enumerate-enumitem">
     <dt class="enumerate-enumitem">
      1.
     </dt>
     <dd class="enumerate-enumitem">
      For
each
instance,
the
algorithm
counts
how
many
instances
are
located
within
a
small
distance
      <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
       <mi>
        𝜖
       </mi>
      </math>
      from
it.
This
region
is
called
the
instance’s
      <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
       <mi>
        𝜖
       </mi>
      </math>
      -neighbourhood.
     </dd>
     <dt class="enumerate-enumitem">
      2.
     </dt>
     <dd class="enumerate-enumitem">
      If
an
instance
has
at
least <span style="color:#054C5C;"><code class="verb">min_samples</code></span>
      instances
in
its
      <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
       <mi>
        𝜖
       </mi>
      </math>
      -neighborhood
(including
itself),
then
it
is
considered
a
core
instance.
In
other
words,
core
instances
are
those
that
are
located
in
dense
regions.
     </dd>
     <dt class="enumerate-enumitem">
      3.
     </dt>
     <dd class="enumerate-enumitem">
      All
instances
in
the
neighborhood
of
a
core
instance
belong
to
the
same
cluster.
This
neighbourhood
may
include
other
core
instances,
therefore,
a
long
sequence
of
neighboring
core
instances
forms
a
single
cluster.
     </dd>
     <dt class="enumerate-enumitem">
      4.
     </dt>
     <dd class="enumerate-enumitem">
      Any
instance
that
is
not
a
core
instance
and
does
not
have
one
in
its
neighborhood
is
considered
an
      <alert style="color: #821131;">
       anomaly
      </alert>
      .
     </dd>
    </dl>
    <p class="noindent">
     This algorithm works well if all the clusters are well separated by low-density regions. The
     <a class="glossary" href="DataScienceIILectureBookli2.html#glo:dbscan">
      DBSCAN
     </a>
     class in <span style="color:#054C5C;"><code class="verb">sklearn</code></span>
     is as simple
to use as we might expect. Let’s test it on the moons dataset, which is a toy dataset:
    </p>
    <p class="noindent">
    </p>
    <pre><div id="fancyvrb101" style="padding:20px;border-radius: 3px;"><a id="x7-73006r1164"></a><span style="color:#2B2BFF;">from</span><span style="color:#BABABA;"> </span>sklearn.cluster<span style="color:#BABABA;"> </span><span style="color:#2B2BFF;">import</span> DBSCAN 
<a id="x7-73008r1165"></a><span style="color:#2B2BFF;">from</span><span style="color:#BABABA;"> </span>sklearn.datasets<span style="color:#BABABA;"> </span><span style="color:#2B2BFF;">import</span> make_moons 
<a id="x7-73010r1166"></a> 
<a id="x7-73012r1167"></a>X, y = make_moons(n_samples=<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">1000</span></span>, noise=<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">0.05</span></span>, random_state=<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">42</span></span>) 
<a id="x7-73014r1168"></a>dbscan = DBSCAN(eps=<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">0.05</span></span>, min_samples=<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">5</span></span>) 
<a id="x7-73016r1169"></a>dbscan.fit(X)</div></pre>
    <p class="noindent">
     The labels of all the instances are now available in the <span style="color:#054C5C;"><code class="verb">labels_</code></span>
     instance variable:
    </p>
    <p class="noindent">
    </p>
    <pre><div id="fancyvrb102" style="padding:20px;border-radius: 3px;"><a id="x7-73018r1181"></a><span style="color:#2B2BFF;">print</span>(dbscan.labels_[:<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">10</span></span>])</div></pre>
    <div class="tcolorbox tcolorbox" id="tcolobox-48">
     <div class="tcolorbox-content">
      <pre><div id="fancyvrb103" style="padding:20px;border-radius: 3px;"><a id="x7-73020r1"></a>[ 0  2 -1 -1  1  0  0  0  2  5]</div></pre>
     </div>
    </div>
    <p class="noindent">
     Notice that some instances have a cluster index equal to -1, which means that they are considered as anomalies by the algorithm.
The core instances indices are available in the <span style="color:#054C5C;"><code class="verb">core_sample_indices_</code></span>
     instance variable, and the core instances themselves are
available in the <span style="color:#054C5C;"><code class="verb">components_</code></span>
     instance variable:
    </p>
    <p class="noindent">
    </p>
    <pre><div id="fancyvrb104" style="padding:20px;border-radius: 3px;"><a id="x7-73022r1195"></a><span style="color:#2B2BFF;">print</span>(dbscan.core_sample_indices_[:<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">10</span></span>])</div></pre>
    <div class="tcolorbox tcolorbox" id="tcolobox-49">
     <div class="tcolorbox-content">
      <pre><div id="fancyvrb105" style="padding:20px;border-radius: 3px;"><a id="x7-73024r1"></a>[ 0  4  5  6  7  8 10 11 12 13]</div></pre>
     </div>
    </div>
    <pre><div id="fancyvrb106" style="padding:20px;border-radius: 3px;"><a id="x7-73026r1205"></a><span style="color:#2B2BFF;">print</span>(dbscan.components_)</div></pre>
    <div class="tcolorbox tcolorbox" id="tcolobox-50">
     <div class="tcolorbox-content">
      <pre><div id="fancyvrb107" style="padding:20px;border-radius: 3px;"><a id="x7-73028r1"></a>[[-0.02137124  0.40618608] 
<a id="x7-73030r2"></a> [-0.84192557  0.53058695] 
<a id="x7-73032r3"></a> [ 0.58930337 -0.32137599] 
<a id="x7-73034r4"></a> ... 
<a id="x7-73036r5"></a> [ 1.66258462 -0.3079193 ] 
<a id="x7-73038r6"></a> [-0.94355873  0.3278936 ] 
<a id="x7-73040r7"></a> [ 0.79419406  0.60777171]]</div></pre>
     </div>
    </div>
    <p class="noindent">
     This clustering is represented in the
     <a class="glossary" href="DataScienceIILectureBookli2.html#glo:lhs">
      Left Hand Side (LHS)
     </a>
     plot of
     <span id="bold" style="font-weight:bold;">
      Fig.
     </span>
     <a href="#x7-73041r15">
      5.15
     </a>. As we can see, it identified quite a lot of anomalies,
plus seven different clusters. Fortunately, if we widen each instance’s neighbourhood by increasing eps to 0.2, we get the
clustering on the right, which looks much better. Let’s continue with this model.
    </p>
    <div class="figure">
     <p class="noindent">
      <img alt="PIC" height="" src="codes/images/Unsupervised-Learning/dbscan-plot-.svg" width="150%"/>
      <a id="x7-73041r15">
      </a>
     </p>
     <figcaption class="caption">
      <span class="id">
       Figure 5.15:
      </span>
      <span class="content">
       DBSCAN clustering using two different neighborhood radiuses.
      </span>
     </figcaption>
    </div>
    <p class="noindent">
     For example, let’s train a <span style="color:#054C5C;"><code class="verb">KNeighborsClassifier</code></span>
     :
    </p>
    <pre><div id="fancyvrb108" style="padding:20px;border-radius: 3px;"><a id="x7-73043r1286"></a><span style="color:#2B2BFF;">from</span><span style="color:#BABABA;"> </span>sklearn.neighbors<span style="color:#BABABA;"> </span><span style="color:#2B2BFF;">import</span> KNeighborsClassifier 
<a id="x7-73045r1287"></a> 
<a id="x7-73047r1288"></a>knn = KNeighborsClassifier(n_neighbors=<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">50</span></span>) 
<a id="x7-73049r1289"></a>knn.fit(dbscan.components_, dbscan.labels_[dbscan.core_sample_indices_])</div></pre>
    <p class="noindent">
     Now, given a few new instances, we can predict which clusters they most likely belong to and even estimate a probability for each
cluster:
    </p>
    <pre><div id="fancyvrb109" style="padding:20px;border-radius: 3px;"><a id="x7-73051r1301"></a>X_new = np.array([[-<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">0.5</span></span>, <span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">0</span></span>], [<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">0</span></span>, <span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">0.5</span></span>], [<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">1</span></span>, -<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">0.1</span></span>], [<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">2</span></span>, <span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">1</span></span>]]) 
<a id="x7-73053r1302"></a><span style="color:#2B2BFF;">print</span>(knn.predict(X_new))</div></pre>
    <div class="tcolorbox tcolorbox" id="tcolobox-51">
     <div class="tcolorbox-content">
      <pre><div id="fancyvrb110" style="padding:20px;border-radius: 3px;"><a id="x7-73055r1"></a>[1 0 1 0]</div></pre>
     </div>
    </div>
    <pre><div id="fancyvrb111" style="padding:20px;border-radius: 3px;"><a id="x7-73057r1314"></a><span style="color:#2B2BFF;">print</span>(knn.predict_proba(X_new))</div></pre>
    <div class="tcolorbox tcolorbox" id="tcolobox-52">
     <div class="tcolorbox-content">
      <pre><div id="fancyvrb112" style="padding:20px;border-radius: 3px;"><a id="x7-73059r1"></a>[[0.18 0.82] 
<a id="x7-73061r2"></a> [1.   0.  ] 
<a id="x7-73063r3"></a> [0.12 0.88] 
<a id="x7-73065r4"></a> [1.   0.  ]]</div></pre>
     </div>
    </div>
    <p class="noindent">
     Note that we only trained the classifier on the core instances, but we could also have chosen to train it on all the instances, or all
but the anomalies.
    </p>
    <div class="knowledge">
     <p class="noindent">
      This choice depends on the final task
     </p>
    </div>
    The decision boundary is represented in
    <span id="bold" style="font-weight:bold;">
     Fig.
    </span>
    <a href="#x7-73066r16">
     5.16
    </a>
    (the crosses represent the four instances
in
    <span style="color:#054C5C;">
     <code class="verb">
      X_new
     </code>
    </span>
    ). Notice that since there is no anomaly in the training set, the classifier
    <div class="figure">
     <p class="noindent">
      <img alt="PIC" height="" src="codes/images/Unsupervised-Learning/cluster-classification-plot-.svg" width="150%"/>
      <a id="x7-73066r16">
      </a>
     </p>
     <figcaption class="caption">
      <span class="id">
       Figure 5.16:
      </span>
      <span class="content">
       Decision boundary between two clusters
      </span>
     </figcaption>
    </div>
    <p class="noindent">
     always chooses a cluster, even when that cluster is far away. It is fairly straightforward to introduce a maximum distance, in
which case the two instances that are far away from both clusters are classified as anomalies. To do this, use the <span style="color:#054C5C;"><code class="verb">kneighbors()</code></span>
     method of the <span style="color:#054C5C;"><code class="verb">KNeighborsClassifier</code></span>. Given a set of instances, it returns the distances and the indices of the
     <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
      <mi>
       k
      </mi>
     </math>
     -nearest neighbours in the training
set (two matrices, each with
     <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
      <mi>
       k
      </mi>
     </math>
     columns):
    </p>
    <pre><div id="fancyvrb113" style="padding:20px;border-radius: 3px;"><a id="x7-73068r1339"></a>y_dist, y_pred_idx = knn.kneighbors(X_new, n_neighbors=<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">1</span></span>) 
<a id="x7-73070r1340"></a>y_pred = dbscan.labels_[dbscan.core_sample_indices_][y_pred_idx] 
<a id="x7-73072r1341"></a>y_pred[y_dist &gt; <span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">0.2</span></span>] = -<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">1</span></span> 
<a id="x7-73074r1342"></a><span style="color:#2B2BFF;">print</span>(y_pred.ravel())</div></pre>
    <div class="tcolorbox tcolorbox" id="tcolobox-53">
     <div class="tcolorbox-content">
      <pre><div id="fancyvrb114" style="padding:20px;border-radius: 3px;"><a id="x7-73076r1"></a>[-1  0  1 -1]</div></pre>
     </div>
    </div>
    <p class="noindent">
     In short,
     <a class="glossary" href="DataScienceIILectureBookli2.html#glo:dbscan">
      DBSCAN
     </a>
     is a very simple yet powerful algorithm capable of identifying any number of clusters of any shape. It is robust
to outliers, and it has just two hyperparameters ( <span style="color:#054C5C;"><code class="verb">eps</code></span>
     and <span style="color:#054C5C;"><code class="verb">min_samples</code></span>
     ). If the density varies significantly across the clusters,
however, or if there’s no sufficiently low- density region around some clusters, DBSCAN can struggle to capture all the
clusters properly. Moreover, its computational complexity is roughly O(m2n), so it does not scale well to large
datasets.
     <a id="subsubsection*.12">
     </a>
    </p>
    <h5 class="subsubsectionHead">
     <a id="x7-74000">
     </a>
     Other Clustering Algorithms
    </h5>
    <p class="noindent"> <span style="color:#054C5C;"><code class="verb">sklearn</code></span>
     implements several more clustering algorithms that we should take a look at. Here is just some of them:
    </p>
    <dl class="description">
     <dt class="description">
      <a id="x7-74001x">
      </a>
      Agglomerative clustering
     </dt>
     <dd class="description">
      <p class="noindent">
       A
hierarchy
of
clusters
is
built
from
the
bottom
up.
Think
of
many
tiny
bubbles
floating
on
water
and
gradually
attaching
to
each
other
until
there’s
one
big
group
of
bubbles.
Similarly,
at
each
iteration,
agglomerative
                                                                                
                                                                                
     
clustering
connects
the
nearest
pair
of
clusters
(starting
with
individual
instances).
If
we
drew
a
tree
with
a
branch
for
every
pair
of
clusters
that
merged,
we
would
get
a
binary
tree
of
clusters,
where
the
leaves
are
the
individual
instances.
This
approach
can
capture
clusters
of
various
shapes;
it
also
produces
a
flexible
and
informative
cluster
tree
instead
of
forcing
we
to
choose
a
particular
cluster
                                                                                
                                                                                
     
scale,
and
it
can
be
used
with
any
pairwise
distance.
It
can
scale
nicely
to
large
numbers
of
instances
if
we
provide
a
connectivity
matrix,
which
is
a
sparse
m
Œ
m
matrix
that
indicates
which
pairs
of
instances
are
neighbors
(e.g.,
returned
by <span style="color:#054C5C;"><code class="verb">sklearn.neighbors.kneighbors_graph()</code></span>
       ).
Without
a
connectivity
matrix,
the
algorithm
does
not
scale
well
to
large
datasets.
      </p>
     </dd>
     <dt class="description">
      <a id="x7-74002x">
      </a>
      BIRCH
     </dt>
     <dd class="description">
      <p class="noindent">
       The
balanced
iterative
reducing
                                                                                
                                                                                
     
and
clustering
using
hierarchies
(BIRCH)
algorithm
was
designed
specifically
for
very
large
datasets,
and
it
can
be
faster
than
batch
       <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
        <mi>
         k
        </mi>
       </math>
       -means,
with
similar
results,
as
long
as
the
number
of
features
is
not
too
large
(&lt;20).
During
training,
it
builds
a
tree
structure
containing
just
enough
information
to
quickly
assign
each
new
instance
to
a
cluster,
without
having
to
store
all
the
instances
in
the
tree:
this
                                                                                
                                                                                
     
approach
allows
it
to
use
limited
memory
while
handling
huge
datasets.
      </p>
     </dd>
     <dt class="description">
      <a id="x7-74003x">
      </a>
      Mean-shift
     </dt>
     <dd class="description">
      <p class="noindent">
       This
algorithm
starts
by
placing
a
circle
centered
on
each
instance;
then
for
each
circle
it
computes
the
mean
of
all
the
instances
located
within
it,
and
it
shifts
the
circle
so
that
it
is
centered
on
the
mean.
Next,
it
iterates
this
mean-shifting
step
until
all
the
circles
stop
moving
                                                                                
                                                                                
     
(i.e.,
until
each
of
them
is
centered
on
the
mean
of
the
instances
it
contains).
Mean-shift
shifts
the
circles
in
the
direction
of
higher
density,
until
each
of
them
has
found
a
local
density
maximum.
Finally,
all
the
instances
whose
circles
have
settled
in
the
same
place
(or
close
enough)
are
assigned
to
the
same
cluster.
Mean-shift
has
some
of
the
same
features
as
DBSCAN,
like
how
                                                                                
                                                                                
     
it
can
find
any
number
of
clusters
of
any
shape,
it
has
very
few
hyperparameters
(just
one-the
radius
of
the
circles,
called
the
bandwidth),
and
it
relies
on
local
density
estimation.
But
unlike
DBSCAN,
mean-shift
tends
to
chop
clusters
into
pieces
when
they
have
internal
density
variations.
Unfortunately,
its
computational
complexity
is
O(m2n),
so
it
is
not
suited
for
large
datasets.
      </p>
     </dd>
     <dt class="description">
      <a id="x7-74004x">
      </a>
      Affinity Propagation
     </dt>
     <dd class="description">
      <p class="noindent">
       In
                                                                                
                                                                                
     
this
algorithm,
instances
repeatedly
exchange
messages
between
one
another
until
every
instance
has
elected
another
instance
(or
itself)
to
represent
it.
These
elected
instances
are
called
exemplars.
Each
exemplar
and
all
the
instances
that
elected
it
form
one
cluster.
In
real-life
politics,
we
typically
want
to
vote
for
a
candidate
whose
opinions
are
similar
to
ours,
but
we
also
want
them
to
win
the
election,
so
we
                                                                                
                                                                                
     
might
choose
a
candidate
we
don’t
fully
agree
with,
but
who
is
more
popular.
We
typically
evaluate
popularity
through
polls.
Affinity
propagation
works
in
a
similar
way,
and
it
tends
to
choose
exemplars
located
near
the
center
of
clusters,
similar
to
       <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
        <mi>
         k
        </mi>
       </math>
       -means.
But
unlike
with
       <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
        <mi>
         k
        </mi>
       </math>
       -means,
we
don’t
have
to
pick
a
number
of
clusters
ahead
of
time:
it
is
determined
during
training.
Moreover,
affinity
propagation
can
                                                                                
                                                                                
     
deal
nicely
with
clusters
of
different
sizes.
Sadly,
this
algorithm
has
a
computational
complexity
of
O(m2),
so
it
is
not
suited
for
large
datasets.
      </p>
     </dd>
     <dt class="description">
      <a id="x7-74005x">
      </a>
      Spectral clustering
     </dt>
     <dd class="description">
      <p class="noindent">
       This
algorithm
takes
a
similarity
matrix
between
the
instances
and
creates
a
low-dimensional
embedding
from
it
(i.e.,
it
reduces
the
matrix’s
dimensionality),
then
it
uses
another
clustering
algorithm
in
this
low-
dimensional
space
( <span style="color:#054C5C;"><code class="verb">sklearn</code></span>
       ’s
implementation
uses
       <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
        <mi>
         k
        </mi>
       </math>
       -means).
Spectral
                                                                                
                                                                                
     
clustering
can
capture
complex
cluster
structures,
and
it
can
also
be
used
to
cut
graphs
(e.g.,
to
identify
clusters
of
friends
on
a
social
network).
It
does
not
scale
well
to
large
numbers
of
instances,
and
it
does
not
behave
well
when
the
clusters
have
very
different
sizes.
      </p>
     </dd>
    </dl>
    <p class="noindent">
     Now let’s dive into Gaussian mixture models, which can be used for density estimation, clustering, and anomaly
detection.
    </p>
    <p class="noindent">
    </p>
    <h2 class="sectionHead">
     <a aria-label="Link to this section in PDF version" class="pdf-link" href="DataScienceIILectureBook.pdf#section.5.3" style="color: red;  position: absolute;  left: -30px;" target="_blank">
      <svg class="bi bi-file-pdf" fill="currentColor" height="20" viewbox="0 0 16 16" width="20" xmlns="http://www.w3.org/2000/svg">
       <path d="M4 0a2 2 0 0 0-2 2v12a2 2 0 0 0 2 2h8a2 2 0 0 0 2-2V2a2 2 0 0 0-2-2H4zm0 1h8a1 1 0 0 1 1 1v12a1 1 0 0 1-1 1H4a1 1 0 0 1-1-1V2a1 1 0 0 1 1-1z">
       </path>
       <path d="M4.603 12.087a.81.81 0 0 1-.438-.42c-.195-.388-.13-.776.08-1.102.198-.307.526-.568.897-.787a7.68 7.68 0 0 1 1.482-.645 19.701 19.701 0 0 0 1.062-2.227 7.269 7.269 0 0 1-.43-1.295c-.086-.4-.119-.796-.046-1.136.075-.354.274-.672.65-.823.192-.077.4-.12.602-.077a.7.7 0 0 1 .477.365c.088.164.12.356.127.538.007.187-.012.395-.047.614-.084.51-.27 1.134-.52 1.794a10.954 10.954 0 0 0 .98 1.686 5.753 5.753 0 0 1 1.334.05c.364.065.734.195.96.465.12.144.193.32.2.518.007.192-.047.382-.138.563a1.04 1.04 0 0 1-.354.416.856.856 0 0 1-.51.138c-.331-.014-.654-.196-.933-.417a5.716 5.716 0 0 1-.911-.95 11.642 11.642 0 0 0-1.997.406 11.311 11.311 0 0 1-1.021 1.51c-.29.35-.608.655-.926.787a.793.793 0 0 1-.58.029zm1.379-1.901c-.166.076-.32.156-.459.238-.328.194-.541.383-.647.547-.094.145-.096.25-.04.361.01.022.02.036.026.044a.27.27 0 0 0 .035-.012c.137-.056.355-.235.635-.572a8.18 8.18 0 0 0 .45-.606zm1.64-1.33a12.647 12.647 0 0 1 1.01-.193 11.666 11.666 0 0 1-.51-.858 20.741 20.741 0 0 1-.5 1.05zm2.446.45c.15.162.296.3.435.41.24.19.407.253.498.256a.107.107 0 0 0 .07-.015.307.307 0 0 0 .094-.125.436.436 0 0 0 .059-.2.095.095 0 0 0-.026-.063c-.052-.062-.2-.152-.518-.209a3.881 3.881 0 0 0-.612-.053zM8.078 5.8a6.7 6.7 0 0 0 .2-.828c.031-.188.043-.343.038-.465a.613.613 0 0 0-.032-.198.517.517 0 0 0-.145.04c-.087.035-.158.106-.196.283-.04.192-.03.469.046.822.024.111.054.227.09.346z">
       </path>
      </svg>
     </a>
     <small class="titlemark">
      5.3
     </small>
     <a id="x7-750005.3">
     </a>
     Gaussian Mixtures
    </h2>
    <p class="noindent">
     A
     <a class="glossary" href="DataScienceIILectureBookli2.html#glo:gmm">
      Gaussian Mixture Model (GMM)
     </a>
     is a probabilistic model that assumes that the instances were generated from a mixture of
several Gaussian distributions whose parameters are unknown. All the instances generated from a single Gaussian distribution
form a cluster that typically looks like an ellipsoid. Each cluster can have a different ellipsoidal shape, size, density, and
orientation, just like in
     <span id="bold" style="font-weight:bold;">
      Fig.
     </span>
     <a href="#x7-69001r8">
      5.8
     </a>. When we observe an instance, we know it was generated from one of the Gaussian
distributions, but we are not told which one, and we do not know what the parameters of these distributions
are.
    </p>
    <p class="noindent">
     There are several GMM variants. In the simplest variant, implemented in the <span style="color:#054C5C;"><code class="verb">GaussianMixture</code></span>
     class, we must know in advance the number
     <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
      <mi>
       k
      </mi>
     </math>
     of Gaussian distributions.
The dataset
     <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
      <mi>
       X
      </mi>
     </math>
     is
assumed to have been generated through the following probabilistic process:
    </p>
    <ul class="itemize1">
     <li class="itemize">
      <p class="noindent">
       For
each
instance,
a
cluster
is
picked
randomly
from
among
       <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
        <mi>
         k
        </mi>
       </math>
       clusters.
The
probability
of
choosing
the
       <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
        <msubsup>
         <mrow>
          <mi>
           j
          </mi>
         </mrow>
         <mrow>
          <mi>
           t
          </mi>
          <mi>
           h
          </mi>
         </mrow>
         <mrow>
         </mrow>
        </msubsup>
       </math>
       cluster
is
the
cluster’s
weight
       <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
        <msubsup>
         <mrow>
          <mi>
           ϕ
          </mi>
         </mrow>
         <mrow>
         </mrow>
         <mrow>
          <mo class="MathClass-open" stretchy="false">
           (
          </mo>
          <mi>
           j
          </mi>
          <mo class="MathClass-close" stretchy="false">
           )
          </mo>
         </mrow>
        </msubsup>
       </math>
       .
The
index
of
the
cluster
chosen
for
the
       <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
        <msubsup>
         <mrow>
          <mi>
           j
          </mi>
         </mrow>
         <mrow>
          <mi>
           t
          </mi>
          <mi>
           h
          </mi>
         </mrow>
         <mrow>
         </mrow>
        </msubsup>
       </math>
       instance
is
noted
       <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
        <msubsup>
         <mrow>
          <mi>
           z
          </mi>
         </mrow>
         <mrow>
         </mrow>
         <mrow>
          <mo class="MathClass-open" stretchy="false">
           (
          </mo>
          <mi>
           i
          </mi>
          <mo class="MathClass-close" stretchy="false">
           )
          </mo>
         </mrow>
        </msubsup>
       </math>
       .
      </p>
     </li>
     <li class="itemize">
      <p class="noindent">
       If the
       <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
        <msubsup>
         <mrow>
          <mi>
           i
          </mi>
         </mrow>
         <mrow>
         </mrow>
         <mrow>
          <mi>
           t
          </mi>
          <mi>
           h
          </mi>
         </mrow>
        </msubsup>
       </math>
       instance was
assigned to the
       <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
        <msubsup>
         <mrow>
          <mi>
           j
          </mi>
         </mrow>
         <mrow>
          <mi>
           t
          </mi>
          <mi>
           h
          </mi>
         </mrow>
         <mrow>
         </mrow>
        </msubsup>
       </math>
       cluster (i.e.,
       <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
        <msubsup>
         <mrow>
          <mi>
           z
          </mi>
         </mrow>
         <mrow>
         </mrow>
         <mrow>
          <mo class="MathClass-open" stretchy="false">
           (
          </mo>
          <mi>
           i
          </mi>
          <mo class="MathClass-close" stretchy="false">
           )
          </mo>
         </mrow>
        </msubsup>
        <mo class="MathClass-rel" stretchy="false">
         =
        </mo>
        <mi>
         j
        </mi>
       </math>
       ),
then the location
       <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
        <msubsup>
         <mrow>
          <mover accent="true">
           <mrow>
            <mi>
             x
            </mi>
           </mrow>
           <mo accent="true">
            →
           </mo>
          </mover>
         </mrow>
         <mrow>
         </mrow>
         <mrow>
          <mo class="MathClass-open" stretchy="false">
           (
          </mo>
          <mi>
           i
          </mi>
          <mo class="MathClass-close" stretchy="false">
           )
          </mo>
         </mrow>
        </msubsup>
       </math>
       of this instance is sampled randomly from the Gaussian distribution with mean
       <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
        <msubsup>
         <mrow>
          <mi>
           μ
          </mi>
         </mrow>
         <mrow>
         </mrow>
         <mrow>
          <mo class="MathClass-open" stretchy="false">
           (
          </mo>
          <mi>
           j
          </mi>
          <mo class="MathClass-close" stretchy="false">
           )
          </mo>
         </mrow>
        </msubsup>
       </math>
       and covariance
matrix
       <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
        <msubsup>
         <mrow>
          <mi mathvariant="normal">
           Σ
          </mi>
         </mrow>
         <mrow>
         </mrow>
         <mrow>
          <mo class="MathClass-open" stretchy="false">
           (
          </mo>
          <mi>
           j
          </mi>
          <mo class="MathClass-close" stretchy="false">
           )
          </mo>
         </mrow>
        </msubsup>
       </math>
       .
This is noted as:
      </p>
      <table class="equation-star">
       <tr>
        <td>
         <math class="equation" display="block" xmlns="http://www.w3.org/1998/Math/MathML">
          <msubsup>
           <mrow>
            <mover accent="true">
             <mrow>
              <mi>
               x
              </mi>
             </mrow>
             <mo accent="true">
              →
             </mo>
            </mover>
           </mrow>
           <mrow>
           </mrow>
           <mrow>
            <mo class="MathClass-open" stretchy="false">
             (
            </mo>
            <mi>
             i
            </mi>
            <mo class="MathClass-close" stretchy="false">
             )
            </mo>
           </mrow>
          </msubsup>
          <mo class="MathClass-rel" stretchy="false">
           ∼
          </mo>
          <mstyle mathvariant="script">
           <mi>
            N
           </mi>
          </mstyle>
          <msup>
           <mrow>
            <mrow>
             <mo fence="true" form="prefix">
              (
             </mo>
             <mrow>
              <msubsup>
               <mrow>
                <mi>
                 μ
                </mi>
               </mrow>
               <mrow>
               </mrow>
               <mrow>
                <mo class="MathClass-open" stretchy="false">
                 (
                </mo>
                <mi>
                 j
                </mi>
                <mo class="MathClass-close" stretchy="false">
                 )
                </mo>
               </mrow>
              </msubsup>
              <mo class="MathClass-punc" stretchy="false">
               ,
              </mo>
              <mspace class="thinspace" width="0.17em">
              </mspace>
              <msubsup>
               <mrow>
                <mi mathvariant="normal">
                 Σ
                </mi>
               </mrow>
               <mrow>
               </mrow>
               <mrow>
                <mo class="MathClass-open" stretchy="false">
                 (
                </mo>
                <mi>
                 j
                </mi>
                <mo class="MathClass-close" stretchy="false">
                 )
                </mo>
               </mrow>
              </msubsup>
             </mrow>
             <mo fence="true" form="postfix">
              )
             </mo>
            </mrow>
           </mrow>
           <mrow>
           </mrow>
          </msup>
         </math>
        </td>
       </tr>
      </table>
     </li>
    </ul>
    <p class="noindent">
     So what can we do with such a model? Well, given the dataset
     <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
      <mover accent="true">
       <mrow>
        <mi>
         X
        </mi>
       </mrow>
       <mo accent="true">
        →
       </mo>
      </mover>
     </math>
     , we typically want to start by
estimating the weights
     <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
      <mi>
       ϕ
      </mi>
     </math>
     and all
the distribution parameters
     <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
      <msubsup>
       <mrow>
        <mi>
         μ
        </mi>
       </mrow>
       <mrow>
       </mrow>
       <mrow>
        <mo class="MathClass-open" stretchy="false">
         (
        </mo>
        <mn>
         1
        </mn>
        <mo class="MathClass-close" stretchy="false">
         )
        </mo>
       </mrow>
      </msubsup>
     </math>
     to
     <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
      <msubsup>
       <mrow>
        <mi>
         μ
        </mi>
       </mrow>
       <mrow>
       </mrow>
       <mrow>
        <mo class="MathClass-open" stretchy="false">
         (
        </mo>
        <mi>
         k
        </mi>
        <mo class="MathClass-close" stretchy="false">
         )
        </mo>
       </mrow>
      </msubsup>
     </math>
     and
     <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
      <msubsup>
       <mrow>
        <mi mathvariant="normal">
         Σ
        </mi>
       </mrow>
       <mrow>
       </mrow>
       <mrow>
        <mo class="MathClass-open" stretchy="false">
         (
        </mo>
        <mn>
         1
        </mn>
        <mo class="MathClass-close" stretchy="false">
         )
        </mo>
       </mrow>
      </msubsup>
     </math>
     to
     <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
      <msubsup>
       <mrow>
        <mi mathvariant="normal">
         Σ
        </mi>
       </mrow>
       <mrow>
       </mrow>
       <mrow>
        <mo class="MathClass-open" stretchy="false">
         (
        </mo>
        <mi>
         k
        </mi>
        <mo class="MathClass-close" stretchy="false">
         )
        </mo>
       </mrow>
      </msubsup>
     </math>
     . <span style="color:#054C5C;"><code class="verb">sklearn</code></span>
     ’s <span style="color:#054C5C;"><code class="verb">GaussianMixture</code></span>
     class makes this super easy:
    </p>
    <p class="noindent">
    </p>
    <pre><div id="fancyvrb115" style="padding:20px;border-radius: 3px;"><a id="x7-75002r1510"></a><span style="color:#2B2BFF;">from</span><span style="color:#BABABA;"> </span>sklearn.mixture<span style="color:#BABABA;"> </span><span style="color:#2B2BFF;">import</span> GaussianMixture</div></pre>
    <p class="noindent">
    </p>
    <pre><div id="fancyvrb116" style="padding:20px;border-radius: 3px;"><a id="x7-75004r1518"></a>gm = GaussianMixture(n_components=<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">3</span></span>, n_init=<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">10</span></span>, random_state=<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">42</span></span>) 
<a id="x7-75006r1519"></a>gm.fit(X)</div></pre>
    <p class="noindent">
     Let’s look at the parameters that the algorithm estimated:
    </p>
    <p class="noindent">
    </p>
    <pre><div id="fancyvrb117" style="padding:20px;border-radius: 3px;"><a id="x7-75008r1531"></a><span style="color:#2B2BFF;">print</span>(gm.weights_)</div></pre>
    <div class="tcolorbox tcolorbox" id="tcolobox-54">
     <div class="tcolorbox-content">
      <pre><div id="fancyvrb118" style="padding:20px;border-radius: 3px;"><a id="x7-75010r1"></a>[0.40005972 0.20961444 0.39032584]</div></pre>
     </div>
    </div>
    <pre><div id="fancyvrb119" style="padding:20px;border-radius: 3px;"><a id="x7-75012r1541"></a><span style="color:#2B2BFF;">print</span>(gm.means_)</div></pre>
    <div class="tcolorbox tcolorbox" id="tcolobox-55">
     <div class="tcolorbox-content">
      <pre><div id="fancyvrb120" style="padding:20px;border-radius: 3px;"><a id="x7-75014r1"></a>[[-1.40764129  1.42712848] 
<a id="x7-75016r2"></a> [ 3.39947665  1.05931088] 
<a id="x7-75018r3"></a> [ 0.05145113  0.07534576]]</div></pre>
     </div>
    </div>
    <p class="noindent">
     Great, it worked fine! Indeed, two of the three clusters were generated with 500 instances each, while the third cluster only
contains 250 instances. So the true cluster weights are 0.4, 0.2, and 0.4, respectively, and that’s roughly what the algorithm
found. Similarly, the true means and covariance matrices are quite close to those found by the algorithm. But
how? This class relies on the
     <a class="glossary" href="DataScienceIILectureBookli2.html#glo:em">
      Expectation Maximisation (EM)
     </a>
     algorithm, which has many similarities with the
     <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
      <mi>
       k
      </mi>
     </math>
     -means
algorithm where it also initializes the cluster parameters randomly, then it repeats two steps until convergence,
first assigning instances to clusters (this is called the expectation step) and then updating the clusters (this
is called the maximisation step). In the context of clustering, we can think of
     <a class="glossary" href="DataScienceIILectureBookli2.html#glo:em">
      EM
     </a>
     as a generalisation of
     <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
      <mi>
       k
      </mi>
     </math>
     -means that not only
finds the cluster centres (
     <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
      <msubsup>
       <mrow>
        <mi>
         μ
        </mi>
       </mrow>
       <mrow>
       </mrow>
       <mrow>
        <mo class="MathClass-open" stretchy="false">
         (
        </mo>
        <mn>
         1
        </mn>
        <mo class="MathClass-close" stretchy="false">
         )
        </mo>
       </mrow>
      </msubsup>
     </math>
     to
     <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
      <msubsup>
       <mrow>
        <mi>
         μ
        </mi>
       </mrow>
       <mrow>
       </mrow>
       <mrow>
        <mo class="MathClass-open" stretchy="false">
         (
        </mo>
        <mi>
         k
        </mi>
        <mo class="MathClass-close" stretchy="false">
         )
        </mo>
       </mrow>
      </msubsup>
     </math>
     ), but also their size,
shape, and orientation (
     <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
      <msubsup>
       <mrow>
        <mi mathvariant="normal">
         Σ
        </mi>
       </mrow>
       <mrow>
       </mrow>
       <mrow>
        <mo class="MathClass-open" stretchy="false">
         (
        </mo>
        <mn>
         1
        </mn>
        <mo class="MathClass-close" stretchy="false">
         )
        </mo>
       </mrow>
      </msubsup>
     </math>
     to
     <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
      <msubsup>
       <mrow>
        <mi mathvariant="normal">
         Σ
        </mi>
       </mrow>
       <mrow>
       </mrow>
       <mrow>
        <mo class="MathClass-open" stretchy="false">
         (
        </mo>
        <mi>
         k
        </mi>
        <mo class="MathClass-close" stretchy="false">
         )
        </mo>
       </mrow>
      </msubsup>
     </math>
     ), as well as their
relative weights (
     <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
      <msubsup>
       <mrow>
        <mi>
         ϕ
        </mi>
       </mrow>
       <mrow>
       </mrow>
       <mrow>
        <mo class="MathClass-open" stretchy="false">
         (
        </mo>
        <mn>
         1
        </mn>
        <mo class="MathClass-close" stretchy="false">
         )
        </mo>
       </mrow>
      </msubsup>
     </math>
     to
     <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
      <msubsup>
       <mrow>
        <mi>
         ϕ
        </mi>
       </mrow>
       <mrow>
       </mrow>
       <mrow>
        <mo class="MathClass-open" stretchy="false">
         (
        </mo>
        <mi>
         j
        </mi>
        <mo class="MathClass-close" stretchy="false">
         )
        </mo>
       </mrow>
      </msubsup>
     </math>
     ). Unlike
     <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
      <mi>
       k
      </mi>
     </math>
     -means,
though,
     <a class="glossary" href="DataScienceIILectureBookli2.html#glo:em">
      EM
     </a>
     uses
     <alert style="color: #821131;">
      soft cluster assignments
     </alert>
     , not hard assignments. For each instance, during the expectation step, the algorithm
estimates the probability that it belongs to each cluster (based on the current cluster parameters). Then, during the
maximisation step, each cluster is updated using all the instances in the dataset, with each instance weighted by the estimated
probability that it belongs to that cluster. These probabilities are called the responsibilities of the clusters for the instances.
During the maximization step, each cluster’s update will mostly be impacted by the instances it is most responsible
for.
    </p>
    <div class="warning">
     <p class="noindent">
      Unfortunately, just like
      <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
       <mi>
        k
       </mi>
      </math>
      -means,
      <a class="glossary" href="DataScienceIILectureBookli2.html#glo:em">
       EM
      </a>
      can end up converging to poor solutions, so it needs to be run several times, keeping only the best solution. This is why we
set <span style="color:#054C5C;"><code class="verb">n_init</code></span>
      to 10. By default <span style="color:#054C5C;"><code class="verb">n_init</code></span>
      is set to 1.
     </p>
    </div>
    <p class="noindent">
     We can check whether or not the algorithm converged and how many iterations it took:
    </p>
    <p class="noindent">
    </p>
    <pre><div id="fancyvrb121" style="padding:20px;border-radius: 3px;"><a id="x7-75020r1570"></a><span style="color:#2B2BFF;">print</span>(gm.converged_)</div></pre>
    <div class="tcolorbox tcolorbox" id="tcolobox-56">
     <div class="tcolorbox-content">
      <pre><div id="fancyvrb122" style="padding:20px;border-radius: 3px;"><a id="x7-75022r1"></a>True</div></pre>
     </div>
    </div>
    <pre><div id="fancyvrb123" style="padding:20px;border-radius: 3px;"><a id="x7-75024r1582"></a><span style="color:#2B2BFF;">print</span>(gm.n_iter_)</div></pre>
    <div class="tcolorbox tcolorbox" id="tcolobox-57">
     <div class="tcolorbox-content">
      <pre><div id="fancyvrb124" style="padding:20px;border-radius: 3px;"><a id="x7-75026r1"></a>4</div></pre>
     </div>
    </div>
    <p class="noindent">
     Now that we have an estimate of the location, size, shape, orientation, and relative weight of each cluster, the model can easily
assign each instance to the most likely cluster (hard clustering) or estimate the probability that it belongs to a particular
cluster (soft clustering). Just use the <span style="color:#054C5C;"><code class="verb">predict()</code></span>
     method for hard clustering, or the <span style="color:#054C5C;"><code class="verb">predict_proba()</code></span>
     method for soft
clustering:
    </p>
    <p class="noindent">
    </p>
    <pre><div id="fancyvrb125" style="padding:20px;border-radius: 3px;"><a id="x7-75028r1597"></a><span style="color:#2B2BFF;">print</span>(gm.predict(X))</div></pre>
    <div class="tcolorbox tcolorbox" id="tcolobox-58">
     <div class="tcolorbox-content">
      <pre><div id="fancyvrb126" style="padding:20px;border-radius: 3px;"><a id="x7-75030r1"></a>[2 2 0 ... 1 1 1]</div></pre>
     </div>
    </div>
    <pre><div id="fancyvrb127" style="padding:20px;border-radius: 3px;"><a id="x7-75032r1607"></a><span style="color:#2B2BFF;">print</span>(gm.predict_proba(X).round(<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">3</span></span>))</div></pre>
    <div class="tcolorbox tcolorbox" id="tcolobox-59">
     <div class="tcolorbox-content">
      <pre><div id="fancyvrb128" style="padding:20px;border-radius: 3px;"><a id="x7-75034r1"></a>[[0.    0.023 0.977] 
<a id="x7-75036r2"></a> [0.001 0.016 0.983] 
<a id="x7-75038r3"></a> [1.    0.    0.   ] 
<a id="x7-75040r4"></a> ... 
<a id="x7-75042r5"></a> [0.    1.    0.   ] 
<a id="x7-75044r6"></a> [0.    1.    0.   ] 
<a id="x7-75046r7"></a> [0.    1.    0.   ]]</div></pre>
     </div>
    </div>
    <p class="noindent">
     A Gaussian mixture model is a
     <alert style="color: #821131;">
      generative model
     </alert>
     , meaning we can sample new instances from it (note that they are ordered by
cluster index):
    </p>
    <p class="noindent">
    </p>
    <pre><div id="fancyvrb129" style="padding:20px;border-radius: 3px;"><a id="x7-75048r1625"></a>X_new, y_new = gm.sample(<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">6</span></span>) 
<a id="x7-75050r1626"></a><span style="color:#2B2BFF;">print</span>(X_new)</div></pre>
    <div class="tcolorbox tcolorbox" id="tcolobox-60">
     <div class="tcolorbox-content">
      <pre><div id="fancyvrb130" style="padding:20px;border-radius: 3px;"><a id="x7-75052r1"></a>[[-2.32491052  1.04752548] 
<a id="x7-75054r2"></a> [-1.16654983  1.62795173] 
<a id="x7-75056r3"></a> [ 1.84860618  2.07374016] 
<a id="x7-75058r4"></a> [ 3.98304484  1.49869936] 
<a id="x7-75060r5"></a> [ 3.8163406   0.53038367] 
<a id="x7-75062r6"></a> [ 0.38079484 -0.56239369]]</div></pre>
     </div>
    </div>
    <pre><div id="fancyvrb131" style="padding:20px;border-radius: 3px;"><a id="x7-75064r1641"></a><span style="color:#2B2BFF;">print</span>(y_new)</div></pre>
    <div class="tcolorbox tcolorbox" id="tcolobox-61">
     <div class="tcolorbox-content">
      <pre><div id="fancyvrb132" style="padding:20px;border-radius: 3px;"><a id="x7-75066r1"></a>[0 0 1 1 1 2]</div></pre>
     </div>
    </div>
    <p class="noindent">
     It is also possible to estimate the density of the model at any given location. This is achieved using the <span style="color:#054C5C;"><code class="verb">score_samples()</code></span>
     method:
for each instance it is given, this method estimates the log of the
     <a class="glossary" href="DataScienceIILectureBookli2.html#glo:pdf">
      Portable Document Format (PDF)
     </a>
     at that location. The greater
the score, the higher the density:
    </p>
    <p class="noindent">
    </p>
    <pre><div id="fancyvrb133" style="padding:20px;border-radius: 3px;"><a id="x7-75068r1655"></a><span style="color:#2B2BFF;">print</span>(gm.score_samples(X).round(<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">2</span></span>))</div></pre>
    <div class="tcolorbox tcolorbox" id="tcolobox-62">
     <div class="tcolorbox-content">
      <pre><div id="fancyvrb134" style="padding:20px;border-radius: 3px;"><a id="x7-75070r1"></a>[-2.61 -3.57 -3.33 ... -3.51 -4.4  -3.81]</div></pre>
     </div>
    </div>
    <div class="figure">
     <p class="noindent">
      <img alt="PIC" height="" src="codes/images/Unsupervised-Learning/gaussian-mixture-plot-.svg" width="150%"/>
      <a id="x7-75071r17">
      </a>
     </p>
     <figcaption class="caption">
      <span class="id">
       Figure 5.17:
      </span>
      <span class="content">
       Cluster means, decision boundaries, and density contours of a trained Gaussian mixture model.
      </span>
     </figcaption>
    </div>
    <p class="noindent">
     If we compute the exponential of these scores, we get the value of the
     <a class="glossary" href="DataScienceIILectureBookli2.html#glo:pdf">
      PDF
     </a>
     at the location of the given instances. These are not
probabilities, but probability densities: they can take on any positive value, not just a value between 0 and 1. To estimate the
probability that an instance will fall within a particular region, we would have to integrate the
     <a class="glossary" href="DataScienceIILectureBookli2.html#glo:pdf">
      PDF
     </a>
     over that region (if we do so
over the entire space of possible instance locations, the result will be 1).
     <span id="bold" style="font-weight:bold;">
      Fig.
     </span>
     <a href="#x7-75071r17">
      5.17
     </a>
     shows the cluster means, the decision
boundaries (dashed lines), and the density contours of this model.
    </p>
    <p class="noindent">
     The algorithm clearly found an excellent solution. Of course, we made its task easy by generating the data using a set of 2D
Gaussian distributions
     <alert style="color: #821131;">
      <span id="bold" style="font-weight:bold;">
       <sup class="textsuperscript">
        22
       </sup>
      </span>
     </alert>
     <span class="marginnote">
      <alert style="color: #821131;">
       <span id="bold" style="font-weight:bold;">
        <sup class="textsuperscript">
         22
        </sup>
       </span>
      </alert>
      <span style="color:#0063B2;">
       unfortunately, real-life data is not always so Gaussian and low-dimensional
      </span>
     </span>
     . We also gave the
algorithm the correct number of clusters. When there are many dimensions, or many clusters, or few instances,
     <a class="glossary" href="DataScienceIILectureBookli2.html#glo:em">
      EM
     </a>
     can struggle
to converge to the optimal solution. We might need to reduce the difficulty of the task by limiting the number of parameters that
the algorithm has to learn. One way to do this is to limit the range of shapes and orientations that the clusters can have. This
can be achieved by imposing constraints on the covariance matrices. To do this, set the <span style="color:#054C5C;"><code class="verb">covariance_type</code></span>
     hyperparameter to one of
the following values:
    </p>
    <dl class="description">
     <dt class="description">
      <a id="x7-75072x5.3">
      </a>
      spherical
     </dt>
     <dd class="description">
      <p class="noindent">
       All
clusters
must
be
spherical,
but
they
can
have
different
diameters
(i.e.,
different
variances).
      </p>
     </dd>
     <dt class="description">
      <a id="x7-75073x5.3">
      </a>
      diag
     </dt>
     <dd class="description">
      <p class="noindent">
       Clusters
can
take
on
any
ellipsoidal
shape
of
any
size,
but
the
ellipsoid’s
axes
must
be
parallel
to
the
coordinate
axes
(i.e.,
the
covariance
matrices
must
be
diagonal).
      </p>
     </dd>
     <dt class="description">
      <a id="x7-75074x5.3">
      </a>
      tied
     </dt>
     <dd class="description">
      <p class="noindent">
       All
clusters
must
have
the
same
ellipsoidal
shape,
size,
and
orientation
(i.e.,
all
clusters
share
the
same
covariance
matrix).
      </p>
     </dd>
    </dl>
    <p class="noindent">
     By default, <span style="color:#054C5C;"><code class="verb">covariance_type</code></span>
     is equal to "full", which means that each cluster can take on any shape, size, and orientation (it has
its own unconstrained covariance matrix).
     <span id="bold" style="font-weight:bold;">
      Fig.
     </span>
     <a href="#x7-75075r18">
      5.18
     </a>
     plots the solutions found by the EM algorithm when <span style="color:#054C5C;"><code class="verb">covariance_type</code></span>
     is set
to <span style="color:#054C5C;"><code class="verb">"tied"</code></span>
     or <span style="color:#054C5C;"><code class="verb">"spherical"</code></span>.
    </p>
    <div class="figure">
     <p class="noindent">
      <img alt="PIC" height="" src="codes/images/Unsupervised-Learning/covariance-type-plot-.svg" width="150%"/>
      <a id="x7-75075r18">
      </a>
     </p>
     <figcaption class="caption">
      <span class="id">
       Figure 5.18:
      </span>
      <span class="content">
       Gaussian mixtures for tied clusters (left) and spherical clusters (right)
      </span>
     </figcaption>
    </div>
    <div class="informationblock" id="tcolobox-63">
     <div class="title">
      <p class="noindent">
       <span id="bold" style="font-weight:bold;">
        Information
       </span>
       : Computational Complexity
      </p>
     </div>
     <div class="box-content">
      <p class="noindent">
       The computational complexity of training a <span style="color:#054C5C;"><code class="verb">GaussianMixture</code></span>
       model depends on the number of instances m,
the  number  of  dimensions  n,  the  number  of  clusters  k,  and  the  constraints  on  the  covariance  matrices.  If <span style="color:#054C5C;"><code class="verb">covariance_type</code></span>
       is <span style="color:#054C5C;"><code class="verb">"spherical"</code></span>
       or <span style="color:#054C5C;"><code class="verb">"diag"</code></span>,  it  is  O(kmn),  assuming  the  data  has  a  clustering  structure.  If <span style="color:#054C5C;"><code class="verb">covariance_type</code></span>
       is "tied" or "full", it is O(kmn2 + kn3), so it will not scale to large numbers of features.
      </p>
     </div>
    </div>
    <p class="noindent">
     Gaussian mixture models can also be used for
     <alert style="color: #821131;">
      anomaly detection
     </alert>
     . We’ll see how in the next section.
    </p>
    <h3 class="subsectionHead">
     <a aria-label="Link to this section in PDF version" class="pdf-link" href="DataScienceIILectureBook.pdf#subsection.5.3.1" style="color: red;  position: absolute;  left: -30px;" target="_blank">
      <svg class="bi bi-file-pdf" fill="currentColor" height="20" viewbox="0 0 16 16" width="20" xmlns="http://www.w3.org/2000/svg">
       <path d="M4 0a2 2 0 0 0-2 2v12a2 2 0 0 0 2 2h8a2 2 0 0 0 2-2V2a2 2 0 0 0-2-2H4zm0 1h8a1 1 0 0 1 1 1v12a1 1 0 0 1-1 1H4a1 1 0 0 1-1-1V2a1 1 0 0 1 1-1z">
       </path>
       <path d="M4.603 12.087a.81.81 0 0 1-.438-.42c-.195-.388-.13-.776.08-1.102.198-.307.526-.568.897-.787a7.68 7.68 0 0 1 1.482-.645 19.701 19.701 0 0 0 1.062-2.227 7.269 7.269 0 0 1-.43-1.295c-.086-.4-.119-.796-.046-1.136.075-.354.274-.672.65-.823.192-.077.4-.12.602-.077a.7.7 0 0 1 .477.365c.088.164.12.356.127.538.007.187-.012.395-.047.614-.084.51-.27 1.134-.52 1.794a10.954 10.954 0 0 0 .98 1.686 5.753 5.753 0 0 1 1.334.05c.364.065.734.195.96.465.12.144.193.32.2.518.007.192-.047.382-.138.563a1.04 1.04 0 0 1-.354.416.856.856 0 0 1-.51.138c-.331-.014-.654-.196-.933-.417a5.716 5.716 0 0 1-.911-.95 11.642 11.642 0 0 0-1.997.406 11.311 11.311 0 0 1-1.021 1.51c-.29.35-.608.655-.926.787a.793.793 0 0 1-.58.029zm1.379-1.901c-.166.076-.32.156-.459.238-.328.194-.541.383-.647.547-.094.145-.096.25-.04.361.01.022.02.036.026.044a.27.27 0 0 0 .035-.012c.137-.056.355-.235.635-.572a8.18 8.18 0 0 0 .45-.606zm1.64-1.33a12.647 12.647 0 0 1 1.01-.193 11.666 11.666 0 0 1-.51-.858 20.741 20.741 0 0 1-.5 1.05zm2.446.45c.15.162.296.3.435.41.24.19.407.253.498.256a.107.107 0 0 0 .07-.015.307.307 0 0 0 .094-.125.436.436 0 0 0 .059-.2.095.095 0 0 0-.026-.063c-.052-.062-.2-.152-.518-.209a3.881 3.881 0 0 0-.612-.053zM8.078 5.8a6.7 6.7 0 0 0 .2-.828c.031-.188.043-.343.038-.465a.613.613 0 0 0-.032-.198.517.517 0 0 0-.145.04c-.087.035-.158.106-.196.283-.04.192-.03.469.046.822.024.111.054.227.09.346z">
       </path>
      </svg>
     </a>
     <small class="titlemark">
      5.3.1
     </small>
     <a id="x7-760005.3.1">
     </a>
     Using Gaussian Mixtures for Anomaly Detection
    </h3>
    <p class="noindent">
     Using a Gaussian mixture model for anomaly detection is quite simple.
    </p>
    <div class="knowledge">
     <p class="noindent">
      Any instance located in a low-density region can be considered an anomaly.
     </p>
    </div>
    We must define what density threshold we want to
use. For example, in a manufacturing company that tries to detect defective products, the ratio of defective products
is usually well known. Say it is equal to 2%. We then set the density threshold to be the value that results in
having 2% of the instances located in areas below that threshold density. If we notice that we get too many false
positives (i.e., perfectly good products that are flagged as defective), we can lower the threshold. Conversely, if
we have too many false negatives (i.e., defective products that the system does not flag as defective), we can
increase the threshold. This is the usual precision/recall trade-off. Here is how we would identify the outliers using
the fourth percentile lowest density as the threshold (i.e., approximately 4% of the instances will be flagged as
anomalies):
    <p class="noindent">
    </p>
    <pre><div id="fancyvrb135" style="padding:20px;border-radius: 3px;"><a id="x7-76002r1802"></a>densities = gm.score_samples(X) 
<a id="x7-76004r1803"></a>density_threshold = np.percentile(densities, <span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">2</span></span>) 
<a id="x7-76006r1804"></a>anomalies = X[densities &lt; density_threshold]</div></pre>
    <p class="noindent">
     <span id="bold" style="font-weight:bold;">
      Fig.
     </span>
     <a href="#x7-76007r19">
      5.19
     </a>
     represents these anomalies as stars. A closely related task is
     <alert style="color: #821131;">
      novelty detection
     </alert>
     . It differs from anomaly detection in
that the algorithm is assumed to be trained on a
     <span id="bold" style="font-weight:bold;">
      clean
     </span>
     dataset, uncontaminated by outliers, whereas anomaly detection does not
make this assumption. Indeed, outlier detection is often used to clean up a dataset.
    </p>
    <div class="figure">
     <p class="noindent">
      <img alt="PIC" height="" src="codes/images/Unsupervised-Learning/mixture-anomaly-detection-plot-.svg" width="150%"/>
      <a id="x7-76007r19">
      </a>
     </p>
     <figcaption class="caption">
      <span class="id">
       Figure 5.19:
      </span>
      <span class="content">
       Anomaly detection using a Gaussian mixture model.
      </span>
     </figcaption>
    </div>
    <div class="informationblock" id="tcolobox-64">
     <div class="title">
      <p class="noindent">
       <span id="bold" style="font-weight:bold;">
        Information
       </span>
       : Working with Outliers
      </p>
     </div>
     <div class="box-content">
      <p class="noindent">
       Gaussian mixture models try to fit all the data, including the outliers; if we have too many of them this will
bias the model’s view of “normality”, and some outliers may wrongly be considered as normal. If this happens,
we can try to fit the model once, use it to detect and remove the most extreme outliers, then fit the model again
on the cleaned-up dataset. Another approach is to use robust covariance estimation methods.
      </p>
     </div>
    </div>
    <p class="noindent">
     Just like
     <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
      <mi>
       k
      </mi>
     </math>
     -means,
the <span style="color:#054C5C;"><code class="verb">GaussianMixture</code></span>
     algorithm requires we to specify the number of clusters. So how can we find that number?
    </p>
    <h3 class="subsectionHead">
     <a aria-label="Link to this section in PDF version" class="pdf-link" href="DataScienceIILectureBook.pdf#subsection.5.3.2" style="color: red;  position: absolute;  left: -30px;" target="_blank">
      <svg class="bi bi-file-pdf" fill="currentColor" height="20" viewbox="0 0 16 16" width="20" xmlns="http://www.w3.org/2000/svg">
       <path d="M4 0a2 2 0 0 0-2 2v12a2 2 0 0 0 2 2h8a2 2 0 0 0 2-2V2a2 2 0 0 0-2-2H4zm0 1h8a1 1 0 0 1 1 1v12a1 1 0 0 1-1 1H4a1 1 0 0 1-1-1V2a1 1 0 0 1 1-1z">
       </path>
       <path d="M4.603 12.087a.81.81 0 0 1-.438-.42c-.195-.388-.13-.776.08-1.102.198-.307.526-.568.897-.787a7.68 7.68 0 0 1 1.482-.645 19.701 19.701 0 0 0 1.062-2.227 7.269 7.269 0 0 1-.43-1.295c-.086-.4-.119-.796-.046-1.136.075-.354.274-.672.65-.823.192-.077.4-.12.602-.077a.7.7 0 0 1 .477.365c.088.164.12.356.127.538.007.187-.012.395-.047.614-.084.51-.27 1.134-.52 1.794a10.954 10.954 0 0 0 .98 1.686 5.753 5.753 0 0 1 1.334.05c.364.065.734.195.96.465.12.144.193.32.2.518.007.192-.047.382-.138.563a1.04 1.04 0 0 1-.354.416.856.856 0 0 1-.51.138c-.331-.014-.654-.196-.933-.417a5.716 5.716 0 0 1-.911-.95 11.642 11.642 0 0 0-1.997.406 11.311 11.311 0 0 1-1.021 1.51c-.29.35-.608.655-.926.787a.793.793 0 0 1-.58.029zm1.379-1.901c-.166.076-.32.156-.459.238-.328.194-.541.383-.647.547-.094.145-.096.25-.04.361.01.022.02.036.026.044a.27.27 0 0 0 .035-.012c.137-.056.355-.235.635-.572a8.18 8.18 0 0 0 .45-.606zm1.64-1.33a12.647 12.647 0 0 1 1.01-.193 11.666 11.666 0 0 1-.51-.858 20.741 20.741 0 0 1-.5 1.05zm2.446.45c.15.162.296.3.435.41.24.19.407.253.498.256a.107.107 0 0 0 .07-.015.307.307 0 0 0 .094-.125.436.436 0 0 0 .059-.2.095.095 0 0 0-.026-.063c-.052-.062-.2-.152-.518-.209a3.881 3.881 0 0 0-.612-.053zM8.078 5.8a6.7 6.7 0 0 0 .2-.828c.031-.188.043-.343.038-.465a.613.613 0 0 0-.032-.198.517.517 0 0 0-.145.04c-.087.035-.158.106-.196.283-.04.192-.03.469.046.822.024.111.054.227.09.346z">
       </path>
      </svg>
     </a>
     <small class="titlemark">
      5.3.2
     </small>
     <a id="x7-770005.3.2">
     </a>
     Selecting the Number of Clusters
    </h3>
    <p class="noindent">
     With
     <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
      <mi>
       k
      </mi>
     </math>
     -means,
we can use the inertia or the silhouette score to select the required number of clusters. But with Gaussian mixtures, it is not
possible to use these metrics as they are
     <span id="bold" style="font-weight:bold;">
      NOT
     </span>
     reliable when the clusters are not spherical or have different sizes. Instead, we
can try to find the model which minimises a theoretical information criterion, such as the
     <a class="glossary" href="DataScienceIILectureBookli2.html#glo:bic">
      Bayesian Information Criterion (BIC)
     </a>
     or the
     <a class="glossary" href="DataScienceIILectureBookli2.html#glo:aic">
      Akaike Information Criterion (AIC)
     </a>, defined as:
    </p>
    <math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
     <mtable class="align-star" columnalign="left" displaystyle="true">
      <mtr>
       <mtd class="align-odd" columnalign="right">
        <mstyle class="text">
         <mtext>
          BIC
         </mtext>
        </mstyle>
       </mtd>
       <mtd class="align-even">
        <mo class="MathClass-rel" stretchy="false">
         =
        </mo>
        <mi class="loglike">
         log
        </mi>
        <mo>
         ⁡
        </mo>
        <msup>
         <mrow>
          <mrow>
           <mo fence="true" form="prefix">
            (
           </mo>
           <mrow>
            <mi>
             m
            </mi>
           </mrow>
           <mo fence="true" form="postfix">
            )
           </mo>
          </mrow>
         </mrow>
         <mrow>
         </mrow>
        </msup>
        <mi>
         p
        </mi>
        <mo class="MathClass-bin" stretchy="false">
         −
        </mo>
        <mn>
         2
        </mn>
        <mo>
         <mi class="loglike">
          log
         </mi>
         <mo>
          ⁡
         </mo>
        </mo>
        <msup>
         <mrow>
          <mrow>
           <mo fence="true" form="prefix">
            (
           </mo>
           <mrow>
            <msubsup>
             <mrow>
              <mover accent="true">
               <mrow>
                <mi>
                 L
                </mi>
               </mrow>
               <mo accent="true">
                ^
               </mo>
              </mover>
             </mrow>
             <mrow>
             </mrow>
             <mrow>
             </mrow>
            </msubsup>
           </mrow>
           <mo fence="true" form="postfix">
            )
           </mo>
          </mrow>
         </mrow>
         <mrow>
         </mrow>
        </msup>
        <mspace width="2em">
        </mspace>
       </mtd>
       <mtd class="align-label" columnalign="right">
       </mtd>
       <mtd class="align-label">
        <mspace width="2em">
        </mspace>
       </mtd>
      </mtr>
      <mtr>
       <mtd class="align-odd" columnalign="right">
        <mstyle class="text">
         <mtext>
          AIC
         </mtext>
        </mstyle>
       </mtd>
       <mtd class="align-even">
        <mo class="MathClass-rel" stretchy="false">
         =
        </mo>
        <mn>
         2
        </mn>
        <mi>
         p
        </mi>
        <mo class="MathClass-bin" stretchy="false">
         −
        </mo>
        <mn>
         2
        </mn>
        <mi class="loglike">
         log
        </mi>
        <mo>
         ⁡
        </mo>
        <msup>
         <mrow>
          <mrow>
           <mo fence="true" form="prefix">
            (
           </mo>
           <mrow>
            <msubsup>
             <mrow>
              <mover accent="true">
               <mrow>
                <mi>
                 L
                </mi>
               </mrow>
               <mo accent="true">
                ^
               </mo>
              </mover>
             </mrow>
             <mrow>
             </mrow>
             <mrow>
             </mrow>
            </msubsup>
           </mrow>
           <mo fence="true" form="postfix">
            )
           </mo>
          </mrow>
         </mrow>
         <mrow>
         </mrow>
        </msup>
        <mspace width="2em">
        </mspace>
       </mtd>
       <mtd class="align-label" columnalign="right">
       </mtd>
       <mtd class="align-label">
        <mspace width="2em">
        </mspace>
       </mtd>
      </mtr>
     </mtable>
    </math>
    <p class="noindent">
     where
     <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
      <mi>
       m
      </mi>
     </math>
     is
the
number
of
instances,
     <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
      <mi>
       p
      </mi>
     </math>
     is
the
number
of
parameters
learned
by
the
model
and,
     <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
      <msubsup>
       <mrow>
        <mover accent="true">
         <mrow>
          <mi>
           L
          </mi>
         </mrow>
         <mo accent="true">
          ^
         </mo>
        </mover>
       </mrow>
       <mrow>
       </mrow>
       <mrow>
       </mrow>
      </msubsup>
     </math>
     is
the
maximised
value
of
the
     <alert style="color: #821131;">
      likelihood
function
     </alert>
     of
the
model.
                                                                                
                                                                                
Both
     <a class="glossary" href="DataScienceIILectureBookli2.html#glo:bic">
      BIC
     </a>
     and
     <a class="glossary" href="DataScienceIILectureBookli2.html#glo:aic">
      AIC
     </a>
     penalise
models
with
more
parameters
to
learn
     <alert style="color: #821131;">
      <span id="bold" style="font-weight:bold;">
       <sup class="textsuperscript">
        23
       </sup>
      </span>
     </alert>
     <span class="marginnote">
      <alert style="color: #821131;">
       <span id="bold" style="font-weight:bold;">
        <sup class="textsuperscript">
         23
        </sup>
       </span>
      </alert>
      <span style="color:#0063B2;">
       e.g.,
more
clusters
      </span>
     </span>
     and
reward
models
which
fit
the
data
well.
Both
methods
often
end
up
selecting
the
same
model.
When
they
differ,
the
model
selected
by
the
     <a class="glossary" href="DataScienceIILectureBookli2.html#glo:bic">
      BIC
     </a>
     tends
to
be
simpler
     <alert style="color: #821131;">
      <span id="bold" style="font-weight:bold;">
       <sup class="textsuperscript">
        24
       </sup>
      </span>
     </alert>
     <span class="marginnote">
      <alert style="color: #821131;">
       <span id="bold" style="font-weight:bold;">
        <sup class="textsuperscript">
         24
        </sup>
       </span>
      </alert>
      <span style="color:#0063B2;">
       fewer
parameters.
      </span>
     </span>
     than
the
one
selected
by
the
     <a class="glossary" href="DataScienceIILectureBookli2.html#glo:aic">
      AIC
     </a>,
but
tends
to
not
fit
the
data
quite
as
well.
     <alert style="color: #821131;">
      <span id="bold" style="font-weight:bold;">
       <sup class="textsuperscript">
        25
       </sup>
      </span>
     </alert>
     <span class="marginnote">
      <alert style="color: #821131;">
       <span id="bold" style="font-weight:bold;">
        <sup class="textsuperscript">
         25
        </sup>
       </span>
      </alert>
      <span style="color:#0063B2;">
       this
                                                                                
                                                                                
is
especially
true
for
larger
datasets.
      </span>
     </span>
    </p>
    <div class="informationblock" id="tcolobox-65">
     <div class="title">
      <p class="noindent">
       <span id="bold" style="font-weight:bold;">
        Information
       </span>
       : The Likelihood Function
      </p>
     </div>
     <div class="box-content">
      <p class="noindent">
       The terms
       <alert style="color: #821131;">
        probability
       </alert>
       and
       <alert style="color: #821131;">
        likelihood
       </alert>
       are often used interchangeably in everyday language, but have very
different meanings in statistics and therefore is worth looking at it in detail.
      </p>
      <p class="noindent">
       Given
a
statistical
model
with
some
parameters
       <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
        <mi>
         𝜃
        </mi>
       </math>
       ,
the
word
“probability”
is
used
to
describe
how
plausible
a
future
outcome
       <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
        <mi>
         x
        </mi>
       </math>
       is,
       <alert style="color: #821131;">
        <span id="bold" style="font-weight:bold;">
         <sup class="textsuperscript">
          26
         </sup>
        </span>
       </alert>
       <span class="marginnote">
        <alert style="color: #821131;">
         <span id="bold" style="font-weight:bold;">
          <sup class="textsuperscript">
           26
          </sup>
         </span>
        </alert>
        <span style="color:#0063B2;">
         knowing
the
parameter
values
         <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
          <mi>
           𝜃
          </mi>
         </math>
        </span>
       </span>
       whereas
the
word
“likelihood”
is
used
to
describe
how
       <span id="bold" style="font-weight:bold;">
        plausible
       </span>
       a
particular
set
of
parameter
values
       <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
        <mi>
         𝜃
        </mi>
       </math>
       are,
after
the
outcome
       <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
        <mi>
         x
        </mi>
       </math>
       is
known.
      </p>
      <div class="figure">
       <p class="noindent">
        <img alt="PIC" height="" src="codes/images/Unsupervised-Learning/likelihood-function-plot-.svg" width="150%"/>
        <a id="x7-77001r20">
        </a>
       </p>
       <figcaption class="caption">
        <span class="id">
         Figure 5.20:
        </span>
        <span class="content">
         A model’s parametric function (top left), and some derived functions: a PDF (lower left), a
                    likelihood function (top right), and a log likelihood function (lower right).
        </span>
       </figcaption>
      </div>
      <p class="noindent">
       As an example, consider a 1D mixture model of two Gaussian distributions centered at (-4,1). For simplicity, this toy model has a
single parameter (
       <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
        <mi>
         𝜃
        </mi>
       </math>
       )
which controls the standard deviations of both distributions. The top-left contour plot in
       <span id="bold" style="font-weight:bold;">
        Fig.
       </span>
       <a href="#x7-77001r20">
        5.20
       </a>
       shows the entire model
       <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
        <mi>
         g
        </mi>
        <msup>
         <mrow>
          <mrow>
           <mo fence="true" form="prefix">
            (
           </mo>
           <mrow>
            <mi>
             x
            </mi>
            <mo class="MathClass-punc" stretchy="false">
             ;
            </mo>
            <mspace class="thinspace" width="0.17em">
            </mspace>
            <mi>
             𝜃
            </mi>
           </mrow>
           <mo fence="true" form="postfix">
            )
           </mo>
          </mrow>
         </mrow>
         <mrow>
         </mrow>
        </msup>
       </math>
       as a function
of both
       <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
        <mi>
         x
        </mi>
       </math>
       and
       <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
        <mi>
         𝜃
        </mi>
       </math>
       . To estimate the probability
distribution of a future outcome
       <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
        <mi>
         x
        </mi>
       </math>
       ,
we need to set the model parameter
       <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
        <mi>
         𝜃
        </mi>
       </math>
       .
      </p>
      <p class="noindent">
       For
example,
setting
       <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
        <mi>
         𝜃
        </mi>
       </math>
       to
1.3
(the
horizontal
line),
we
get
the
probability
density
function
       <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
        <mi>
         f
        </mi>
        <msup>
         <mrow>
          <mrow>
           <mo fence="true" form="prefix">
            (
           </mo>
           <mrow>
            <mi>
             x
            </mi>
            <mo class="MathClass-punc" stretchy="false">
             ;
            </mo>
            <mi>
             𝜃
            </mi>
            <mo class="MathClass-rel" stretchy="false">
             =
            </mo>
            <mn>
             1
            </mn>
            <mo class="MathClass-punc" stretchy="false">
             .
            </mo>
            <mn>
             3
            </mn>
           </mrow>
           <mo fence="true" form="postfix">
            )
           </mo>
          </mrow>
         </mrow>
         <mrow>
         </mrow>
        </msup>
       </math>
       shown
in
the
lower-left
plot.
Say
we
want
to
estimate
the
probability
that
x
will
fall
between
-2
and
+2.
We
must
calculate
the
integral
of
the
PDF
on
this
range.
       <alert style="color: #821131;">
        <span id="bold" style="font-weight:bold;">
         <sup class="textsuperscript">
          27
         </sup>
        </span>
       </alert>
       <span class="marginnote">
        <alert style="color: #821131;">
         <span id="bold" style="font-weight:bold;">
          <sup class="textsuperscript">
           27
          </sup>
         </span>
        </alert>
        <span style="color:#0063B2;">
         i.e.,
the
surface
of
the
shaded
region.
        </span>
       </span>
       But
what
if
we
don’t
know
       <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
        <mi>
         𝜃
        </mi>
       </math>
       ,
and
instead
if
we
have
observed
a
single
instance
x=2.5
(the
vertical
line
in
the
upper-left
plot)?
In
this
case,
we
get
the
likelihood
function
       <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
        <mi>
         L
        </mi>
        <mo class="MathClass-open" stretchy="false">
         (
        </mo>
        <mi>
         𝜃
        </mi>
        <mo class="MathClass-rel" stretchy="false">
         |
        </mo>
        <mi>
         x
        </mi>
        <mo class="MathClass-rel" stretchy="false">
         =
        </mo>
        <mn>
         2
        </mn>
        <mo class="MathClass-punc" stretchy="false">
         .
        </mo>
        <mn>
         5
        </mn>
        <mo class="MathClass-close" stretchy="false">
         )
        </mo>
        <mo class="MathClass-rel" stretchy="false">
         =
        </mo>
        <mi>
         f
        </mi>
        <mo class="MathClass-open" stretchy="false">
         (
        </mo>
        <mi>
         x
        </mi>
        <mo class="MathClass-rel" stretchy="false">
         =
        </mo>
        <mn>
         2
        </mn>
        <mo class="MathClass-punc" stretchy="false">
         .
        </mo>
        <mn>
         5
        </mn>
        <mo class="MathClass-punc" stretchy="false">
         ;
        </mo>
        <mi>
         𝜃
        </mi>
        <mo class="MathClass-close" stretchy="false">
         )
        </mo>
       </math>
       ,
represented
in
the
upper-right
plot.
      </p>
      <p class="noindent">
       In
short,
the
       <a class="glossary" href="DataScienceIILectureBookli2.html#glo:pdf">
        PDF
       </a>
       is
a
function
of
       <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
        <mi>
         x
        </mi>
       </math>
       (with
       <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
        <mi>
         𝜃
        </mi>
       </math>
       fixed),
while
the
likelihood
function
is
a
function
of
       <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
        <mi>
         𝜃
        </mi>
       </math>
       (with
       <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
        <mi>
         x
        </mi>
       </math>
       fixed).
It
is
important
to
understand
that
the
likelihood
function
is
not
a
probability
distribution:
if
we
integrate
a
probability
distribution
over
all
possible
values
of
       <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
        <mi>
         x
        </mi>
       </math>
       ,
we
always
get
1,
but
if
we
integrate
the
likelihood
function
over
all
possible
values
of
       <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
        <mi>
         𝜃
        </mi>
       </math>
       the
result
can
be
any
positive
value.
Given
a
dataset
       <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
        <mover accent="true">
         <mrow>
          <mi>
           X
          </mi>
         </mrow>
         <mo accent="true">
          →
         </mo>
        </mover>
       </math>
       ,
a
common
task
is
to
try
to
estimate
the
most
likely
values
for
the
model
parameters.
To
do
this,
we
must
find
the
values
that
maximize
the
likelihood
function,
given
X.
In
this
example,
if
we
have
observed
a
single
instance
       <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
        <mi>
         x
        </mi>
        <mo class="MathClass-rel" stretchy="false">
         =
        </mo>
        <mn>
         2
        </mn>
        <mo class="MathClass-punc" stretchy="false">
         .
        </mo>
        <mn>
         5
        </mn>
       </math>
       ,
the
       <a class="glossary" href="DataScienceIILectureBookli2.html#glo:mle">
        Maximum
Likelihood
Estimate
(MLE)
       </a>
       of
       <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
        <mi>
         𝜃
        </mi>
       </math>
       is
       <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
        <msubsup>
         <mrow>
          <mover accent="true">
           <mrow>
            <mi>
             𝜃
            </mi>
           </mrow>
           <mo accent="true">
            ^
           </mo>
          </mover>
         </mrow>
         <mrow>
         </mrow>
         <mrow>
         </mrow>
        </msubsup>
        <mo class="MathClass-rel" stretchy="false">
         =
        </mo>
        <mn>
         1
        </mn>
        <mo class="MathClass-punc" stretchy="false">
         .
        </mo>
        <mn>
         5
        </mn>
       </math>
       .
If
a
prior
probability
distribution
g
over
       <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
        <mi>
         𝜃
        </mi>
       </math>
       exists,
it
is
possible
to
take
it
into
account
by
maximising
       <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
        <mstyle mathvariant="script">
         <mi>
          L
         </mi>
        </mstyle>
        <msup>
         <mrow>
          <mrow>
           <mo fence="true" form="prefix">
            (
           </mo>
           <mrow>
            <mi>
             𝜃
            </mi>
            <mo class="MathClass-rel" stretchy="false">
             |
            </mo>
            <mi>
             x
            </mi>
           </mrow>
           <mo fence="true" form="postfix">
            )
           </mo>
          </mrow>
         </mrow>
         <mrow>
         </mrow>
        </msup>
        <mi>
         g
        </mi>
        <msup>
         <mrow>
          <mrow>
           <mo fence="true" form="prefix">
            (
           </mo>
           <mrow>
            <mi>
             𝜃
            </mi>
           </mrow>
           <mo fence="true" form="postfix">
            )
           </mo>
          </mrow>
         </mrow>
         <mrow>
         </mrow>
        </msup>
       </math>
       rather
than
just
maximising
       <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
        <mstyle mathvariant="script">
         <mi>
          L
         </mi>
        </mstyle>
        <msup>
         <mrow>
          <mrow>
           <mo fence="true" form="prefix">
            (
           </mo>
           <mrow>
            <mi>
             𝜃
            </mi>
            <mo class="MathClass-rel" stretchy="false">
             |
            </mo>
            <mi>
             x
            </mi>
           </mrow>
           <mo fence="true" form="postfix">
            )
           </mo>
          </mrow>
         </mrow>
         <mrow>
         </mrow>
        </msup>
       </math>
       .
This
is
called
       <a class="glossary" href="DataScienceIILectureBookli2.html#glo:map">
        Maximum
a-Posteriori
(MAP)
       </a>
       estimation.
Since
       <a class="glossary" href="DataScienceIILectureBookli2.html#glo:map">
        MAP
       </a>
       constrains
the
parameter
values,
we
can
think
of
it
as
a
regularised
version
of
       <a class="glossary" href="DataScienceIILectureBookli2.html#glo:mle">
        MLE
       </a>.
Notice
that
maximising
the
likelihood
function
is
equivalent
to
maximising
its
logarithm
(represented
in
the
lower-right
plot
in
       <span id="bold" style="font-weight:bold;">
        Fig.
       </span>
       <a href="#x7-77001r20">
        5.20
       </a>
       ).
Indeed,
the
logarithm
is
a
strictly
increasing
function,
so
if
       <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
        <mi>
         𝜃
        </mi>
       </math>
       maximises
the
log-likelihood,
it
also
maximises
the
likelihood.
It
turns
out
that
it
is
generally
easier
to
maximize
the
log
likelihood.
For
example,
if
we
observed
several
independent
instances
x(1)
to
x(m),
we
would
need
to
find
the
value
of
       <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
        <mi>
         𝜃
        </mi>
       </math>
       that
maximises
the
product
of
the
individual
likelihood
functions.
But
it
is
equivalent,
and
much
simpler,
to
maximize
the
sum
(not
the
product)
of
the
log
likelihood
functions,
thanks
to
the
magic
of
the
logarithm
which
converts
products
into
sums:
log(ab)
=
log(a)
+
log(b).
Once
we
have
estimated
       <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
        <msubsup>
         <mrow>
          <mover accent="true">
           <mrow>
            <mi>
             𝜃
            </mi>
           </mrow>
           <mo accent="true">
            ^
           </mo>
          </mover>
         </mrow>
         <mrow>
         </mrow>
         <mrow>
         </mrow>
        </msubsup>
       </math>
       ,
the
value
of
       <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
        <mi>
         𝜃
        </mi>
       </math>
       that
maximises
the
likelihood
function,
then
we
are
ready
to
compute
       <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
        <mover accent="true">
         <mrow>
          <mi>
           L
          </mi>
         </mrow>
         <mo accent="true">
          →
         </mo>
        </mover>
        <mo class="MathClass-rel" stretchy="false">
         =
        </mo>
        <mstyle mathvariant="script">
         <mi>
          L
         </mi>
        </mstyle>
        <msup>
         <mrow>
          <mrow>
           <mo fence="true" form="prefix">
            (
           </mo>
           <mrow>
            <msubsup>
             <mrow>
              <mover accent="true">
               <mrow>
                <mi>
                 𝜃
                </mi>
               </mrow>
               <mo accent="true">
                ^
               </mo>
              </mover>
             </mrow>
             <mrow>
             </mrow>
             <mrow>
             </mrow>
            </msubsup>
            <mo class="MathClass-punc" stretchy="false">
             ,
            </mo>
            <mspace class="thinspace" width="0.17em">
            </mspace>
            <mover accent="true">
             <mrow>
              <mi>
               X
              </mi>
             </mrow>
             <mo accent="true">
              →
             </mo>
            </mover>
           </mrow>
           <mo fence="true" form="postfix">
            )
           </mo>
          </mrow>
         </mrow>
         <mrow>
         </mrow>
        </msup>
       </math>
       ,
which
is
the
value
used
to
compute
the
AIC
and
BIC;
we
can
think
of
it
as
a
measure
of
how
well
the
model
fits
the
data.
      </p>
     </div>
    </div>
    <p class="noindent">
     To calculate the values for
     <a class="glossary" href="DataScienceIILectureBookli2.html#glo:bic">
      BIC
     </a>
     and
     <a class="glossary" href="DataScienceIILectureBookli2.html#glo:aic">
      AIC
     </a>, call the <span style="color:#054C5C;"><code class="verb">bic()</code></span>
     and <span style="color:#054C5C;"><code class="verb">aic()</code></span>
     methods:
    </p>
    <p class="noindent">
     Figure 9-20 shows the BIC for different numbers of clusters k. As we can see, both the BIC and the AIC are lowest when k=3, so
it is most likely the best choice.
    </p>
    <h3 class="subsectionHead">
     <a aria-label="Link to this section in PDF version" class="pdf-link" href="DataScienceIILectureBook.pdf#subsection.5.3.3" style="color: red;  position: absolute;  left: -30px;" target="_blank">
      <svg class="bi bi-file-pdf" fill="currentColor" height="20" viewbox="0 0 16 16" width="20" xmlns="http://www.w3.org/2000/svg">
       <path d="M4 0a2 2 0 0 0-2 2v12a2 2 0 0 0 2 2h8a2 2 0 0 0 2-2V2a2 2 0 0 0-2-2H4zm0 1h8a1 1 0 0 1 1 1v12a1 1 0 0 1-1 1H4a1 1 0 0 1-1-1V2a1 1 0 0 1 1-1z">
       </path>
       <path d="M4.603 12.087a.81.81 0 0 1-.438-.42c-.195-.388-.13-.776.08-1.102.198-.307.526-.568.897-.787a7.68 7.68 0 0 1 1.482-.645 19.701 19.701 0 0 0 1.062-2.227 7.269 7.269 0 0 1-.43-1.295c-.086-.4-.119-.796-.046-1.136.075-.354.274-.672.65-.823.192-.077.4-.12.602-.077a.7.7 0 0 1 .477.365c.088.164.12.356.127.538.007.187-.012.395-.047.614-.084.51-.27 1.134-.52 1.794a10.954 10.954 0 0 0 .98 1.686 5.753 5.753 0 0 1 1.334.05c.364.065.734.195.96.465.12.144.193.32.2.518.007.192-.047.382-.138.563a1.04 1.04 0 0 1-.354.416.856.856 0 0 1-.51.138c-.331-.014-.654-.196-.933-.417a5.716 5.716 0 0 1-.911-.95 11.642 11.642 0 0 0-1.997.406 11.311 11.311 0 0 1-1.021 1.51c-.29.35-.608.655-.926.787a.793.793 0 0 1-.58.029zm1.379-1.901c-.166.076-.32.156-.459.238-.328.194-.541.383-.647.547-.094.145-.096.25-.04.361.01.022.02.036.026.044a.27.27 0 0 0 .035-.012c.137-.056.355-.235.635-.572a8.18 8.18 0 0 0 .45-.606zm1.64-1.33a12.647 12.647 0 0 1 1.01-.193 11.666 11.666 0 0 1-.51-.858 20.741 20.741 0 0 1-.5 1.05zm2.446.45c.15.162.296.3.435.41.24.19.407.253.498.256a.107.107 0 0 0 .07-.015.307.307 0 0 0 .094-.125.436.436 0 0 0 .059-.2.095.095 0 0 0-.026-.063c-.052-.062-.2-.152-.518-.209a3.881 3.881 0 0 0-.612-.053zM8.078 5.8a6.7 6.7 0 0 0 .2-.828c.031-.188.043-.343.038-.465a.613.613 0 0 0-.032-.198.517.517 0 0 0-.145.04c-.087.035-.158.106-.196.283-.04.192-.03.469.046.822.024.111.054.227.09.346z">
       </path>
      </svg>
     </a>
     <small class="titlemark">
      5.3.3
     </small>
     <a id="x7-780005.3.3">
     </a>
     Bayesian Gaussian Mixture Models
    </h3>
    <p class="noindent">
     Rather
than
manually
searching
for
the
optimal
number
of
clusters
and
slow
down
the
process,
we
can
use
the <span style="color:#054C5C;"><code class="verb">BayesianGaussianMixture</code></span>
     class,
which
is
capable
of
giving
weights
equal
(or
close)
to
zero
to
unnecessary
clusters.
Set
the
number
of
clusters <span style="color:#054C5C;"><code class="verb">n_components</code></span>
     to
a
value
that
we
have
good
reason
to
believe
is
greater
than
the
optimal
number
                                                                                
                                                                                
of
clusters,
     <alert style="color: #821131;">
      <span id="bold" style="font-weight:bold;">
       <sup class="textsuperscript">
        28
       </sup>
      </span>
     </alert>
     <span class="marginnote">
      <alert style="color: #821131;">
       <span id="bold" style="font-weight:bold;">
        <sup class="textsuperscript">
         28
        </sup>
       </span>
      </alert>
      <span style="color:#0063B2;">
       this
assumes
some
minimal
knowledge
about
the
problem
at
hand.
      </span>
     </span>
     and
the
algorithm
will
eliminate
the
unnecessary
clusters
automatically.
    </p>
    <p class="noindent">
     For
example,
let’s
set
the
number
of
clusters
to
10
and
see
what
happens:
    </p>
    <p class="noindent">
     Excellent.
The
algorithm
automatically
detected
only
three
clusters
are
needed,
and
the
resulting
clusters
are
almost
identical
to
the
ones
in
     <span id="bold" style="font-weight:bold;">
      Fig.
     </span>
     <a href="#x7-76007r19">
      5.19
     </a>.
    </p>
    <p class="noindent">
     An important point to mention about Gaussian mixture models:
    </p>
    <div class="quoteblock">
     <p class="noindent">
      although
they
work
                                                                                
                                                                                
     
great
on
clusters
with
ellipsoidal
shapes,
they
don’t
do
so
well
with
clusters
of
very
different
shapes.
     </p>
    </div>
    <p class="noindent">
     For example, let’s see what happens if we use a Bayesian Gaussian mixture model to cluster the moons dataset (see Figure
9-21).
    </p>
    <p class="noindent">
     The algorithm desperately searched for ellipsoids, so it found eight different clusters instead of two. The density
estimation is not too bad, so this model could perhaps be used for anomaly detection, but it failed to identify the two
moons.
    </p>
    <h3 class="subsectionHead">
     <a aria-label="Link to this section in PDF version" class="pdf-link" href="DataScienceIILectureBook.pdf#subsection.5.3.4" style="color: red;  position: absolute;  left: -30px;" target="_blank">
      <svg class="bi bi-file-pdf" fill="currentColor" height="20" viewbox="0 0 16 16" width="20" xmlns="http://www.w3.org/2000/svg">
       <path d="M4 0a2 2 0 0 0-2 2v12a2 2 0 0 0 2 2h8a2 2 0 0 0 2-2V2a2 2 0 0 0-2-2H4zm0 1h8a1 1 0 0 1 1 1v12a1 1 0 0 1-1 1H4a1 1 0 0 1-1-1V2a1 1 0 0 1 1-1z">
       </path>
       <path d="M4.603 12.087a.81.81 0 0 1-.438-.42c-.195-.388-.13-.776.08-1.102.198-.307.526-.568.897-.787a7.68 7.68 0 0 1 1.482-.645 19.701 19.701 0 0 0 1.062-2.227 7.269 7.269 0 0 1-.43-1.295c-.086-.4-.119-.796-.046-1.136.075-.354.274-.672.65-.823.192-.077.4-.12.602-.077a.7.7 0 0 1 .477.365c.088.164.12.356.127.538.007.187-.012.395-.047.614-.084.51-.27 1.134-.52 1.794a10.954 10.954 0 0 0 .98 1.686 5.753 5.753 0 0 1 1.334.05c.364.065.734.195.96.465.12.144.193.32.2.518.007.192-.047.382-.138.563a1.04 1.04 0 0 1-.354.416.856.856 0 0 1-.51.138c-.331-.014-.654-.196-.933-.417a5.716 5.716 0 0 1-.911-.95 11.642 11.642 0 0 0-1.997.406 11.311 11.311 0 0 1-1.021 1.51c-.29.35-.608.655-.926.787a.793.793 0 0 1-.58.029zm1.379-1.901c-.166.076-.32.156-.459.238-.328.194-.541.383-.647.547-.094.145-.096.25-.04.361.01.022.02.036.026.044a.27.27 0 0 0 .035-.012c.137-.056.355-.235.635-.572a8.18 8.18 0 0 0 .45-.606zm1.64-1.33a12.647 12.647 0 0 1 1.01-.193 11.666 11.666 0 0 1-.51-.858 20.741 20.741 0 0 1-.5 1.05zm2.446.45c.15.162.296.3.435.41.24.19.407.253.498.256a.107.107 0 0 0 .07-.015.307.307 0 0 0 .094-.125.436.436 0 0 0 .059-.2.095.095 0 0 0-.026-.063c-.052-.062-.2-.152-.518-.209a3.881 3.881 0 0 0-.612-.053zM8.078 5.8a6.7 6.7 0 0 0 .2-.828c.031-.188.043-.343.038-.465a.613.613 0 0 0-.032-.198.517.517 0 0 0-.145.04c-.087.035-.158.106-.196.283-.04.192-.03.469.046.822.024.111.054.227.09.346z">
       </path>
      </svg>
     </a>
     <small class="titlemark">
      5.3.4
     </small>
     <a id="x7-790005.3.4">
     </a>
     Other Algorithms for Anomaly and Novelty Detection
    </h3>
    <p class="noindent">
     Of course there are other algorithms as well with <span style="color:#054C5C;"><code class="verb">sklearn</code></span>
     implementing other algorithms dedicated to anomaly detection or
novelty detection:
     <a id="paragraph*.13">
     </a>
    </p>
    <p class="noindent">
     <span class="paragraphHead">
      <a id="x7-80000">
      </a>
      Fast-MCD
     </span>
     Stands
for
     <span id="bold" style="font-weight:bold;">
      minimum
covariance
determinant
     </span>
     .
Implemented
by
the <span style="color:#054C5C;"><code class="verb">EllipticEnvelope</code></span>
     class,
this
algorithm
is
useful
for
outlier
detection,
in
particular
to
clean
up
a
dataset.
It
assumes
the
normal
instances
(inliers)
are
generated
                                                                                
                                                                                
from
a
single
Gaussian
distribution.
     <alert style="color: #821131;">
      <span id="bold" style="font-weight:bold;">
       <sup class="textsuperscript">
        29
       </sup>
      </span>
     </alert>
     <span class="marginnote">
      <alert style="color: #821131;">
       <span id="bold" style="font-weight:bold;">
        <sup class="textsuperscript">
         29
        </sup>
       </span>
      </alert>
      <span style="color:#0063B2;">
       Which
is
not
a
mixture.
      </span>
     </span>
     It
also
assumes
the
dataset
is
contaminated
with
outliers
which
were
not
generated
from
this
Gaussian
distribution.
    </p>
    <p class="noindent">
     MCD
is
a
method
for
estimating
the
mean
and
covariance
matrix
in
a
way
that
tries
to
minimize
the
influence
of
anomalies.
When
the
algorithm
estimates
the
parameters
of
the
Gaussian
distribution,
     <alert style="color: #821131;">
      <span id="bold" style="font-weight:bold;">
       <sup class="textsuperscript">
        30
       </sup>
      </span>
     </alert>
     <span class="marginnote">
      <alert style="color: #821131;">
       <span id="bold" style="font-weight:bold;">
        <sup class="textsuperscript">
         30
        </sup>
       </span>
      </alert>
      <span style="color:#0063B2;">
       i.e.,
the
shape
of
the
elliptic
                                                                                
                                                                                
envelope
around
the
inliers.
      </span>
     </span>
     it
is
careful
to
ignore
the
instances
that
are
most
likely
outliers.
    </p>
    <p class="noindent">
     This
technique
gives
a
better
estimation
of
the
elliptic
envelope
and
thus
makes
the
algorithm
better
at
identifying
the
outliers.
     <a id="paragraph*.14">
     </a>
    </p>
    <p class="noindent">
     <span class="paragraphHead">
      <a id="x7-81000">
      </a>
      Isolation Forest
     </span>
     An
efficient
algorithm
for
outlier
detection,
especially
in
high-dimensional
datasets.
The
algorithm
builds
a
random
forest
in
which
each
decision
tree
is
grown
randomly.
At
each
node,
                                                                                
                                                                                
it
picks
a
feature
randomly,
then
it
picks
a
random
threshold
value
     <alert style="color: #821131;">
      <span id="bold" style="font-weight:bold;">
       <sup class="textsuperscript">
        31
       </sup>
      </span>
     </alert>
     <span class="marginnote">
      <alert style="color: #821131;">
       <span id="bold" style="font-weight:bold;">
        <sup class="textsuperscript">
         31
        </sup>
       </span>
      </alert>
      <span style="color:#0063B2;">
       between
the
min
and
max
values
      </span>
     </span>
     to
split
the
dataset
in
two.
The
dataset
gradually
gets
chopped
into
pieces
this
way,
until
all
instances
end
up
isolated
from
the
other
instances.
Anomalies
are
usually
far
from
other
instances,
so
on
average
     <alert style="color: #821131;">
      <span id="bold" style="font-weight:bold;">
       <sup class="textsuperscript">
        32
       </sup>
      </span>
     </alert>
     <span class="marginnote">
      <alert style="color: #821131;">
       <span id="bold" style="font-weight:bold;">
        <sup class="textsuperscript">
         32
        </sup>
       </span>
      </alert>
      <span style="color:#0063B2;">
       across
all
the
decision
trees
      </span>
     </span>
     they
tend
to
get
isolated
in
fewer
                                                                                
                                                                                
steps
than
normal
instances <a id="x7-81001"></a><a href="#X0-liu2008isolation">[22]</a>.
     <a id="paragraph*.15">
     </a>
    </p>
    <p class="noindent">
     <span class="paragraphHead">
      <a id="x7-82000">
      </a>
      Local Outlier Factor
     </span>
     Used in outlier detection as it compares the density of instances around a given instance to the density around its neighbors. An anomaly is often more
isolated than its
     <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
      <mi>
       k
      </mi>
     </math>
     nearest
neighbours.
     <a id="paragraph*.16">
     </a>
    </p>
    <p class="noindent">
     <span class="paragraphHead">
      <a id="x7-83000">
      </a>
      One-Class SVM
     </span>
     Suited for novelty detection. Recall that a kernelized SVM classifier separates two classes by first (implicitly) mapping all the
instances to a high-dimensional space, then separating the two classes using a linear SVM classifier within this
high-dimensional space. Since we just have one class of instances, the one-class SVM algorithm instead tries to separate the
instances in high-dimensional space from the origin. In the original space, this will correspond to finding a small
region that encompasses all the instances. If a new instance does not fall within this region, it is an anomaly.
There are a few hyperparameters to tweak: the usual ones for a kernelized SVM, plus a margin hyperparameter
that corresponds to the probability of a new instance being mistakenly considered as novel when it is in fact
normal. It works great, especially with high-dimensional datasets, but like all SVMs it does not scale to large
datasets.
    </p>
    <p class="noindent">
     PCA and other dimensionality reduction techniques with an <span style="color:#054C5C;"><code class="verb">inverse_transform()</code></span>
     method If we compare the reconstruction error
of a normal instance with the reconstruction error of an anomaly, the latter will usually be much larger. This is a simple and
often quite efficient anomaly detection approach.
    </p>
   </article>
   <footer>
    <div class="next">
     <p class="noindent">
      <a href="DataScienceIILectureBookch6.html" style="float: right;">
       Next Chapter →
      </a>
      <a href="DataScienceIILectureBookch4.html" style="float: left;">
       ← Previous Chapter
      </a>
      <a href="index.html">
       TOP
      </a>
     </p>
    </div>
   </footer>
  </div>
  <p class="noindent">
   <a id="tailDataScienceIILectureBookch5.html">
   </a>
  </p>
 </body>
</html>
