<!DOCTYPE html>
<html lang="en-US" xml:lang="en-US">
 <head>
  <title>
   2 Decision Trees
  </title>
    <link rel='icon' type='image/x-icon' href='figures/logo.svg'></link>
  <meta charset="utf-8"/>
  <meta content="ParSnip" name="generator"/>
  <meta content="width=device-width,initial-scale=1" name="viewport"/>
  <link href="DataScienceIILectureBook.css" rel="stylesheet" type="text/css"/>
  <meta content="DataScienceIILectureBook.tex" name="src"/>
  <script async="async" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/mml-chtml.js" type="text/javascript">
  </script>
  <link href="flatweb.css" rel="stylesheet" type="text/css"/>
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.7.1/jquery.min.js">
  </script>
  <script src="flatjs.js">
  </script>
  <link href="http://fonts.googleapis.com/css?family=Roboto" rel="stylesheet" type="text/css"/>
  <script type="module">
   import pdfjsDist from 'https://cdn.jsdelivr.net/npm/pdfjs-dist@5.3.93/+esm'
  </script>
  <script>
   function moveTOC() { 
var htmlShow = document.getElementsByClassName("wide"); 
if (htmlShow[0].style.display === "none") { 
htmlShow[0].style.display = "block"; 
} else { 
htmlShow[0].style.display = "none"; 
} 
}
  </script>
  <link href="https://unpkg.com/nord-highlightjs@0.1.0/dist/nord.css" rel="stylesheet" type="text/css"/>
  <script src="https://unpkg.com/@highlightjs/cdn-assets@11.3.1/highlight.min.js">
  </script>
  <script src="https://unpkg.com/highlightjs-copy/dist/highlightjs-copy.min.js">
  </script>
  <link href="https://unpkg.com/highlightjs-copy/dist/highlightjs-copy.min.css" rel="stylesheet"/>
  <script src="flatjs.js">
  </script>
  <header>
  </header>
 </head>
 <body id="top">
  <nav class="wide">
   <div class="contents">
    <h3 style="font-weight:lighter; padding: 20px 0 20px 0;">
     2
   
     
   

   Decision Trees
    </h3>
    <h4 style="font-weight:lighter">
     Chapter Table of Contents
    </h4>
    <ul>
     <div class="chapterTOCS">
      <li>
       <span class="sectionToc">
        <small>
         2.1
        </small>
        <a href="#x4-120002.1">
         Introduction
        </a>
       </span>
      </li>
      <li>
       <span class="subsectionToc" style="font-size:12px;margin:0px;">
        <small>
         2.1.1
        </small>
        <a href="#x4-130002.1.1">
         Advantages and Disadvantages
        </a>
       </span>
      </li>
      <li>
       <span class="sectionToc">
        <small>
         2.2
        </small>
        <a href="#x4-140002.2">
         Training and Visualising Decision Trees
        </a>
       </span>
      </li>
      <li>
       <span class="sectionToc">
        <small>
         2.3
        </small>
        <a href="#x4-150002.3">
         Making Predictions
        </a>
       </span>
      </li>
      <li>
       <span class="subsectionToc" style="font-size:12px;margin:0px;">
        <small>
         2.3.1
        </small>
        <a href="#x4-160002.3.1">
         Gini Impurity
        </a>
       </span>
      </li>
      <li>
       <span class="sectionToc">
        <small>
         2.4
        </small>
        <a href="#x4-170002.4">
         Estimating Class Probabilities
        </a>
       </span>
      </li>
      <li>
       <span class="sectionToc">
        <small>
         2.5
        </small>
        <a href="#x4-180002.5">
         The CART Training Algorithm
        </a>
       </span>
      </li>
      <li>
       <span class="sectionToc">
        <small>
         2.6
        </small>
        <a href="#x4-190002.6">
         Gini Impurity or Entropy?
        </a>
       </span>
      </li>
      <li>
       <span class="sectionToc">
        <small>
         2.7
        </small>
        <a href="#x4-200002.7">
         Regularization Hyperparameters
        </a>
       </span>
      </li>
      <li>
       <span class="sectionToc">
        <small>
         2.8
        </small>
        <a href="#x4-210002.8">
         Regression
        </a>
       </span>
      </li>
      <li>
       <span class="sectionToc">
        <small>
         2.9
        </small>
        <a href="#x4-220002.9">
         Sensitivity to Axis Orientation
        </a>
       </span>
      </li>
      <li>
       <span class="sectionToc">
        <small>
         2.10
        </small>
        <a href="#x4-230002.10">
         DTs Have a High Variance
        </a>
       </span>
      </li>
     </div>
    </ul>
    <div class="prev-next" style="text-align: center">
     <p class="noindent">
      <a href="DataScienceIILectureBookch3.html" style="float:right; font-size:10px">
       NEXT →
      </a>
      <a href="DataScienceIILectureBookch1.html" style="float:left; font-size:10px">
       ← PREV
      </a>
      <a href="index.html">
       TOP
      </a>
     </p>
    </div>
    <div id="author-info" style="bottom: 0; padding-bottom: 10px;">
     <div id="author-logo">
      <img src="figures/logo.svg" style="margin: 10px 0;"/>
      <div>
       <div>
        <h4 style="color:#7aa0b8; font-weight:lighter;">
         WebBook | D. T. McGuiness, P.hD
        </h4>
       </div>
      </div>
      <div id="author-text" style="bottom: 0; padding-top: 50px;border-top: solid 1px #3b4b5e;font-size: 12px;text-align: right;">
       <p>
        <b>
         Authors Note
        </b>
        The website you are viewing is auto-generated
    using ParSnip and therefore subject to slight errors in
    typography and formatting. When in doubt, please consult the
    LectureBook.
       </p>
      </div>
     </div>
    </div>
   </div>
  </nav>
  <div class="page">
   <article class="chapter">
    <h1 class="chapterHead">
     <div class="number">
      2
     </div>
     <a id="x4-110002">
     </a>
     Decision Trees
    </h1><button id='toc-button' onclick='moveTOC()'>TOC</button>
    <div class="section-control" style="text-align: center">
     <a id="prev-section" onclick="goPrev()" style="float:left;">
      ← Previous Section
     </a>
     <a id="next-section" onclick="goNext()" style="float:right;">
      Next Section →
     </a>
    </div>
    <h2 class="sectionHead">
     <a aria-label="Link to this section in PDF version" class="pdf-link" href="DataScienceIILectureBook.pdf#section.2.1" style="color: red;  position: absolute;  left: -30px;" target="_blank">
      <svg class="bi bi-file-pdf" fill="currentColor" height="20" viewbox="0 0 16 16" width="20" xmlns="http://www.w3.org/2000/svg">
       <path d="M4 0a2 2 0 0 0-2 2v12a2 2 0 0 0 2 2h8a2 2 0 0 0 2-2V2a2 2 0 0 0-2-2H4zm0 1h8a1 1 0 0 1 1 1v12a1 1 0 0 1-1 1H4a1 1 0 0 1-1-1V2a1 1 0 0 1 1-1z">
       </path>
       <path d="M4.603 12.087a.81.81 0 0 1-.438-.42c-.195-.388-.13-.776.08-1.102.198-.307.526-.568.897-.787a7.68 7.68 0 0 1 1.482-.645 19.701 19.701 0 0 0 1.062-2.227 7.269 7.269 0 0 1-.43-1.295c-.086-.4-.119-.796-.046-1.136.075-.354.274-.672.65-.823.192-.077.4-.12.602-.077a.7.7 0 0 1 .477.365c.088.164.12.356.127.538.007.187-.012.395-.047.614-.084.51-.27 1.134-.52 1.794a10.954 10.954 0 0 0 .98 1.686 5.753 5.753 0 0 1 1.334.05c.364.065.734.195.96.465.12.144.193.32.2.518.007.192-.047.382-.138.563a1.04 1.04 0 0 1-.354.416.856.856 0 0 1-.51.138c-.331-.014-.654-.196-.933-.417a5.716 5.716 0 0 1-.911-.95 11.642 11.642 0 0 0-1.997.406 11.311 11.311 0 0 1-1.021 1.51c-.29.35-.608.655-.926.787a.793.793 0 0 1-.58.029zm1.379-1.901c-.166.076-.32.156-.459.238-.328.194-.541.383-.647.547-.094.145-.096.25-.04.361.01.022.02.036.026.044a.27.27 0 0 0 .035-.012c.137-.056.355-.235.635-.572a8.18 8.18 0 0 0 .45-.606zm1.64-1.33a12.647 12.647 0 0 1 1.01-.193 11.666 11.666 0 0 1-.51-.858 20.741 20.741 0 0 1-.5 1.05zm2.446.45c.15.162.296.3.435.41.24.19.407.253.498.256a.107.107 0 0 0 .07-.015.307.307 0 0 0 .094-.125.436.436 0 0 0 .059-.2.095.095 0 0 0-.026-.063c-.052-.062-.2-.152-.518-.209a3.881 3.881 0 0 0-.612-.053zM8.078 5.8a6.7 6.7 0 0 0 .2-.828c.031-.188.043-.343.038-.465a.613.613 0 0 0-.032-.198.517.517 0 0 0-.145.04c-.087.035-.158.106-.196.283-.04.192-.03.469.046.822.024.111.054.227.09.346z">
       </path>
      </svg>
     </a>
     <small class="titlemark">
      2.1
     </small>
     <a id="x4-120002.1">
     </a>
     Introduction
    </h2>
    <p class="noindent">
     <a class="glossary" href="DataScienceIILectureBookli2.html#glo:dt">
      Decision Tree (DT)
     </a>
     is a versatile
     <a class="glossary" href="DataScienceIILectureBookli2.html#glo:ml">
      ML
     </a>
     algorithm which can perform both classification and regression
tasks, and even multioutput tasks, capable of fitting complex datasets. They are classified as a
non-parametric supervised learning algorithm, which is utilized for both classification and
regression tasks. It has a hierarchical, tree structure, which consists of a root node, branches,
internal nodes and leaf nodes.
     <a class="glossary" href="DataScienceIILectureBookli2.html#glo:dt">
      DT
     </a>
     s are also the fundamental components of random forests,
which are among the most powerful
     <a class="glossary" href="DataScienceIILectureBookli2.html#glo:ml">
      ML
     </a>
     algorithms available today. Some of the applications
include:
    </p>
    <dl class="description">
     <dt class="description">
      <a id="x4-12001x2.1">
      </a>
      Loan Approval in Banking
     </dt>
     <dd class="description">
      <p class="noindent">
       Banks use Decision Trees to assess whether a loan application should be approved. The
decision is based on factors like credit score, income, employment status and loan history.
This helps predict approval or rejection helps in enabling quick and reliable decisions.
      </p>
     </dd>
     <dt class="description">
      <a id="x4-12002x2.1">
      </a>
      Medical Diagnosis
     </dt>
     <dd class="description">
      <p class="noindent">
       In healthcare they assist in diagnosing diseases. For example, they can predict whether a
patient has diabetes based on clinical data like glucose levels, BMI and blood pressure. This
helps classify patients into diabetic or non-diabetic categories, supporting early diagnosis
and treatment.
      </p>
     </dd>
     <dt class="description">
      <a id="x4-12003x2.1">
      </a>
      Predicting Exam Results in Education
     </dt>
     <dd class="description">
      <p class="noindent">
       Educational institutions use to predict whether a student will pass or fail based on factors
like attendance, study time and past grades. This helps teachers identify at-risk students
and offer targeted support.
      </p>
     </dd>
     <dt class="description">
      <a id="x4-12004x2.1">
      </a>
      Customer Churn Prediction
     </dt>
     <dd class="description">
      <p class="noindent">
       Companies use Decision Trees to predict whether a customer will leave or stay based
on behavior patterns, purchase history, and interactions. This allows businesses to take
proactive steps to retain customers.
      </p>
     </dd>
     <dt class="description">
      <a id="x4-12005x2.1">
      </a>
      Fraud Detection
     </dt>
     <dd class="description">
      <p class="noindent">
       In finance, Decision Trees are used to detect fraudulent activities, such as credit card fraud.
By analyzing past transaction data and patterns, Decision Trees can identify suspicious
activities and flag them for further investigation.
      </p>
     </dd>
    </dl>
    <p class="noindent">
     In this chapter we will start by discussing how to train, visualise, and make predictions with
     <a class="glossary" href="DataScienceIILectureBookli2.html#glo:dt">
      DT
     </a>
     s. Then
we will go through the CART training algorithm used by <span style="color:#054C5C;"><code class="verb">sklearn</code></span>, and we will explore how to
regularise trees and use them for regression tasks.
    </p>
    <p class="noindent">
     Finally, we will discuss some of the limitations of
     <a class="glossary" href="DataScienceIILectureBookli2.html#glo:dt">
      DT
     </a>
     s.
    </p>
    <h3 class="subsectionHead">
     <a aria-label="Link to this section in PDF version" class="pdf-link" href="DataScienceIILectureBook.pdf#subsection.2.1.1" style="color: red;  position: absolute;  left: -30px;" target="_blank">
      <svg class="bi bi-file-pdf" fill="currentColor" height="20" viewbox="0 0 16 16" width="20" xmlns="http://www.w3.org/2000/svg">
       <path d="M4 0a2 2 0 0 0-2 2v12a2 2 0 0 0 2 2h8a2 2 0 0 0 2-2V2a2 2 0 0 0-2-2H4zm0 1h8a1 1 0 0 1 1 1v12a1 1 0 0 1-1 1H4a1 1 0 0 1-1-1V2a1 1 0 0 1 1-1z">
       </path>
       <path d="M4.603 12.087a.81.81 0 0 1-.438-.42c-.195-.388-.13-.776.08-1.102.198-.307.526-.568.897-.787a7.68 7.68 0 0 1 1.482-.645 19.701 19.701 0 0 0 1.062-2.227 7.269 7.269 0 0 1-.43-1.295c-.086-.4-.119-.796-.046-1.136.075-.354.274-.672.65-.823.192-.077.4-.12.602-.077a.7.7 0 0 1 .477.365c.088.164.12.356.127.538.007.187-.012.395-.047.614-.084.51-.27 1.134-.52 1.794a10.954 10.954 0 0 0 .98 1.686 5.753 5.753 0 0 1 1.334.05c.364.065.734.195.96.465.12.144.193.32.2.518.007.192-.047.382-.138.563a1.04 1.04 0 0 1-.354.416.856.856 0 0 1-.51.138c-.331-.014-.654-.196-.933-.417a5.716 5.716 0 0 1-.911-.95 11.642 11.642 0 0 0-1.997.406 11.311 11.311 0 0 1-1.021 1.51c-.29.35-.608.655-.926.787a.793.793 0 0 1-.58.029zm1.379-1.901c-.166.076-.32.156-.459.238-.328.194-.541.383-.647.547-.094.145-.096.25-.04.361.01.022.02.036.026.044a.27.27 0 0 0 .035-.012c.137-.056.355-.235.635-.572a8.18 8.18 0 0 0 .45-.606zm1.64-1.33a12.647 12.647 0 0 1 1.01-.193 11.666 11.666 0 0 1-.51-.858 20.741 20.741 0 0 1-.5 1.05zm2.446.45c.15.162.296.3.435.41.24.19.407.253.498.256a.107.107 0 0 0 .07-.015.307.307 0 0 0 .094-.125.436.436 0 0 0 .059-.2.095.095 0 0 0-.026-.063c-.052-.062-.2-.152-.518-.209a3.881 3.881 0 0 0-.612-.053zM8.078 5.8a6.7 6.7 0 0 0 .2-.828c.031-.188.043-.343.038-.465a.613.613 0 0 0-.032-.198.517.517 0 0 0-.145.04c-.087.035-.158.106-.196.283-.04.192-.03.469.046.822.024.111.054.227.09.346z">
       </path>
      </svg>
     </a>
     <small class="titlemark">
      2.1.1
     </small>
     <a id="x4-130002.1.1">
     </a>
     Advantages and Disadvantages
    </h3>
    <p class="noindent">
     Before we start with our chapter on
     <a class="glossary" href="DataScienceIILectureBookli2.html#glo:dt">
      DT
     </a>, lets list down the advantages it has over other
methods:
    </p>
    <dl class="description">
     <dt class="description">
      <a id="x4-13001x2.1.1">
      </a>
      Easy to Understand
     </dt>
     <dd class="description">
      <p class="noindent">
       Decision Trees are visual which makes it easy to follow the decision-making process
      </p>
     </dd>
     <dt class="description">
      <a id="x4-13002x2.1.1">
      </a>
      Versatility
     </dt>
     <dd class="description">
      <p class="noindent">
       Can be used for both classification and regression problems.
      </p>
     </dd>
     <dt class="description">
      <a id="x4-13003x2.1.1">
      </a>
      No Need for Feature Scaling
     </dt>
     <dd class="description">
      <p class="noindent">
       Unlike many machine learning models, it dont require us to scale or normalize our data.
      </p>
     </dd>
     <dt class="description">
      <a id="x4-13004x2.1.1">
      </a>
      Handles Non-linear Relationships
     </dt>
     <dd class="description">
      <p class="noindent">
       It capture complex, non-linear relationships between features and outcomes effectively.
      </p>
     </dd>
     <dt class="description">
      <a id="x4-13005x2.1.1">
      </a>
      Interpretability
     </dt>
     <dd class="description">
      <p class="noindent">
       The tree structure is easy to interpret helps in allowing users to understand the reasoning
behind each decision.
      </p>
     </dd>
     <dt class="description">
      <a id="x4-13006x2.1.1">
      </a>
      Handles Missing Data
     </dt>
     <dd class="description">
      <p class="noindent">
       It can handle missing values by using strategies like assigning the most common value or
ignoring missing data during splits.
      </p>
     </dd>
    </dl>
    <p class="noindent">
     Of course, as with every method, there are it’s disadvantages which are:
    </p>
    <dl class="description">
     <dt class="description">
      <a id="x4-13007x2.1.1">
      </a>
      Overfitting
     </dt>
     <dd class="description">
      <p class="noindent">
       They can overfit the training data if they are too deep which means they memorize the
data instead of learning general patterns. This leads to poor performance on unseen data.
      </p>
     </dd>
     <dt class="description">
      <a id="x4-13008x2.1.1">
      </a>
      Instability
     </dt>
     <dd class="description">
      <p class="noindent">
       It can be unstable which means that small changes in the data may lead to significant
differences in the tree structure and predictions.
      </p>
     </dd>
     <dt class="description">
      <a id="x4-13009x2.1.1">
      </a>
      Bias towards Features with Many Categories
     </dt>
     <dd class="description">
      <p class="noindent">
       It can become biased toward features with many distinct values which focuses too much
on them and potentially missing other important features which can reduce prediction
accuracy.
      </p>
     </dd>
     <dt class="description">
      <a id="x4-13010x2.1.1">
      </a>
      Difficulty in Capturing Complex Interactions
     </dt>
     <dd class="description">
      <p class="noindent">
       Decision Trees may struggle to capture complex interactions between features which helps
in making them less effective for certain types of data.
      </p>
     </dd>
     <dt class="description">
      <a id="x4-13011x2.1.1">
      </a>
      Computationally Expensive for Large Datasets
     </dt>
     <dd class="description">
      <p class="noindent">
       For large datasets, building and pruning a Decision Tree can be computationally intensive,
especially as the tree depth increases.
      </p>
     </dd>
    </dl>
    <p class="noindent">
    </p>
    <h2 class="sectionHead">
     <a aria-label="Link to this section in PDF version" class="pdf-link" href="DataScienceIILectureBook.pdf#section.2.2" style="color: red;  position: absolute;  left: -30px;" target="_blank">
      <svg class="bi bi-file-pdf" fill="currentColor" height="20" viewbox="0 0 16 16" width="20" xmlns="http://www.w3.org/2000/svg">
       <path d="M4 0a2 2 0 0 0-2 2v12a2 2 0 0 0 2 2h8a2 2 0 0 0 2-2V2a2 2 0 0 0-2-2H4zm0 1h8a1 1 0 0 1 1 1v12a1 1 0 0 1-1 1H4a1 1 0 0 1-1-1V2a1 1 0 0 1 1-1z">
       </path>
       <path d="M4.603 12.087a.81.81 0 0 1-.438-.42c-.195-.388-.13-.776.08-1.102.198-.307.526-.568.897-.787a7.68 7.68 0 0 1 1.482-.645 19.701 19.701 0 0 0 1.062-2.227 7.269 7.269 0 0 1-.43-1.295c-.086-.4-.119-.796-.046-1.136.075-.354.274-.672.65-.823.192-.077.4-.12.602-.077a.7.7 0 0 1 .477.365c.088.164.12.356.127.538.007.187-.012.395-.047.614-.084.51-.27 1.134-.52 1.794a10.954 10.954 0 0 0 .98 1.686 5.753 5.753 0 0 1 1.334.05c.364.065.734.195.96.465.12.144.193.32.2.518.007.192-.047.382-.138.563a1.04 1.04 0 0 1-.354.416.856.856 0 0 1-.51.138c-.331-.014-.654-.196-.933-.417a5.716 5.716 0 0 1-.911-.95 11.642 11.642 0 0 0-1.997.406 11.311 11.311 0 0 1-1.021 1.51c-.29.35-.608.655-.926.787a.793.793 0 0 1-.58.029zm1.379-1.901c-.166.076-.32.156-.459.238-.328.194-.541.383-.647.547-.094.145-.096.25-.04.361.01.022.02.036.026.044a.27.27 0 0 0 .035-.012c.137-.056.355-.235.635-.572a8.18 8.18 0 0 0 .45-.606zm1.64-1.33a12.647 12.647 0 0 1 1.01-.193 11.666 11.666 0 0 1-.51-.858 20.741 20.741 0 0 1-.5 1.05zm2.446.45c.15.162.296.3.435.41.24.19.407.253.498.256a.107.107 0 0 0 .07-.015.307.307 0 0 0 .094-.125.436.436 0 0 0 .059-.2.095.095 0 0 0-.026-.063c-.052-.062-.2-.152-.518-.209a3.881 3.881 0 0 0-.612-.053zM8.078 5.8a6.7 6.7 0 0 0 .2-.828c.031-.188.043-.343.038-.465a.613.613 0 0 0-.032-.198.517.517 0 0 0-.145.04c-.087.035-.158.106-.196.283-.04.192-.03.469.046.822.024.111.054.227.09.346z">
       </path>
      </svg>
     </a>
     <small class="titlemark">
      2.2
     </small>
     <a id="x4-140002.2">
     </a>
     Training and Visualising Decision Trees
    </h2>
    <p class="noindent">
     To understand
     <a class="glossary" href="DataScienceIILectureBookli2.html#glo:dt">
      DT
     </a>
     s, let’s build one and take a look at how it makes predictions. The following code
trains a <span style="color:#054C5C;"><code class="verb">DecisionTreeClassifier</code></span>
     on the iris dataset:
    </p>
    <p class="noindent">
    </p>
    <pre><div id="fancyvrb11" style="padding:20px;border-radius: 3px;"><a id="x4-14002r23"></a><span style="color:#2B2BFF;">from</span><span style="color:#BABABA;"> </span>sklearn.datasets<span style="color:#BABABA;"> </span><span style="color:#2B2BFF;">import</span> load_iris 
<a id="x4-14004r24"></a><span style="color:#2B2BFF;">from</span><span style="color:#BABABA;"> </span>sklearn.tree<span style="color:#BABABA;"> </span><span style="color:#2B2BFF;">import</span> DecisionTreeClassifier 
<a id="x4-14006r25"></a>iris = load_iris(as_frame=<span style="color:#2B2BFF;"><span id="bold" style="font-weight:bold;">True</span></span>) 
<a id="x4-14008r26"></a>X_iris = iris.data[[<span style="color:#800080;">"petal length (cm)"</span>, <span style="color:#800080;">"petal width (cm)"</span>]].values 
<a id="x4-14010r27"></a>y_iris = iris.target 
<a id="x4-14012r28"></a>tree_clf = DecisionTreeClassifier(max_depth=<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">2</span></span>, random_state=<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">42</span></span>) 
<a id="x4-14014r29"></a>tree_clf.fit(X_iris, y_iris)</div></pre>
    <p class="noindent">
     We can visualize the trained
     <a class="glossary" href="DataScienceIILectureBookli2.html#glo:dt">
      DT
     </a>
     by first using the <span style="color:#054C5C;"><code class="verb">export_graphviz()</code></span>
     function to output a graph
definition file called <span style="color:#054C5C;"><code class="verb">iris_tree.dot</code></span>
     :
    </p>
    <p class="noindent">
    </p>
    <pre><div id="fancyvrb12" style="padding:20px;border-radius: 3px;"><a id="x4-14016r36"></a><span style="color:#2B2BFF;">from</span><span style="color:#BABABA;"> </span>sklearn.tree<span style="color:#BABABA;"> </span><span style="color:#2B2BFF;">import</span> export_graphviz 
<a id="x4-14018r37"></a>export_graphviz( 
<a id="x4-14020r38"></a>tree_clf, 
<a id="x4-14022r39"></a>out_file=<span style="color:#800080;">"iris_tree.dot"</span>, 
<a id="x4-14024r40"></a>feature_names=[<span style="color:#800080;">"petal length (cm)"</span>, <span style="color:#800080;">"petal width (cm)"</span>], 
<a id="x4-14026r41"></a>class_names=iris.target_names, 
<a id="x4-14028r42"></a>rounded=<span style="color:#2B2BFF;"><span id="bold" style="font-weight:bold;">True</span></span>, 
<a id="x4-14030r43"></a>filled=<span style="color:#2B2BFF;"><span id="bold" style="font-weight:bold;">True</span></span> 
<a id="x4-14032r44"></a>)</div></pre>
    <p class="noindent">
     If you are using a Jupyter Notebook to study, you can use <span style="color:#054C5C;"><code class="verb">graphviz.Source.from_file()</code></span>
     to load and
display the file inline:
    </p>
    <p class="noindent">
    </p>
    <pre><div id="fancyvrb13" style="padding:20px;border-radius: 3px;"><a id="x4-14034r51"></a><span style="color:#2B2BFF;">from</span><span style="color:#BABABA;"> </span>graphviz<span style="color:#BABABA;"> </span><span style="color:#2B2BFF;">import</span> Source 
<a id="x4-14036r52"></a>Source.from_file(<span style="color:#800080;">"iris_tree.dot"</span>)</div></pre>
    <div class="informationblock" id="tcolobox-8">
     <div class="title">
      <p class="noindent">
       <span id="bold" style="font-weight:bold;">
        Information
       </span>
       : Graphviz &amp; DOT
      </p>
     </div>
     <div class="box-content">
      <p class="noindent">
       Graphviz (short for Graph Visualization Software) is a package of open-source tools for creating graphs. It takes
text input in DOT format, generates images.
      </p>
      <p class="noindent">
       DOT is a graph description language. DOT files are usually with .gv filename extension.
      </p>
     </div>
    </div>
    <p class="noindent">
    </p>
    <h2 class="sectionHead">
     <a aria-label="Link to this section in PDF version" class="pdf-link" href="DataScienceIILectureBook.pdf#section.2.3" style="color: red;  position: absolute;  left: -30px;" target="_blank">
      <svg class="bi bi-file-pdf" fill="currentColor" height="20" viewbox="0 0 16 16" width="20" xmlns="http://www.w3.org/2000/svg">
       <path d="M4 0a2 2 0 0 0-2 2v12a2 2 0 0 0 2 2h8a2 2 0 0 0 2-2V2a2 2 0 0 0-2-2H4zm0 1h8a1 1 0 0 1 1 1v12a1 1 0 0 1-1 1H4a1 1 0 0 1-1-1V2a1 1 0 0 1 1-1z">
       </path>
       <path d="M4.603 12.087a.81.81 0 0 1-.438-.42c-.195-.388-.13-.776.08-1.102.198-.307.526-.568.897-.787a7.68 7.68 0 0 1 1.482-.645 19.701 19.701 0 0 0 1.062-2.227 7.269 7.269 0 0 1-.43-1.295c-.086-.4-.119-.796-.046-1.136.075-.354.274-.672.65-.823.192-.077.4-.12.602-.077a.7.7 0 0 1 .477.365c.088.164.12.356.127.538.007.187-.012.395-.047.614-.084.51-.27 1.134-.52 1.794a10.954 10.954 0 0 0 .98 1.686 5.753 5.753 0 0 1 1.334.05c.364.065.734.195.96.465.12.144.193.32.2.518.007.192-.047.382-.138.563a1.04 1.04 0 0 1-.354.416.856.856 0 0 1-.51.138c-.331-.014-.654-.196-.933-.417a5.716 5.716 0 0 1-.911-.95 11.642 11.642 0 0 0-1.997.406 11.311 11.311 0 0 1-1.021 1.51c-.29.35-.608.655-.926.787a.793.793 0 0 1-.58.029zm1.379-1.901c-.166.076-.32.156-.459.238-.328.194-.541.383-.647.547-.094.145-.096.25-.04.361.01.022.02.036.026.044a.27.27 0 0 0 .035-.012c.137-.056.355-.235.635-.572a8.18 8.18 0 0 0 .45-.606zm1.64-1.33a12.647 12.647 0 0 1 1.01-.193 11.666 11.666 0 0 1-.51-.858 20.741 20.741 0 0 1-.5 1.05zm2.446.45c.15.162.296.3.435.41.24.19.407.253.498.256a.107.107 0 0 0 .07-.015.307.307 0 0 0 .094-.125.436.436 0 0 0 .059-.2.095.095 0 0 0-.026-.063c-.052-.062-.2-.152-.518-.209a3.881 3.881 0 0 0-.612-.053zM8.078 5.8a6.7 6.7 0 0 0 .2-.828c.031-.188.043-.343.038-.465a.613.613 0 0 0-.032-.198.517.517 0 0 0-.145.04c-.087.035-.158.106-.196.283-.04.192-.03.469.046.822.024.111.054.227.09.346z">
       </path>
      </svg>
     </a>
     <small class="titlemark">
      2.3
     </small>
     <a id="x4-150002.3">
     </a>
     Making Predictions
    </h2>
    <aside class="wrapfig-r">
     <img alt="PIC" height="" src="codes/images/Decision-Trees/iris_tree-.svg" width="100%"/>
     <a id="x4-15001r1">
     </a>
     <figcaption class="caption">
      <span class="id">
       Figure 2.1:
      </span>
      <span class="content">
      </span>
     </figcaption>
    </aside>
    <p class="noindent">
     Let’s see how the tree represented in Figure
     <a href="#x4-15001r1">
      2.1
     </a>
     makes predictions.
    </p>
    <p class="noindent">
     Suppose we find an iris flower and want to classify it based on its
     <alert style="color: #821131;">
      petals
     </alert>
     . Looking at our
three in
     <span id="bold" style="font-weight:bold;">
      Fig.
     </span>
     <a href="#x4-15001r1">
      2.1
     </a>, we start at the
     <span id="bold" style="font-weight:bold;">
      root node
     </span>
     , which is depth 0, at the top. This node asks
whether the flower’s petal length is smaller than 2,45 cm. If it is, then we move down to the
root’s
     <span id="bold" style="font-weight:bold;">
      left child node
     </span>
     , which is in this case it is depth 1, left, which is a
     <span id="bold" style="font-weight:bold;">
      leaf node
     </span>
     , as
in it does not have any child nodes, so it does not ask any questions as it simply look at
the predicted class for that node, and the
     <a class="glossary" href="DataScienceIILectureBookli2.html#glo:dt">
      DT
     </a>
     predicts that your flower is an
     <italic>
      Iris setosa
     </italic> <span style="color:#054C5C;">(<code class="verb">class=setosa</code>)</span>.
    </p>
    <p class="noindent">
     Now
suppose
we
find
another
flower,
and
this
time
the
petal
length
is
greater
than
2,45 
cm
.
We
again
start
at
the
root
but
now
move
down
to
its
right
child
node
     <alert style="color: #821131;">
      <span id="bold" style="font-weight:bold;">
       <sup class="textsuperscript">
        1
       </sup>
      </span>
     </alert>
     <span class="marginnote">
      <alert style="color: #821131;">
       <span id="bold" style="font-weight:bold;">
        <sup class="textsuperscript">
         1
        </sup>
       </span>
      </alert>
      <span style="color:#0063B2;">
       which
is
depth
1,
right
      </span>
     </span>
     .
This
is
     <span id="bold" style="font-weight:bold;">
      NOT
     </span>
     a
leaf
node,
it’s
a
     <span id="bold" style="font-weight:bold;">
      split
                                                                                
                                                                                
node
     </span>
     ,
so
it
asks
another
question:
    </p>
    <p class="noindent">
    </p>
    <div class="quoteblock">
     <p class="noindent">
      is
the
petal
width
smaller
than
1,75
cm
?
     </p>
    </div>
    <p class="noindent">
     If it is, then our flower is most likely an
     <italic>
      Iris versicolor
     </italic>
     . If not, it is likely an
     <italic>
      Iris virginica
     </italic>
     .
    </p>
    <div class="knowledge">
     <p class="noindent">
      One of the many qualities of
      <a class="glossary" href="DataScienceIILectureBookli2.html#glo:dt">
       DT
      </a>
      s is that they require very little data preparation. In fact, they don’t require feature scaling or
centering at all.
     </p>
    </div>
    <p class="noindent">
     A node’s samples attribute counts how many training instances it applies to.
    </p>
    <p class="noindent">
    </p>
    <div class="quoteblock">
     <p class="noindent">
      For
example,
100
training
instances
have
a
petal
length
greater
than
2,45
cm
(depth
1,
right),
and
of
those
100,
54
have
a
petal
width
smaller
than
1,75
cm
(depth
2,
left).
     </p>
    </div>
    <p class="noindent">
     A node’s value attribute tells you how many training instances of each class this node applies to.
    </p>
    <p class="noindent">
     For example, the bottom-right node applies to 0 Iris setosa, 1 Iris versicolor, and 45 Iris virginica.
    </p>
    <p class="noindent">
     Finally,
a
                                                                                
                                                                                
node’s
gini
attribute
measures
its
     <span id="bold" style="font-weight:bold;">
      Gini
impurity
     </span>
     ,
named
after
Corrado
Gini
     <alert style="color: #821131;">
      <span id="bold" style="font-weight:bold;">
       <sup class="textsuperscript">
        2
       </sup>
      </span>
     </alert>
     <span class="marginnote">
      <img alt="PIC" height="" src="figures/Decision-Trees/raster/portrait-gini.jpg" width="100%"/>
      <alert style="color: #821131;">
       <span id="bold" style="font-weight:bold;">
        <sup class="textsuperscript">
         2
        </sup>
       </span>
      </alert>
      <span style="color:#0063B2;">
       <span id="bold" style="font-weight:bold;">
        Corrado
Gini
       </span>
       (
       <italic>
        1884
-
1965
       </italic>
       )
An
Italian
statistician,
demographer
and
sociologist
who
developed
the
Gini
coefficient,
a
measure
of
the
income
inequality
in
a
society.
Gini
was
a
proponent
of
organicism
and
applied
it
to
nations.
      </span>
     </span>
     which
we
will
have
a
look
at
now.
    </p>
    <h3 class="subsectionHead">
     <a aria-label="Link to this section in PDF version" class="pdf-link" href="DataScienceIILectureBook.pdf#subsection.2.3.1" style="color: red;  position: absolute;  left: -30px;" target="_blank">
      <svg class="bi bi-file-pdf" fill="currentColor" height="20" viewbox="0 0 16 16" width="20" xmlns="http://www.w3.org/2000/svg">
       <path d="M4 0a2 2 0 0 0-2 2v12a2 2 0 0 0 2 2h8a2 2 0 0 0 2-2V2a2 2 0 0 0-2-2H4zm0 1h8a1 1 0 0 1 1 1v12a1 1 0 0 1-1 1H4a1 1 0 0 1-1-1V2a1 1 0 0 1 1-1z">
       </path>
       <path d="M4.603 12.087a.81.81 0 0 1-.438-.42c-.195-.388-.13-.776.08-1.102.198-.307.526-.568.897-.787a7.68 7.68 0 0 1 1.482-.645 19.701 19.701 0 0 0 1.062-2.227 7.269 7.269 0 0 1-.43-1.295c-.086-.4-.119-.796-.046-1.136.075-.354.274-.672.65-.823.192-.077.4-.12.602-.077a.7.7 0 0 1 .477.365c.088.164.12.356.127.538.007.187-.012.395-.047.614-.084.51-.27 1.134-.52 1.794a10.954 10.954 0 0 0 .98 1.686 5.753 5.753 0 0 1 1.334.05c.364.065.734.195.96.465.12.144.193.32.2.518.007.192-.047.382-.138.563a1.04 1.04 0 0 1-.354.416.856.856 0 0 1-.51.138c-.331-.014-.654-.196-.933-.417a5.716 5.716 0 0 1-.911-.95 11.642 11.642 0 0 0-1.997.406 11.311 11.311 0 0 1-1.021 1.51c-.29.35-.608.655-.926.787a.793.793 0 0 1-.58.029zm1.379-1.901c-.166.076-.32.156-.459.238-.328.194-.541.383-.647.547-.094.145-.096.25-.04.361.01.022.02.036.026.044a.27.27 0 0 0 .035-.012c.137-.056.355-.235.635-.572a8.18 8.18 0 0 0 .45-.606zm1.64-1.33a12.647 12.647 0 0 1 1.01-.193 11.666 11.666 0 0 1-.51-.858 20.741 20.741 0 0 1-.5 1.05zm2.446.45c.15.162.296.3.435.41.24.19.407.253.498.256a.107.107 0 0 0 .07-.015.307.307 0 0 0 .094-.125.436.436 0 0 0 .059-.2.095.095 0 0 0-.026-.063c-.052-.062-.2-.152-.518-.209a3.881 3.881 0 0 0-.612-.053zM8.078 5.8a6.7 6.7 0 0 0 .2-.828c.031-.188.043-.343.038-.465a.613.613 0 0 0-.032-.198.517.517 0 0 0-.145.04c-.087.035-.158.106-.196.283-.04.192-.03.469.046.822.024.111.054.227.09.346z">
       </path>
      </svg>
     </a>
     <small class="titlemark">
      2.3.1
     </small>
     <a id="x4-160002.3.1">
     </a>
     Gini Impurity
    </h3>
    <p class="noindent">
     Gini Impurity is a measurement used to build Decision Trees to determine how the features of a dataset should split nodes to
form the tree. More precisely, the Gini Impurity of a dataset is a number between 0-0.5, which indicates the likelihood of new,
                                                                                
                                                                                
random data being misclassified if it were given a random class label according to the class distribution in the
dataset.
    </p>
    <div class="figure">
     <p class="noindent">
      <img alt="PIC" height="" src="figures/Decision-Trees/raster/gini-impurity-diagram.png" width="150%"/>
      <a id="x4-16001r2">
      </a>
     </p>
     <figcaption class="caption">
      <span class="id">
       Figure 2.2:
      </span>
      <span class="content">
      </span>
     </figcaption>
    </div>
    <p class="noindent">
     For example, say we want to build a classifier that determines if someone will default on their credit card. You have some labeled
data with features, such as bins for age, income, credit rating, and whether or not each person is a student. To find the best
feature for the first split of the tree  the root node  you could calculate how poorly each feature divided the data into the correct
class, default ("yes") or didn’t default ("no"). This calculation would measure the impurity of the split, and the feature with the
lowest impurity would determine the best feature for splitting the current node. This process would continue for each subsequent
node using the remaining features.
    </p>
    <p class="noindent">
     In the image above, age has minumum gini impurity, so age is selected as the root in the decision tree.
    </p>
    <p class="noindent">
     : a node is "pure" <span style="color:#054C5C;">(<code class="verb">gini=0</code>)</span> if all training instances it applies to belong to the same class.
    </p>
    <p class="noindent">
     For example, since the depth-1 left node applies only to
     <italic>
      Iris setosa
     </italic>
     training instances, it is pure
and its Gini impurity is 0. Eq. (
     <a href="#x4-16002r1">
      2.1
     </a>
     ) shows how the training algorithm computes the Gini impurity
     <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
      <msubsup>
       <mrow>
        <mi>
         G
        </mi>
       </mrow>
       <mrow>
        <mi>
         i
        </mi>
       </mrow>
       <mrow>
       </mrow>
      </msubsup>
     </math>
     of the
     <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
      <msubsup>
       <mrow>
        <mi>
         i
        </mi>
       </mrow>
       <mrow>
       </mrow>
       <mrow>
        <mi>
         t
        </mi>
        <mi>
         h
        </mi>
       </mrow>
      </msubsup>
     </math>
     node. The depth-2 left node
has a Gini impurity of
     <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
      <mn>
       1
      </mn>
      <mo class="MathClass-bin" stretchy="false">
       −
      </mo>
      <mo class="MathClass-open" stretchy="false">
       (
      </mo>
      <mn>
       0
      </mn>
      <mo class="MathClass-bin" stretchy="false">
       ∕
      </mo>
      <mn>
       5
      </mn>
      <mn>
       4
      </mn>
      <mo class="MathClass-close" stretchy="false">
       )
      </mo>
      <mn>
       2
      </mn>
      <mo class="MathClass-bin" stretchy="false">
       −
      </mo>
      <mo class="MathClass-open" stretchy="false">
       (
      </mo>
      <mn>
       4
      </mn>
      <mn>
       9
      </mn>
      <mo class="MathClass-bin" stretchy="false">
       ∕
      </mo>
      <mn>
       5
      </mn>
      <mn>
       4
      </mn>
      <mo class="MathClass-close" stretchy="false">
       )
      </mo>
      <mn>
       2
      </mn>
      <mo class="MathClass-bin" stretchy="false">
       −
      </mo>
      <mo class="MathClass-open" stretchy="false">
       (
      </mo>
      <mn>
       5
      </mn>
      <mo class="MathClass-bin" stretchy="false">
       ∕
      </mo>
      <mn>
       5
      </mn>
      <mn>
       4
      </mn>
      <mo class="MathClass-close" stretchy="false">
       )
      </mo>
      <mn>
       2
      </mn>
      <mo class="MathClass-rel" stretchy="false">
       ≈
      </mo>
      <mn>
       0
      </mn>
      <mo class="MathClass-punc" stretchy="false">
       .
      </mo>
      <mn>
       1
      </mn>
      <mn>
       6
      </mn>
      <mn>
       8
      </mn>
     </math>
     .
    </p>
    <table class="equation">
     <tr>
      <td>
       <math class="equation" display="block" xmlns="http://www.w3.org/1998/Math/MathML">
        <mstyle class="label" id="x4-16002r1">
        </mstyle>
        <msubsup>
         <mrow>
          <mi>
           G
          </mi>
         </mrow>
         <mrow>
          <mi>
           i
          </mi>
         </mrow>
         <mrow>
         </mrow>
        </msubsup>
        <mo class="MathClass-rel" stretchy="false">
         =
        </mo>
        <mn>
         1
        </mn>
        <mo class="MathClass-bin" stretchy="false">
         −
        </mo>
        <munderover accent="false" accentunder="false">
         <mrow>
          <mo>
           ∑
          </mo>
         </mrow>
         <mrow>
          <mi>
           k
          </mi>
          <mo class="MathClass-rel" stretchy="false">
           =
          </mo>
          <mn>
           1
          </mn>
         </mrow>
         <mrow>
          <mi>
           n
          </mi>
         </mrow>
        </munderover>
        <msubsup>
         <mrow>
          <mi>
           p
          </mi>
         </mrow>
         <mrow>
          <mi>
           i
          </mi>
          <mo class="MathClass-punc" stretchy="false">
           ,
          </mo>
          <mi>
           k
          </mi>
         </mrow>
         <mrow>
          <mn>
           2
          </mn>
         </mrow>
        </msubsup>
       </math>
      </td>
      <td class="eq-no">
       (2.1)
      </td>
     </tr>
    </table>
    <p class="noindent">
     where
     <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
      <msubsup>
       <mrow>
        <mi>
         G
        </mi>
       </mrow>
       <mrow>
        <mi>
         i
        </mi>
       </mrow>
       <mrow>
       </mrow>
      </msubsup>
     </math>
     is the Gini
impurity of the
     <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
      <msubsup>
       <mrow>
        <mi>
         i
        </mi>
       </mrow>
       <mrow>
       </mrow>
       <mrow>
        <mi>
         t
        </mi>
        <mi>
         h
        </mi>
       </mrow>
      </msubsup>
     </math>
     node,
     <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
      <msubsup>
       <mrow>
        <mi>
         p
        </mi>
       </mrow>
       <mrow>
        <mi>
         i
        </mi>
        <mo class="MathClass-punc" stretchy="false">
         ,
        </mo>
        <mi>
         k
        </mi>
       </mrow>
       <mrow>
       </mrow>
      </msubsup>
     </math>
     is the ratio of class
     <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
      <mi>
       k
      </mi>
     </math>
     instances among the
training instances in the
     <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
      <msubsup>
       <mrow>
        <mi>
         i
        </mi>
       </mrow>
       <mrow>
       </mrow>
       <mrow>
        <mi>
         t
        </mi>
        <mi>
         h
        </mi>
       </mrow>
      </msubsup>
     </math>
     node.
    </p>
    <p class="noindent"> <span style="color:#054C5C;"><code class="verb">sklearn</code></span>
     uses the CART algorithm, which produces only
     <span id="bold" style="font-weight:bold;">
      binary trees
     </span>
     , meaning trees where split nodes always have exactly two
children (i.e., questions only have yes/no answers).
    </p>
    <div class="knowledge">
     <p class="noindent">
      However, other algorithms, such as ID3, can produce
      <a class="glossary" href="DataScienceIILectureBookli2.html#glo:dt">
       DT
      </a>
      s with nodes that have more than two children.
     </p>
    </div>
    Figure
    <a href="#x4-16003r3">
     2.3
    </a>
    shows
this
    <a class="glossary" href="DataScienceIILectureBookli2.html#glo:dt">
     DT
    </a>
    ’s decision boundaries. The thick vertical line represents the decision boundary of the root node (depth
0): petal length = 2.45 cm. Since the lefthand area is pure (only
    <italic>
     Iris setosa
    </italic>
    ), it cannot be split any further.
However, the righthand area is impure, so the depth-1 right node splits it at petal width = 1.75 cm (represented
by the dashed line). Since
    <span style="color:#054C5C;">
     <code class="verb">
      max_depth
     </code>
    </span>
    was set to 2, the
    <a class="glossary" href="DataScienceIILectureBookli2.html#glo:dt">
     DT
    </a>
    stops right there. If you set
    <span style="color:#054C5C;">
     <code class="verb">
      max_depth
     </code>
    </span>
    to 3, then the
two depth-2 nodes would each add another decision boundary (represented by the two vertical dotted lines).
    <div class="figure">
     <p class="noindent">
      <img alt="PIC" height="" src="codes/images/Decision-Trees/decision_tree_decision_boundaries_plot-.svg" width="150%"/>
      <a id="x4-16003r3">
      </a>
     </p>
     <figcaption class="caption">
      <span class="id">
       Figure 2.3:
      </span>
      <span class="content">
       <a class="glossary" href="DataScienceIILectureBookli2.html#glo:dt">
        DT
       </a>
       decision boundaries
      </span>
     </figcaption>
    </div>
    <div class="knowledge">
     <p class="noindent">
      The tree structure, including all the information shown in Figure
      <a href="#x4-15001r1">
       2.1
      </a>, is available via the classifier’s <span style="color:#054C5C;"><code class="verb">tree_</code></span>
      attribute.
     </p>
     <p class="noindent">
      Type <span style="color:#054C5C;"><code class="verb">help(tree_clf.tree_)</code></span>
      for details.
     </p>
    </div>
    <div class="informationblock" id="tcolobox-9">
     <div class="title">
      <p class="noindent">
       <span id="bold" style="font-weight:bold;">
        Information
       </span>
       : White v. Black Box
      </p>
     </div>
     <div class="box-content">
      <p class="noindent">
       <a class="glossary" href="DataScienceIILectureBookli2.html#glo:dt">
        DT
       </a>
       s are intuitive, and their decisions are easy to interpret. Such models are often called
       <span id="bold" style="font-weight:bold;">
        white box
       </span>
       models.
In contrast, random forests and neural networks are generally considered
       <span id="bold" style="font-weight:bold;">
        black box
       </span>
       models. They make great
predictions, and you can easily check the calculations that they performed to make these predictions, however,
it is usually hard to explain in simple terms why the predictions were made.
      </p>
      <p class="noindent">
       For example, if a neural network says that a particular person appears in a picture, it is hard to know what
contributed to this prediction: Did the model recognize that person’s eyes? Their mouth? Their nose? Their
shoes? Or even the couch that they were sitting on? Conversely,
       <a class="glossary" href="DataScienceIILectureBookli2.html#glo:dt">
        DT
       </a>
       s provide nice, simple classification rules
that can even be applied manually if need be (e.g., for flower classification). The field of interpretable
       <a class="glossary" href="DataScienceIILectureBookli2.html#glo:ml">
        ML
       </a>
       aims
at creating
       <a class="glossary" href="DataScienceIILectureBookli2.html#glo:ml">
        ML
       </a>
       systems that can explain their decisions in a way humans can understand. This is important in
many domains-for example, to ensure the system does not make unfair decisions.
      </p>
     </div>
    </div>
    <p class="noindent">
    </p>
    <h2 class="sectionHead">
     <a aria-label="Link to this section in PDF version" class="pdf-link" href="DataScienceIILectureBook.pdf#section.2.4" style="color: red;  position: absolute;  left: -30px;" target="_blank">
      <svg class="bi bi-file-pdf" fill="currentColor" height="20" viewbox="0 0 16 16" width="20" xmlns="http://www.w3.org/2000/svg">
       <path d="M4 0a2 2 0 0 0-2 2v12a2 2 0 0 0 2 2h8a2 2 0 0 0 2-2V2a2 2 0 0 0-2-2H4zm0 1h8a1 1 0 0 1 1 1v12a1 1 0 0 1-1 1H4a1 1 0 0 1-1-1V2a1 1 0 0 1 1-1z">
       </path>
       <path d="M4.603 12.087a.81.81 0 0 1-.438-.42c-.195-.388-.13-.776.08-1.102.198-.307.526-.568.897-.787a7.68 7.68 0 0 1 1.482-.645 19.701 19.701 0 0 0 1.062-2.227 7.269 7.269 0 0 1-.43-1.295c-.086-.4-.119-.796-.046-1.136.075-.354.274-.672.65-.823.192-.077.4-.12.602-.077a.7.7 0 0 1 .477.365c.088.164.12.356.127.538.007.187-.012.395-.047.614-.084.51-.27 1.134-.52 1.794a10.954 10.954 0 0 0 .98 1.686 5.753 5.753 0 0 1 1.334.05c.364.065.734.195.96.465.12.144.193.32.2.518.007.192-.047.382-.138.563a1.04 1.04 0 0 1-.354.416.856.856 0 0 1-.51.138c-.331-.014-.654-.196-.933-.417a5.716 5.716 0 0 1-.911-.95 11.642 11.642 0 0 0-1.997.406 11.311 11.311 0 0 1-1.021 1.51c-.29.35-.608.655-.926.787a.793.793 0 0 1-.58.029zm1.379-1.901c-.166.076-.32.156-.459.238-.328.194-.541.383-.647.547-.094.145-.096.25-.04.361.01.022.02.036.026.044a.27.27 0 0 0 .035-.012c.137-.056.355-.235.635-.572a8.18 8.18 0 0 0 .45-.606zm1.64-1.33a12.647 12.647 0 0 1 1.01-.193 11.666 11.666 0 0 1-.51-.858 20.741 20.741 0 0 1-.5 1.05zm2.446.45c.15.162.296.3.435.41.24.19.407.253.498.256a.107.107 0 0 0 .07-.015.307.307 0 0 0 .094-.125.436.436 0 0 0 .059-.2.095.095 0 0 0-.026-.063c-.052-.062-.2-.152-.518-.209a3.881 3.881 0 0 0-.612-.053zM8.078 5.8a6.7 6.7 0 0 0 .2-.828c.031-.188.043-.343.038-.465a.613.613 0 0 0-.032-.198.517.517 0 0 0-.145.04c-.087.035-.158.106-.196.283-.04.192-.03.469.046.822.024.111.054.227.09.346z">
       </path>
      </svg>
     </a>
     <small class="titlemark">
      2.4
     </small>
     <a id="x4-170002.4">
     </a>
     Estimating Class Probabilities
    </h2>
    <p class="noindent">
     A
     <a class="glossary" href="DataScienceIILectureBookli2.html#glo:dt">
      DT
     </a>
     can also estimate the probability that an instance belongs to a particular class
     <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
      <mi>
       k
      </mi>
     </math>
     . First, it
traverses the tree to find the leaf node for this instance, and then it returns the ratio of training instances of class
     <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
      <mi>
       k
      </mi>
     </math>
     in this
node.
    </p>
    <p class="noindent">
     For example, suppose you have found a flower whose petals are 5 cm long and 1.5 cm wide. The corresponding leaf node is the
depth-2 left node, so the
     <a class="glossary" href="DataScienceIILectureBookli2.html#glo:dt">
      DT
     </a>
     outputs the following probabilities: 0% for Iris setosa (0/54), 90.7% for Iris versicolor (49/54), and
9.3% for Iris virginica (5/54). And if you ask it to predict the class, it outputs Iris versicolor (class 1) because it has
     <alert style="color: #821131;">
      the highest
probability
     </alert>
     . Let’s check this:
    </p>
    <pre><div id="fancyvrb14" style="padding:20px;border-radius: 3px;"><a id="x4-17002r102"></a><span style="color:#2B2BFF;">print</span>(tree_clf.predict_proba([[<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">5</span></span>, <span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">1.5</span></span>]]).round(<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">3</span></span>)) 
<a id="x4-17004r103"></a><span style="color:#2B2BFF;">print</span>(tree_clf.predict([[<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">5</span></span>, <span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">1.5</span></span>]]))</div></pre>
    <div class="tcolorbox tcolorbox" id="tcolobox-10">
     <div class="tcolorbox-content">
      <pre><div id="fancyvrb15" style="padding:20px;border-radius: 3px;"><a id="x4-17006r1"></a>[[0.    0.907 0.093]] 
<a id="x4-17008r2"></a>[1]</div></pre>
     </div>
    </div>
    Notice the
estimated probabilities would be identical anywhere else in the bottom-right rectangle of Figure
    <a href="#x4-15001r1">
     2.1
    </a>, for example, if the petals
were 6 cm long and 1.5 cm wide.
    <p class="noindent">
    </p>
    <h2 class="sectionHead">
     <a aria-label="Link to this section in PDF version" class="pdf-link" href="DataScienceIILectureBook.pdf#section.2.5" style="color: red;  position: absolute;  left: -30px;" target="_blank">
      <svg class="bi bi-file-pdf" fill="currentColor" height="20" viewbox="0 0 16 16" width="20" xmlns="http://www.w3.org/2000/svg">
       <path d="M4 0a2 2 0 0 0-2 2v12a2 2 0 0 0 2 2h8a2 2 0 0 0 2-2V2a2 2 0 0 0-2-2H4zm0 1h8a1 1 0 0 1 1 1v12a1 1 0 0 1-1 1H4a1 1 0 0 1-1-1V2a1 1 0 0 1 1-1z">
       </path>
       <path d="M4.603 12.087a.81.81 0 0 1-.438-.42c-.195-.388-.13-.776.08-1.102.198-.307.526-.568.897-.787a7.68 7.68 0 0 1 1.482-.645 19.701 19.701 0 0 0 1.062-2.227 7.269 7.269 0 0 1-.43-1.295c-.086-.4-.119-.796-.046-1.136.075-.354.274-.672.65-.823.192-.077.4-.12.602-.077a.7.7 0 0 1 .477.365c.088.164.12.356.127.538.007.187-.012.395-.047.614-.084.51-.27 1.134-.52 1.794a10.954 10.954 0 0 0 .98 1.686 5.753 5.753 0 0 1 1.334.05c.364.065.734.195.96.465.12.144.193.32.2.518.007.192-.047.382-.138.563a1.04 1.04 0 0 1-.354.416.856.856 0 0 1-.51.138c-.331-.014-.654-.196-.933-.417a5.716 5.716 0 0 1-.911-.95 11.642 11.642 0 0 0-1.997.406 11.311 11.311 0 0 1-1.021 1.51c-.29.35-.608.655-.926.787a.793.793 0 0 1-.58.029zm1.379-1.901c-.166.076-.32.156-.459.238-.328.194-.541.383-.647.547-.094.145-.096.25-.04.361.01.022.02.036.026.044a.27.27 0 0 0 .035-.012c.137-.056.355-.235.635-.572a8.18 8.18 0 0 0 .45-.606zm1.64-1.33a12.647 12.647 0 0 1 1.01-.193 11.666 11.666 0 0 1-.51-.858 20.741 20.741 0 0 1-.5 1.05zm2.446.45c.15.162.296.3.435.41.24.19.407.253.498.256a.107.107 0 0 0 .07-.015.307.307 0 0 0 .094-.125.436.436 0 0 0 .059-.2.095.095 0 0 0-.026-.063c-.052-.062-.2-.152-.518-.209a3.881 3.881 0 0 0-.612-.053zM8.078 5.8a6.7 6.7 0 0 0 .2-.828c.031-.188.043-.343.038-.465a.613.613 0 0 0-.032-.198.517.517 0 0 0-.145.04c-.087.035-.158.106-.196.283-.04.192-.03.469.046.822.024.111.054.227.09.346z">
       </path>
      </svg>
     </a>
     <small class="titlemark">
      2.5
     </small>
     <a id="x4-180002.5">
     </a>
     The CART Training Algorithm
    </h2>
    <p class="noindent">
     <a class="glossary" href="DataScienceIILectureBookli2.html#glo:cart">
      Classification and Regression Tree (CART)
     </a>
     is a predictive algorithm used for explaining how the target variable’s values can be
predicted based on other matters. It is a decision tree where each fork is split into a predictor variable and each
node has a prediction for the target variable at the end. The three <alert style="color: #821131;">(3)</alert> primary points of the algorithm is as
follows:
    </p>
    <dl class="description">
     <dt class="description">
      <a id="x4-18001x2.5">
      </a>
      Tree structure
     </dt>
     <dd class="description">
      <p class="noindent">
       CART
builds
a
tree-like
structure
consisting
of
nodes
and
branches.
The
nodes
represent
different
decision
points,
and
the
branches
represent
the
possible
outcomes
of
those
decisions.
The
leaf
nodes
in
the
tree
contain
a
predicted
class
label
or
value
for
the
target
variable.
      </p>
     </dd>
     <dt class="description">
      <a id="x4-18002x2.5">
      </a>
      Splitting
     </dt>
     <dd class="description">
      <p class="noindent">
       criteria:
CART
                                                                                
                                                                                
     
uses
a
greedy
approach
to
split
the
data
at
each
node.
It
evaluates
all
possible
splits
and
selects
the
one
that
best
reduces
the
impurity
of
the
resulting
subsets.
For
classification
tasks,
CART
uses
Gini
impurity
as
the
splitting
criterion.
The
lower
the
Gini
impurity,
the
more
pure
the
subset
is.
For
regression
tasks,
CART
uses
residual
reduction
as
the
splitting
criterion.
The
lower
the
residual
reduction,
                                                                                
                                                                                
     
the
better
the
fit
of
the
model
to
the
data.
      </p>
     </dd>
     <dt class="description">
      <a id="x4-18003x2.5">
      </a>
      Pruning
     </dt>
     <dd class="description">
      <p class="noindent">
       To
prevent
overfitting
of
the
data,
pruning
is
a
technique
used
to
remove
the
nodes
that
contribute
little
to
the
model
accuracy.
Cost
complexity
pruning
and
information
gain
pruning
are
two
popular
pruning
techniques.
Cost
complexity
pruning
involves
calculating
the
cost
of
each
node
and
removing
nodes
that
have
a
negative
cost.
                                                                                
                                                                                
     
Information
gain
pruning
involves
calculating
the
information
gain
of
each
node
and
removing
nodes
that
have
a
low
information
gain.
      </p>
     </dd>
    </dl>
    <p class="noindent"> <span style="color:#054C5C;"><code class="verb">sklearn</code></span>
     uses the Classification and Regression Tree (CART) algorithm to train
     <a class="glossary" href="DataScienceIILectureBookli2.html#glo:dt">
      DT
     </a>
     s (also called growing
trees). The algorithm works by first splitting the training set into two subsets using a single feature
     <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
      <mi>
       k
      </mi>
     </math>
     and a threshold
     <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
      <msubsup>
       <mrow>
        <mi>
         t
        </mi>
       </mrow>
       <mrow>
        <mi>
         k
        </mi>
       </mrow>
       <mrow>
       </mrow>
      </msubsup>
     </math>
     (e.g., "petal
length
     <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
      <mo class="MathClass-rel" stretchy="false">
       ≤
      </mo>
     </math>
     2.45 cm").
    </p>
    <p class="noindent">
     How does it choose
     <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
      <mi>
       k
      </mi>
     </math>
     and
     <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
      <msubsup>
       <mrow>
        <mi>
         t
        </mi>
       </mrow>
       <mrow>
        <mi>
         k
        </mi>
       </mrow>
       <mrow>
       </mrow>
      </msubsup>
     </math>
     ? It searches
for the pair
     <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
      <msup>
       <mrow>
        <mrow>
         <mo fence="true" form="prefix">
          (
         </mo>
         <mrow>
          <mi>
           k
          </mi>
          <mo class="MathClass-punc" stretchy="false">
           ,
          </mo>
          <mspace class="thinspace" width="0.17em">
          </mspace>
          <msubsup>
           <mrow>
            <mi>
             t
            </mi>
           </mrow>
           <mrow>
            <mi>
             k
            </mi>
           </mrow>
           <mrow>
           </mrow>
          </msubsup>
         </mrow>
         <mo fence="true" form="postfix">
          )
         </mo>
        </mrow>
       </mrow>
       <mrow>
       </mrow>
      </msup>
     </math>
     that produces the purest subsets, weighted by their size. Equation
     <a href="#x4-18004r2">
      2.2
     </a>
     gives the cost function that the algorithm tries to
minimize.
    </p>
    <table class="equation">
     <tr>
      <td>
       <math class="equation" display="block" xmlns="http://www.w3.org/1998/Math/MathML">
        <mstyle class="label" id="x4-18004r2">
        </mstyle>
        <mi>
         J
        </mi>
        <msup>
         <mrow>
          <mrow>
           <mo fence="true" form="prefix">
            (
           </mo>
           <mrow>
            <mi>
             k
            </mi>
            <mo class="MathClass-punc" stretchy="false">
             ,
            </mo>
            <mspace class="thinspace" width="0.17em">
            </mspace>
            <msubsup>
             <mrow>
              <mi>
               t
              </mi>
             </mrow>
             <mrow>
              <mi>
               k
              </mi>
             </mrow>
             <mrow>
             </mrow>
            </msubsup>
           </mrow>
           <mo fence="true" form="postfix">
            )
           </mo>
          </mrow>
         </mrow>
         <mrow>
         </mrow>
        </msup>
        <mo class="MathClass-rel" stretchy="false">
         =
        </mo>
        <mfrac>
         <mrow>
          <msubsup>
           <mrow>
            <mi>
             m
            </mi>
           </mrow>
           <mrow>
            <mi>
             l
            </mi>
            <mi>
             e
            </mi>
            <mi>
             f
            </mi>
            <mi>
             t
            </mi>
           </mrow>
           <mrow>
           </mrow>
          </msubsup>
         </mrow>
         <mrow>
          <mi>
           m
          </mi>
         </mrow>
        </mfrac>
        <msubsup>
         <mrow>
          <mi>
           G
          </mi>
         </mrow>
         <mrow>
          <mi>
           l
          </mi>
          <mi>
           e
          </mi>
          <mi>
           f
          </mi>
          <mi>
           t
          </mi>
         </mrow>
         <mrow>
         </mrow>
        </msubsup>
        <mo class="MathClass-bin" stretchy="false">
         +
        </mo>
        <mfrac>
         <mrow>
          <msubsup>
           <mrow>
            <mi>
             m
            </mi>
           </mrow>
           <mrow>
            <mi>
             r
            </mi>
            <mi>
             i
            </mi>
            <mi>
             g
            </mi>
            <mi>
             h
            </mi>
            <mi>
             t
            </mi>
           </mrow>
           <mrow>
           </mrow>
          </msubsup>
         </mrow>
         <mrow>
          <mi>
           m
          </mi>
         </mrow>
        </mfrac>
        <msubsup>
         <mrow>
          <mi>
           G
          </mi>
         </mrow>
         <mrow>
          <mi>
           r
          </mi>
          <mi>
           i
          </mi>
          <mi>
           g
          </mi>
          <mi>
           h
          </mi>
          <mi>
           t
          </mi>
         </mrow>
         <mrow>
         </mrow>
        </msubsup>
        <mspace class="quad" width="1em">
        </mspace>
        <mstyle class="text">
         <mtext>
          where
         </mtext>
        </mstyle>
        <mspace class="quad" width="1em">
        </mspace>
        <mrow class="cases">
         <mrow>
          <mo fence="true" form="prefix">
           {
          </mo>
          <mrow>
           <mtable align="axis" class="array" columnlines="none" displaystyle="true" equalcolumns="false" equalrows="false" style="">
            <mtr class="array-row">
             <mtd class="array-td" columnalign="left">
              <msubsup>
               <mrow>
                <mi>
                 G
                </mi>
               </mrow>
               <mrow>
                <mi>
                 l
                </mi>
                <mi>
                 e
                </mi>
                <mi>
                 f
                </mi>
                <mi>
                 t
                </mi>
                <mo class="MathClass-bin" stretchy="false">
                 ∕
                </mo>
                <mi>
                 r
                </mi>
                <mi>
                 i
                </mi>
                <mi>
                 g
                </mi>
                <mi>
                 h
                </mi>
                <mi>
                 t
                </mi>
               </mrow>
               <mrow>
               </mrow>
              </msubsup>
              <mspace class="quad" width="1em">
              </mspace>
              <mstyle class="text">
               <mtext>
                measures the impurity
               </mtext>
              </mstyle>
              <mspace class="quad" width="1em">
              </mspace>
             </mtd>
            </mtr>
            <mtr class="array-row">
             <mtd class="array-td" columnalign="left">
              <msubsup>
               <mrow>
                <mi>
                 m
                </mi>
               </mrow>
               <mrow>
                <mi>
                 l
                </mi>
                <mi>
                 e
                </mi>
                <mi>
                 f
                </mi>
                <mi>
                 t
                </mi>
                <mo class="MathClass-bin" stretchy="false">
                 ∕
                </mo>
                <mi>
                 r
                </mi>
                <mi>
                 i
                </mi>
                <mi>
                 g
                </mi>
                <mi>
                 h
                </mi>
                <mi>
                 t
                </mi>
               </mrow>
               <mrow>
               </mrow>
              </msubsup>
              <mspace class="quad" width="1em">
              </mspace>
              <mstyle class="text">
               <mtext>
                number of instances
               </mtext>
              </mstyle>
              <mspace class="quad" width="1em">
              </mspace>
             </mtd>
            </mtr>
           </mtable>
          </mrow>
          <mo fence="true" form="postfix">
          </mo>
         </mrow>
        </mrow>
       </math>
      </td>
      <td class="eq-no">
       (2.2)
      </td>
     </tr>
    </table>
    <p class="noindent">
     Once the CART algorithm successfully splits the training set in two, it splits the subsets using the same logic, then the
sub-subsets, and so on, recursively. It stops recursing once it reaches the maximum depth (defined by the <span style="color:#054C5C;"><code class="verb">max_depth</code></span>
     hyperparameter), or if it cannot find a split that will reduce impurity. A few other hyperparameters (described in a moment)
control additional stopping conditions: <span style="color:#054C5C;"><code class="verb">min_samples_split</code></span>, <span style="color:#054C5C;"><code class="verb">min_samples_leaf</code></span>, <span style="color:#054C5C;"><code class="verb">min_weight_fraction_leaf</code></span>, and <span style="color:#054C5C;"><code class="verb">max_leaf_nodes</code></span>.
    </p>
    <div class="warning">
     <p class="noindent">
      CART algorithm is a
      <span id="bold" style="font-weight:bold;">
       greedy algorithm
      </span>
      : it greedily searches for an optimum split at the top level, then repeats the process at
each subsequent level. It does not check whether or not the split will lead to the lowest possible impurity several levels
down. A greedy algorithm often produces a solution that’s reasonably good but not guaranteed to be optimal.
     </p>
    </div>
    <p class="noindent">
    </p>
    <h2 class="sectionHead">
     <a aria-label="Link to this section in PDF version" class="pdf-link" href="DataScienceIILectureBook.pdf#section.2.6" style="color: red;  position: absolute;  left: -30px;" target="_blank">
      <svg class="bi bi-file-pdf" fill="currentColor" height="20" viewbox="0 0 16 16" width="20" xmlns="http://www.w3.org/2000/svg">
       <path d="M4 0a2 2 0 0 0-2 2v12a2 2 0 0 0 2 2h8a2 2 0 0 0 2-2V2a2 2 0 0 0-2-2H4zm0 1h8a1 1 0 0 1 1 1v12a1 1 0 0 1-1 1H4a1 1 0 0 1-1-1V2a1 1 0 0 1 1-1z">
       </path>
       <path d="M4.603 12.087a.81.81 0 0 1-.438-.42c-.195-.388-.13-.776.08-1.102.198-.307.526-.568.897-.787a7.68 7.68 0 0 1 1.482-.645 19.701 19.701 0 0 0 1.062-2.227 7.269 7.269 0 0 1-.43-1.295c-.086-.4-.119-.796-.046-1.136.075-.354.274-.672.65-.823.192-.077.4-.12.602-.077a.7.7 0 0 1 .477.365c.088.164.12.356.127.538.007.187-.012.395-.047.614-.084.51-.27 1.134-.52 1.794a10.954 10.954 0 0 0 .98 1.686 5.753 5.753 0 0 1 1.334.05c.364.065.734.195.96.465.12.144.193.32.2.518.007.192-.047.382-.138.563a1.04 1.04 0 0 1-.354.416.856.856 0 0 1-.51.138c-.331-.014-.654-.196-.933-.417a5.716 5.716 0 0 1-.911-.95 11.642 11.642 0 0 0-1.997.406 11.311 11.311 0 0 1-1.021 1.51c-.29.35-.608.655-.926.787a.793.793 0 0 1-.58.029zm1.379-1.901c-.166.076-.32.156-.459.238-.328.194-.541.383-.647.547-.094.145-.096.25-.04.361.01.022.02.036.026.044a.27.27 0 0 0 .035-.012c.137-.056.355-.235.635-.572a8.18 8.18 0 0 0 .45-.606zm1.64-1.33a12.647 12.647 0 0 1 1.01-.193 11.666 11.666 0 0 1-.51-.858 20.741 20.741 0 0 1-.5 1.05zm2.446.45c.15.162.296.3.435.41.24.19.407.253.498.256a.107.107 0 0 0 .07-.015.307.307 0 0 0 .094-.125.436.436 0 0 0 .059-.2.095.095 0 0 0-.026-.063c-.052-.062-.2-.152-.518-.209a3.881 3.881 0 0 0-.612-.053zM8.078 5.8a6.7 6.7 0 0 0 .2-.828c.031-.188.043-.343.038-.465a.613.613 0 0 0-.032-.198.517.517 0 0 0-.145.04c-.087.035-.158.106-.196.283-.04.192-.03.469.046.822.024.111.054.227.09.346z">
       </path>
      </svg>
     </a>
     <small class="titlemark">
      2.6
     </small>
     <a id="x4-190002.6">
     </a>
     Gini Impurity or Entropy?
    </h2>
    <p class="noindent">
     By default, the <span style="color:#054C5C;"><code class="verb">DecisionTreeClassifier</code></span>
     class uses the
     <span id="bold" style="font-weight:bold;">
      Gini impurity
     </span>
     measure, but you can select the entropy impurity measure
instead by setting the criterion hyperparameter to
     <alert style="color: #821131;">
      entropy
     </alert>
     .
    </p>
    <div class="informationblock" id="tcolobox-11">
     <div class="title">
      <p class="noindent">
       <span id="bold" style="font-weight:bold;">
        Information
       </span>
       : Entropy: A Measure of Disorder
      </p>
     </div>
     <div class="box-content">
      <p class="noindent">
       The concept of entropy originated in thermodynamics as a measure of molecular disorder: entropy approaches
zero when molecules are still and well ordered. Entropy later spread to a wide variety of domains, including in
Shannon’s information theory, where it measures the average information content of a message and the entropy
is zero when all messages are identical.
      </p>
     </div>
    </div>
    In
    <a class="glossary" href="DataScienceIILectureBookli2.html#glo:ml">
     ML
    </a>, entropy is frequently used as an impurity measure: a set’s entropy is zero when it
contains instances of only one class. Equation
    <a href="#x4-19001r3">
     2.3
    </a>
    shows the definition of the entropy of the
    <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
     <msubsup>
      <mrow>
       <mi>
        i
       </mi>
      </mrow>
      <mrow>
      </mrow>
      <mrow>
       <mi>
        t
       </mi>
       <mi>
        h
       </mi>
      </mrow>
     </msubsup>
    </math>
    node. For
example, the depth-2 left node in Figure
    <a href="#x4-15001r1">
     2.1
    </a>
    has an entropy equal to:
    <table class="equation-star">
     <tr>
      <td>
       <math class="equation" display="block" xmlns="http://www.w3.org/1998/Math/MathML">
        <mo class="MathClass-bin" stretchy="false">
         −
        </mo>
        <mfrac>
         <mrow>
          <mn>
           4
          </mn>
          <mn>
           9
          </mn>
         </mrow>
         <mrow>
          <mn>
           5
          </mn>
          <mn>
           4
          </mn>
         </mrow>
        </mfrac>
        <msub>
         <mrow>
          <mi class="loglike">
           log
          </mi>
          <mo>
           ⁡
          </mo>
         </mrow>
         <mrow>
          <mn>
           2
          </mn>
         </mrow>
        </msub>
        <mfrac>
         <mrow>
          <mn>
           4
          </mn>
          <mn>
           9
          </mn>
         </mrow>
         <mrow>
          <mn>
           5
          </mn>
          <mn>
           4
          </mn>
         </mrow>
        </mfrac>
        <mo class="MathClass-bin" stretchy="false">
         −
        </mo>
        <mfrac>
         <mrow>
          <mn>
           5
          </mn>
         </mrow>
         <mrow>
          <mn>
           5
          </mn>
          <mn>
           4
          </mn>
         </mrow>
        </mfrac>
        <msub>
         <mrow>
          <mo>
           <mi class="loglike">
            log
           </mi>
           <mo>
            ⁡
           </mo>
          </mo>
         </mrow>
         <mrow>
          <mn>
           2
          </mn>
         </mrow>
        </msub>
        <mfrac>
         <mrow>
          <mn>
           5
          </mn>
         </mrow>
         <mrow>
          <mn>
           5
          </mn>
          <mn>
           4
          </mn>
         </mrow>
        </mfrac>
        <mo class="MathClass-rel" stretchy="false">
         ≈
        </mo>
        <mn>
         0
        </mn>
        <mo class="MathClass-punc" stretchy="false">
         .
        </mo>
        <mn>
         4
        </mn>
        <mn>
         4
        </mn>
        <mn>
         5
        </mn>
       </math>
      </td>
     </tr>
    </table>
    <p class="noindent">
     And the general equation for entropy could be written as:
    </p>
    <table class="equation">
     <tr>
      <td>
       <math class="equation" display="block" xmlns="http://www.w3.org/1998/Math/MathML">
        <mstyle class="label" id="x4-19001r3">
        </mstyle>
        <msubsup>
         <mrow>
          <mi>
           H
          </mi>
         </mrow>
         <mrow>
          <mi>
           i
          </mi>
         </mrow>
         <mrow>
         </mrow>
        </msubsup>
        <mo class="MathClass-rel" stretchy="false">
         =
        </mo>
        <mo class="MathClass-bin" stretchy="false">
         −
        </mo>
        <munderover accent="false" accentunder="false">
         <mrow>
          <mo>
           ∑
          </mo>
         </mrow>
         <mrow>
          <mi>
           k
          </mi>
          <mo class="MathClass-rel" stretchy="false">
           =
          </mo>
          <mn>
           1
          </mn>
         </mrow>
         <mrow>
          <mi>
           n
          </mi>
         </mrow>
        </munderover>
        <msubsup>
         <mrow>
          <mi>
           p
          </mi>
         </mrow>
         <mrow>
          <mi>
           i
          </mi>
          <mo class="MathClass-punc" stretchy="false">
           ,
          </mo>
          <mi>
           k
          </mi>
         </mrow>
         <mrow>
         </mrow>
        </msubsup>
        <msub>
         <mrow>
          <mo>
           <mi class="loglike">
            log
           </mi>
           <mo>
            ⁡
           </mo>
          </mo>
         </mrow>
         <mrow>
          <mn>
           2
          </mn>
         </mrow>
        </msub>
        <msubsup>
         <mrow>
          <mi>
           p
          </mi>
         </mrow>
         <mrow>
          <mi>
           i
          </mi>
          <mo class="MathClass-punc" stretchy="false">
           ,
          </mo>
          <mi>
           k
          </mi>
         </mrow>
         <mrow>
         </mrow>
        </msubsup>
        <mspace class="qquad" width="2em">
        </mspace>
        <mstyle class="text">
         <mtext>
          where
         </mtext>
         <mstyle class="math">
          <msubsup>
           <mrow>
            <mi>
             p
            </mi>
           </mrow>
           <mrow>
            <mi>
             i
            </mi>
            <mo class="MathClass-punc" stretchy="false">
             ,
            </mo>
            <mi>
             k
            </mi>
           </mrow>
           <mrow>
           </mrow>
          </msubsup>
          <mo class="MathClass-rel" stretchy="false">
           ≠
          </mo>
          <mn>
           0
          </mn>
         </mstyle>
         <mtext>
         </mtext>
        </mstyle>
        <mspace class="qquad" width="2em">
        </mspace>
       </math>
      </td>
      <td class="eq-no">
       (2.3)
      </td>
     </tr>
    </table>
    <p class="noindent">
     So, which one to use ? Gini impurity or entropy? Most of the time it does not make a big difference: they lead to similar trees.
Gini impurity is slightly faster to compute, so it is a good default. However, when they differ, Gini impurity tends to isolate
the most frequent class in its own branch of the tree, while entropy tends to produce slightly more balanced
trees.
    </p>
    <p class="noindent">
    </p>
    <h2 class="sectionHead">
     <a aria-label="Link to this section in PDF version" class="pdf-link" href="DataScienceIILectureBook.pdf#section.2.7" style="color: red;  position: absolute;  left: -30px;" target="_blank">
      <svg class="bi bi-file-pdf" fill="currentColor" height="20" viewbox="0 0 16 16" width="20" xmlns="http://www.w3.org/2000/svg">
       <path d="M4 0a2 2 0 0 0-2 2v12a2 2 0 0 0 2 2h8a2 2 0 0 0 2-2V2a2 2 0 0 0-2-2H4zm0 1h8a1 1 0 0 1 1 1v12a1 1 0 0 1-1 1H4a1 1 0 0 1-1-1V2a1 1 0 0 1 1-1z">
       </path>
       <path d="M4.603 12.087a.81.81 0 0 1-.438-.42c-.195-.388-.13-.776.08-1.102.198-.307.526-.568.897-.787a7.68 7.68 0 0 1 1.482-.645 19.701 19.701 0 0 0 1.062-2.227 7.269 7.269 0 0 1-.43-1.295c-.086-.4-.119-.796-.046-1.136.075-.354.274-.672.65-.823.192-.077.4-.12.602-.077a.7.7 0 0 1 .477.365c.088.164.12.356.127.538.007.187-.012.395-.047.614-.084.51-.27 1.134-.52 1.794a10.954 10.954 0 0 0 .98 1.686 5.753 5.753 0 0 1 1.334.05c.364.065.734.195.96.465.12.144.193.32.2.518.007.192-.047.382-.138.563a1.04 1.04 0 0 1-.354.416.856.856 0 0 1-.51.138c-.331-.014-.654-.196-.933-.417a5.716 5.716 0 0 1-.911-.95 11.642 11.642 0 0 0-1.997.406 11.311 11.311 0 0 1-1.021 1.51c-.29.35-.608.655-.926.787a.793.793 0 0 1-.58.029zm1.379-1.901c-.166.076-.32.156-.459.238-.328.194-.541.383-.647.547-.094.145-.096.25-.04.361.01.022.02.036.026.044a.27.27 0 0 0 .035-.012c.137-.056.355-.235.635-.572a8.18 8.18 0 0 0 .45-.606zm1.64-1.33a12.647 12.647 0 0 1 1.01-.193 11.666 11.666 0 0 1-.51-.858 20.741 20.741 0 0 1-.5 1.05zm2.446.45c.15.162.296.3.435.41.24.19.407.253.498.256a.107.107 0 0 0 .07-.015.307.307 0 0 0 .094-.125.436.436 0 0 0 .059-.2.095.095 0 0 0-.026-.063c-.052-.062-.2-.152-.518-.209a3.881 3.881 0 0 0-.612-.053zM8.078 5.8a6.7 6.7 0 0 0 .2-.828c.031-.188.043-.343.038-.465a.613.613 0 0 0-.032-.198.517.517 0 0 0-.145.04c-.087.035-.158.106-.196.283-.04.192-.03.469.046.822.024.111.054.227.09.346z">
       </path>
      </svg>
     </a>
     <small class="titlemark">
      2.7
     </small>
     <a id="x4-200002.7">
     </a>
     Regularization Hyperparameters
    </h2>
    <p class="noindent">
     <a class="glossary" href="DataScienceIILectureBookli2.html#glo:dt">
      DT
     </a>
     s make very few assumptions about the training data (as opposed to linear models, which assume that the data is linear, for
example). If left unconstrained, the tree structure will adapt itself to the training data, fitting it very closely-indeed, most likely
     <span id="bold" style="font-weight:bold;">
      overfitting
     </span>
     it.
    </p>
    <p class="noindent">
     Such a model is often called a non-parametric model, not because it does not have any parameters (it often has a lot) but because
the number of parameters is not determined prior to training, so the model structure is free to stick closely to the
data.
    </p>
    <p class="noindent">
     In contrast, a parametric model, such as a linear model, has a predetermined number of parameters, so its degree of freedom is
limited, reducing the risk of over-fitting
    </p>
    <div class="warning">
     <p class="noindent">
      This increases the risk of underfitting.
     </p>
    </div>
    To avoid overfitting the training data, you need to restrict the
    <a class="glossary" href="DataScienceIILectureBookli2.html#glo:dt">
     DT
    </a>
    ’s freedom during
training. As you know by now, this is called regularization. The regularization hyperparameters depend on the algorithm used,
but generally you can at least restrict the maximum depth of the
    <a class="glossary" href="DataScienceIILectureBookli2.html#glo:dt">
     DT
    </a>. In
    <span style="color:#054C5C;">
     <code class="verb">
      sklearn
     </code>
    </span>
    , this is controlled by the
    <span style="color:#054C5C;">
     <code class="verb">
      max_depth
     </code>
    </span>
    hyperparameter. The default value is
    <span style="color:#054C5C;">
     <code class="verb">
      None
     </code>
    </span>
    , which means unlimited. Reducing
    <span style="color:#054C5C;">
     <code class="verb">
      max_depth
     </code>
    </span>
    will regularize the model and thus reduce
the risk of overfitting.
    <p class="noindent">
     The <span style="color:#054C5C;"><code class="verb">DecisionTreeClassifier</code></span>
     class has a few other parameters that similarly restrict the shape of the
     <a class="glossary" href="DataScienceIILectureBookli2.html#glo:dt">
      DT
     </a>
     :
    </p>
    <dl class="description">
     <dt class="description">
      <a id="x4-20001x2.7">
      </a> <span style="color:#054C5C;"><code class="verb">max_features</code></span>
     </dt>
     <dd class="description">
      <p class="noindent">
       Maximum
number
of
features
that
are
evaluated
for
splitting
at
each
node
      </p>
     </dd>
     <dt class="description">
      <a id="x4-20002x2.7">
      </a> <span style="color:#054C5C;"><code class="verb">max_leaf_nodes</code></span>
     </dt>
     <dd class="description">
      <p class="noindent">
       Maximum
number
of
leaf
nodes
      </p>
     </dd>
     <dt class="description">
      <a id="x4-20003x2.7">
      </a> <span style="color:#054C5C;"><code class="verb">min_samples_split</code></span>
     </dt>
     <dd class="description">
      <p class="noindent">
       Minimum
number
of
samples
a
node
must
have
before
it
can
be
split
      </p>
     </dd>
     <dt class="description">
      <a id="x4-20004x2.7">
      </a> <span style="color:#054C5C;"><code class="verb">min_samples_leaf</code></span>
     </dt>
     <dd class="description">
      <p class="noindent">
       Minimum
number
of
samples
a
leaf
node
must
have
to
be
created
      </p>
     </dd>
     <dt class="description">
      <a id="x4-20005x2.7">
      </a> <span style="color:#054C5C;"><code class="verb">min_weight_fraction_leaf</code></span>
     </dt>
     <dd class="description">
      <p class="noindent">
       Same
as <span style="color:#054C5C;"><code class="verb">min_samples_leaf</code></span>
       but
expressed
as
a
fraction
of
the
total
number
of
weighted
instances.
      </p>
     </dd>
    </dl>
    <div class="warning">
     <p class="noindent">
      Increasing <span style="color:#054C5C;"><code class="verb">min_*</code></span>
      hyperparameters or reducing <span style="color:#054C5C;"><code class="verb">max_*</code></span>
      hyperparameters will regularize the model.
     </p>
    </div>
    <div class="knowledge">
     <p class="noindent">
      Other algorithms work by first training the
      <a class="glossary" href="DataScienceIILectureBookli2.html#glo:dt">
       DT
      </a>
      without restrictions, then pruning unnecessary nodes. A node whose children are
all leaf nodes is considered unnecessary if the purity improvement it provides is not statistically significant. Standard statistical tests,
such as the
      <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
       <msup>
        <mrow>
         <mi>
          χ
         </mi>
        </mrow>
        <mrow>
         <mn>
          2
         </mn>
        </mrow>
       </msup>
      </math>
      test (chi-squared test), are used to estimate the probability that the improvement is purely the result of chance (which is called
the null hypothesis). If this probability, called the p-value, is higher than a given threshold (typically 5%, controlled by a
hyperparameter), then the node is considered unnecessary and its children are deleted. The pruning continues until all
unnecessary nodes have been pruned.
     </p>
    </div>
    Let’s test regularization on the moons dataset, introduced previously. We’ll train one
    <a class="glossary" href="DataScienceIILectureBookli2.html#glo:dt">
     DT
    </a>
    without regularization, and another with
    <span style="color:#054C5C;">
     <code class="verb">
      min_samples_leaf
     </code>
    </span>
    =5. Here’s the code; Figure
    <a href="#x4-20022r4">
     2.4
    </a>
    shows the decision boundaries of each
tree:
    <pre><div id="fancyvrb16" style="padding:20px;border-radius: 3px;"><a id="x4-20007r116"></a><span style="color:#2B2BFF;">from</span><span style="color:#BABABA;"> </span>sklearn.datasets<span style="color:#BABABA;"> </span><span style="color:#2B2BFF;">import</span> make_moons 
<a id="x4-20009r117"></a> 
<a id="x4-20011r118"></a>X_moons, y_moons = make_moons(n_samples=<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">150</span></span>, noise=<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">0.2</span></span>, random_state=<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">42</span></span>) 
<a id="x4-20013r119"></a> 
<a id="x4-20015r120"></a>tree_clf1 = DecisionTreeClassifier(random_state=<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">42</span></span>) 
<a id="x4-20017r121"></a>tree_clf2 = DecisionTreeClassifier(min_samples_leaf=<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">5</span></span>, random_state=<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">42</span></span>) 
<a id="x4-20019r122"></a>tree_clf1.fit(X_moons, y_moons) 
<a id="x4-20021r123"></a>tree_clf2.fit(X_moons, y_moons)</div></pre>
    <div class="figure">
     <img alt="PIC" height="" src="codes/images/Decision-Trees/min_samples_leaf_plot-.svg" width="150%"/>
     <a id="x4-20022r4">
     </a>
     <figcaption class="caption">
      <span class="id">
       Figure 2.4:
      </span>
      <span class="content">
       Decision boundaries of an unregularized tree (left) and a regularized tree (right)
      </span>
     </figcaption>
    </div>
    <p class="noindent">
     The unregularized model on the left is clearly overfitting, and the regularized model on the right will probably
generalize better. We can verify this by evaluating both trees on a test set generated using a different random
seed:
    </p>
    <pre><div id="fancyvrb17" style="padding:20px;border-radius: 3px;"><a id="x4-20024r130"></a>X_moons_test, y_moons_test = make_moons(n_samples=<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">1000</span></span>, noise=<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">0.2</span></span>, random_state=<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">43</span></span>) 
<a id="x4-20026r131"></a><span style="color:#2B2BFF;">print</span>(tree_clf1.score(X_moons_test, y_moons_test)) 
<a id="x4-20028r132"></a><span style="color:#2B2BFF;">print</span>(tree_clf2.score(X_moons_test, y_moons_test))</div></pre>
    <div class="tcolorbox tcolorbox" id="tcolobox-12">
     <div class="tcolorbox-content">
      <pre><div id="fancyvrb18" style="padding:20px;border-radius: 3px;"><a id="x4-20030r1"></a>0.898 
<a id="x4-20032r2"></a>0.92</div></pre>
     </div>
    </div>
    Indeed, the
second tree has a better accuracy on the test set.
    <p class="noindent">
    </p>
    <h2 class="sectionHead">
     <a aria-label="Link to this section in PDF version" class="pdf-link" href="DataScienceIILectureBook.pdf#section.2.8" style="color: red;  position: absolute;  left: -30px;" target="_blank">
      <svg class="bi bi-file-pdf" fill="currentColor" height="20" viewbox="0 0 16 16" width="20" xmlns="http://www.w3.org/2000/svg">
       <path d="M4 0a2 2 0 0 0-2 2v12a2 2 0 0 0 2 2h8a2 2 0 0 0 2-2V2a2 2 0 0 0-2-2H4zm0 1h8a1 1 0 0 1 1 1v12a1 1 0 0 1-1 1H4a1 1 0 0 1-1-1V2a1 1 0 0 1 1-1z">
       </path>
       <path d="M4.603 12.087a.81.81 0 0 1-.438-.42c-.195-.388-.13-.776.08-1.102.198-.307.526-.568.897-.787a7.68 7.68 0 0 1 1.482-.645 19.701 19.701 0 0 0 1.062-2.227 7.269 7.269 0 0 1-.43-1.295c-.086-.4-.119-.796-.046-1.136.075-.354.274-.672.65-.823.192-.077.4-.12.602-.077a.7.7 0 0 1 .477.365c.088.164.12.356.127.538.007.187-.012.395-.047.614-.084.51-.27 1.134-.52 1.794a10.954 10.954 0 0 0 .98 1.686 5.753 5.753 0 0 1 1.334.05c.364.065.734.195.96.465.12.144.193.32.2.518.007.192-.047.382-.138.563a1.04 1.04 0 0 1-.354.416.856.856 0 0 1-.51.138c-.331-.014-.654-.196-.933-.417a5.716 5.716 0 0 1-.911-.95 11.642 11.642 0 0 0-1.997.406 11.311 11.311 0 0 1-1.021 1.51c-.29.35-.608.655-.926.787a.793.793 0 0 1-.58.029zm1.379-1.901c-.166.076-.32.156-.459.238-.328.194-.541.383-.647.547-.094.145-.096.25-.04.361.01.022.02.036.026.044a.27.27 0 0 0 .035-.012c.137-.056.355-.235.635-.572a8.18 8.18 0 0 0 .45-.606zm1.64-1.33a12.647 12.647 0 0 1 1.01-.193 11.666 11.666 0 0 1-.51-.858 20.741 20.741 0 0 1-.5 1.05zm2.446.45c.15.162.296.3.435.41.24.19.407.253.498.256a.107.107 0 0 0 .07-.015.307.307 0 0 0 .094-.125.436.436 0 0 0 .059-.2.095.095 0 0 0-.026-.063c-.052-.062-.2-.152-.518-.209a3.881 3.881 0 0 0-.612-.053zM8.078 5.8a6.7 6.7 0 0 0 .2-.828c.031-.188.043-.343.038-.465a.613.613 0 0 0-.032-.198.517.517 0 0 0-.145.04c-.087.035-.158.106-.196.283-.04.192-.03.469.046.822.024.111.054.227.09.346z">
       </path>
      </svg>
     </a>
     <small class="titlemark">
      2.8
     </small>
     <a id="x4-210002.8">
     </a>
     Regression
    </h2>
    <p class="noindent">
     <a class="glossary" href="DataScienceIILectureBookli2.html#glo:dt">
      DT
     </a>
     s are also capable of performing regression tasks. Let’s build a regression tree using <span style="color:#054C5C;"><code class="verb">sklearn</code></span>
     ’s <span style="color:#054C5C;"><code class="verb">DecisionTreeRegressor</code></span>
     class,
training it on a noisy quadratic dataset with <span style="color:#054C5C;"><code class="verb">max_depth=2</code></span>
     :
    </p>
    <p class="noindent">
     The resulting tree is represented in Figure
     <a href="#x4-21001r5">
      2.5
     </a>.
    </p>
    <div class="figure">
     <p class="noindent">
      <img alt="PIC" height="" src="codes/images/Decision-Trees/regression_tree-.svg" width="150%"/>
      <a id="x4-21001r5">
      </a>
     </p>
     <figcaption class="caption">
      <span class="id">
       Figure 2.5:
      </span>
      <span class="content">
       A
       <a class="glossary" href="DataScienceIILectureBookli2.html#glo:dt">
        DT
       </a>
       for regression
      </span>
     </figcaption>
    </div>
    <p class="noindent">
     This tree looks very similar to the classification tree built earlier. The main difference is that instead of predicting a class
in each node, it predicts a value. For example, suppose you want to make a prediction for a new instance with
     <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
      <msubsup>
       <mrow>
        <mi>
         x
        </mi>
       </mrow>
       <mrow>
        <mn>
         1
        </mn>
       </mrow>
       <mrow>
       </mrow>
      </msubsup>
     </math>
     = 0.2. The root
node asks whether
     <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
      <msubsup>
       <mrow>
        <mi>
         x
        </mi>
       </mrow>
       <mrow>
        <mn>
         1
        </mn>
       </mrow>
       <mrow>
       </mrow>
      </msubsup>
      <mo class="MathClass-rel" stretchy="false">
       ≤
      </mo>
     </math>
     0.197. Since it is not, the algorithm goes to the right child node, which asks whether
     <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
      <msubsup>
       <mrow>
        <mi>
         x
        </mi>
       </mrow>
       <mrow>
        <mn>
         1
        </mn>
       </mrow>
       <mrow>
       </mrow>
      </msubsup>
      <mo class="MathClass-rel" stretchy="false">
       ≤
      </mo>
     </math>
     0.772.
Since it is, the algorithm goes to the left child node. This is a leaf node, and it predicts <span style="color:#054C5C;"><code class="verb">value=0.111</code></span>. This prediction is the
average target value of the 110 training instances associated with this leaf node, and it results in a mean squared error equal to
0.015 over these 110 instances.
    </p>
    <p class="noindent">
     This model’s predictions are represented on the left in Figure
     <a href="#x4-21002r6">
      2.6
     </a>. If you set <span style="color:#054C5C;"><code class="verb">max_depth=3</code></span>, you get the predictions represented on
the right. Notice how the predicted value for each region is always the average target value of the instances in that region. The
algorithm splits each region in a way that makes most training instances as close as possible to that predicted value.
    </p>
    <div class="figure">
     <p class="noindent">
      <img alt="PIC" height="" src="codes/images/Decision-Trees/tree_regression_plot-.svg" width="150%"/>
      <a id="x4-21002r6">
      </a>
     </p>
     <figcaption class="caption">
      <span class="id">
       Figure 2.6:
      </span>
      <span class="content">
       Predictions of two
       <a class="glossary" href="DataScienceIILectureBookli2.html#glo:dt">
        DT
       </a>
       regression models
      </span>
     </figcaption>
    </div>
    <p class="noindent">
     The CART algorithm works as described earlier, except that instead of trying to split the training set in a way that minimizes
impurity, it now tries to split the training set in a way that minimizes the MSE. Equation 6-4 shows the cost function that the
algorithm tries to minimize. Eq. (??) CART cost function for regression Just like for classification tasks,
     <a class="glossary" href="DataScienceIILectureBookli2.html#glo:dt">
      DT
     </a>
     s are prone to
overfitting when dealing with regression tasks. Without any regularization (i.e., using the default hyperparameters), you get the
predictions on the left in Figure
     <a href="#x4-21003r7">
      2.7
     </a>
    </p>
    <p class="noindent">
     These predictions are overfitting the training set very badly. Just setting <span style="color:#054C5C;"><code class="verb">min_samples_leaf=10</code></span>
     results in a much more reasonable
model, represented on the right in Figure 6-6.
    </p>
    <div class="figure">
     <p class="noindent">
      <img alt="PIC" height="" src="codes/images/Decision-Trees/tree_regression_regularization_plot-.svg" width="150%"/>
      <a id="x4-21003r7">
      </a>
     </p>
     <figcaption class="caption">
      <span class="id">
       Figure 2.7:
      </span>
      <span class="content">
       Predictions of an unregularized regression tree (left) and a regularized tree (right)
      </span>
     </figcaption>
    </div>
    <p class="noindent">
    </p>
    <h2 class="sectionHead">
     <a aria-label="Link to this section in PDF version" class="pdf-link" href="DataScienceIILectureBook.pdf#section.2.9" style="color: red;  position: absolute;  left: -30px;" target="_blank">
      <svg class="bi bi-file-pdf" fill="currentColor" height="20" viewbox="0 0 16 16" width="20" xmlns="http://www.w3.org/2000/svg">
       <path d="M4 0a2 2 0 0 0-2 2v12a2 2 0 0 0 2 2h8a2 2 0 0 0 2-2V2a2 2 0 0 0-2-2H4zm0 1h8a1 1 0 0 1 1 1v12a1 1 0 0 1-1 1H4a1 1 0 0 1-1-1V2a1 1 0 0 1 1-1z">
       </path>
       <path d="M4.603 12.087a.81.81 0 0 1-.438-.42c-.195-.388-.13-.776.08-1.102.198-.307.526-.568.897-.787a7.68 7.68 0 0 1 1.482-.645 19.701 19.701 0 0 0 1.062-2.227 7.269 7.269 0 0 1-.43-1.295c-.086-.4-.119-.796-.046-1.136.075-.354.274-.672.65-.823.192-.077.4-.12.602-.077a.7.7 0 0 1 .477.365c.088.164.12.356.127.538.007.187-.012.395-.047.614-.084.51-.27 1.134-.52 1.794a10.954 10.954 0 0 0 .98 1.686 5.753 5.753 0 0 1 1.334.05c.364.065.734.195.96.465.12.144.193.32.2.518.007.192-.047.382-.138.563a1.04 1.04 0 0 1-.354.416.856.856 0 0 1-.51.138c-.331-.014-.654-.196-.933-.417a5.716 5.716 0 0 1-.911-.95 11.642 11.642 0 0 0-1.997.406 11.311 11.311 0 0 1-1.021 1.51c-.29.35-.608.655-.926.787a.793.793 0 0 1-.58.029zm1.379-1.901c-.166.076-.32.156-.459.238-.328.194-.541.383-.647.547-.094.145-.096.25-.04.361.01.022.02.036.026.044a.27.27 0 0 0 .035-.012c.137-.056.355-.235.635-.572a8.18 8.18 0 0 0 .45-.606zm1.64-1.33a12.647 12.647 0 0 1 1.01-.193 11.666 11.666 0 0 1-.51-.858 20.741 20.741 0 0 1-.5 1.05zm2.446.45c.15.162.296.3.435.41.24.19.407.253.498.256a.107.107 0 0 0 .07-.015.307.307 0 0 0 .094-.125.436.436 0 0 0 .059-.2.095.095 0 0 0-.026-.063c-.052-.062-.2-.152-.518-.209a3.881 3.881 0 0 0-.612-.053zM8.078 5.8a6.7 6.7 0 0 0 .2-.828c.031-.188.043-.343.038-.465a.613.613 0 0 0-.032-.198.517.517 0 0 0-.145.04c-.087.035-.158.106-.196.283-.04.192-.03.469.046.822.024.111.054.227.09.346z">
       </path>
      </svg>
     </a>
     <small class="titlemark">
      2.9
     </small>
     <a id="x4-220002.9">
     </a>
     Sensitivity to Axis Orientation
    </h2>
    <p class="noindent">
     <a class="glossary" href="DataScienceIILectureBookli2.html#glo:dt">
      DT
     </a>
     s have a lot going for them: they are relatively easy to understand and interpret, simple to use, versatile,
and powerful. However, they do have a few limitations. First, as you may have noticed,
     <a class="glossary" href="DataScienceIILectureBookli2.html#glo:dt">
      DT
     </a>
     s love orthogonal
decision boundaries (all splits are perpendicular to an axis), which makes them sensitive to the orientation of data.
    </p>
    <div class="figure">
     <p class="noindent">
      <img alt="PIC" height="" src="codes/images/Decision-Trees/sensitivity_to_rotation_plot-.svg" width="150%"/>
      <a id="x4-22001r8">
      </a>
     </p>
     <figcaption class="caption">
      <span class="id">
       Figure 2.8:
      </span>
      <span class="content">
       Sensitivity to training set rotation
      </span>
     </figcaption>
    </div>
    <p class="noindent">
     For example, Figure
     <a href="#x4-22001r8">
      2.8
     </a>
     shows a simple linearly separable dataset: on the left, a
     <a class="glossary" href="DataScienceIILectureBookli2.html#glo:dt">
      DT
     </a>
     can split it easily, while
on the right, after the dataset is rotated by 45 degrees, the decision boundary looks unnecessarily convoluted.
Although both
     <a class="glossary" href="DataScienceIILectureBookli2.html#glo:dt">
      DT
     </a>
     s fit the training set perfectly, it is very likely that the model on the right will not generalize
well.
    </p>
    <p class="noindent">
     One way to limit this problem is to scale the data, then apply a principal component analysis transformation. We will look at
PCA in detail later, but for now you only need to know that it rotates the data in a way that reduces the correlation between the
features, which often makes things easier for trees. Let’s create a small pipeline that scales the data and rotates it using PCA,
then train a DecisionTreeClassifier on that data.
    </p>
    <pre><div id="fancyvrb19" style="padding:20px;border-radius: 3px;"><a id="x4-22003r319"></a><span style="color:#2B2BFF;">from</span><span style="color:#BABABA;"> </span>sklearn.decomposition<span style="color:#BABABA;"> </span><span style="color:#2B2BFF;">import</span> PCA 
<a id="x4-22005r320"></a><span style="color:#2B2BFF;">from</span><span style="color:#BABABA;"> </span>sklearn.pipeline<span style="color:#BABABA;"> </span><span style="color:#2B2BFF;">import</span> make_pipeline 
<a id="x4-22007r321"></a><span style="color:#2B2BFF;">from</span><span style="color:#BABABA;"> </span>sklearn.preprocessing<span style="color:#BABABA;"> </span><span style="color:#2B2BFF;">import</span> StandardScaler 
<a id="x4-22009r322"></a> 
<a id="x4-22011r323"></a>pca_pipeline = make_pipeline(StandardScaler(), PCA()) 
<a id="x4-22013r324"></a>X_iris_rotated = pca_pipeline.fit_transform(X_iris) 
<a id="x4-22015r325"></a>tree_clf_pca = DecisionTreeClassifier(max_depth=<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">2</span></span>, random_state=<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">42</span></span>) 
<a id="x4-22017r326"></a>tree_clf_pca.fit(X_iris_rotated, y_iris)</div></pre>
    <p class="noindent">
     Figure
     <a href="#x4-23001r10">
      2.10
     </a>
     shows the decision boundaries of that tree: as you can see, the rotation makes it possible to fit the dataset
pretty well using only one feature <alert style="color: #821131;">(1)</alert>, which is a linear function of the original petal length and width.
    </p>
    <div class="figure">
     <img alt="PIC" height="" src="codes/images/Decision-Trees/pca_preprocessing_plot-.svg" width="150%"/>
     <a id="x4-22018r9">
     </a>
     <figcaption class="caption">
      <span class="id">
       Figure 2.9:
      </span>
      <span class="content">
       A tree’s decision boundaries on the scaled and PCA-rotated iris dataset
      </span>
     </figcaption>
    </div>
    <p class="noindent">
    </p>
    <h2 class="sectionHead">
     <a aria-label="Link to this section in PDF version" class="pdf-link" href="DataScienceIILectureBook.pdf#section.2.10" style="color: red;  position: absolute;  left: -30px;" target="_blank">
      <svg class="bi bi-file-pdf" fill="currentColor" height="20" viewbox="0 0 16 16" width="20" xmlns="http://www.w3.org/2000/svg">
       <path d="M4 0a2 2 0 0 0-2 2v12a2 2 0 0 0 2 2h8a2 2 0 0 0 2-2V2a2 2 0 0 0-2-2H4zm0 1h8a1 1 0 0 1 1 1v12a1 1 0 0 1-1 1H4a1 1 0 0 1-1-1V2a1 1 0 0 1 1-1z">
       </path>
       <path d="M4.603 12.087a.81.81 0 0 1-.438-.42c-.195-.388-.13-.776.08-1.102.198-.307.526-.568.897-.787a7.68 7.68 0 0 1 1.482-.645 19.701 19.701 0 0 0 1.062-2.227 7.269 7.269 0 0 1-.43-1.295c-.086-.4-.119-.796-.046-1.136.075-.354.274-.672.65-.823.192-.077.4-.12.602-.077a.7.7 0 0 1 .477.365c.088.164.12.356.127.538.007.187-.012.395-.047.614-.084.51-.27 1.134-.52 1.794a10.954 10.954 0 0 0 .98 1.686 5.753 5.753 0 0 1 1.334.05c.364.065.734.195.96.465.12.144.193.32.2.518.007.192-.047.382-.138.563a1.04 1.04 0 0 1-.354.416.856.856 0 0 1-.51.138c-.331-.014-.654-.196-.933-.417a5.716 5.716 0 0 1-.911-.95 11.642 11.642 0 0 0-1.997.406 11.311 11.311 0 0 1-1.021 1.51c-.29.35-.608.655-.926.787a.793.793 0 0 1-.58.029zm1.379-1.901c-.166.076-.32.156-.459.238-.328.194-.541.383-.647.547-.094.145-.096.25-.04.361.01.022.02.036.026.044a.27.27 0 0 0 .035-.012c.137-.056.355-.235.635-.572a8.18 8.18 0 0 0 .45-.606zm1.64-1.33a12.647 12.647 0 0 1 1.01-.193 11.666 11.666 0 0 1-.51-.858 20.741 20.741 0 0 1-.5 1.05zm2.446.45c.15.162.296.3.435.41.24.19.407.253.498.256a.107.107 0 0 0 .07-.015.307.307 0 0 0 .094-.125.436.436 0 0 0 .059-.2.095.095 0 0 0-.026-.063c-.052-.062-.2-.152-.518-.209a3.881 3.881 0 0 0-.612-.053zM8.078 5.8a6.7 6.7 0 0 0 .2-.828c.031-.188.043-.343.038-.465a.613.613 0 0 0-.032-.198.517.517 0 0 0-.145.04c-.087.035-.158.106-.196.283-.04.192-.03.469.046.822.024.111.054.227.09.346z">
       </path>
      </svg>
     </a>
     <small class="titlemark">
      2.10
     </small>
     <a id="x4-230002.10">
     </a>
     DTs Have a High Variance
    </h2>
    <p class="noindent">
     More generally, the main issue with
     <a class="glossary" href="DataScienceIILectureBookli2.html#glo:dt">
      DT
     </a>
     s is that they have quite a
     <span id="bold" style="font-weight:bold;">
      high variance
     </span>
     : small changes to the hyperparameters or to the
data may produce very different models. In fact, since the training algorithm used by <span style="color:#054C5C;"><code class="verb">sklearn</code></span>
     is stochastic-it randomly selects the
set of features to evaluate at each node-even retraining the same
     <a class="glossary" href="DataScienceIILectureBookli2.html#glo:dt">
      DT
     </a>
     on the exact same data may produce a very different model,
such as the one represented in Figure
     <a href="#x4-23001r10">
      2.10
     </a>
     (unless you set the <span style="color:#054C5C;"><code class="verb">random_state</code></span>
     hyperparameter). As you can see, it looks very
different from the previous
     <a class="glossary" href="DataScienceIILectureBookli2.html#glo:dt">
      DT
     </a>
     (shown in Figure
     <a href="#x4-15001r1">
      2.1
     </a>.
    </p>
    <div class="figure">
     <p class="noindent">
      <img alt="PIC" height="" src="codes/images/Decision-Trees/decision_tree_high_variance_plot-.svg" width="150%"/>
      <a id="x4-23001r10">
      </a>
     </p>
     <figcaption class="caption">
      <span class="id">
       Figure 2.10:
      </span>
      <span class="content">
       Retraining the same model on the same data may produce a very different model
      </span>
     </figcaption>
    </div>
    <p class="noindent">
     Luckily, by averaging predictions over many trees, it’s possible to reduce variance signiflicantly. Such an ensemble of trees is
called a random forest, and it’s one of the most powerful types of models available today, as you will see in the next chapter.
    </p>
   </article>
   <footer>
    <div class="next">
     <p class="noindent">
      <a href="DataScienceIILectureBookch3.html" style="float: right;">
       Next Chapter →
      </a>
      <a href="DataScienceIILectureBookch1.html" style="float: left;">
       ← Previous Chapter
      </a>
      <a href="index.html">
       TOP
      </a>
     </p>
    </div>
   </footer>
  </div>
  <p class="noindent">
   <a id="tailDataScienceIILectureBookch2.html">
   </a>
  </p>
 </body>
</html>
