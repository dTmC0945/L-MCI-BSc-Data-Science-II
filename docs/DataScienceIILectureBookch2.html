<!DOCTYPE html>
<html lang="en-US" xml:lang="en-US">
 <head>
  <title>
   2 Decision Trees
  </title>
    <link rel='icon' type='image/x-icon' href='figures/logo.svg'></link>
  <meta charset="utf-8"/>
  <meta content="ParSnip" name="generator"/>
  <meta content="width=device-width,initial-scale=1" name="viewport"/>
  <link href="DataScienceIILectureBook.css" rel="stylesheet" type="text/css"/>
  <meta content="DataScienceIILectureBook.tex" name="src"/>
  <script async="async" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/mml-chtml.js" type="text/javascript">
  </script>
  <link href="flatweb.css" rel="stylesheet" type="text/css"/>
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.7.1/jquery.min.js">
  </script>
  <script src="flatjs.js">
  </script>
  <link href="http://fonts.googleapis.com/css?family=Roboto" rel="stylesheet" type="text/css"/>
  <script type="module">
   import pdfjsDist from 'https://cdn.jsdelivr.net/npm/pdfjs-dist@5.3.93/+esm'
  </script>
  <script>
   function moveTOC() { 
var htmlShow = document.getElementsByClassName("wide"); 
if (htmlShow[0].style.display === "none") { 
htmlShow[0].style.display = "block"; 
} else { 
htmlShow[0].style.display = "none"; 
} 
}
  </script>
  <link href="https://unpkg.com/nord-highlightjs@0.1.0/dist/nord.css" rel="stylesheet" type="text/css"/>
  <script src="https://unpkg.com/@highlightjs/cdn-assets@11.3.1/highlight.min.js">
  </script>
  <script src="https://unpkg.com/highlightjs-copy/dist/highlightjs-copy.min.js">
  </script>
  <link href="https://unpkg.com/highlightjs-copy/dist/highlightjs-copy.min.css" rel="stylesheet"/>
  <script src="flatjs.js">
  </script>
  <header>
  </header>
 </head>
 <body id="top">
  <nav class="wide">
   <div class="contents">
    <h3 style="font-weight:lighter; padding: 20px 0 20px 0;">
     2
   
     
   

   Decision Trees
    </h3>
    <h4 style="font-weight:lighter">
     Chapter Table of Contents
    </h4>
    <ul>
     <div class="chapterTOCS">
      <li>
       <span class="sectionToc">
        <small>
         2.1
        </small>
        <a href="#x4-130002.1">
         Introduction
        </a>
       </span>
      </li>
      <li>
       <span class="subsectionToc" style="font-size:12px;margin:0px;">
        <small>
         2.1.1
        </small>
        <a href="#x4-140002.1.1">
         Advantages and Disadvantages
        </a>
       </span>
      </li>
      <li>
       <span class="sectionToc">
        <small>
         2.2
        </small>
        <a href="#x4-150002.2">
         Training and Visualising Decision Trees
        </a>
       </span>
      </li>
      <li>
       <span class="sectionToc">
        <small>
         2.3
        </small>
        <a href="#x4-160002.3">
         Making Predictions
        </a>
       </span>
      </li>
      <li>
       <span class="subsectionToc" style="font-size:12px;margin:0px;">
        <small>
         2.3.1
        </small>
        <a href="#x4-170002.3.1">
         Gini Impurity
        </a>
       </span>
      </li>
      <li>
       <span class="subsectionToc" style="font-size:12px;margin:0px;">
        <small>
         2.3.2
        </small>
        <a href="#x4-180002.3.2">
         Estimating Class Probabilities
        </a>
       </span>
      </li>
      <li>
       <span class="sectionToc">
        <small>
         2.4
        </small>
        <a href="#x4-190002.4">
         The CART Training Algorithm
        </a>
       </span>
      </li>
      <li>
       <span class="subsectionToc" style="font-size:12px;margin:0px;">
        <small>
         2.4.1
        </small>
        <a href="#x4-200002.4.1">
         Gini Impurity v. Information Entropy
        </a>
       </span>
      </li>
      <li>
       <span class="subsectionToc" style="font-size:12px;margin:0px;">
        <small>
         2.4.2
        </small>
        <a href="#x4-210002.4.2">
         Regularisation Hyperparameters
        </a>
       </span>
      </li>
      <li>
       <span class="sectionToc">
        <small>
         2.5
        </small>
        <a href="#x4-220002.5">
         Regression
        </a>
       </span>
      </li>
      <li>
       <span class="sectionToc">
        <small>
         2.6
        </small>
        <a href="#x4-230002.6">
         Sensitivity to Axis Orientation
        </a>
       </span>
      </li>
     </div>
    </ul>
    <div class="prev-next" style="text-align: center">
     <p class="noindent">
      <a href="DataScienceIILectureBookch3.html" style="float:right; font-size:10px">
       NEXT →
      </a>
      <a href="DataScienceIILectureBookch1.html" style="float:left; font-size:10px">
       ← PREV
      </a>
      <a href="index.html">
       TOP
      </a>
     </p>
    </div>
    <div id="author-info" style="bottom: 0; padding-bottom: 10px;">
     <div id="author-logo">
      <img src="figures/logo.svg" style="margin: 10px 0;"/>
      <div>
       <div>
        <h4 style="color:#7aa0b8; font-weight:lighter;">
         WebBook | D. T. McGuiness, P.hD
        </h4>
       </div>
      </div>
      <div id="author-text" style="bottom: 0; padding-top: 50px;border-top: solid 1px #3b4b5e;font-size: 12px;text-align: right;">
       <p>
        <b>
         Authors Note
        </b>
        The website you are viewing is auto-generated
    using ParSnip and therefore subject to slight errors in
    typography and formatting. When in doubt, please consult the
    LectureBook.
       </p>
      </div>
     </div>
    </div>
   </div>
  </nav>
  <div class="page">
   <article class="chapter">
    <h1 class="chapterHead">
     <div class="number">
      2
     </div>
     <a id="x4-120002">
     </a>
     Decision Trees
    </h1><button id='toc-button' onclick='moveTOC()'>TOC</button>
    <div class="section-control" style="text-align: center">
     <a id="prev-section" onclick="goPrev()" style="float:left;">
      ← Previous Section
     </a>
     <a id="next-section" onclick="goNext()" style="float:right;">
      Next Section →
     </a>
    </div>
    <h2 class="sectionHead">
     <a aria-label="Link to this section in PDF version" class="pdf-link" href="DataScienceIILectureBook.pdf#section.2.1" style="color: red;  position: absolute;  left: -30px;" target="_blank">
      <svg class="bi bi-file-pdf" fill="currentColor" height="20" viewbox="0 0 16 16" width="20" xmlns="http://www.w3.org/2000/svg">
       <path d="M4 0a2 2 0 0 0-2 2v12a2 2 0 0 0 2 2h8a2 2 0 0 0 2-2V2a2 2 0 0 0-2-2H4zm0 1h8a1 1 0 0 1 1 1v12a1 1 0 0 1-1 1H4a1 1 0 0 1-1-1V2a1 1 0 0 1 1-1z">
       </path>
       <path d="M4.603 12.087a.81.81 0 0 1-.438-.42c-.195-.388-.13-.776.08-1.102.198-.307.526-.568.897-.787a7.68 7.68 0 0 1 1.482-.645 19.701 19.701 0 0 0 1.062-2.227 7.269 7.269 0 0 1-.43-1.295c-.086-.4-.119-.796-.046-1.136.075-.354.274-.672.65-.823.192-.077.4-.12.602-.077a.7.7 0 0 1 .477.365c.088.164.12.356.127.538.007.187-.012.395-.047.614-.084.51-.27 1.134-.52 1.794a10.954 10.954 0 0 0 .98 1.686 5.753 5.753 0 0 1 1.334.05c.364.065.734.195.96.465.12.144.193.32.2.518.007.192-.047.382-.138.563a1.04 1.04 0 0 1-.354.416.856.856 0 0 1-.51.138c-.331-.014-.654-.196-.933-.417a5.716 5.716 0 0 1-.911-.95 11.642 11.642 0 0 0-1.997.406 11.311 11.311 0 0 1-1.021 1.51c-.29.35-.608.655-.926.787a.793.793 0 0 1-.58.029zm1.379-1.901c-.166.076-.32.156-.459.238-.328.194-.541.383-.647.547-.094.145-.096.25-.04.361.01.022.02.036.026.044a.27.27 0 0 0 .035-.012c.137-.056.355-.235.635-.572a8.18 8.18 0 0 0 .45-.606zm1.64-1.33a12.647 12.647 0 0 1 1.01-.193 11.666 11.666 0 0 1-.51-.858 20.741 20.741 0 0 1-.5 1.05zm2.446.45c.15.162.296.3.435.41.24.19.407.253.498.256a.107.107 0 0 0 .07-.015.307.307 0 0 0 .094-.125.436.436 0 0 0 .059-.2.095.095 0 0 0-.026-.063c-.052-.062-.2-.152-.518-.209a3.881 3.881 0 0 0-.612-.053zM8.078 5.8a6.7 6.7 0 0 0 .2-.828c.031-.188.043-.343.038-.465a.613.613 0 0 0-.032-.198.517.517 0 0 0-.145.04c-.087.035-.158.106-.196.283-.04.192-.03.469.046.822.024.111.054.227.09.346z">
       </path>
      </svg>
     </a>
     <small class="titlemark">
      2.1
     </small>
     <a id="x4-130002.1">
     </a>
     Introduction
    </h2>
    <p class="noindent">
     <a class="glossary" href="DataScienceIILectureBookli2.html#glo:dt">
      Decision Tree (DT)
     </a>
     is a versatile
     <a class="glossary" href="DataScienceIILectureBookli2.html#glo:ml">
      ML
     </a>
     algorithm which can perform both classification and regression
tasks, and even multi-output tasks, capable of fitting complex datasets. They are classified as a
non-parametric supervised learning algorithm, which is utilised for both classification and regression
tasks.
    </p>
    <p class="noindent">
     It has a hierarchical tree structure, consisting of a root node, branches, internal nodes and leaf
nodes.
     <a class="glossary" href="DataScienceIILectureBookli2.html#glo:dt">
      DT
     </a>
     s are also the fundamental components of random forests, which are among the most
powerful
     <a class="glossary" href="DataScienceIILectureBookli2.html#glo:ml">
      ML
     </a>
     algorithms available today. Some of the applications include:
    </p>
    <dl class="description">
     <dt class="description">
      <a id="x4-13001x2.1">
      </a>
      Loan Approval in Banking
     </dt>
     <dd class="description">
      <p class="noindent">
       Banks use
       <a class="glossary" href="DataScienceIILectureBookli2.html#glo:dt">
        DT
       </a>
       s to assess whether a loan application should be approved. The decision is
based on factors such as credit score, income, and loan history. This helps predict approval
or rejection and enables quick and reliable decisions <a id="x4-13002"></a><a href="#X0-alaradi2020tree">[1]</a>,
       <a id="x4-13003">
       </a> <a href="#X0-rajesh2020real">[2]</a>.
      </p>
     </dd>
     <dt class="description">
      <a id="x4-13004x2.1">
      </a>
      Medical Diagnosis
     </dt>
     <dd class="description">
      <p class="noindent">
       In healthcare they assist in diagnosing diseases. For example, they can predict whether
a patient has diabetes based on clinical data like glucose levels,
       <a class="glossary" href="DataScienceIILectureBookli2.html#glo:bmi">
        Body-Mass Index (BMI)
       </a>
       and blood pressure <a id="x4-13005"></a><a href="#X0-rodriguez2019decision">[3]</a>. This helps classify patients into diabetic or non-diabetic categories,
supporting early diagnosis and treatment <a id="x4-13006"></a><a href="#X0-azar2013decision">[4]</a>.
      </p>
     </dd>
     <dt class="description">
      <a id="x4-13007x2.1">
      </a>
      Predicting Exam Results in Education
     </dt>
     <dd class="description">
      <p class="noindent">
       Educational institutions use to predict whether a student will pass or fail based on factors
like attendance, study time and past grades. This helps teachers identify at-risk students
and offer targeted support <a id="x4-13008"></a><a href="#X0-mesaric2016decision">[5]</a>,
       <a id="x4-13009">
       </a> <a href="#X0-kumar2011efficiency">[6]</a>.
      </p>
     </dd>
     <dt class="description">
      <a id="x4-13010x2.1">
      </a>
      Customer Churn Prediction
     </dt>
     <dd class="description">
      <p class="noindent">
       Companies use
       <a class="glossary" href="DataScienceIILectureBookli2.html#glo:dt">
        DT
       </a>
       s to predict whether a customer will leave or stay based on behaviour
patterns, purchase history, and interactions. This allows businesses to take proactive steps
to retain customers <a id="x4-13011"></a><a href="#X0-dalvi2016analysis">[7]</a>.
      </p>
     </dd>
     <dt class="description">
      <a id="x4-13012x2.1">
      </a>
      Fraud Detection
     </dt>
     <dd class="description">
      <p class="noindent">
       In finance,
       <a class="glossary" href="DataScienceIILectureBookli2.html#glo:dt">
        DT
       </a>
       s are used to detect fraudulent activities, such as credit card fraud <a id="x4-13013"></a><a href="#X0-save2017novel">[8]</a>. By
analysing past transaction data and patterns,
       <a class="glossary" href="DataScienceIILectureBookli2.html#glo:dt">
        DT
       </a>
       s can identify suspicious activities and
flag them for further investigation <a id="x4-13014"></a><a href="#X0-sahin2013cost">[9]</a>.
      </p>
     </dd>
    </dl>
    <p class="noindent">
     In this chapter we will start by discussing how to train, visualise, and make predictions with
     <a class="glossary" href="DataScienceIILectureBookli2.html#glo:dt">
      DT
     </a>
     s. Then
we will go through the
     <a class="glossary" href="DataScienceIILectureBookli2.html#glo:cart">
      Classification and Regression Tree (CART)
     </a>
     training algorithm used by <span style="color:#054C5C;"><code class="verb">scikit-learn</code></span>, and we will explore how to regularise trees and use them for regression
tasks.
    </p>
    <h3 class="subsectionHead">
     <a aria-label="Link to this section in PDF version" class="pdf-link" href="DataScienceIILectureBook.pdf#subsection.2.1.1" style="color: red;  position: absolute;  left: -30px;" target="_blank">
      <svg class="bi bi-file-pdf" fill="currentColor" height="20" viewbox="0 0 16 16" width="20" xmlns="http://www.w3.org/2000/svg">
       <path d="M4 0a2 2 0 0 0-2 2v12a2 2 0 0 0 2 2h8a2 2 0 0 0 2-2V2a2 2 0 0 0-2-2H4zm0 1h8a1 1 0 0 1 1 1v12a1 1 0 0 1-1 1H4a1 1 0 0 1-1-1V2a1 1 0 0 1 1-1z">
       </path>
       <path d="M4.603 12.087a.81.81 0 0 1-.438-.42c-.195-.388-.13-.776.08-1.102.198-.307.526-.568.897-.787a7.68 7.68 0 0 1 1.482-.645 19.701 19.701 0 0 0 1.062-2.227 7.269 7.269 0 0 1-.43-1.295c-.086-.4-.119-.796-.046-1.136.075-.354.274-.672.65-.823.192-.077.4-.12.602-.077a.7.7 0 0 1 .477.365c.088.164.12.356.127.538.007.187-.012.395-.047.614-.084.51-.27 1.134-.52 1.794a10.954 10.954 0 0 0 .98 1.686 5.753 5.753 0 0 1 1.334.05c.364.065.734.195.96.465.12.144.193.32.2.518.007.192-.047.382-.138.563a1.04 1.04 0 0 1-.354.416.856.856 0 0 1-.51.138c-.331-.014-.654-.196-.933-.417a5.716 5.716 0 0 1-.911-.95 11.642 11.642 0 0 0-1.997.406 11.311 11.311 0 0 1-1.021 1.51c-.29.35-.608.655-.926.787a.793.793 0 0 1-.58.029zm1.379-1.901c-.166.076-.32.156-.459.238-.328.194-.541.383-.647.547-.094.145-.096.25-.04.361.01.022.02.036.026.044a.27.27 0 0 0 .035-.012c.137-.056.355-.235.635-.572a8.18 8.18 0 0 0 .45-.606zm1.64-1.33a12.647 12.647 0 0 1 1.01-.193 11.666 11.666 0 0 1-.51-.858 20.741 20.741 0 0 1-.5 1.05zm2.446.45c.15.162.296.3.435.41.24.19.407.253.498.256a.107.107 0 0 0 .07-.015.307.307 0 0 0 .094-.125.436.436 0 0 0 .059-.2.095.095 0 0 0-.026-.063c-.052-.062-.2-.152-.518-.209a3.881 3.881 0 0 0-.612-.053zM8.078 5.8a6.7 6.7 0 0 0 .2-.828c.031-.188.043-.343.038-.465a.613.613 0 0 0-.032-.198.517.517 0 0 0-.145.04c-.087.035-.158.106-.196.283-.04.192-.03.469.046.822.024.111.054.227.09.346z">
       </path>
      </svg>
     </a>
     <small class="titlemark">
      2.1.1
     </small>
     <a id="x4-140002.1.1">
     </a>
     Advantages and Disadvantages
    </h3>
    <p class="noindent">
     Before we start with our chapter on
     <a class="glossary" href="DataScienceIILectureBookli2.html#glo:dt">
      DT
     </a>, lets list down the advantages it has over other
methods:
    </p>
    <dl class="description">
     <dt class="description">
      <a id="x4-14001x2.1.1">
      </a>
      Easy to Understand
     </dt>
     <dd class="description">
      <p class="noindent">
       <a class="glossary" href="DataScienceIILectureBookli2.html#glo:dt">
        DT
       </a>
       s are visual which makes it easy to follow the decision-making process
      </p>
     </dd>
     <dt class="description">
      <a id="x4-14002x2.1.1">
      </a>
      Versatility
     </dt>
     <dd class="description">
      <p class="noindent">
       Can be used for both classification and regression problems.
      </p>
     </dd>
     <dt class="description">
      <a id="x4-14003x2.1.1">
      </a>
      No Need for Feature Scaling
     </dt>
     <dd class="description">
      <p class="noindent">
       Unlike many
       <a class="glossary" href="DataScienceIILectureBookli2.html#glo:ml">
        ML
       </a>
       models, it doesn’t require us to scale or normalise our data.
      </p>
     </dd>
     <dt class="description">
      <a id="x4-14004x2.1.1">
      </a>
      Handles Non-linear Relationships
     </dt>
     <dd class="description">
      <p class="noindent">
       It capture complex, non-linear relationships between features and outcomes effectively.
      </p>
     </dd>
     <dt class="description">
      <a id="x4-14005x2.1.1">
      </a>
      Interpretability
     </dt>
     <dd class="description">
      <p class="noindent">
       The tree structure is easy to interpret helps in allowing users to understand the reasoning
behind each decision.
      </p>
     </dd>
     <dt class="description">
      <a id="x4-14006x2.1.1">
      </a>
      Handles Missing Data
     </dt>
     <dd class="description">
      <p class="noindent">
       It can handle missing values by using strategies like assigning the most common value or
ignoring missing data during splits.
      </p>
     </dd>
    </dl>
    <p class="noindent">
     Of course, as with every method, there are it’s disadvantages which are:
    </p>
    <dl class="description">
     <dt class="description">
      <a id="x4-14007x2.1.1">
      </a>
      Over-fitting
     </dt>
     <dd class="description">
      <p class="noindent">
       They can over-fit the training data if they are too deep which means they memorise the
data instead of learning general patterns. This leads to poor performance on unseen data.
      </p>
     </dd>
     <dt class="description">
      <a id="x4-14008x2.1.1">
      </a>
      Instability
     </dt>
     <dd class="description">
      <p class="noindent">
       It can be unstable which means that small changes in the data may lead to significant
differences in the tree structure and predictions.
      </p>
     </dd>
     <dt class="description">
      <a id="x4-14009x2.1.1">
      </a>
      Bias towards Features with Many Categories
     </dt>
     <dd class="description">
      <p class="noindent">
       It can become biased toward features with many distinct values which focuses too much
on them and potentially missing other important features which can reduce prediction
accuracy.
      </p>
     </dd>
     <dt class="description">
      <a id="x4-14010x2.1.1">
      </a>
      Difficulty in Capturing Complex Interactions
     </dt>
     <dd class="description">
      <p class="noindent">
       <a class="glossary" href="DataScienceIILectureBookli2.html#glo:dt">
        DT
       </a>
       s may struggle to capture complex interactions between features which helps in making
them less effective for certain types of data.
      </p>
     </dd>
     <dt class="description">
      <a id="x4-14011x2.1.1">
      </a>
      Computationally Expensive for Large Datasets
     </dt>
     <dd class="description">
      <p class="noindent">
       For large datasets, building and pruning a
       <a class="glossary" href="DataScienceIILectureBookli2.html#glo:dt">
        DT
       </a>
       can be computationally intensive, especially
as the tree depth increases.
      </p>
     </dd>
    </dl>
    <div class="informationblock" id="tcolobox-8">
     <div class="title">
      <p class="noindent">
       <span id="bold" style="font-weight:bold;">
        Information
       </span>
       : White v. Black Box
      </p>
     </div>
     <div class="box-content">
      <p class="noindent">
       <a class="glossary" href="DataScienceIILectureBookli2.html#glo:dt">
        DT
       </a>
       s are intuitive, and their decisions are easy to interpret. Such models are often called
       <span id="bold" style="font-weight:bold;">
        white box
       </span>
       models.
In contrast, random forests and neural networks are generally considered
       <span id="bold" style="font-weight:bold;">
        black box
       </span>
       models. They make great
predictions, and we can easily check the calculations that they performed to make these predictions, however, it
is usually hard to explain in simple terms why the predictions were made.
      </p>
      <p class="noindent">
       For example, if a neural network says that a particular person appears in a picture, it is hard to know what
contributed to this prediction: Did the model recognise that person’s eyes? Their mouth? Their nose? Their
shoes? Or even the couch that they were sitting on? Conversely,
       <a class="glossary" href="DataScienceIILectureBookli2.html#glo:dt">
        DT
       </a>
       s provide nice, simple classification rules
that can even be applied manually if need be (e.g., for flower classification). The field of interpretable
       <a class="glossary" href="DataScienceIILectureBookli2.html#glo:ml">
        ML
       </a>
       aims
at creating
       <a class="glossary" href="DataScienceIILectureBookli2.html#glo:ml">
        ML
       </a>
       systems that can explain their decisions in a way humans can understand. This is important in
many domains-for example, to ensure the system does not make unfair decisions.
      </p>
     </div>
    </div>
    <p class="noindent">
    </p>
    <h2 class="sectionHead">
     <a aria-label="Link to this section in PDF version" class="pdf-link" href="DataScienceIILectureBook.pdf#section.2.2" style="color: red;  position: absolute;  left: -30px;" target="_blank">
      <svg class="bi bi-file-pdf" fill="currentColor" height="20" viewbox="0 0 16 16" width="20" xmlns="http://www.w3.org/2000/svg">
       <path d="M4 0a2 2 0 0 0-2 2v12a2 2 0 0 0 2 2h8a2 2 0 0 0 2-2V2a2 2 0 0 0-2-2H4zm0 1h8a1 1 0 0 1 1 1v12a1 1 0 0 1-1 1H4a1 1 0 0 1-1-1V2a1 1 0 0 1 1-1z">
       </path>
       <path d="M4.603 12.087a.81.81 0 0 1-.438-.42c-.195-.388-.13-.776.08-1.102.198-.307.526-.568.897-.787a7.68 7.68 0 0 1 1.482-.645 19.701 19.701 0 0 0 1.062-2.227 7.269 7.269 0 0 1-.43-1.295c-.086-.4-.119-.796-.046-1.136.075-.354.274-.672.65-.823.192-.077.4-.12.602-.077a.7.7 0 0 1 .477.365c.088.164.12.356.127.538.007.187-.012.395-.047.614-.084.51-.27 1.134-.52 1.794a10.954 10.954 0 0 0 .98 1.686 5.753 5.753 0 0 1 1.334.05c.364.065.734.195.96.465.12.144.193.32.2.518.007.192-.047.382-.138.563a1.04 1.04 0 0 1-.354.416.856.856 0 0 1-.51.138c-.331-.014-.654-.196-.933-.417a5.716 5.716 0 0 1-.911-.95 11.642 11.642 0 0 0-1.997.406 11.311 11.311 0 0 1-1.021 1.51c-.29.35-.608.655-.926.787a.793.793 0 0 1-.58.029zm1.379-1.901c-.166.076-.32.156-.459.238-.328.194-.541.383-.647.547-.094.145-.096.25-.04.361.01.022.02.036.026.044a.27.27 0 0 0 .035-.012c.137-.056.355-.235.635-.572a8.18 8.18 0 0 0 .45-.606zm1.64-1.33a12.647 12.647 0 0 1 1.01-.193 11.666 11.666 0 0 1-.51-.858 20.741 20.741 0 0 1-.5 1.05zm2.446.45c.15.162.296.3.435.41.24.19.407.253.498.256a.107.107 0 0 0 .07-.015.307.307 0 0 0 .094-.125.436.436 0 0 0 .059-.2.095.095 0 0 0-.026-.063c-.052-.062-.2-.152-.518-.209a3.881 3.881 0 0 0-.612-.053zM8.078 5.8a6.7 6.7 0 0 0 .2-.828c.031-.188.043-.343.038-.465a.613.613 0 0 0-.032-.198.517.517 0 0 0-.145.04c-.087.035-.158.106-.196.283-.04.192-.03.469.046.822.024.111.054.227.09.346z">
       </path>
      </svg>
     </a>
     <small class="titlemark">
      2.2
     </small>
     <a id="x4-150002.2">
     </a>
     Training and Visualising Decision Trees
    </h2>
    <p class="noindent">
     To understand
     <a class="glossary" href="DataScienceIILectureBookli2.html#glo:dt">
      DT
     </a>
     s, let’s build one and take a look at how it makes predictions. The following code
trains a <span style="color:#054C5C;"><code class="verb">DecisionTreeClassifier</code></span>
     on the iris dataset:
    </p>
    <p class="noindent">
    </p>
    <pre><div id="fancyvrb11" style="padding:20px;border-radius: 3px;"><a id="x4-15002r23"></a><span style="color:#2B2BFF;">from</span><span style="color:#BABABA;"> </span>sklearn.datasets<span style="color:#BABABA;"> </span><span style="color:#2B2BFF;">import</span> load_iris 
<a id="x4-15004r24"></a><span style="color:#2B2BFF;">from</span><span style="color:#BABABA;"> </span>sklearn.tree<span style="color:#BABABA;"> </span><span style="color:#2B2BFF;">import</span> DecisionTreeClassifier 
<a id="x4-15006r25"></a>iris = load_iris(as_frame=<span style="color:#2B2BFF;"><span id="bold" style="font-weight:bold;">True</span></span>) 
<a id="x4-15008r26"></a>X_iris = iris.data[[<span style="color:#800080;">"petal length (cm)"</span>, <span style="color:#800080;">"petal width (cm)"</span>]].values 
<a id="x4-15010r27"></a>y_iris = iris.target 
<a id="x4-15012r28"></a>tree_clf = DecisionTreeClassifier(max_depth=<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">2</span></span>, random_state=<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">42</span></span>) 
<a id="x4-15014r29"></a>tree_clf.fit(X_iris, y_iris)</div></pre>
    <p class="noindent">
     We can visualise the trained
     <a class="glossary" href="DataScienceIILectureBookli2.html#glo:dt">
      DT
     </a>
     by first using the <span style="color:#054C5C;"><code class="verb">export_graphviz()</code></span>
     function to output a graph
definition file called <span style="color:#054C5C;"><code class="verb">iris_tree.dot</code></span>
     :
    </p>
    <p class="noindent">
    </p>
    <pre><div id="fancyvrb12" style="padding:20px;border-radius: 3px;"><a id="x4-15016r36"></a><span style="color:#2B2BFF;">from</span><span style="color:#BABABA;"> </span>sklearn.tree<span style="color:#BABABA;"> </span><span style="color:#2B2BFF;">import</span> export_graphviz 
<a id="x4-15018r37"></a>export_graphviz( 
<a id="x4-15020r38"></a>tree_clf, 
<a id="x4-15022r39"></a>out_file=<span style="color:#800080;">"iris_tree.dot"</span>, 
<a id="x4-15024r40"></a>feature_names=[<span style="color:#800080;">"petal length (cm)"</span>, <span style="color:#800080;">"petal width (cm)"</span>], 
<a id="x4-15026r41"></a>class_names=iris.target_names, 
<a id="x4-15028r42"></a>rounded=<span style="color:#2B2BFF;"><span id="bold" style="font-weight:bold;">True</span></span>, 
<a id="x4-15030r43"></a>filled=<span style="color:#2B2BFF;"><span id="bold" style="font-weight:bold;">True</span></span> 
<a id="x4-15032r44"></a>)</div></pre>
    <div class="knowledge">
     <p class="noindent">
      If we are using a Jupyter Notebook to study, we can use <span style="color:#054C5C;"><code class="verb">graphviz.Source.from_file()</code></span>
      to load and
display the file inline such as given below:
     </p>
    </div>
    <p class="noindent">
    </p>
    <pre><div id="fancyvrb13" style="padding:20px;border-radius: 3px;"><a id="x4-15034r51"></a><span style="color:#2B2BFF;">from</span><span style="color:#BABABA;"> </span>graphviz<span style="color:#BABABA;"> </span><span style="color:#2B2BFF;">import</span> Source 
<a id="x4-15036r52"></a>Source.from_file(<span style="color:#800080;">"iris_tree.dot"</span>)</div></pre>
    <div class="informationblock" id="tcolobox-9">
     <div class="title">
      <p class="noindent">
       <span id="bold" style="font-weight:bold;">
        Information
       </span>
       : Graphviz &amp; DOT
      </p>
     </div>
     <div class="box-content">
      <p class="noindent">
       Graphviz (short for Graph Visualisation Software) is a package of open-source tools for creating graphs. It takes
text input in DOT format, generates images.
      </p>
      <p class="noindent">
       DOT is a graph description language. DOT files are usually with <span style="color:#054C5C;"><code class="verb">.gv</code></span>
       filename extension.
      </p>
     </div>
    </div>
    <p class="noindent">
    </p>
    <h2 class="sectionHead">
     <a aria-label="Link to this section in PDF version" class="pdf-link" href="DataScienceIILectureBook.pdf#section.2.3" style="color: red;  position: absolute;  left: -30px;" target="_blank">
      <svg class="bi bi-file-pdf" fill="currentColor" height="20" viewbox="0 0 16 16" width="20" xmlns="http://www.w3.org/2000/svg">
       <path d="M4 0a2 2 0 0 0-2 2v12a2 2 0 0 0 2 2h8a2 2 0 0 0 2-2V2a2 2 0 0 0-2-2H4zm0 1h8a1 1 0 0 1 1 1v12a1 1 0 0 1-1 1H4a1 1 0 0 1-1-1V2a1 1 0 0 1 1-1z">
       </path>
       <path d="M4.603 12.087a.81.81 0 0 1-.438-.42c-.195-.388-.13-.776.08-1.102.198-.307.526-.568.897-.787a7.68 7.68 0 0 1 1.482-.645 19.701 19.701 0 0 0 1.062-2.227 7.269 7.269 0 0 1-.43-1.295c-.086-.4-.119-.796-.046-1.136.075-.354.274-.672.65-.823.192-.077.4-.12.602-.077a.7.7 0 0 1 .477.365c.088.164.12.356.127.538.007.187-.012.395-.047.614-.084.51-.27 1.134-.52 1.794a10.954 10.954 0 0 0 .98 1.686 5.753 5.753 0 0 1 1.334.05c.364.065.734.195.96.465.12.144.193.32.2.518.007.192-.047.382-.138.563a1.04 1.04 0 0 1-.354.416.856.856 0 0 1-.51.138c-.331-.014-.654-.196-.933-.417a5.716 5.716 0 0 1-.911-.95 11.642 11.642 0 0 0-1.997.406 11.311 11.311 0 0 1-1.021 1.51c-.29.35-.608.655-.926.787a.793.793 0 0 1-.58.029zm1.379-1.901c-.166.076-.32.156-.459.238-.328.194-.541.383-.647.547-.094.145-.096.25-.04.361.01.022.02.036.026.044a.27.27 0 0 0 .035-.012c.137-.056.355-.235.635-.572a8.18 8.18 0 0 0 .45-.606zm1.64-1.33a12.647 12.647 0 0 1 1.01-.193 11.666 11.666 0 0 1-.51-.858 20.741 20.741 0 0 1-.5 1.05zm2.446.45c.15.162.296.3.435.41.24.19.407.253.498.256a.107.107 0 0 0 .07-.015.307.307 0 0 0 .094-.125.436.436 0 0 0 .059-.2.095.095 0 0 0-.026-.063c-.052-.062-.2-.152-.518-.209a3.881 3.881 0 0 0-.612-.053zM8.078 5.8a6.7 6.7 0 0 0 .2-.828c.031-.188.043-.343.038-.465a.613.613 0 0 0-.032-.198.517.517 0 0 0-.145.04c-.087.035-.158.106-.196.283-.04.192-.03.469.046.822.024.111.054.227.09.346z">
       </path>
      </svg>
     </a>
     <small class="titlemark">
      2.3
     </small>
     <a id="x4-160002.3">
     </a>
     Making Predictions
    </h2>
    <aside class="wrapfig-r">
     <img alt="PIC" height="" src="codes/images/Decision-Trees/iris_tree-.svg" width="100%"/>
     <a id="x4-16001r1">
     </a>
     <figcaption class="caption">
      <span class="id">
       Figure 2.1:
      </span>
      <span class="content">
       The iris decision tree.
      </span>
     </figcaption>
    </aside>
    <p class="noindent">
     Let’s see how the tree represented in
     <span id="bold" style="font-weight:bold;">
      Fig.
     </span>
     <a href="#x4-16001r1">
      2.1
     </a>
     makes predictions.
    </p>
    <p class="noindent">
     Suppose we find an iris flower and want to classify it based on its
     <alert style="color: #821131;">
      petals
     </alert>
     . Looking at our
three in
     <span id="bold" style="font-weight:bold;">
      Fig.
     </span>
     <a href="#x4-16001r1">
      2.1
     </a>, we start at the
     <span id="bold" style="font-weight:bold;">
      root node
     </span>
     , which is depth 0, at the top. This node asks
whether the flower’s petal length is smaller than 2,45 cm. If it is, then we move down to the
root’s
     <span id="bold" style="font-weight:bold;">
      left child node
     </span>
     , which is in this case it is depth 1, left, which is a
     <span id="bold" style="font-weight:bold;">
      leaf node
     </span>
     , as
in it does not have any child nodes, so it does not ask any questions as it simply look at
the predicted class for that node, and the
     <a class="glossary" href="DataScienceIILectureBookli2.html#glo:dt">
      DT
     </a>
     predicts that our flower is an
     <italic>
      Iris setosa
     </italic> <span style="color:#054C5C;">(<code class="verb">class=setosa</code>)</span>.
    </p>
    <p class="noindent">
     Now
suppose
we
find
another
flower,
and
this
time
the
petal
length
is
greater
than
2,45 
cm
.
We
again
start
at
the
root
but
now
move
down
to
its
right
child
node
     <alert style="color: #821131;">
      <span id="bold" style="font-weight:bold;">
       <sup class="textsuperscript">
        1
       </sup>
      </span>
     </alert>
     <span class="marginnote">
      <span style="color:#999999;">
      </span>
      <alert style="color: #821131;">
       <span id="bold" style="font-weight:bold;">
        <sup class="textsuperscript">
         1
        </sup>
       </span>
      </alert>
      <span style="color:#0063B2;">
       which
is
depth
1,
right
      </span>
     </span>
     .
This
is
     <span id="bold" style="font-weight:bold;">
      NOT
     </span>
     a
leaf
node,
it’s
a
     <span id="bold" style="font-weight:bold;">
      split
node
     </span>
     ,
                                                                                
                                                                                
so
it
asks
another
question:
    </p>
    
    <p class="noindent">
    </p>
    <div class="quoteblock">
     <p class="noindent">
      is
the
petal
width
smaller
than
1,75
cm
?
     </p>
    </div>
    <p class="noindent">
     If it is, then our flower is most likely an
     <italic>
      Iris versicolor
     </italic>
     . If not, it is likely an
     <italic>
      Iris virginica
     </italic>
     .
    </p>
    <div class="knowledge">
     <p class="noindent">
      One of the many qualities of
      <a class="glossary" href="DataScienceIILectureBookli2.html#glo:dt">
       DT
      </a>
      s is that they require very little data preparation. In fact, they don’t require feature scaling or
centring at all.
     </p>
    </div>
    <p class="noindent">
     A node’s samples attribute counts how many training instances it applies to.
    </p>
    <p class="noindent">
    </p>
    <div class="quoteblock">
     <p class="noindent">
      For
example,
100
training
instances
have
a
petal
length
greater
than
2,45
cm
(depth
1,
right),
and
of
those
100,
54
have
a
petal
width
smaller
than
1,75
cm
(depth
2,
left).
     </p>
    </div>
    <p class="noindent">
     A node’s value attribute tells us how many training instances of each class this node applies to.
    </p>
    <p class="noindent">
     For example, the bottom-right node applies to 0 Iris setosa, 1 Iris versicolor, and 45 Iris virginica.
    </p>
    <p class="noindent">
     Finally,
a
node’s
                                                                                
                                                                                
gini
attribute
measures
its
     <span id="bold" style="font-weight:bold;">
      Gini
impurity
     </span>
     ,
named
after
Corrado
Gini
     <alert style="color: #821131;">
      <span id="bold" style="font-weight:bold;">
       <sup class="textsuperscript">
        2
       </sup>
      </span>
     </alert>
     <span class="marginnote">
      <span style="color:#999999;">
      </span>
      <img alt="PIC" height="" src="figures/Decision-Trees/raster/portrait-gini.jpg" width="100%"/>
      <alert style="color: #821131;">
       <span id="bold" style="font-weight:bold;">
        <sup class="textsuperscript">
         2
        </sup>
       </span>
      </alert>
      <span style="color:#0063B2;">
       <span id="bold" style="font-weight:bold;">
        Corrado
Gini
       </span>
       (
       <italic>
        1884
-
1965
       </italic>
       )
An
Italian
statistician,
demographer
and
sociologist
who
developed
the
Gini
coefficient,
a
measure
of
the
income
inequality
in
a
society.
Gini
was
a
proponent
of
organicism
and
applied
it
to
nations.
      </span>
     </span>
     which
we
will
have
a
look
at
now.
    </p>
    
    <h3 class="subsectionHead">
     <a aria-label="Link to this section in PDF version" class="pdf-link" href="DataScienceIILectureBook.pdf#subsection.2.3.1" style="color: red;  position: absolute;  left: -30px;" target="_blank">
      <svg class="bi bi-file-pdf" fill="currentColor" height="20" viewbox="0 0 16 16" width="20" xmlns="http://www.w3.org/2000/svg">
       <path d="M4 0a2 2 0 0 0-2 2v12a2 2 0 0 0 2 2h8a2 2 0 0 0 2-2V2a2 2 0 0 0-2-2H4zm0 1h8a1 1 0 0 1 1 1v12a1 1 0 0 1-1 1H4a1 1 0 0 1-1-1V2a1 1 0 0 1 1-1z">
       </path>
       <path d="M4.603 12.087a.81.81 0 0 1-.438-.42c-.195-.388-.13-.776.08-1.102.198-.307.526-.568.897-.787a7.68 7.68 0 0 1 1.482-.645 19.701 19.701 0 0 0 1.062-2.227 7.269 7.269 0 0 1-.43-1.295c-.086-.4-.119-.796-.046-1.136.075-.354.274-.672.65-.823.192-.077.4-.12.602-.077a.7.7 0 0 1 .477.365c.088.164.12.356.127.538.007.187-.012.395-.047.614-.084.51-.27 1.134-.52 1.794a10.954 10.954 0 0 0 .98 1.686 5.753 5.753 0 0 1 1.334.05c.364.065.734.195.96.465.12.144.193.32.2.518.007.192-.047.382-.138.563a1.04 1.04 0 0 1-.354.416.856.856 0 0 1-.51.138c-.331-.014-.654-.196-.933-.417a5.716 5.716 0 0 1-.911-.95 11.642 11.642 0 0 0-1.997.406 11.311 11.311 0 0 1-1.021 1.51c-.29.35-.608.655-.926.787a.793.793 0 0 1-.58.029zm1.379-1.901c-.166.076-.32.156-.459.238-.328.194-.541.383-.647.547-.094.145-.096.25-.04.361.01.022.02.036.026.044a.27.27 0 0 0 .035-.012c.137-.056.355-.235.635-.572a8.18 8.18 0 0 0 .45-.606zm1.64-1.33a12.647 12.647 0 0 1 1.01-.193 11.666 11.666 0 0 1-.51-.858 20.741 20.741 0 0 1-.5 1.05zm2.446.45c.15.162.296.3.435.41.24.19.407.253.498.256a.107.107 0 0 0 .07-.015.307.307 0 0 0 .094-.125.436.436 0 0 0 .059-.2.095.095 0 0 0-.026-.063c-.052-.062-.2-.152-.518-.209a3.881 3.881 0 0 0-.612-.053zM8.078 5.8a6.7 6.7 0 0 0 .2-.828c.031-.188.043-.343.038-.465a.613.613 0 0 0-.032-.198.517.517 0 0 0-.145.04c-.087.035-.158.106-.196.283-.04.192-.03.469.046.822.024.111.054.227.09.346z">
       </path>
      </svg>
     </a>
     <small class="titlemark">
      2.3.1
     </small>
     <a id="x4-170002.3.1">
     </a>
     Gini Impurity
    </h3>
    <p class="noindent">
     Gini Impurity is a measurement used to build
     <a class="glossary" href="DataScienceIILectureBookli2.html#glo:dt">
      DT
     </a>
     s to determine how the features of a dataset should split nodes to form the tree.
More precisely, the Gini Impurity of a dataset is a number between 0 - 0.5, which indicates the likelihood of new,
random data being misclassified if it were given a random class label according to the class distribution in the
                                                                                
                                                                                
dataset.
    </p>
    <div class="figure">
     <p class="noindent">
      <img alt="PIC" height="" src="figures/Decision-Trees/raster/gini-impurity-diagram.png" width="150%"/>
      <a id="x4-17001r2">
      </a>
     </p>
     <figcaption class="caption">
      <span class="id">
       Figure 2.2:
      </span>
      <span class="content">
       A visual description of Gini impurity.
      </span>
     </figcaption>
    </div>
    <p class="noindent">
     For example, say we want to build a classifier which determines if someone will default on their credit card. We
have some labelled data with features, such as bins for age, income, credit rating, and whether or not each
person is a student. For us to find the best feature for the first split of the tree
     <alert style="color: #821131;">
      <span id="bold" style="font-weight:bold;">
       <sup class="textsuperscript">
        3
       </sup>
      </span>
     </alert>
     <span class="marginnote">
      <span style="color:#999999;">
      </span>
      <alert style="color: #821131;">
       <span id="bold" style="font-weight:bold;">
        <sup class="textsuperscript">
         3
        </sup>
       </span>
      </alert>
      <span style="color:#0063B2;">
       i.e., the root node
      </span>
     </span>
     we could calculate how poorly each feature divided the data into the correct class:
    </p>
    
    <ul class="itemize1">
     <li class="itemize">
      <p class="noindent">
       default
("yes"),
or
      </p>
     </li>
     <li class="itemize">
      <p class="noindent">
       didn’t
default
("no").
      </p>
     </li>
    </ul>
    <p class="noindent">
     This calculation would measure the impurity of the split, and the feature with the lowest impurity would determine the best
feature for splitting the current node. This process would continue for each subsequent node using the remaining
features.
    </p>
    <p class="noindent">
     In
     <span id="bold" style="font-weight:bold;">
      Fig.
     </span>
     <a href="#x4-17001r2">
      2.2
     </a>, age has minimum Gini impurity, so age is selected as the root in the decision tree.
    </p>
    <div class="warning">
     <p class="noindent">
      A node is said to be
      <span id="bold" style="font-weight:bold;">
       pure
      </span> <span style="color:#054C5C;">(<code class="verb">gini=0</code>)</span> if all training instances belong to the same class.
     </p>
    </div>
    <p class="noindent">
     Going back to our
     <italic>
      iris
     </italic>
     flower example, since the depth-1 left node applies only to
     <italic>
      Iris setosa
     </italic>
     training instances,
it is
     <span id="bold" style="font-weight:bold;">
      pure
     </span>
     and its Gini impurity is 0. Eq. (
     <a href="#x4-17002r1">
      2.1
     </a>
     ) shows how the training algorithm computes the Gini impurity
     <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
      <msubsup>
       <mrow>
        <mi>
         G
        </mi>
       </mrow>
       <mrow>
        <mi>
         i
        </mi>
       </mrow>
       <mrow>
       </mrow>
      </msubsup>
     </math>
     of the
     <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
      <msubsup>
       <mrow>
        <mi>
         i
        </mi>
       </mrow>
       <mrow>
       </mrow>
       <mrow>
        <mi>
         t
        </mi>
        <mi>
         h
        </mi>
       </mrow>
      </msubsup>
     </math>
     node.
The depth-2 left node has a Gini impurity of
    </p>
    <p class="noindent">
     Mathematically we can define the Gini impurity as:
    </p>
    <table class="equation">
     <tr>
      <td>
       <math class="equation" display="block" xmlns="http://www.w3.org/1998/Math/MathML">
        <mstyle class="label" id="x4-17002r1">
        </mstyle>
        <msubsup>
         <mrow>
          <mi>
           G
          </mi>
         </mrow>
         <mrow>
          <mi>
           i
          </mi>
         </mrow>
         <mrow>
         </mrow>
        </msubsup>
        <mo class="MathClass-rel" stretchy="false">
         =
        </mo>
        <mn>
         1
        </mn>
        <mo class="MathClass-bin" stretchy="false">
         −
        </mo>
        <munderover accent="false" accentunder="false">
         <mrow>
          <mo>
           ∑
          </mo>
         </mrow>
         <mrow>
          <mi>
           k
          </mi>
          <mo class="MathClass-rel" stretchy="false">
           =
          </mo>
          <mn>
           1
          </mn>
         </mrow>
         <mrow>
          <mi>
           n
          </mi>
         </mrow>
        </munderover>
        <msubsup>
         <mrow>
          <mi>
           p
          </mi>
         </mrow>
         <mrow>
          <mi>
           i
          </mi>
          <mo class="MathClass-punc" stretchy="false">
           ,
          </mo>
          <mi>
           k
          </mi>
         </mrow>
         <mrow>
          <mn>
           2
          </mn>
         </mrow>
        </msubsup>
       </math>
      </td>
      <td class="eq-no">
       (2.1)
      </td>
     </tr>
    </table>
    <p class="noindent">
     where
     <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
      <msubsup>
       <mrow>
        <mi>
         G
        </mi>
       </mrow>
       <mrow>
        <mi>
         i
        </mi>
       </mrow>
       <mrow>
       </mrow>
      </msubsup>
     </math>
     is the Gini
impurity of the
     <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
      <msubsup>
       <mrow>
        <mi>
         i
        </mi>
       </mrow>
       <mrow>
       </mrow>
       <mrow>
        <mi>
         t
        </mi>
        <mi>
         h
        </mi>
       </mrow>
      </msubsup>
     </math>
     node,
     <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
      <msubsup>
       <mrow>
        <mi>
         p
        </mi>
       </mrow>
       <mrow>
        <mi>
         i
        </mi>
        <mo class="MathClass-punc" stretchy="false">
         ,
        </mo>
        <mi>
         k
        </mi>
       </mrow>
       <mrow>
       </mrow>
      </msubsup>
     </math>
     is the ratio of class
     <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
      <mi>
       k
      </mi>
     </math>
     instances among the
training instances in the
     <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
      <msubsup>
       <mrow>
        <mi>
         i
        </mi>
       </mrow>
       <mrow>
       </mrow>
       <mrow>
        <mi>
         t
        </mi>
        <mi>
         h
        </mi>
       </mrow>
      </msubsup>
     </math>
     node. Using this definition we can calculate the depth-2 left node as:
    </p>
    <table class="equation-star">
     <tr>
      <td>
       <math class="equation" display="block" xmlns="http://www.w3.org/1998/Math/MathML">
        <mn>
         1
        </mn>
        <mo class="MathClass-bin" stretchy="false">
         −
        </mo>
        <msup>
         <mrow>
          <mrow>
           <mo fence="true" form="prefix">
            (
           </mo>
           <mrow>
            <mfrac>
             <mrow>
              <mn>
               0
              </mn>
             </mrow>
             <mrow>
              <mn>
               5
              </mn>
              <mn>
               4
              </mn>
             </mrow>
            </mfrac>
           </mrow>
           <mo fence="true" form="postfix">
            )
           </mo>
          </mrow>
         </mrow>
         <mrow>
          <mn>
           2
          </mn>
         </mrow>
        </msup>
        <mo class="MathClass-bin" stretchy="false">
         −
        </mo>
        <msup>
         <mrow>
          <mrow>
           <mo fence="true" form="prefix">
            (
           </mo>
           <mrow>
            <mfrac>
             <mrow>
              <mn>
               4
              </mn>
              <mn>
               9
              </mn>
             </mrow>
             <mrow>
              <mn>
               5
              </mn>
              <mn>
               4
              </mn>
             </mrow>
            </mfrac>
           </mrow>
           <mo fence="true" form="postfix">
            )
           </mo>
          </mrow>
         </mrow>
         <mrow>
          <mn>
           2
          </mn>
         </mrow>
        </msup>
        <mo class="MathClass-bin" stretchy="false">
         −
        </mo>
        <msup>
         <mrow>
          <mrow>
           <mo fence="true" form="prefix">
            (
           </mo>
           <mrow>
            <msup>
             <mrow>
              <mrow>
               <mo fence="true" form="prefix">
                (
               </mo>
               <mrow>
                <mn>
                 5
                </mn>
               </mrow>
               <mo fence="true" form="postfix">
                )
               </mo>
              </mrow>
             </mrow>
             <mrow>
             </mrow>
            </msup>
            <mn>
             5
            </mn>
            <mn>
             4
            </mn>
           </mrow>
           <mo fence="true" form="postfix">
            )
           </mo>
          </mrow>
         </mrow>
         <mrow>
          <mn>
           2
          </mn>
         </mrow>
        </msup>
        <mo class="MathClass-rel" stretchy="false">
         ≈
        </mo>
        <mn>
         0
        </mn>
        <mo class="MathClass-punc" stretchy="false">
         .
        </mo>
        <mn>
         1
        </mn>
        <mn>
         6
        </mn>
        <mn>
         8
        </mn>
        <mo class="MathClass-punc" stretchy="false">
         .
        </mo>
       </math>
      </td>
     </tr>
    </table>
    <p class="noindent"> <span style="color:#054C5C;"><code class="verb">scikit-learn</code></span>
     uses
the
     <a class="glossary" href="DataScienceIILectureBookli2.html#glo:cart">
      CART
     </a>
     algorithm,
which
produces
only
     <span id="bold" style="font-weight:bold;">
      binary
trees
     </span>
     ,
meaning
trees
where
split
nodes
always
have
exactly
two
<alert style="color: #821131;">(2)</alert> children.
     <alert style="color: #821131;">
      <span id="bold" style="font-weight:bold;">
       <sup class="textsuperscript">
        4
       </sup>
      </span>
     </alert>
     <span class="marginnote">
      <span style="color:#999999;">
      </span>
      <alert style="color: #821131;">
       <span id="bold" style="font-weight:bold;">
        <sup class="textsuperscript">
         4
        </sup>
       </span>
      </alert>
      <span style="color:#0063B2;">
       i.e.,
questions
only
have
yes/no
answers.
      </span>
     </span>
    </p>
    
    <div class="knowledge">
     <p class="noindent">
      However, other algorithms, such as ID3,
      <alert style="color: #821131;">
       <span id="bold" style="font-weight:bold;">
        <sup class="textsuperscript">
         5
        </sup>
       </span>
      </alert>
      <span class="marginnote">
       <span style="color:#999999;">
       </span>
       <alert style="color: #821131;">
        <span id="bold" style="font-weight:bold;">
         <sup class="textsuperscript">
          5
         </sup>
        </span>
       </alert>
       <span style="color:#0063B2;">
        ID3 (Iterative Dichotomiser 3) is an algorithm invented by Ross Quinlan used to generate a decision tree from a dataset,
generally used in natural language processing.
       </span>
      </span>
      can produce
      <a class="glossary" href="DataScienceIILectureBookli2.html#glo:dt">
       DT
      </a>
      s with nodes that have more than two children.
     </p>
     
    </div>
    <p class="noindent">
     <span id="bold" style="font-weight:bold;">
      Fig.
     </span>
     <a href="#x4-17003r3">
      2.3
     </a>
     shows this
     <a class="glossary" href="DataScienceIILectureBookli2.html#glo:dt">
      DT
     </a>
     ’s decision boundaries. The thick vertical line represents the decision boundary of the root node (depth 0)
with petal length = 2,45cm. Given the left hand area is pure (only
     <italic>
      Iris setosa
     </italic>
     ), it cannot be split any further. However, the right
hand area is
     <span id="bold" style="font-weight:bold;">
      impure
     </span>
     , so the depth-1 right node splits it at petal width = 1,75cm, which is represented by the dashed
line.
    </p>
    <p class="noindent">
     Given <span style="color:#054C5C;"><code class="verb">max_depth</code></span>
     was set to 2, the
     <a class="glossary" href="DataScienceIILectureBookli2.html#glo:dt">
      DT
     </a>
     stops right there. If we set <span style="color:#054C5C;"><code class="verb">max_depth</code></span>
     to 3, then
the two depth-2 nodes would each add another decision boundary.
     <alert style="color: #821131;">
      <span id="bold" style="font-weight:bold;">
       <sup class="textsuperscript">
        6
       </sup>
      </span>
     </alert>
     <span class="marginnote">
      <span style="color:#999999;">
      </span>
      <alert style="color: #821131;">
       <span id="bold" style="font-weight:bold;">
        <sup class="textsuperscript">
         6
        </sup>
       </span>
      </alert>
      <span style="color:#0063B2;">
       Which is represented by the two vertical dotted lines.
      </span>
     </span>
    </p>
    
    <div class="figure">
     <p class="noindent">
      <img alt="PIC" height="" src="codes/images/Decision-Trees/decision_tree_decision_boundaries_plot-.svg" width="150%"/>
      <a id="x4-17003r3">
      </a>
     </p>
     <figcaption class="caption">
      <span class="id">
       Figure 2.3:
      </span>
      <span class="content">
       Decision tree decision boundaries.
      </span>
     </figcaption>
    </div>
    <div class="knowledge">
     <p class="noindent">
      The tree structure, including all the information shown in Figure
      <a href="#x4-16001r1">
       2.1
      </a>, is available via the classifier’s <span style="color:#054C5C;"><code class="verb">tree_</code></span>
      attribute. For more
information, type <span style="color:#054C5C;"><code class="verb">help(tree_clf.tree_)</code></span>.
     </p>
    </div>
    <h3 class="subsectionHead">
     <a aria-label="Link to this section in PDF version" class="pdf-link" href="DataScienceIILectureBook.pdf#subsection.2.3.2" style="color: red;  position: absolute;  left: -30px;" target="_blank">
      <svg class="bi bi-file-pdf" fill="currentColor" height="20" viewbox="0 0 16 16" width="20" xmlns="http://www.w3.org/2000/svg">
       <path d="M4 0a2 2 0 0 0-2 2v12a2 2 0 0 0 2 2h8a2 2 0 0 0 2-2V2a2 2 0 0 0-2-2H4zm0 1h8a1 1 0 0 1 1 1v12a1 1 0 0 1-1 1H4a1 1 0 0 1-1-1V2a1 1 0 0 1 1-1z">
       </path>
       <path d="M4.603 12.087a.81.81 0 0 1-.438-.42c-.195-.388-.13-.776.08-1.102.198-.307.526-.568.897-.787a7.68 7.68 0 0 1 1.482-.645 19.701 19.701 0 0 0 1.062-2.227 7.269 7.269 0 0 1-.43-1.295c-.086-.4-.119-.796-.046-1.136.075-.354.274-.672.65-.823.192-.077.4-.12.602-.077a.7.7 0 0 1 .477.365c.088.164.12.356.127.538.007.187-.012.395-.047.614-.084.51-.27 1.134-.52 1.794a10.954 10.954 0 0 0 .98 1.686 5.753 5.753 0 0 1 1.334.05c.364.065.734.195.96.465.12.144.193.32.2.518.007.192-.047.382-.138.563a1.04 1.04 0 0 1-.354.416.856.856 0 0 1-.51.138c-.331-.014-.654-.196-.933-.417a5.716 5.716 0 0 1-.911-.95 11.642 11.642 0 0 0-1.997.406 11.311 11.311 0 0 1-1.021 1.51c-.29.35-.608.655-.926.787a.793.793 0 0 1-.58.029zm1.379-1.901c-.166.076-.32.156-.459.238-.328.194-.541.383-.647.547-.094.145-.096.25-.04.361.01.022.02.036.026.044a.27.27 0 0 0 .035-.012c.137-.056.355-.235.635-.572a8.18 8.18 0 0 0 .45-.606zm1.64-1.33a12.647 12.647 0 0 1 1.01-.193 11.666 11.666 0 0 1-.51-.858 20.741 20.741 0 0 1-.5 1.05zm2.446.45c.15.162.296.3.435.41.24.19.407.253.498.256a.107.107 0 0 0 .07-.015.307.307 0 0 0 .094-.125.436.436 0 0 0 .059-.2.095.095 0 0 0-.026-.063c-.052-.062-.2-.152-.518-.209a3.881 3.881 0 0 0-.612-.053zM8.078 5.8a6.7 6.7 0 0 0 .2-.828c.031-.188.043-.343.038-.465a.613.613 0 0 0-.032-.198.517.517 0 0 0-.145.04c-.087.035-.158.106-.196.283-.04.192-.03.469.046.822.024.111.054.227.09.346z">
       </path>
      </svg>
     </a>
     <small class="titlemark">
      2.3.2
     </small>
     <a id="x4-180002.3.2">
     </a>
     Estimating Class Probabilities
    </h3>
    <p class="noindent">
     A
     <a class="glossary" href="DataScienceIILectureBookli2.html#glo:dt">
      DT
     </a>
     can also estimate the probability which an instance belongs to a particular class
     <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
      <mi>
       k
      </mi>
     </math>
     . First, it
traverses the tree to find the leaf node for this instance, and then it returns the ratio of training instances of class
     <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
      <mi>
       k
      </mi>
     </math>
     in this
node.
    </p>
    <p class="noindent">
     For example, suppose we have found a flower whose petals are 5cm long and 1,5cm wide. The corresponding leaf node is the
depth-2 left node, so the
     <a class="glossary" href="DataScienceIILectureBookli2.html#glo:dt">
      DT
     </a>
     outputs the following probabilities:
    </p>
    <ul class="itemize1">
     <li class="itemize">
      <p class="noindent">
       0%
for
Iris
setosa
(0/54),
      </p>
     </li>
     <li class="itemize">
      <p class="noindent">
       90.7%
for
Iris
versicolor
(49/54),
and
      </p>
     </li>
     <li class="itemize">
      <p class="noindent">
       9.3%
for
Iris
virginica
(5/54).
      </p>
     </li>
    </ul>
    <p class="noindent">
     If we ask it to predict the class, it outputs Iris versicolor (class 1) because it has
     <alert style="color: #821131;">
      the highest probability
     </alert>
     . Let’s check
this:
    </p>
    <p class="noindent">
    </p>
    <pre><div id="fancyvrb14" style="padding:20px;border-radius: 3px;"><a id="x4-18002r102"></a><span style="color:#2B2BFF;">print</span>(tree_clf.predict_proba([[<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">5</span></span>, <span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">1.5</span></span>]]).round(<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">3</span></span>)) 
<a id="x4-18004r103"></a><span style="color:#2B2BFF;">print</span>(tree_clf.predict([[<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">5</span></span>, <span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">1.5</span></span>]]))</div></pre>
    <div class="tcolorbox tcolorbox" id="tcolobox-10">
     <div class="tcolorbox-content">
      <pre><div id="fancyvrb15" style="padding:20px;border-radius: 3px;"><a id="x4-18006r1"></a>[[0.    0.907 0.093]] 
<a id="x4-18008r2"></a>[1]</div></pre>
     </div>
    </div>
    <p class="noindent">
     Notice the estimated probabilities would be identical anywhere else in the bottom-right rectangle of
     <span id="bold" style="font-weight:bold;">
      Fig.
     </span>
     <a href="#x4-16001r1">
      2.1
     </a>, for example, if the
petals were 6cm long and 1,5cm wide.
    </p>
    <p class="noindent">
    </p>
    <h2 class="sectionHead">
     <a aria-label="Link to this section in PDF version" class="pdf-link" href="DataScienceIILectureBook.pdf#section.2.4" style="color: red;  position: absolute;  left: -30px;" target="_blank">
      <svg class="bi bi-file-pdf" fill="currentColor" height="20" viewbox="0 0 16 16" width="20" xmlns="http://www.w3.org/2000/svg">
       <path d="M4 0a2 2 0 0 0-2 2v12a2 2 0 0 0 2 2h8a2 2 0 0 0 2-2V2a2 2 0 0 0-2-2H4zm0 1h8a1 1 0 0 1 1 1v12a1 1 0 0 1-1 1H4a1 1 0 0 1-1-1V2a1 1 0 0 1 1-1z">
       </path>
       <path d="M4.603 12.087a.81.81 0 0 1-.438-.42c-.195-.388-.13-.776.08-1.102.198-.307.526-.568.897-.787a7.68 7.68 0 0 1 1.482-.645 19.701 19.701 0 0 0 1.062-2.227 7.269 7.269 0 0 1-.43-1.295c-.086-.4-.119-.796-.046-1.136.075-.354.274-.672.65-.823.192-.077.4-.12.602-.077a.7.7 0 0 1 .477.365c.088.164.12.356.127.538.007.187-.012.395-.047.614-.084.51-.27 1.134-.52 1.794a10.954 10.954 0 0 0 .98 1.686 5.753 5.753 0 0 1 1.334.05c.364.065.734.195.96.465.12.144.193.32.2.518.007.192-.047.382-.138.563a1.04 1.04 0 0 1-.354.416.856.856 0 0 1-.51.138c-.331-.014-.654-.196-.933-.417a5.716 5.716 0 0 1-.911-.95 11.642 11.642 0 0 0-1.997.406 11.311 11.311 0 0 1-1.021 1.51c-.29.35-.608.655-.926.787a.793.793 0 0 1-.58.029zm1.379-1.901c-.166.076-.32.156-.459.238-.328.194-.541.383-.647.547-.094.145-.096.25-.04.361.01.022.02.036.026.044a.27.27 0 0 0 .035-.012c.137-.056.355-.235.635-.572a8.18 8.18 0 0 0 .45-.606zm1.64-1.33a12.647 12.647 0 0 1 1.01-.193 11.666 11.666 0 0 1-.51-.858 20.741 20.741 0 0 1-.5 1.05zm2.446.45c.15.162.296.3.435.41.24.19.407.253.498.256a.107.107 0 0 0 .07-.015.307.307 0 0 0 .094-.125.436.436 0 0 0 .059-.2.095.095 0 0 0-.026-.063c-.052-.062-.2-.152-.518-.209a3.881 3.881 0 0 0-.612-.053zM8.078 5.8a6.7 6.7 0 0 0 .2-.828c.031-.188.043-.343.038-.465a.613.613 0 0 0-.032-.198.517.517 0 0 0-.145.04c-.087.035-.158.106-.196.283-.04.192-.03.469.046.822.024.111.054.227.09.346z">
       </path>
      </svg>
     </a>
     <small class="titlemark">
      2.4
     </small>
     <a id="x4-190002.4">
     </a>
     The CART Training Algorithm
    </h2>
    <p class="noindent">
     <a class="glossary" href="DataScienceIILectureBookli2.html#glo:cart">
      CART
     </a>
     is a predictive algorithm used for explaining how the target variable’s values can be predicted based on other matters. It is
a
     <a class="glossary" href="DataScienceIILectureBookli2.html#glo:dt">
      DT
     </a>
     where each fork is split into a predictor variable and each node has a prediction for the target variable at the end. The
three <alert style="color: #821131;">(3)</alert> primary points of the algorithm is as follows:
    </p>
    <dl class="description">
     <dt class="description">
      <a id="x4-19001x2.4">
      </a>
      Tree structure
     </dt>
     <dd class="description">
      <p class="noindent">
       <a class="glossary" href="DataScienceIILectureBookli2.html#glo:cart">
        CART
       </a>
       builds
a
tree-like
structure
consisting
of
nodes
and
branches.
The
nodes
represent
different
decision
points,
and
the
branches
represent
the
possible
outcomes
of
those
decisions.
The
leaf
nodes
in
the
tree
contain
a
predicted
class
label
or
value
for
the
target
variable.
      </p>
     </dd>
     <dt class="description">
      <a id="x4-19002x2.4">
      </a>
      Splitting Criteria
     </dt>
     <dd class="description">
      <p class="noindent">
       <a class="glossary" href="DataScienceIILectureBookli2.html#glo:cart">
        CART
       </a>
       uses
a
                                                                                
                                                                                
     
greedy
approach
to
split
the
data
at
each
node.
It
evaluates
all
possible
splits
and
selects
the
one
that
best
reduces
the
impurity
of
the
resulting
subsets.
For
classification
tasks,
       <a class="glossary" href="DataScienceIILectureBookli2.html#glo:cart">
        CART
       </a>
       uses
Gini
impurity
as
the
splitting
criterion.
The
lower
the
Gini
impurity,
the
more
pure
the
subset
is.
For
regression
tasks,
       <a class="glossary" href="DataScienceIILectureBookli2.html#glo:cart">
        CART
       </a>
       uses
residual
reduction
as
the
splitting
criterion.
The
lower
the
residual
reduction,
the
better
                                                                                
                                                                                
     
the
fit
of
the
model
to
the
data.
      </p>
     </dd>
     <dt class="description">
      <a id="x4-19003x2.4">
      </a>
      Pruning
     </dt>
     <dd class="description">
      <p class="noindent">
       To
prevent
over-fitting
of
the
data,
pruning
is
a
technique
used
to
remove
the
nodes
that
contribute
little
to
the
model
accuracy.
Cost
complexity
pruning
and
information
gain
pruning
are
two
popular
pruning
techniques.
Cost
complexity
pruning
involves
calculating
the
cost
of
each
node
and
removing
nodes
that
have
a
negative
cost.
Information
gain
                                                                                
                                                                                
     
pruning
involves
calculating
the
information
gain
of
each
node
and
removing
nodes
that
have
a
low
information
gain.
      </p>
     </dd>
    </dl>
    <p class="noindent"> <span style="color:#054C5C;"><code class="verb">scikit-learn</code></span>
     uses
the
     <a class="glossary" href="DataScienceIILectureBookli2.html#glo:cart">
      CART
     </a>
     algorithm
to
train
     <a class="glossary" href="DataScienceIILectureBookli2.html#glo:dt">
      DT
     </a>
     s
(also
called
growing
trees).
The
algorithm
works
by
splitting
the
training
set
into
two
<alert style="color: #821131;">(2)</alert> subsets
using
a
single
feature
     <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
      <mi>
       k
      </mi>
     </math>
     and
a
threshold
     <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
      <msubsup>
       <mrow>
        <mi>
         t
        </mi>
       </mrow>
       <mrow>
        <mi>
         k
        </mi>
       </mrow>
       <mrow>
       </mrow>
      </msubsup>
     </math>
     .
     <alert style="color: #821131;">
      <span id="bold" style="font-weight:bold;">
       <sup class="textsuperscript">
        7
       </sup>
      </span>
     </alert>
     <span class="marginnote">
      <span style="color:#999999;">
      </span>
      <alert style="color: #821131;">
       <span id="bold" style="font-weight:bold;">
        <sup class="textsuperscript">
         7
        </sup>
       </span>
      </alert>
      <span style="color:#0063B2;">
       e.g.,
petal
length
       <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
        <mo class="MathClass-rel" stretchy="false">
         ≤
        </mo>
       </math>
       2,45
cm
.
      </span>
     </span>
    </p>
    
    <p class="noindent">
     How
does
it
choose
     <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
      <mi>
       k
      </mi>
     </math>
     and
     <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
      <msubsup>
       <mrow>
        <mi>
         t
        </mi>
       </mrow>
       <mrow>
        <mi>
         k
        </mi>
       </mrow>
       <mrow>
       </mrow>
      </msubsup>
     </math>
     ?
    </p>
    <p class="noindent">
     It searches for the pair
     <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
      <msup>
       <mrow>
        <mrow>
         <mo fence="true" form="prefix">
          (
         </mo>
         <mrow>
          <mi>
           k
          </mi>
          <mo class="MathClass-punc" stretchy="false">
           ,
          </mo>
          <mspace class="thinspace" width="0.17em">
          </mspace>
          <msubsup>
           <mrow>
            <mi>
             t
            </mi>
           </mrow>
           <mrow>
            <mi>
             k
            </mi>
           </mrow>
           <mrow>
           </mrow>
          </msubsup>
         </mrow>
         <mo fence="true" form="postfix">
          )
         </mo>
        </mrow>
       </mrow>
       <mrow>
       </mrow>
      </msup>
     </math>
     that produces the purest subsets, weighted by their size. Eq. (
     <a href="#x4-19004r2">
      2.2
     </a>
     ) gives the cost function that the algorithm tries to minimise.
    </p>
    <table class="equation">
     <tr>
      <td>
       <math class="equation" display="block" xmlns="http://www.w3.org/1998/Math/MathML">
        <mstyle class="label" id="x4-19004r2">
        </mstyle>
        <mi>
         J
        </mi>
        <msup>
         <mrow>
          <mrow>
           <mo fence="true" form="prefix">
            (
           </mo>
           <mrow>
            <mi>
             k
            </mi>
            <mo class="MathClass-punc" stretchy="false">
             ,
            </mo>
            <mspace class="thinspace" width="0.17em">
            </mspace>
            <msubsup>
             <mrow>
              <mi>
               t
              </mi>
             </mrow>
             <mrow>
              <mi>
               k
              </mi>
             </mrow>
             <mrow>
             </mrow>
            </msubsup>
           </mrow>
           <mo fence="true" form="postfix">
            )
           </mo>
          </mrow>
         </mrow>
         <mrow>
         </mrow>
        </msup>
        <mo class="MathClass-rel" stretchy="false">
         =
        </mo>
        <mfrac>
         <mrow>
          <msubsup>
           <mrow>
            <mi>
             m
            </mi>
           </mrow>
           <mrow>
            <mi>
             l
            </mi>
            <mi>
             e
            </mi>
            <mi>
             f
            </mi>
            <mi>
             t
            </mi>
           </mrow>
           <mrow>
           </mrow>
          </msubsup>
         </mrow>
         <mrow>
          <mi>
           m
          </mi>
         </mrow>
        </mfrac>
        <msubsup>
         <mrow>
          <mi>
           G
          </mi>
         </mrow>
         <mrow>
          <mi>
           l
          </mi>
          <mi>
           e
          </mi>
          <mi>
           f
          </mi>
          <mi>
           t
          </mi>
         </mrow>
         <mrow>
         </mrow>
        </msubsup>
        <mo class="MathClass-bin" stretchy="false">
         +
        </mo>
        <mfrac>
         <mrow>
          <msubsup>
           <mrow>
            <mi>
             m
            </mi>
           </mrow>
           <mrow>
            <mi>
             r
            </mi>
            <mi>
             i
            </mi>
            <mi>
             g
            </mi>
            <mi>
             h
            </mi>
            <mi>
             t
            </mi>
           </mrow>
           <mrow>
           </mrow>
          </msubsup>
         </mrow>
         <mrow>
          <mi>
           m
          </mi>
         </mrow>
        </mfrac>
        <msubsup>
         <mrow>
          <mi>
           G
          </mi>
         </mrow>
         <mrow>
          <mi>
           r
          </mi>
          <mi>
           i
          </mi>
          <mi>
           g
          </mi>
          <mi>
           h
          </mi>
          <mi>
           t
          </mi>
         </mrow>
         <mrow>
         </mrow>
        </msubsup>
        <mspace class="quad" width="1em">
        </mspace>
        <mstyle class="text">
         <mtext>
          where
         </mtext>
        </mstyle>
        <mspace class="quad" width="1em">
        </mspace>
        <mrow class="cases">
         <mrow>
          <mo fence="true" form="prefix">
           {
          </mo>
          <mrow>
           <mtable align="axis" class="array" columnlines="none" displaystyle="true" equalcolumns="false" equalrows="false" style="">
            <mtr class="array-row">
             <mtd class="array-td" columnalign="left">
              <msubsup>
               <mrow>
                <mi>
                 G
                </mi>
               </mrow>
               <mrow>
                <mi>
                 l
                </mi>
                <mi>
                 e
                </mi>
                <mi>
                 f
                </mi>
                <mi>
                 t
                </mi>
                <mo class="MathClass-bin" stretchy="false">
                 ∕
                </mo>
                <mi>
                 r
                </mi>
                <mi>
                 i
                </mi>
                <mi>
                 g
                </mi>
                <mi>
                 h
                </mi>
                <mi>
                 t
                </mi>
               </mrow>
               <mrow>
               </mrow>
              </msubsup>
              <mspace class="quad" width="1em">
              </mspace>
              <mstyle class="text">
               <mtext>
                measures the impurity
               </mtext>
              </mstyle>
              <mspace class="quad" width="1em">
              </mspace>
             </mtd>
            </mtr>
            <mtr class="array-row">
             <mtd class="array-td" columnalign="left">
              <msubsup>
               <mrow>
                <mi>
                 m
                </mi>
               </mrow>
               <mrow>
                <mi>
                 l
                </mi>
                <mi>
                 e
                </mi>
                <mi>
                 f
                </mi>
                <mi>
                 t
                </mi>
                <mo class="MathClass-bin" stretchy="false">
                 ∕
                </mo>
                <mi>
                 r
                </mi>
                <mi>
                 i
                </mi>
                <mi>
                 g
                </mi>
                <mi>
                 h
                </mi>
                <mi>
                 t
                </mi>
               </mrow>
               <mrow>
               </mrow>
              </msubsup>
              <mspace class="quad" width="1em">
              </mspace>
              <mstyle class="text">
               <mtext>
                number of instances
               </mtext>
              </mstyle>
              <mspace class="quad" width="1em">
              </mspace>
             </mtd>
            </mtr>
           </mtable>
          </mrow>
          <mo fence="true" form="postfix">
          </mo>
         </mrow>
        </mrow>
       </math>
      </td>
      <td class="eq-no">
       (2.2)
      </td>
     </tr>
    </table>
    <p class="noindent">
     Once
the
     <a class="glossary" href="DataScienceIILectureBookli2.html#glo:cart">
      CART
     </a>
     algorithm
successfully
splits
the
training
set
in
two,
it
splits
the
subsets
using
the
same
logic,
then
the
sub-subsets,
and
so
on,
recursively.
It
stops
its
recursive
behaviour
once
it
reaches
the
maximum
depth,
     <alert style="color: #821131;">
      <span id="bold" style="font-weight:bold;">
       <sup class="textsuperscript">
        8
       </sup>
      </span>
     </alert>
     <span class="marginnote">
      <span style="color:#999999;">
      </span>
      <alert style="color: #821131;">
       <span id="bold" style="font-weight:bold;">
        <sup class="textsuperscript">
         8
        </sup>
       </span>
      </alert>
      <span style="color:#0063B2;">
       defined
by
the <span style="color:#054C5C;"><code class="verb">max_depth</code></span>
       hyperparameter.
      </span>
     </span>
     or
if
it
cannot
find
a
split
that
will
reduce
impurity.
    </p>
    
    <p class="noindent">
     A
                                                                                
                                                                                
few
other
hyperparameters
control
additional
stopping
conditions:
    </p>
    <p class="noindent">
    </p>
    <div class="quoteblock">
     <p class="noindent"> <span style="color:#054C5C;"><code class="verb">min_samples_split</code></span>, <span style="color:#054C5C;"><code class="verb">min_samples_leaf</code></span>, <span style="color:#054C5C;"><code class="verb">min_weight_fraction_leaf</code></span>, <span style="color:#054C5C;"><code class="verb">max_leaf_nodes</code></span>.
     </p>
    </div>
    <div class="warning">
     <p class="noindent">
      <a class="glossary" href="DataScienceIILectureBookli2.html#glo:cart">
       CART
      </a>
      algorithm is a
      <span id="bold" style="font-weight:bold;">
       greedy algorithm
      </span>
      . This means it greedily searches for an optimum split at the top level, then repeats the
process at each subsequent level. It does not whether the split will lead to the lowest possible impurity several levels
down. A greedy algorithm often produces a solution that’s reasonably good but not guaranteed to be optimal.
     </p>
    </div>
    <h3 class="subsectionHead">
     <a aria-label="Link to this section in PDF version" class="pdf-link" href="DataScienceIILectureBook.pdf#subsection.2.4.1" style="color: red;  position: absolute;  left: -30px;" target="_blank">
      <svg class="bi bi-file-pdf" fill="currentColor" height="20" viewbox="0 0 16 16" width="20" xmlns="http://www.w3.org/2000/svg">
       <path d="M4 0a2 2 0 0 0-2 2v12a2 2 0 0 0 2 2h8a2 2 0 0 0 2-2V2a2 2 0 0 0-2-2H4zm0 1h8a1 1 0 0 1 1 1v12a1 1 0 0 1-1 1H4a1 1 0 0 1-1-1V2a1 1 0 0 1 1-1z">
       </path>
       <path d="M4.603 12.087a.81.81 0 0 1-.438-.42c-.195-.388-.13-.776.08-1.102.198-.307.526-.568.897-.787a7.68 7.68 0 0 1 1.482-.645 19.701 19.701 0 0 0 1.062-2.227 7.269 7.269 0 0 1-.43-1.295c-.086-.4-.119-.796-.046-1.136.075-.354.274-.672.65-.823.192-.077.4-.12.602-.077a.7.7 0 0 1 .477.365c.088.164.12.356.127.538.007.187-.012.395-.047.614-.084.51-.27 1.134-.52 1.794a10.954 10.954 0 0 0 .98 1.686 5.753 5.753 0 0 1 1.334.05c.364.065.734.195.96.465.12.144.193.32.2.518.007.192-.047.382-.138.563a1.04 1.04 0 0 1-.354.416.856.856 0 0 1-.51.138c-.331-.014-.654-.196-.933-.417a5.716 5.716 0 0 1-.911-.95 11.642 11.642 0 0 0-1.997.406 11.311 11.311 0 0 1-1.021 1.51c-.29.35-.608.655-.926.787a.793.793 0 0 1-.58.029zm1.379-1.901c-.166.076-.32.156-.459.238-.328.194-.541.383-.647.547-.094.145-.096.25-.04.361.01.022.02.036.026.044a.27.27 0 0 0 .035-.012c.137-.056.355-.235.635-.572a8.18 8.18 0 0 0 .45-.606zm1.64-1.33a12.647 12.647 0 0 1 1.01-.193 11.666 11.666 0 0 1-.51-.858 20.741 20.741 0 0 1-.5 1.05zm2.446.45c.15.162.296.3.435.41.24.19.407.253.498.256a.107.107 0 0 0 .07-.015.307.307 0 0 0 .094-.125.436.436 0 0 0 .059-.2.095.095 0 0 0-.026-.063c-.052-.062-.2-.152-.518-.209a3.881 3.881 0 0 0-.612-.053zM8.078 5.8a6.7 6.7 0 0 0 .2-.828c.031-.188.043-.343.038-.465a.613.613 0 0 0-.032-.198.517.517 0 0 0-.145.04c-.087.035-.158.106-.196.283-.04.192-.03.469.046.822.024.111.054.227.09.346z">
       </path>
      </svg>
     </a>
     <small class="titlemark">
      2.4.1
     </small>
     <a id="x4-200002.4.1">
     </a>
     Gini Impurity v. Information Entropy
    </h3>
    <p class="noindent">
     By default, the <span style="color:#054C5C;"><code class="verb">DecisionTreeClassifier</code></span>
     class uses the
     <span id="bold" style="font-weight:bold;">
      Gini impurity
     </span>
     measure, but we can select the entropy impurity measure
instead by setting the criterion hyperparameter to
     <alert style="color: #821131;">
      entropy
     </alert>
     .
    </p>
    <div class="informationblock" id="tcolobox-11">
     <div class="title">
      <p class="noindent">
       <span id="bold" style="font-weight:bold;">
        Information
       </span>
       : Entropy: A Measure of Disorder
      </p>
     </div>
     <div class="box-content">
      <p class="noindent">
       The concept of entropy originated in thermodynamics as a measure of molecular disorder: entropy approaches
zero when molecules are still and well ordered.
      </p>
      <p class="noindent">
       Entropy
later
spread
to
a
wide
variety
of
domains,
including
in
Shannon’s
       <alert style="color: #821131;">
        <span id="bold" style="font-weight:bold;">
         <sup class="textsuperscript">
          9
         </sup>
        </span>
       </alert>
       <span class="marginnote">
        <span style="color:#999999;">
        </span>
        <img alt="PIC" height="" src="figures/Decision-Trees/raster/portrait-shannon.jpg" width="100%"/>
        <alert style="color: #821131;">
         <span id="bold" style="font-weight:bold;">
          <sup class="textsuperscript">
           9
          </sup>
         </span>
        </alert>
        <span style="color:#0063B2;">
         <span id="bold" style="font-weight:bold;">
          Claude
Elwood
Shannon
         </span>
         <italic>
          (1916
-
2001)
         </italic>
         An
American
mathematician,
electrical
engineer,
computer
scientist,
cryptographer
and
inventor
known
as
the
“father
of
information
theory”
and
the
man
who
laid
the
foundations
of
the
Information
Age.
Shannon
was
the
first
to
describe
the
use
of
Boolean
algebra-essential
to
all
digital
electronic
circuits-and
helped
found
artificial
intelligence
(AI).
        </span>
       </span>
       information
theory,
where
it
measures
the
average
information
content
of
a
message
and
the
entropy
is
zero
when
all
messages
are
identical.
      </p>
      
     </div>
    </div>
    <p class="noindent">
     In
     <a class="glossary" href="DataScienceIILectureBookli2.html#glo:ml">
      ML
     </a>, entropy is frequently used as a measurement of impurity, a set’s entropy is zero when
it contains instances of only one class. Eq. (
     <a href="#x4-20001r3">
      2.3
     </a>
     ) shows the definition of the entropy of the
     <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
      <msubsup>
       <mrow>
        <mi>
         i
        </mi>
       </mrow>
       <mrow>
       </mrow>
       <mrow>
        <mi>
         t
        </mi>
        <mi>
         h
        </mi>
       </mrow>
      </msubsup>
     </math>
     node.
    </p>
    <p class="noindent">
     For example, the depth-2 left node in
     <span id="bold" style="font-weight:bold;">
      Fig.
     </span>
     <a href="#x4-16001r1">
      2.1
     </a>
     has an entropy equal to:
    </p>
    <table class="equation-star">
     <tr>
      <td>
       <math class="equation" display="block" xmlns="http://www.w3.org/1998/Math/MathML">
        <mo class="MathClass-bin" stretchy="false">
         −
        </mo>
        <mfrac>
         <mrow>
          <mn>
           4
          </mn>
          <mn>
           9
          </mn>
         </mrow>
         <mrow>
          <mn>
           5
          </mn>
          <mn>
           4
          </mn>
         </mrow>
        </mfrac>
        <msub>
         <mrow>
          <mi class="loglike">
           log
          </mi>
          <mo>
           ⁡
          </mo>
         </mrow>
         <mrow>
          <mn>
           2
          </mn>
         </mrow>
        </msub>
        <mfrac>
         <mrow>
          <mn>
           4
          </mn>
          <mn>
           9
          </mn>
         </mrow>
         <mrow>
          <mn>
           5
          </mn>
          <mn>
           4
          </mn>
         </mrow>
        </mfrac>
        <mo class="MathClass-bin" stretchy="false">
         −
        </mo>
        <mfrac>
         <mrow>
          <mn>
           5
          </mn>
         </mrow>
         <mrow>
          <mn>
           5
          </mn>
          <mn>
           4
          </mn>
         </mrow>
        </mfrac>
        <msub>
         <mrow>
          <mo>
           <mi class="loglike">
            log
           </mi>
           <mo>
            ⁡
           </mo>
          </mo>
         </mrow>
         <mrow>
          <mn>
           2
          </mn>
         </mrow>
        </msub>
        <mfrac>
         <mrow>
          <mn>
           5
          </mn>
         </mrow>
         <mrow>
          <mn>
           5
          </mn>
          <mn>
           4
          </mn>
         </mrow>
        </mfrac>
        <mo class="MathClass-rel" stretchy="false">
         ≈
        </mo>
        <mn>
         0
        </mn>
        <mo class="MathClass-punc" stretchy="false">
         .
        </mo>
        <mn>
         4
        </mn>
        <mn>
         4
        </mn>
        <mn>
         5
        </mn>
       </math>
      </td>
     </tr>
    </table>
    <p class="noindent">
     And the general equation for entropy could be written as:
    </p>
    <table class="equation">
     <tr>
      <td>
       <math class="equation" display="block" xmlns="http://www.w3.org/1998/Math/MathML">
        <mstyle class="label" id="x4-20001r3">
        </mstyle>
        <msubsup>
         <mrow>
          <mi>
           H
          </mi>
         </mrow>
         <mrow>
          <mi>
           i
          </mi>
         </mrow>
         <mrow>
         </mrow>
        </msubsup>
        <mo class="MathClass-rel" stretchy="false">
         =
        </mo>
        <mo class="MathClass-bin" stretchy="false">
         −
        </mo>
        <munderover accent="false" accentunder="false">
         <mrow>
          <mo>
           ∑
          </mo>
         </mrow>
         <mrow>
          <mi>
           k
          </mi>
          <mo class="MathClass-rel" stretchy="false">
           =
          </mo>
          <mn>
           1
          </mn>
         </mrow>
         <mrow>
          <mi>
           n
          </mi>
         </mrow>
        </munderover>
        <msubsup>
         <mrow>
          <mi>
           p
          </mi>
         </mrow>
         <mrow>
          <mi>
           i
          </mi>
          <mo class="MathClass-punc" stretchy="false">
           ,
          </mo>
          <mi>
           k
          </mi>
         </mrow>
         <mrow>
         </mrow>
        </msubsup>
        <msub>
         <mrow>
          <mo>
           <mi class="loglike">
            log
           </mi>
           <mo>
            ⁡
           </mo>
          </mo>
         </mrow>
         <mrow>
          <mn>
           2
          </mn>
         </mrow>
        </msub>
        <msubsup>
         <mrow>
          <mi>
           p
          </mi>
         </mrow>
         <mrow>
          <mi>
           i
          </mi>
          <mo class="MathClass-punc" stretchy="false">
           ,
          </mo>
          <mi>
           k
          </mi>
         </mrow>
         <mrow>
         </mrow>
        </msubsup>
        <mspace class="qquad" width="2em">
        </mspace>
        <mstyle class="text">
         <mtext>
          where
         </mtext>
         <mstyle class="math">
          <msubsup>
           <mrow>
            <mi>
             p
            </mi>
           </mrow>
           <mrow>
            <mi>
             i
            </mi>
            <mo class="MathClass-punc" stretchy="false">
             ,
            </mo>
            <mi>
             k
            </mi>
           </mrow>
           <mrow>
           </mrow>
          </msubsup>
          <mo class="MathClass-rel" stretchy="false">
           ≠
          </mo>
          <mn>
           0
          </mn>
         </mstyle>
         <mtext>
         </mtext>
        </mstyle>
        <mspace class="qquad" width="2em">
        </mspace>
       </math>
      </td>
      <td class="eq-no">
       (2.3)
      </td>
     </tr>
    </table>
    <p class="noindent">
    </p>
    <p class="noindent">
    </p>
    <div class="quoteblock">
     <p class="noindent">
      So,
which
one
to
use?
Gini
impurity
or
entropy?
     </p>
    </div>
    <p class="noindent">
     Most of the time it does not make a big difference as they lead to similar trees. Gini impurity is slightly faster to compute, so it is
a good default. However, when they differ, Gini impurity tends to isolate the most frequent class in its own branch of the tree,
while entropy tends to produce slightly more balanced trees.
    </p>
    <h3 class="subsectionHead">
     <a aria-label="Link to this section in PDF version" class="pdf-link" href="DataScienceIILectureBook.pdf#subsection.2.4.2" style="color: red;  position: absolute;  left: -30px;" target="_blank">
      <svg class="bi bi-file-pdf" fill="currentColor" height="20" viewbox="0 0 16 16" width="20" xmlns="http://www.w3.org/2000/svg">
       <path d="M4 0a2 2 0 0 0-2 2v12a2 2 0 0 0 2 2h8a2 2 0 0 0 2-2V2a2 2 0 0 0-2-2H4zm0 1h8a1 1 0 0 1 1 1v12a1 1 0 0 1-1 1H4a1 1 0 0 1-1-1V2a1 1 0 0 1 1-1z">
       </path>
       <path d="M4.603 12.087a.81.81 0 0 1-.438-.42c-.195-.388-.13-.776.08-1.102.198-.307.526-.568.897-.787a7.68 7.68 0 0 1 1.482-.645 19.701 19.701 0 0 0 1.062-2.227 7.269 7.269 0 0 1-.43-1.295c-.086-.4-.119-.796-.046-1.136.075-.354.274-.672.65-.823.192-.077.4-.12.602-.077a.7.7 0 0 1 .477.365c.088.164.12.356.127.538.007.187-.012.395-.047.614-.084.51-.27 1.134-.52 1.794a10.954 10.954 0 0 0 .98 1.686 5.753 5.753 0 0 1 1.334.05c.364.065.734.195.96.465.12.144.193.32.2.518.007.192-.047.382-.138.563a1.04 1.04 0 0 1-.354.416.856.856 0 0 1-.51.138c-.331-.014-.654-.196-.933-.417a5.716 5.716 0 0 1-.911-.95 11.642 11.642 0 0 0-1.997.406 11.311 11.311 0 0 1-1.021 1.51c-.29.35-.608.655-.926.787a.793.793 0 0 1-.58.029zm1.379-1.901c-.166.076-.32.156-.459.238-.328.194-.541.383-.647.547-.094.145-.096.25-.04.361.01.022.02.036.026.044a.27.27 0 0 0 .035-.012c.137-.056.355-.235.635-.572a8.18 8.18 0 0 0 .45-.606zm1.64-1.33a12.647 12.647 0 0 1 1.01-.193 11.666 11.666 0 0 1-.51-.858 20.741 20.741 0 0 1-.5 1.05zm2.446.45c.15.162.296.3.435.41.24.19.407.253.498.256a.107.107 0 0 0 .07-.015.307.307 0 0 0 .094-.125.436.436 0 0 0 .059-.2.095.095 0 0 0-.026-.063c-.052-.062-.2-.152-.518-.209a3.881 3.881 0 0 0-.612-.053zM8.078 5.8a6.7 6.7 0 0 0 .2-.828c.031-.188.043-.343.038-.465a.613.613 0 0 0-.032-.198.517.517 0 0 0-.145.04c-.087.035-.158.106-.196.283-.04.192-.03.469.046.822.024.111.054.227.09.346z">
       </path>
      </svg>
     </a>
     <small class="titlemark">
      2.4.2
     </small>
     <a id="x4-210002.4.2">
     </a>
     Regularisation Hyperparameters
    </h3>
    <p class="noindent">
     <a class="glossary" href="DataScienceIILectureBookli2.html#glo:dt">
      DT
     </a>
     s
make
very
few
assumptions
                                                                                
                                                                                
about
the
training
data.
     <alert style="color: #821131;">
      <span id="bold" style="font-weight:bold;">
       <sup class="textsuperscript">
        10
       </sup>
      </span>
     </alert>
     <span class="marginnote">
      <span style="color:#999999;">
      </span>
      <alert style="color: #821131;">
       <span id="bold" style="font-weight:bold;">
        <sup class="textsuperscript">
         10
        </sup>
       </span>
      </alert>
      <span style="color:#0063B2;">
       as
opposed
to
linear
models,
which
assume
that
the
data
is
linear,
for
example
      </span>
     </span>
     If
left
unconstrained,
the
tree
structure
will
adapt
itself
to
the
training
data,
fitting
it
very
closely-indeed,
resulting
in
     <span id="bold" style="font-weight:bold;">
      over-fitting
     </span>
     .
    </p>
    
    <p class="noindent">
     Such
a
model
is
often
called
a
non-parametric
model,
not
because
it
does
not
have
any
parameters
     <alert style="color: #821131;">
      <span id="bold" style="font-weight:bold;">
       <sup class="textsuperscript">
        11
       </sup>
      </span>
     </alert>
     <span class="marginnote">
      <span style="color:#999999;">
      </span>
      <alert style="color: #821131;">
       <span id="bold" style="font-weight:bold;">
        <sup class="textsuperscript">
         11
        </sup>
       </span>
      </alert>
      <span style="color:#0063B2;">
       as
it
often
has
a
lot.
      </span>
     </span>
     but
because
the
number
                                                                                
                                                                                
of
parameters
is
not
determined
prior
to
training,
so
the
model
structure
is
free
to
stick
closely
to
the
data.
    </p>
    
    <p class="noindent">
     In
contrast,
a
parametric
model,
such
as
a
linear
model,
has
a
predetermined
number
of
parameters,
so
its
degree
of
freedom
is
limited,
reducing
the
risk
of
over-fitting.
    </p>
    <div class="warning">
     <p class="noindent">
      This, however, increases the risk of under-fitting.
     </p>
    </div>
    <p class="noindent">
     To avoid over-fitting the training data, we need to restrict the
     <a class="glossary" href="DataScienceIILectureBookli2.html#glo:dt">
      DT
     </a>
     ’s freedom during training. As we should know by now, this is
called
     <span id="bold" style="font-weight:bold;">
      regularisation
     </span>
     .
    </p>
    <p class="noindent">
     The regularisation hyperparameters depend on the algorithm used, but generally we can at least restrict the
maximum depth of the
     <a class="glossary" href="DataScienceIILectureBookli2.html#glo:dt">
      DT
     </a>. In <span style="color:#054C5C;"><code class="verb">scikit-learn</code></span>, this is controlled by the <span style="color:#054C5C;"><code class="verb">max_depth</code></span>
     hyperparameter. The default
value is <span style="color:#054C5C;"><code class="verb">None</code></span>, which means unlimited. Reducing <span style="color:#054C5C;"><code class="verb">max_depth</code></span>
     will regularise the model and thus reduce the risk of
over-fitting.
    </p>
    <p class="noindent">
     The <span style="color:#054C5C;"><code class="verb">DecisionTreeClassifier</code></span>
     class has a few other parameters that similarly restrict the shape of the
     <a class="glossary" href="DataScienceIILectureBookli2.html#glo:dt">
      DT
     </a>
     :
    </p>
    <dl class="description">
     <dt class="description">
      <a id="x4-21001x2.4.2">
      </a> <span style="color:#054C5C;"><code class="verb">max_features</code></span>
     </dt>
     <dd class="description">
      <p class="noindent">
       Maximum
number
of
features
                                                                                
                                                                                
     
that
are
evaluated
for
splitting
at
each
node
      </p>
     </dd>
     <dt class="description">
      <a id="x4-21002x2.4.2">
      </a> <span style="color:#054C5C;"><code class="verb">max_leaf_nodes</code></span>
     </dt>
     <dd class="description">
      <p class="noindent">
       Maximum
number
of
leaf
nodes
      </p>
     </dd>
     <dt class="description">
      <a id="x4-21003x2.4.2">
      </a> <span style="color:#054C5C;"><code class="verb">min_samples_split</code></span>
     </dt>
     <dd class="description">
      <p class="noindent">
       Minimum
number
of
samples
a
node
must
have
before
it
can
be
split
      </p>
     </dd>
     <dt class="description">
      <a id="x4-21004x2.4.2">
      </a> <span style="color:#054C5C;"><code class="verb">min_samples_leaf</code></span>
     </dt>
     <dd class="description">
      <p class="noindent">
       Minimum
number
of
samples
a
leaf
node
must
have
to
be
created
      </p>
     </dd>
     <dt class="description">
      <a id="x4-21005x2.4.2">
      </a> <span style="color:#054C5C;"><code class="verb">min_weight_fraction_leaf</code></span>
     </dt>
     <dd class="description">
      <p class="noindent">
       Same
as <span style="color:#054C5C;"><code class="verb">min_samples_leaf</code></span>
       but
expressed
as
a
fraction
                                                                                
                                                                                
     
of
the
total
number
of
weighted
instances.
      </p>
     </dd>
    </dl>
    <div class="warning">
     <p class="noindent">
      Increasing <span style="color:#054C5C;"><code class="verb">min_*</code></span>
      or reducing <span style="color:#054C5C;"><code class="verb">max_*</code></span>
      hyperparameters will regularise the model.
     </p>
    </div>
    <div class="informationblock" id="tcolobox-12">
     <div class="title">
      <p class="noindent">
       <span id="bold" style="font-weight:bold;">
        Information
       </span>
       : Other Methods of Training
      </p>
     </div>
     <div class="box-content">
      <p class="noindent">
       Other algorithms work by first training the
       <a class="glossary" href="DataScienceIILectureBookli2.html#glo:dt">
        DT
       </a>
       without restrictions, then pruning unnecessary nodes. A node
whose children are all leaf nodes is considered unnecessary if the purity improvement it provides is not statistically
significant.               Standard              statistical              tests,              such              as              the
       <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
        <msup>
         <mrow>
          <mi>
           χ
          </mi>
         </mrow>
         <mrow>
          <mn>
           2
          </mn>
         </mrow>
        </msup>
       </math>
       test (chi-squared test), are used to estimate the probability that the improvement is purely the result of chance
(which is called the null hypothesis). If this probability, called the p-value, is higher than a given threshold
(typically 5%, controlled by a hyperparameter), then the node is considered unnecessary and its children are
deleted. The pruning continues until all unnecessary nodes have been pruned.
      </p>
     </div>
    </div>
    <p class="noindent">
     Let’s test regularisation on the moons dataset, introduced previously. We’ll train one
     <a class="glossary" href="DataScienceIILectureBookli2.html#glo:dt">
      DT
     </a>
     without regularisation,
and another with <span style="color:#054C5C;"><code class="verb">min_samples_leaf=5</code></span>. Here’s the code with
     <span id="bold" style="font-weight:bold;">
      Fig.
     </span>
     <a href="#x4-21022r4">
      2.4
     </a>
     showing the decision boundaries of each
tree.
    </p>
    <p class="noindent">
    </p>
    <pre><div id="fancyvrb16" style="padding:20px;border-radius: 3px;"><a id="x4-21007r116"></a><span style="color:#2B2BFF;">from</span><span style="color:#BABABA;"> </span>sklearn.datasets<span style="color:#BABABA;"> </span><span style="color:#2B2BFF;">import</span> make_moons 
<a id="x4-21009r117"></a> 
<a id="x4-21011r118"></a>X_moons, y_moons = make_moons(n_samples=<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">150</span></span>, noise=<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">0.2</span></span>, random_state=<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">42</span></span>) 
<a id="x4-21013r119"></a> 
<a id="x4-21015r120"></a>tree_clf1 = DecisionTreeClassifier(random_state=<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">42</span></span>) 
<a id="x4-21017r121"></a>tree_clf2 = DecisionTreeClassifier(min_samples_leaf=<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">5</span></span>, random_state=<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">42</span></span>) 
<a id="x4-21019r122"></a>tree_clf1.fit(X_moons, y_moons) 
<a id="x4-21021r123"></a>tree_clf2.fit(X_moons, y_moons)</div></pre>
    <div class="figure">
     <p class="noindent">
      <img alt="PIC" height="" src="codes/images/Decision-Trees/min_samples_leaf_plot-.svg" width="150%"/>
      <a id="x4-21022r4">
      </a>
     </p>
     <figcaption class="caption">
      <span class="id">
       Figure 2.4:
      </span>
      <span class="content">
       Decision boundaries of an unregulated tree (left) and a regularised tree (right).
      </span>
     </figcaption>
    </div>
    <p class="noindent">
     The unregulated model on the left is clearly over-fitting, and the regularised model on the right will probably
generalise better. We can verify this by evaluating both trees on a test set generated using a different random
seed:
    </p>
    <pre><div id="fancyvrb17" style="padding:20px;border-radius: 3px;"><a id="x4-21024r130"></a>X_moons_test, y_moons_test = make_moons(n_samples=<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">1000</span></span>, noise=<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">0.2</span></span>, random_state=<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">43</span></span>) 
<a id="x4-21026r131"></a><span style="color:#2B2BFF;">print</span>(tree_clf1.score(X_moons_test, y_moons_test)) 
<a id="x4-21028r132"></a><span style="color:#2B2BFF;">print</span>(tree_clf2.score(X_moons_test, y_moons_test))</div></pre>
    <div class="tcolorbox tcolorbox" id="tcolobox-13">
     <div class="tcolorbox-content">
      <pre><div id="fancyvrb18" style="padding:20px;border-radius: 3px;"><a id="x4-21030r1"></a>0.898 
<a id="x4-21032r2"></a>0.92</div></pre>
     </div>
    </div>
    <p class="noindent">
     As we can see, the
     <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
      <msup>
       <mrow>
        <mstyle class="text">
         <mtext>
          2
         </mtext>
        </mstyle>
       </mrow>
       <mrow>
        <mstyle class="text">
         <mtext>
          nd
         </mtext>
        </mstyle>
       </mrow>
      </msup>
     </math>
     tree has a better accuracy on the test set.
    </p>
    <p class="noindent">
    </p>
    <h2 class="sectionHead">
     <a aria-label="Link to this section in PDF version" class="pdf-link" href="DataScienceIILectureBook.pdf#section.2.5" style="color: red;  position: absolute;  left: -30px;" target="_blank">
      <svg class="bi bi-file-pdf" fill="currentColor" height="20" viewbox="0 0 16 16" width="20" xmlns="http://www.w3.org/2000/svg">
       <path d="M4 0a2 2 0 0 0-2 2v12a2 2 0 0 0 2 2h8a2 2 0 0 0 2-2V2a2 2 0 0 0-2-2H4zm0 1h8a1 1 0 0 1 1 1v12a1 1 0 0 1-1 1H4a1 1 0 0 1-1-1V2a1 1 0 0 1 1-1z">
       </path>
       <path d="M4.603 12.087a.81.81 0 0 1-.438-.42c-.195-.388-.13-.776.08-1.102.198-.307.526-.568.897-.787a7.68 7.68 0 0 1 1.482-.645 19.701 19.701 0 0 0 1.062-2.227 7.269 7.269 0 0 1-.43-1.295c-.086-.4-.119-.796-.046-1.136.075-.354.274-.672.65-.823.192-.077.4-.12.602-.077a.7.7 0 0 1 .477.365c.088.164.12.356.127.538.007.187-.012.395-.047.614-.084.51-.27 1.134-.52 1.794a10.954 10.954 0 0 0 .98 1.686 5.753 5.753 0 0 1 1.334.05c.364.065.734.195.96.465.12.144.193.32.2.518.007.192-.047.382-.138.563a1.04 1.04 0 0 1-.354.416.856.856 0 0 1-.51.138c-.331-.014-.654-.196-.933-.417a5.716 5.716 0 0 1-.911-.95 11.642 11.642 0 0 0-1.997.406 11.311 11.311 0 0 1-1.021 1.51c-.29.35-.608.655-.926.787a.793.793 0 0 1-.58.029zm1.379-1.901c-.166.076-.32.156-.459.238-.328.194-.541.383-.647.547-.094.145-.096.25-.04.361.01.022.02.036.026.044a.27.27 0 0 0 .035-.012c.137-.056.355-.235.635-.572a8.18 8.18 0 0 0 .45-.606zm1.64-1.33a12.647 12.647 0 0 1 1.01-.193 11.666 11.666 0 0 1-.51-.858 20.741 20.741 0 0 1-.5 1.05zm2.446.45c.15.162.296.3.435.41.24.19.407.253.498.256a.107.107 0 0 0 .07-.015.307.307 0 0 0 .094-.125.436.436 0 0 0 .059-.2.095.095 0 0 0-.026-.063c-.052-.062-.2-.152-.518-.209a3.881 3.881 0 0 0-.612-.053zM8.078 5.8a6.7 6.7 0 0 0 .2-.828c.031-.188.043-.343.038-.465a.613.613 0 0 0-.032-.198.517.517 0 0 0-.145.04c-.087.035-.158.106-.196.283-.04.192-.03.469.046.822.024.111.054.227.09.346z">
       </path>
      </svg>
     </a>
     <small class="titlemark">
      2.5
     </small>
     <a id="x4-220002.5">
     </a>
     Regression
    </h2>
    <p class="noindent">
     <a class="glossary" href="DataScienceIILectureBookli2.html#glo:dt">
      DT
     </a>
     s are also capable of performing regression tasks. Let’s build a regression tree with the <span style="color:#054C5C;"><code class="verb">DecisionTreeRegressor</code></span>
     class, training it
on a noisy quadratic dataset with <span style="color:#054C5C;"><code class="verb">max_depth=2</code></span>
     with the resulting tree being represented in
     <span id="bold" style="font-weight:bold;">
      Fig.
     </span>
     <a href="#x4-22001r5">
      2.5
     </a>.
    </p>
    <div class="figure">
     <p class="noindent">
      <img alt="PIC" height="" src="codes/images/Decision-Trees/regression_tree-.svg" width="150%"/>
      <a id="x4-22001r5">
      </a>
     </p>
     <figcaption class="caption">
      <span class="id">
       Figure 2.5:
      </span>
      <span class="content">
       A decision tree for regression.
      </span>
     </figcaption>
    </div>
    <p class="noindent">
     This tree looks very similar to the classification tree built earlier. The main difference is that instead of predicting a class in each
node, it predicts a
     <span id="bold" style="font-weight:bold;">
      value
     </span>
     .
    </p>
    <div class="figure">
     <p class="noindent">
      <img alt="PIC" height="" src="codes/images/Decision-Trees/tree_regression_plot-.svg" width="150%"/>
      <a id="x4-22002r6">
      </a>
     </p>
     <figcaption class="caption">
      <span class="id">
       Figure 2.6:
      </span>
      <span class="content">
       Predictions of two decision tree regression models.
      </span>
     </figcaption>
    </div>
    <p class="noindent">
     For example, say we want to make a prediction for a new instance with
     <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
      <msubsup>
       <mrow>
        <mi>
         x
        </mi>
       </mrow>
       <mrow>
        <mn>
         1
        </mn>
       </mrow>
       <mrow>
       </mrow>
      </msubsup>
     </math>
     = 0.2. The root
node asks whether
     <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
      <msubsup>
       <mrow>
        <mi>
         x
        </mi>
       </mrow>
       <mrow>
        <mn>
         1
        </mn>
       </mrow>
       <mrow>
       </mrow>
      </msubsup>
      <mo class="MathClass-rel" stretchy="false">
       ≤
      </mo>
      <mn>
       0
      </mn>
      <mo class="MathClass-punc" stretchy="false">
       .
      </mo>
      <mn>
       1
      </mn>
      <mn>
       9
      </mn>
      <mn>
       7
      </mn>
     </math>
     .
As it is not, the algorithm goes to the right child node, which asks whether
     <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
      <msubsup>
       <mrow>
        <mi>
         x
        </mi>
       </mrow>
       <mrow>
        <mn>
         1
        </mn>
       </mrow>
       <mrow>
       </mrow>
      </msubsup>
      <mo class="MathClass-rel" stretchy="false">
       ≤
      </mo>
      <mn>
       0
      </mn>
      <mo class="MathClass-punc" stretchy="false">
       .
      </mo>
      <mn>
       7
      </mn>
      <mn>
       7
      </mn>
      <mn>
       2
      </mn>
     </math>
     . Since it
is, the algorithm goes to the left child node. This is a leaf node, and it predicts <span style="color:#054C5C;"><code class="verb">value=0.111</code></span>. This prediction is the average target
value of the 110 training instances associated with this leaf node, and results in a mean squared error equal to 0.015 over these
110 instances.
    </p>
    <p class="noindent">
     This model’s predictions are represented on the left in
     <span id="bold" style="font-weight:bold;">
      Fig.
     </span>
     <a href="#x4-22002r6">
      2.6
     </a>. If we set <span style="color:#054C5C;"><code class="verb">max_depth=3</code></span>, we get the predictions represented on the
right. Notice how the predicted value for each region is always the average target value of the instances in that region. The
algorithm splits each region in a way that makes most training instances as close as possible to that predicted
value.
    </p>
    <p class="noindent">
     The
     <a class="glossary" href="DataScienceIILectureBookli2.html#glo:cart">
      CART
     </a>
     algorithm works as described earlier, except that instead of trying to split the training set in a way that
minimises impurity, it now tries to split the training set in a way that minimises the
     <a class="glossary" href="DataScienceIILectureBookli2.html#glo:mse">
      Mean Square Error (MSE)
     </a>. To
get a better feel of the underlying mathematics, Eq. (
     <a href="#x4-22003r4">
      2.4
     </a>
     ) shows the cost function that the algorithm tries to
minimise:
    </p>
    <table class="equation">
     <tr>
      <td>
       <math class="equation" display="block" xmlns="http://www.w3.org/1998/Math/MathML">
        <mstyle class="label" id="x4-22003r4">
        </mstyle>
        <mi>
         J
        </mi>
        <msup>
         <mrow>
          <mrow>
           <mo fence="true" form="prefix">
            (
           </mo>
           <mrow>
            <mi>
             k
            </mi>
            <mo class="MathClass-punc" stretchy="false">
             ,
            </mo>
            <mspace class="thinspace" width="0.17em">
            </mspace>
            <msubsup>
             <mrow>
              <mi>
               t
              </mi>
             </mrow>
             <mrow>
              <mi>
               k
              </mi>
             </mrow>
             <mrow>
             </mrow>
            </msubsup>
           </mrow>
           <mo fence="true" form="postfix">
            )
           </mo>
          </mrow>
         </mrow>
         <mrow>
         </mrow>
        </msup>
        <mo class="MathClass-rel" stretchy="false">
         =
        </mo>
        <mfrac>
         <mrow>
          <msubsup>
           <mrow>
            <mi>
             m
            </mi>
           </mrow>
           <mrow>
            <mi>
             l
            </mi>
            <mi>
             e
            </mi>
            <mi>
             f
            </mi>
            <mi>
             t
            </mi>
           </mrow>
           <mrow>
           </mrow>
          </msubsup>
         </mrow>
         <mrow>
          <mi>
           m
          </mi>
         </mrow>
        </mfrac>
        <msub>
         <mrow>
          <mstyle class="text">
           <mtext>
            MSE
           </mtext>
          </mstyle>
         </mrow>
         <mrow>
          <mstyle class="text">
           <mtext>
            left
           </mtext>
          </mstyle>
         </mrow>
        </msub>
        <mo class="MathClass-bin" stretchy="false">
         +
        </mo>
        <mfrac>
         <mrow>
          <msubsup>
           <mrow>
            <mi>
             m
            </mi>
           </mrow>
           <mrow>
            <mi>
             r
            </mi>
            <mi>
             i
            </mi>
            <mi>
             g
            </mi>
            <mi>
             h
            </mi>
            <mi>
             t
            </mi>
           </mrow>
           <mrow>
           </mrow>
          </msubsup>
         </mrow>
         <mrow>
          <mi>
           m
          </mi>
         </mrow>
        </mfrac>
        <msub>
         <mrow>
          <mstyle class="text">
           <mtext>
            MSE
           </mtext>
          </mstyle>
         </mrow>
         <mrow>
          <mstyle class="text">
           <mtext>
            right
           </mtext>
          </mstyle>
         </mrow>
        </msub>
        <mspace class="quad" width="1em">
        </mspace>
        <mstyle class="text">
         <mtext>
          where
         </mtext>
        </mstyle>
        <mspace class="quad" width="1em">
        </mspace>
        <mrow class="cases">
         <mrow>
          <mo fence="true" form="prefix">
           {
          </mo>
          <mrow>
           <mtable align="axis" class="array" columnlines="none" displaystyle="true" equalcolumns="false" equalrows="false" style="">
            <mtr class="array-row">
             <mtd class="array-td" columnalign="left">
              <msub>
               <mrow>
                <mstyle class="text">
                 <mtext>
                  MSE
                 </mtext>
                </mstyle>
               </mrow>
               <mrow>
                <mstyle class="text">
                 <mtext>
                  node
                 </mtext>
                </mstyle>
               </mrow>
              </msub>
              <mspace class="quad" width="1em">
              </mspace>
             </mtd>
             <mtd class="array-td" columnalign="left">
              <mo class="MathClass-rel" stretchy="false">
               =
              </mo>
              <munder class="msub">
               <mrow>
                <mo>
                 ∑
                </mo>
               </mrow>
               <mrow>
                <mi>
                 i
                </mi>
                <mspace class="thinspace" width="0.17em">
                </mspace>
                <mo class="MathClass-rel" stretchy="false">
                 ∈
                </mo>
                <mspace class="thinspace" width="0.17em">
                </mspace>
                <mstyle class="text">
                 <mtext>
                  node
                 </mtext>
                </mstyle>
               </mrow>
              </munder>
              <msup>
               <mrow>
                <mrow>
                 <mo fence="true" form="prefix">
                  (
                 </mo>
                 <mrow>
                  <msubsup>
                   <mrow>
                    <mi>
                     ŷ
                    </mi>
                   </mrow>
                   <mrow>
                    <mi>
                     n
                    </mi>
                    <mi>
                     o
                    </mi>
                    <mi>
                     d
                    </mi>
                    <mi>
                     e
                    </mi>
                   </mrow>
                   <mrow>
                   </mrow>
                  </msubsup>
                  <mo class="MathClass-bin" stretchy="false">
                   −
                  </mo>
                  <msubsup>
                   <mrow>
                    <mi>
                     y
                    </mi>
                   </mrow>
                   <mrow>
                   </mrow>
                   <mrow>
                    <mo class="MathClass-open" stretchy="false">
                     (
                    </mo>
                    <mi>
                     i
                    </mi>
                    <mo class="MathClass-close" stretchy="false">
                     )
                    </mo>
                   </mrow>
                  </msubsup>
                 </mrow>
                 <mo fence="true" form="postfix">
                  )
                 </mo>
                </mrow>
               </mrow>
               <mrow>
                <mn>
                 2
                </mn>
               </mrow>
              </msup>
             </mtd>
            </mtr>
            <mtr class="array-row">
             <mtd class="array-td" columnalign="left">
              <msubsup>
               <mrow>
                <mi>
                 ŷ
                </mi>
               </mrow>
               <mrow>
                <mi>
                 n
                </mi>
                <mi>
                 o
                </mi>
                <mi>
                 d
                </mi>
                <mi>
                 e
                </mi>
               </mrow>
               <mrow>
               </mrow>
              </msubsup>
              <mspace class="quad" width="1em">
              </mspace>
             </mtd>
             <mtd class="array-td" columnalign="left">
              <mo class="MathClass-rel" stretchy="false">
               =
              </mo>
              <mfrac>
               <mrow>
                <mn>
                 1
                </mn>
               </mrow>
               <mrow>
                <msubsup>
                 <mrow>
                  <mi>
                   m
                  </mi>
                 </mrow>
                 <mrow>
                  <mi>
                   n
                  </mi>
                  <mi>
                   o
                  </mi>
                  <mi>
                   d
                  </mi>
                  <mi>
                   e
                  </mi>
                 </mrow>
                 <mrow>
                 </mrow>
                </msubsup>
               </mrow>
              </mfrac>
              <munder class="msub">
               <mrow>
                <mo>
                 ∑
                </mo>
               </mrow>
               <mrow>
                <mi>
                 i
                </mi>
                <mspace class="thinspace" width="0.17em">
                </mspace>
                <mo class="MathClass-rel" stretchy="false">
                 ∈
                </mo>
                <mspace class="thinspace" width="0.17em">
                </mspace>
                <mstyle class="text">
                 <mtext>
                  node
                 </mtext>
                </mstyle>
               </mrow>
              </munder>
              <msubsup>
               <mrow>
                <mi>
                 y
                </mi>
               </mrow>
               <mrow>
               </mrow>
               <mrow>
                <mo class="MathClass-open" stretchy="false">
                 (
                </mo>
                <mi>
                 i
                </mi>
                <mo class="MathClass-close" stretchy="false">
                 )
                </mo>
               </mrow>
              </msubsup>
             </mtd>
            </mtr>
           </mtable>
          </mrow>
          <mo fence="true" form="postfix">
          </mo>
         </mrow>
        </mrow>
       </math>
      </td>
      <td class="eq-no">
       (2.4)
      </td>
     </tr>
    </table>
    <p class="noindent">
    </p>
    <p class="noindent">
     Just
like
for
classification
tasks,
     <a class="glossary" href="DataScienceIILectureBookli2.html#glo:dt">
      DT
     </a>
     s
are
prone
to
over-fitting
when
dealing
with
regression
tasks.
Without
any
regularization,
     <alert style="color: #821131;">
      <span id="bold" style="font-weight:bold;">
       <sup class="textsuperscript">
        12
       </sup>
      </span>
     </alert>
     <span class="marginnote">
      <span style="color:#999999;">
      </span>
      <alert style="color: #821131;">
       <span id="bold" style="font-weight:bold;">
        <sup class="textsuperscript">
         12
        </sup>
       </span>
      </alert>
      <span style="color:#0063B2;">
       i.e.,
using
the
default
hyperparameters.
      </span>
     </span>
     we
get
the
predictions
on
the
left
in
     <span id="bold" style="font-weight:bold;">
      Fig.
     </span>
     <a href="#x4-22004r7">
      2.7
     </a>.
    </p>
    
    <p class="noindent">
     As can clearly be seen, these predictions are over-fitting the training set very badly. Setting <span style="color:#054C5C;"><code class="verb">min_samples_leaf=10</code></span>
     results in a
significantly more reasonable model, represented on the right in
     <span id="bold" style="font-weight:bold;">
      Fig.
     </span>
     <a href="#x4-22004r7">
      2.7
     </a>.
    </p>
    <div class="figure">
     <p class="noindent">
      <img alt="PIC" height="" src="codes/images/Decision-Trees/tree_regression_regularization_plot-.svg" width="150%"/>
      <a id="x4-22004r7">
      </a>
     </p>
     <figcaption class="caption">
      <span class="id">
       Figure 2.7:
      </span>
      <span class="content">
       Predictions of an unregularised regression tree (left) and a regularised tree (right).
      </span>
     </figcaption>
    </div>
    <p class="noindent">
    </p>
    <h2 class="sectionHead">
     <a aria-label="Link to this section in PDF version" class="pdf-link" href="DataScienceIILectureBook.pdf#section.2.6" style="color: red;  position: absolute;  left: -30px;" target="_blank">
      <svg class="bi bi-file-pdf" fill="currentColor" height="20" viewbox="0 0 16 16" width="20" xmlns="http://www.w3.org/2000/svg">
       <path d="M4 0a2 2 0 0 0-2 2v12a2 2 0 0 0 2 2h8a2 2 0 0 0 2-2V2a2 2 0 0 0-2-2H4zm0 1h8a1 1 0 0 1 1 1v12a1 1 0 0 1-1 1H4a1 1 0 0 1-1-1V2a1 1 0 0 1 1-1z">
       </path>
       <path d="M4.603 12.087a.81.81 0 0 1-.438-.42c-.195-.388-.13-.776.08-1.102.198-.307.526-.568.897-.787a7.68 7.68 0 0 1 1.482-.645 19.701 19.701 0 0 0 1.062-2.227 7.269 7.269 0 0 1-.43-1.295c-.086-.4-.119-.796-.046-1.136.075-.354.274-.672.65-.823.192-.077.4-.12.602-.077a.7.7 0 0 1 .477.365c.088.164.12.356.127.538.007.187-.012.395-.047.614-.084.51-.27 1.134-.52 1.794a10.954 10.954 0 0 0 .98 1.686 5.753 5.753 0 0 1 1.334.05c.364.065.734.195.96.465.12.144.193.32.2.518.007.192-.047.382-.138.563a1.04 1.04 0 0 1-.354.416.856.856 0 0 1-.51.138c-.331-.014-.654-.196-.933-.417a5.716 5.716 0 0 1-.911-.95 11.642 11.642 0 0 0-1.997.406 11.311 11.311 0 0 1-1.021 1.51c-.29.35-.608.655-.926.787a.793.793 0 0 1-.58.029zm1.379-1.901c-.166.076-.32.156-.459.238-.328.194-.541.383-.647.547-.094.145-.096.25-.04.361.01.022.02.036.026.044a.27.27 0 0 0 .035-.012c.137-.056.355-.235.635-.572a8.18 8.18 0 0 0 .45-.606zm1.64-1.33a12.647 12.647 0 0 1 1.01-.193 11.666 11.666 0 0 1-.51-.858 20.741 20.741 0 0 1-.5 1.05zm2.446.45c.15.162.296.3.435.41.24.19.407.253.498.256a.107.107 0 0 0 .07-.015.307.307 0 0 0 .094-.125.436.436 0 0 0 .059-.2.095.095 0 0 0-.026-.063c-.052-.062-.2-.152-.518-.209a3.881 3.881 0 0 0-.612-.053zM8.078 5.8a6.7 6.7 0 0 0 .2-.828c.031-.188.043-.343.038-.465a.613.613 0 0 0-.032-.198.517.517 0 0 0-.145.04c-.087.035-.158.106-.196.283-.04.192-.03.469.046.822.024.111.054.227.09.346z">
       </path>
      </svg>
     </a>
     <small class="titlemark">
      2.6
     </small>
     <a id="x4-230002.6">
     </a>
     Sensitivity to Axis Orientation
    </h2>
    <p class="noindent">
     <a class="glossary" href="DataScienceIILectureBookli2.html#glo:dt">
      DT
     </a>
     s
have
a
lot
going
for
them
given
they
are
relatively
easy
to
understand
and
interpret,
simple
to
use,
versatile,
and
powerful.
However,
they
do
have
a
few
limitations.
First,
as
we
may
have
noticed,
     <a class="glossary" href="DataScienceIILectureBookli2.html#glo:dt">
      DT
     </a>
     s
love
orthogonal
decision
boundaries,
     <alert style="color: #821131;">
      <span id="bold" style="font-weight:bold;">
       <sup class="textsuperscript">
        13
       </sup>
      </span>
     </alert>
     <span class="marginnote">
      <span style="color:#999999;">
      </span>
      <alert style="color: #821131;">
       <span id="bold" style="font-weight:bold;">
        <sup class="textsuperscript">
         13
        </sup>
       </span>
      </alert>
      <span style="color:#0063B2;">
       all
splits
are
perpendicular
to
an
axis.
      </span>
     </span>
     which
makes
them
sensitive
to
the
orientation
of
data.
    </p>
    
    <div class="figure">
     <p class="noindent">
      <img alt="PIC" height="" src="codes/images/Decision-Trees/sensitivity_to_rotation_plot-.svg" width="150%"/>
      <a id="x4-23001r8">
      </a>
     </p>
     <figcaption class="caption">
      <span class="id">
       Figure 2.8:
      </span>
      <span class="content">
       Sensitivity to training set rotation.
      </span>
     </figcaption>
    </div>
    <p class="noindent">
     For example,
     <span id="bold" style="font-weight:bold;">
      Fig.
     </span>
     <a href="#x4-23001r8">
      2.8
     </a>
     shows a simple linearly separable dataset where on the left, a
     <a class="glossary" href="DataScienceIILectureBookli2.html#glo:dt">
      DT
     </a>
     can split it easily, while
on the right, after the dataset is rotated by 45 degrees, the decision boundary looks unnecessarily convoluted.
Although both
     <a class="glossary" href="DataScienceIILectureBookli2.html#glo:dt">
      DT
     </a>
     s fit the training set perfectly, it is very likely that the model on the right will not generalise
well.
    </p>
    <p class="noindent">
     One way to limit this problem is to scale the data, then apply a principal component analysis transformation.
We will look at
     <a class="glossary" href="DataScienceIILectureBookli2.html#glo:pca">
      Principal Component Analysis (PCA)
     </a>
     in detail later, but for now we only need to know that it
rotates the data in a way that reduces the correlation between the features, which often makes things easier for
trees.
    </p>
    <p class="noindent">
     Let’s create a small pipeline that scales the data and rotates it using
     <a class="glossary" href="DataScienceIILectureBookli2.html#glo:pca">
      PCA
     </a>, then, continue on to train a <span style="color:#054C5C;"><code class="verb">DecisionTreeClassifier</code></span>
     on that data.
    </p>
    <pre><div id="fancyvrb19" style="padding:20px;border-radius: 3px;"><a id="x4-23003r319"></a><span style="color:#2B2BFF;">from</span><span style="color:#BABABA;"> </span>sklearn.decomposition<span style="color:#BABABA;"> </span><span style="color:#2B2BFF;">import</span> PCA 
<a id="x4-23005r320"></a><span style="color:#2B2BFF;">from</span><span style="color:#BABABA;"> </span>sklearn.pipeline<span style="color:#BABABA;"> </span><span style="color:#2B2BFF;">import</span> make_pipeline 
<a id="x4-23007r321"></a><span style="color:#2B2BFF;">from</span><span style="color:#BABABA;"> </span>sklearn.preprocessing<span style="color:#BABABA;"> </span><span style="color:#2B2BFF;">import</span> StandardScaler 
<a id="x4-23009r322"></a> 
<a id="x4-23011r323"></a>pca_pipeline = make_pipeline(StandardScaler(), PCA()) 
<a id="x4-23013r324"></a>X_iris_rotated = pca_pipeline.fit_transform(X_iris) 
<a id="x4-23015r325"></a>tree_clf_pca = DecisionTreeClassifier(max_depth=<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">2</span></span>, random_state=<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">42</span></span>) 
<a id="x4-23017r326"></a>tree_clf_pca.fit(X_iris_rotated, y_iris)</div></pre>
    <p class="noindent">
     <span id="bold" style="font-weight:bold;">
      Fig.
     </span>
     <a href="#x4-23018r9">
      2.9
     </a>
     shows the decision boundaries of that tree and as we can see, the rotation makes it possible to fit the dataset pretty well
using only one <alert style="color: #821131;">(1)</alert> feature, which is a linear function of the original petal length and width.
    </p>
    <div class="figure">
     <p class="noindent">
      <img alt="PIC" height="" src="codes/images/Decision-Trees/pca_preprocessing_plot-.svg" width="150%"/>
      <a id="x4-23018r9">
      </a>
     </p>
     <figcaption class="caption">
      <span class="id">
       Figure 2.9:
      </span>
      <span class="content">
       A tree’s decision boundaries on the scaled and PCA-rotated iris dataset.
      </span>
     </figcaption>
    </div>
    <div class="informationblock" id="tcolobox-14">
     <div class="title">
      <p class="noindent">
       <span id="bold" style="font-weight:bold;">
        Information
       </span>
       : The Problem of High Variance
      </p>
     </div>
     <div class="box-content">
      <p class="noindent">
       More generally, the primary issue with
       <a class="glossary" href="DataScienceIILectureBookli2.html#glo:dt">
        DT
       </a>
       s is that they have
       <span id="bold" style="font-weight:bold;">
        high variance
       </span>
       where small changes to the
hyperparameters or to the data may produce very different models.
      </p>
      <p class="noindent">
       In fact, given the training algorithm used by <span style="color:#054C5C;"><code class="verb">scikit-learn</code></span>
       is stochastic-it randomly selects the set of features to
evaluate at each node-even retraining the same
       <a class="glossary" href="DataScienceIILectureBookli2.html#glo:dt">
        DT
       </a>
       on the exact same data may produce a very different model,
such as the one represented in
       <span id="bold" style="font-weight:bold;">
        Fig.
       </span>
       <a href="#x4-23019r10">
        2.10
       </a>, unless we set the <span style="color:#054C5C;"><code class="verb">random_state</code></span>
       hyperparameter.
      </p>
      <p class="noindent">
       As we can see, it looks very different from the previous
       <a class="glossary" href="DataScienceIILectureBookli2.html#glo:dt">
        DT
       </a>, shown in
       <span id="bold" style="font-weight:bold;">
        Fig.
       </span>
       <a href="#x4-16001r1">
        2.1
       </a>.
      </p>
      <div class="figure">
       <p class="noindent">
        <img alt="PIC" height="" src="codes/images/Decision-Trees/decision_tree_high_variance_plot-.svg" width="150%"/>
        <a id="x4-23019r10">
        </a>
       </p>
       <figcaption class="caption">
        <span class="id">
         Figure 2.10:
        </span>
        <span class="content">
         Retraining the same model on the same data may produce a very different model.
        </span>
       </figcaption>
      </div>
      <p class="noindent">
       Luckily, by averaging predictions over many trees, it’s possible to reduce variance significantly. Such an
ensemble is called a
       <a class="glossary" href="DataScienceIILectureBookli2.html#glo:rf">
        Random Forest (RF)
       </a>, and it’s one of the most powerful types of models available
today.
      </p>
     </div>
    </div>
   </article>
   <footer>
    <div class="next">
     <p class="noindent">
      <a href="DataScienceIILectureBookch3.html" style="float: right;">
       Next Chapter →
      </a>
      <a href="DataScienceIILectureBookch1.html" style="float: left;">
       ← Previous Chapter
      </a>
      <a href="index.html">
       TOP
      </a>
     </p>
    </div>
   </footer>
  </div>
  <p class="noindent">
   <a id="tailDataScienceIILectureBookch2.html">
   </a>
  </p>
 </body>
</html>
