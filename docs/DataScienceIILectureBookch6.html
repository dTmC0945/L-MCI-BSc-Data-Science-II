<!DOCTYPE html>
<html lang="en-US" xml:lang="en-US">
 <head>
  <title>
   6 Introduction to Artificial Neural Networks
  </title>
    <link rel='icon' type='image/x-icon' href='figures/logo.svg'></link>
  <meta charset="utf-8"/>
  <meta content="ParSnip" name="generator"/>
  <meta content="width=device-width,initial-scale=1" name="viewport"/>
  <link href="DataScienceIILectureBook.css" rel="stylesheet" type="text/css"/>
  <meta content="DataScienceIILectureBook.tex" name="src"/>
  <script async="async" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/mml-chtml.js" type="text/javascript">
  </script>
  <link href="flatweb.css" rel="stylesheet" type="text/css"/>
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.7.1/jquery.min.js">
  </script>
  <script src="flatjs.js">
  </script>
  <link href="http://fonts.googleapis.com/css?family=Roboto" rel="stylesheet" type="text/css"/>
  <script type="module">
   import pdfjsDist from 'https://cdn.jsdelivr.net/npm/pdfjs-dist@5.3.93/+esm'
  </script>
  <script>
   function moveTOC() { 
var htmlShow = document.getElementsByClassName("wide"); 
if (htmlShow[0].style.display === "none") { 
htmlShow[0].style.display = "block"; 
} else { 
htmlShow[0].style.display = "none"; 
} 
}
  </script>
  <link href="https://unpkg.com/nord-highlightjs@0.1.0/dist/nord.css" rel="stylesheet" type="text/css"/>
  <script src="https://unpkg.com/@highlightjs/cdn-assets@11.3.1/highlight.min.js">
  </script>
  <script src="https://unpkg.com/highlightjs-copy/dist/highlightjs-copy.min.js">
  </script>
  <link href="https://unpkg.com/highlightjs-copy/dist/highlightjs-copy.min.css" rel="stylesheet"/>
  <script src="flatjs.js">
  </script>
  <header>
  </header>
 </head>
 <body id="top">
  <nav class="wide">
   <div class="contents">
    <h3 style="font-weight:lighter; padding: 20px 0 20px 0;">
     6
   
     
   

   Introduction to Artificial Neural Networks
    </h3>
    <h4 style="font-weight:lighter">
     Chapter Table of Contents
    </h4>
    <ul>
     <div class="chapterTOCS">
      <li>
       <span class="sectionToc">
        <small>
         6.1
        </small>
        <a href="#x8-850006.1">
         Introduction
        </a>
       </span>
      </li>
      <li>
       <span class="sectionToc">
        <small>
         6.2
        </small>
        <a href="#x8-860006.2">
         From Biology to Silicon: Artificial Neurons
        </a>
       </span>
      </li>
      <li>
       <span class="subsectionToc" style="font-size:12px;margin:0px;">
        <small>
         6.2.1
        </small>
        <a href="#x8-870006.2.1">
         Biological Neurons
        </a>
       </span>
      </li>
      <li>
       <span class="subsectionToc" style="font-size:12px;margin:0px;">
        <small>
         6.2.2
        </small>
        <a href="#x8-880006.2.2">
         Logical Computations with Neurons
        </a>
       </span>
      </li>
      <li>
       <span class="subsectionToc" style="font-size:12px;margin:0px;">
        <small>
         6.2.3
        </small>
        <a href="#x8-890006.2.3">
         The Perceptron
        </a>
       </span>
      </li>
      <li>
       <span class="subsectionToc" style="font-size:12px;margin:0px;">
        <small>
         6.2.4
        </small>
        <a href="#x8-900006.2.4">
         Multilayer Perceptron and Backpropagation
        </a>
       </span>
      </li>
      <li>
       <span class="subsectionToc" style="font-size:12px;margin:0px;">
        <small>
         6.2.5
        </small>
        <a href="#x8-910006.2.5">
         Regression MLPs
        </a>
       </span>
      </li>
      <li>
       <span class="subsectionToc" style="font-size:12px;margin:0px;">
        <small>
         6.2.6
        </small>
        <a href="#x8-920006.2.6">
         Classification MLPs
        </a>
       </span>
      </li>
      <li>
       <span class="sectionToc">
        <small>
         6.3
        </small>
        <a href="#x8-930006.3">
         Implementing MLPs with Keras
        </a>
       </span>
      </li>
      <li>
       <span class="subsectionToc" style="font-size:12px;margin:0px;">
        <small>
         6.3.1
        </small>
        <a href="#x8-940006.3.1">
         Building an Image Classifier Using Sequential API
        </a>
       </span>
      </li>
      <li>
       <span class="subsectionToc" style="font-size:12px;margin:0px;">
        <small>
         6.3.2
        </small>
        <a href="#x8-960006.3.2">
         Creating the model using the sequential API
        </a>
       </span>
      </li>
      <li>
       <span class="subsectionToc" style="font-size:12px;margin:0px;">
        <small>
         6.3.3
        </small>
        <a href="#x8-1000006.3.3">
         Building a Regression MLP Using the Sequential API
        </a>
       </span>
      </li>
     </div>
    </ul>
    <div class="prev-next" style="text-align: center">
     <p class="noindent">
      <a href="DataScienceIILectureBookch7.html" style="float:right; font-size:10px">
       NEXT →
      </a>
      <a href="DataScienceIILectureBookch5.html" style="float:left; font-size:10px">
       ← PREV
      </a>
      <a href="index.html">
       TOP
      </a>
     </p>
    </div>
    <div id="author-info" style="bottom: 0; padding-bottom: 10px;">
     <div id="author-logo">
      <img src="figures/logo.svg" style="margin: 10px 0;"/>
      <div>
       <div>
        <h4 style="color:#7aa0b8; font-weight:lighter;">
         WebBook | D. T. McGuiness, P.hD
        </h4>
       </div>
      </div>
      <div id="author-text" style="bottom: 0; padding-top: 50px;border-top: solid 1px #3b4b5e;font-size: 12px;text-align: right;">
       <p>
        <b>
         Authors Note
        </b>
        The website you are viewing is auto-generated
    using ParSnip and therefore subject to slight errors in
    typography and formatting. When in doubt, please consult the
    LectureBook.
       </p>
      </div>
     </div>
    </div>
   </div>
  </nav>
  <div class="page">
   <article class="chapter">
    <h1 class="chapterHead">
     <div class="number">
      6
     </div>
     <a id="x8-840006">
     </a>
     Introduction to Artificial Neural Networks
    </h1><button id='toc-button' onclick='moveTOC()'>TOC</button>
    <div class="section-control" style="text-align: center">
     <a id="prev-section" onclick="goPrev()" style="float:left;">
      ← Previous Section
     </a>
     <a id="next-section" onclick="goNext()" style="float:right;">
      Next Section →
     </a>
    </div>
    <h2 class="sectionHead">
     <a aria-label="Link to this section in PDF version" class="pdf-link" href="DataScienceIILectureBook.pdf#section.6.1" style="color: red;  position: absolute;  left: -30px;" target="_blank">
      <svg class="bi bi-file-pdf" fill="currentColor" height="20" viewbox="0 0 16 16" width="20" xmlns="http://www.w3.org/2000/svg">
       <path d="M4 0a2 2 0 0 0-2 2v12a2 2 0 0 0 2 2h8a2 2 0 0 0 2-2V2a2 2 0 0 0-2-2H4zm0 1h8a1 1 0 0 1 1 1v12a1 1 0 0 1-1 1H4a1 1 0 0 1-1-1V2a1 1 0 0 1 1-1z">
       </path>
       <path d="M4.603 12.087a.81.81 0 0 1-.438-.42c-.195-.388-.13-.776.08-1.102.198-.307.526-.568.897-.787a7.68 7.68 0 0 1 1.482-.645 19.701 19.701 0 0 0 1.062-2.227 7.269 7.269 0 0 1-.43-1.295c-.086-.4-.119-.796-.046-1.136.075-.354.274-.672.65-.823.192-.077.4-.12.602-.077a.7.7 0 0 1 .477.365c.088.164.12.356.127.538.007.187-.012.395-.047.614-.084.51-.27 1.134-.52 1.794a10.954 10.954 0 0 0 .98 1.686 5.753 5.753 0 0 1 1.334.05c.364.065.734.195.96.465.12.144.193.32.2.518.007.192-.047.382-.138.563a1.04 1.04 0 0 1-.354.416.856.856 0 0 1-.51.138c-.331-.014-.654-.196-.933-.417a5.716 5.716 0 0 1-.911-.95 11.642 11.642 0 0 0-1.997.406 11.311 11.311 0 0 1-1.021 1.51c-.29.35-.608.655-.926.787a.793.793 0 0 1-.58.029zm1.379-1.901c-.166.076-.32.156-.459.238-.328.194-.541.383-.647.547-.094.145-.096.25-.04.361.01.022.02.036.026.044a.27.27 0 0 0 .035-.012c.137-.056.355-.235.635-.572a8.18 8.18 0 0 0 .45-.606zm1.64-1.33a12.647 12.647 0 0 1 1.01-.193 11.666 11.666 0 0 1-.51-.858 20.741 20.741 0 0 1-.5 1.05zm2.446.45c.15.162.296.3.435.41.24.19.407.253.498.256a.107.107 0 0 0 .07-.015.307.307 0 0 0 .094-.125.436.436 0 0 0 .059-.2.095.095 0 0 0-.026-.063c-.052-.062-.2-.152-.518-.209a3.881 3.881 0 0 0-.612-.053zM8.078 5.8a6.7 6.7 0 0 0 .2-.828c.031-.188.043-.343.038-.465a.613.613 0 0 0-.032-.198.517.517 0 0 0-.145.04c-.087.035-.158.106-.196.283-.04.192-.03.469.046.822.024.111.054.227.09.346z">
       </path>
      </svg>
     </a>
     <small class="titlemark">
      6.1
     </small>
     <a id="x8-850006.1">
     </a>
     Introduction
    </h2>
    <p class="noindent">
     It is quite apparent that life imitates life and engineers are inspired by nature. It seems only
logical, then, to look at the brain’s architecture for inspiration on how to build an intelligent
machine.
    </p>
    <div class="figure">
     <p class="noindent">
      <img alt="PIC" height="" src="figures/Introduction-to-Artificial-Neural-Networks/raster/nature-robin.jpg" width="150%"/>
      <a id="x8-85001r1">
      </a>
     </p>
     <figcaption class="caption">
      <span class="id">
       Figure 6.1:
      </span>
      <span class="content">
       Nature  always  is  a  great  source  of  inspiration  for  good  design.  For  example,  the  beak  of  a  bird  is
         aerodynamically efficient and was used in designing the Bullet train <a href="#X0-BBC2019">[23]</a>.The field is of emulating models,
         systems, and elements of nature for the purpose of solving complex human problems is called biomimetics <a href="#X0-vincent2006biomimetics">[24]</a>.
      </span>
     </figcaption>
    </div>
    <p class="noindent">
     This is the logic that sparked
     <a class="glossary" href="DataScienceIILectureBookli2.html#glo:ann">
      Artificial Neural Networks (ANN)
     </a>
     s,
     <a class="glossary" href="DataScienceIILectureBookli2.html#glo:ml">
      ML
     </a>
     models inspired by the networks
of biological neurons found in our brains. However, although planes were inspired by birds,
they don’t have to flap their wings to fly. Similarly,
     <a class="glossary" href="DataScienceIILectureBookli2.html#glo:ann">
      ANN
     </a>
     s have gradually become quite
different from their biological cousins. Some researchers even argue that we should drop
the biological analogy altogether such as calling them
     <alert style="color: #821131;">
      units
     </alert>
     rather than
     <alert style="color: #821131;">
      neurons
     </alert> <a id="x8-85004"></a><a href="#X0-agatonovic2000basic">[25]</a>, as
some consider this naming to decrease the amount of creativity we can give to the topic
.
    </p>
    <p class="noindent">
     <a class="glossary" href="DataScienceIILectureBookli2.html#glo:ann">
      ANN
     </a>
     s are at the very core of
     <span id="bold" style="font-weight:bold;">
      deep learning
     </span>
     . They are versatile, powerful, and scalable,
making them ideal to tackle large and highly complex
     <a class="glossary" href="DataScienceIILectureBookli2.html#glo:ml">
      ML
     </a>
     tasks such as classifying billions
of images (e.g., Google Images), powering speech recognition services (e.g., Apple’s Siri),
recommending the best videos to watch to hundreds of millions of users every day (e.g.,
YouTube), or learning to beat the world champion at the game of Go (DeepMind’s AlphaGo <a id="x8-85005"></a><a href="#X0-holcomb2018overview">[26]</a>).
    </p>
    <p class="noindent">
     The will treat this chapter as a formal introduction to
     <a class="glossary" href="DataScienceIILectureBookli2.html#glo:ann">
      ANN
     </a>, starting with a tour of the very
first
     <a class="glossary" href="DataScienceIILectureBookli2.html#glo:ann">
      ANN
     </a>
     architectures and leading up to multilayer perceptrons, which are heavily used
today.
    </p>
    <p class="noindent">
     In the second part, we will look at how to implement neural networks using TensorFlow’s Keras API.
This is a beautifully designed and simple high-level API for building, training, evaluating, and running
neural networks. While it may look simple at first glance, it is expressive and flexible enough to let you
build a wide variety of neural network architectures.
    </p>
    <div class="knowledge">
     <p class="noindent">
      For most of your use cases, using <span style="color:#054C5C;"><code class="verb">keras</code></span>
      will be enough.
     </p>
    </div>
    <div class="figure">
     <p class="noindent">
      <img alt="PIC" height="" src="figures/Introduction-to-Artificial-Neural-Networks/raster/alpha-go.jpg" width="150%"/>
      <a id="x8-85006r2">
      </a>
     </p>
     <figcaption class="caption">
      <span class="id">
       Figure 6.2:
      </span>
      <span class="content">
       The prolific advancements of computers and neural networks have allowed us to tackle problems once
         deemed impossible. A game of GO requires uncountable amount of moves, yet using ML it was possible to
         create a software capable of beating the world champion.
      </span>
     </figcaption>
    </div>
    <p class="noindent">
    </p>
    <h2 class="sectionHead">
     <a aria-label="Link to this section in PDF version" class="pdf-link" href="DataScienceIILectureBook.pdf#section.6.2" style="color: red;  position: absolute;  left: -30px;" target="_blank">
      <svg class="bi bi-file-pdf" fill="currentColor" height="20" viewbox="0 0 16 16" width="20" xmlns="http://www.w3.org/2000/svg">
       <path d="M4 0a2 2 0 0 0-2 2v12a2 2 0 0 0 2 2h8a2 2 0 0 0 2-2V2a2 2 0 0 0-2-2H4zm0 1h8a1 1 0 0 1 1 1v12a1 1 0 0 1-1 1H4a1 1 0 0 1-1-1V2a1 1 0 0 1 1-1z">
       </path>
       <path d="M4.603 12.087a.81.81 0 0 1-.438-.42c-.195-.388-.13-.776.08-1.102.198-.307.526-.568.897-.787a7.68 7.68 0 0 1 1.482-.645 19.701 19.701 0 0 0 1.062-2.227 7.269 7.269 0 0 1-.43-1.295c-.086-.4-.119-.796-.046-1.136.075-.354.274-.672.65-.823.192-.077.4-.12.602-.077a.7.7 0 0 1 .477.365c.088.164.12.356.127.538.007.187-.012.395-.047.614-.084.51-.27 1.134-.52 1.794a10.954 10.954 0 0 0 .98 1.686 5.753 5.753 0 0 1 1.334.05c.364.065.734.195.96.465.12.144.193.32.2.518.007.192-.047.382-.138.563a1.04 1.04 0 0 1-.354.416.856.856 0 0 1-.51.138c-.331-.014-.654-.196-.933-.417a5.716 5.716 0 0 1-.911-.95 11.642 11.642 0 0 0-1.997.406 11.311 11.311 0 0 1-1.021 1.51c-.29.35-.608.655-.926.787a.793.793 0 0 1-.58.029zm1.379-1.901c-.166.076-.32.156-.459.238-.328.194-.541.383-.647.547-.094.145-.096.25-.04.361.01.022.02.036.026.044a.27.27 0 0 0 .035-.012c.137-.056.355-.235.635-.572a8.18 8.18 0 0 0 .45-.606zm1.64-1.33a12.647 12.647 0 0 1 1.01-.193 11.666 11.666 0 0 1-.51-.858 20.741 20.741 0 0 1-.5 1.05zm2.446.45c.15.162.296.3.435.41.24.19.407.253.498.256a.107.107 0 0 0 .07-.015.307.307 0 0 0 .094-.125.436.436 0 0 0 .059-.2.095.095 0 0 0-.026-.063c-.052-.062-.2-.152-.518-.209a3.881 3.881 0 0 0-.612-.053zM8.078 5.8a6.7 6.7 0 0 0 .2-.828c.031-.188.043-.343.038-.465a.613.613 0 0 0-.032-.198.517.517 0 0 0-.145.04c-.087.035-.158.106-.196.283-.04.192-.03.469.046.822.024.111.054.227.09.346z">
       </path>
      </svg>
     </a>
     <small class="titlemark">
      6.2
     </small>
     <a id="x8-860006.2">
     </a>
     From Biology to Silicon: Artificial Neurons
    </h2>
    <p class="noindent">
     While it may seem they are the cutting edge in
     <a class="glossary" href="DataScienceIILectureBookli2.html#glo:ml">
      ML
     </a>,
     <a class="glossary" href="DataScienceIILectureBookli2.html#glo:ann">
      ANN
     </a>
     s have been around for quite a while: they
were first introduced back in 1943 by the neurophysiologist Warren McCulloch and the mathematician
Walter Pitts. In their landmark paper
     <italic>
      A Logical Calculus of Ideas Immanent in Nervous Activity
     </italic>
     , They
presented a simplified computational model of how biological neurons might work together in animal
brains to perform complex computations using propositional logic. This was the first artificial neural
network architecture <a id="x8-86001"></a><a href="#X0-mcculloch1943logical">[27]</a>.
    </p>
    <p class="noindent">
     Since then many other architectures have been invented. The early successes of
     <a class="glossary" href="DataScienceIILectureBookli2.html#glo:ann">
      ANN
     </a>
     s led to the
widespread belief that we would soon be conversing with truly intelligent machines. When
it became clear in the 1960s that this promise would go unfulfilled (at least for quite a
while), funding flew elsewhere, and
     <a class="glossary" href="DataScienceIILectureBookli2.html#glo:ann">
      ANN
     </a>
     s entered a long winter. This is also known as the
     <alert style="color: #821131;">
      <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
       <msup>
        <mrow>
         <mstyle class="text">
          <mtext>
           1
          </mtext>
         </mstyle>
        </mrow>
        <mrow>
         <mstyle class="text">
          <mtext>
           st
          </mtext>
         </mstyle>
        </mrow>
       </msup>
      </math>
      AI
winter
     </alert> <a id="x8-86002"></a><a href="#X0-howe2007artificial">[28]</a>. In the early 1980s, new architectures were invented and better training techniques were
developed, sparking a revival of interest in connectionism, the study of neural networks. But progress
was slow, and by the 1990s other powerful
     <a class="glossary" href="DataScienceIILectureBookli2.html#glo:ml">
      ML
     </a>
     techniques had been invented, such as
     <a class="glossary" href="DataScienceIILectureBookli2.html#glo:svm">
      SVM
     </a>.
These techniques seemed to offer better results and stronger theoretical foundations than
     <a class="glossary" href="DataScienceIILectureBookli2.html#glo:ann">
      ANN
     </a>
     s, so once again the study of neural networks was put on hold and this is known as the
     <alert style="color: #821131;">
      <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
       <msup>
        <mrow>
         <mstyle class="text">
          <mtext>
           2
          </mtext>
         </mstyle>
        </mrow>
        <mrow>
         <mstyle class="text">
          <mtext>
           nd
          </mtext>
         </mstyle>
        </mrow>
       </msup>
      </math>
      AI
winter
     </alert>
     .
    </p>
    <p class="noindent">
     We are now witnessing yet another wave of interest in
     <a class="glossary" href="DataScienceIILectureBookli2.html#glo:ann">
      ANN
     </a>
     s. Will this wave die out like the
previous ones did? Well, here are a few good reasons to believe that this time is different
and that the renewed interest in
     <a class="glossary" href="DataScienceIILectureBookli2.html#glo:ann">
      ANN
     </a>
     s will have a much more profound impact on our
lives:
    </p>
    <p class="noindent">
     There is now a
     <span id="bold" style="font-weight:bold;">
      huge quantity of data available
     </span>
     to train neural networks, and
     <a class="glossary" href="DataScienceIILectureBookli2.html#glo:ann">
      ANN
     </a>
     s frequently
outperform other ML techniques on very large and complex problems. One of the major turning points
of
     <a class="glossary" href="DataScienceIILectureBookli2.html#glo:ann">
      ANN
     </a>
     was the fundamental question of:
    </p>
    <p class="noindent">
    </p>
    <div class="quoteblock">
     <p class="noindent">
      <italic>
       Is our understanding of the model at fault or is it merely the lack of data to train?
      </italic>
     </p>
    </div>
    <p class="noindent">
     The tremendous increase in computing power since the 1990s now makes it possible to train large
neural networks in a reasonable amount of time. This is in part due to
     <span id="bold" style="font-weight:bold;">
      Moore’s law
     </span>
     , but also thanks
to the gaming industry, which has stimulated the production of powerful GPU cards by the millions
which have become the norm to train
     <a class="glossary" href="DataScienceIILectureBookli2.html#glo:ml">
      ML
     </a>
     instead of CPUs.
    </p>
    <div class="informationblock" id="tcolobox-66">
     <div class="title">
      <p class="noindent">
       <span id="bold" style="font-weight:bold;">
        Information
       </span>
       : Moore’s Law
      </p>
     </div>
     <div class="box-content">
      <p class="noindent">
       the number of components in integrated circuits has doubled about every 2 years over the last 50 years.
      </p>
     </div>
    </div>
    <p class="noindent">
     In addition to previous additions, cloud platforms have made this power accessible to everyone.
The training algorithms have been improved. To be fair they are only slightly different
                                                                                
                                                                                
from the ones used in the 1990s, but these relatively small tweaks have had a huge positive
impact.
    </p>
    <p class="noindent">
     Some theoretical limitations of
     <a class="glossary" href="DataScienceIILectureBookli2.html#glo:ann">
      ANN
     </a>
     s have turned out to be benign in practice. For example, many
people thought that
     <a class="glossary" href="DataScienceIILectureBookli2.html#glo:ann">
      ANN
     </a>
     training algorithms were doomed because they were likely to
get stuck in local optima <a id="x8-86003"></a><a href="#X0-goodfellow2014qualitatively">[29]</a>, but it turns out that this is not a big problem in practice,
especially for larger neural networks: the local optima often perform almost as well as the global
optimum.
    </p>
    <p class="noindent">
     <a class="glossary" href="DataScienceIILectureBookli2.html#glo:ann">
      ANN
     </a>
     s seem to have entered a virtuous circle of funding and progress. Amazing products
based on
     <a class="glossary" href="DataScienceIILectureBookli2.html#glo:ann">
      ANN
     </a>
     s regularly make the headline news, which pulls more and more attention
and funding toward them, resulting in more and more progress and even more amazing
products.
    </p>
    <h3 class="subsectionHead">
     <a aria-label="Link to this section in PDF version" class="pdf-link" href="DataScienceIILectureBook.pdf#subsection.6.2.1" style="color: red;  position: absolute;  left: -30px;" target="_blank">
      <svg class="bi bi-file-pdf" fill="currentColor" height="20" viewbox="0 0 16 16" width="20" xmlns="http://www.w3.org/2000/svg">
       <path d="M4 0a2 2 0 0 0-2 2v12a2 2 0 0 0 2 2h8a2 2 0 0 0 2-2V2a2 2 0 0 0-2-2H4zm0 1h8a1 1 0 0 1 1 1v12a1 1 0 0 1-1 1H4a1 1 0 0 1-1-1V2a1 1 0 0 1 1-1z">
       </path>
       <path d="M4.603 12.087a.81.81 0 0 1-.438-.42c-.195-.388-.13-.776.08-1.102.198-.307.526-.568.897-.787a7.68 7.68 0 0 1 1.482-.645 19.701 19.701 0 0 0 1.062-2.227 7.269 7.269 0 0 1-.43-1.295c-.086-.4-.119-.796-.046-1.136.075-.354.274-.672.65-.823.192-.077.4-.12.602-.077a.7.7 0 0 1 .477.365c.088.164.12.356.127.538.007.187-.012.395-.047.614-.084.51-.27 1.134-.52 1.794a10.954 10.954 0 0 0 .98 1.686 5.753 5.753 0 0 1 1.334.05c.364.065.734.195.96.465.12.144.193.32.2.518.007.192-.047.382-.138.563a1.04 1.04 0 0 1-.354.416.856.856 0 0 1-.51.138c-.331-.014-.654-.196-.933-.417a5.716 5.716 0 0 1-.911-.95 11.642 11.642 0 0 0-1.997.406 11.311 11.311 0 0 1-1.021 1.51c-.29.35-.608.655-.926.787a.793.793 0 0 1-.58.029zm1.379-1.901c-.166.076-.32.156-.459.238-.328.194-.541.383-.647.547-.094.145-.096.25-.04.361.01.022.02.036.026.044a.27.27 0 0 0 .035-.012c.137-.056.355-.235.635-.572a8.18 8.18 0 0 0 .45-.606zm1.64-1.33a12.647 12.647 0 0 1 1.01-.193 11.666 11.666 0 0 1-.51-.858 20.741 20.741 0 0 1-.5 1.05zm2.446.45c.15.162.296.3.435.41.24.19.407.253.498.256a.107.107 0 0 0 .07-.015.307.307 0 0 0 .094-.125.436.436 0 0 0 .059-.2.095.095 0 0 0-.026-.063c-.052-.062-.2-.152-.518-.209a3.881 3.881 0 0 0-.612-.053zM8.078 5.8a6.7 6.7 0 0 0 .2-.828c.031-.188.043-.343.038-.465a.613.613 0 0 0-.032-.198.517.517 0 0 0-.145.04c-.087.035-.158.106-.196.283-.04.192-.03.469.046.822.024.111.054.227.09.346z">
       </path>
      </svg>
     </a>
     <small class="titlemark">
      6.2.1
     </small>
     <a id="x8-870006.2.1">
     </a>
     Biological Neurons
    </h3>
    <p class="noindent">
     Before we discuss artificial neurons, let’s take a quick look at a biological neuron. It is an
unusual-looking cell mostly found in animal brains.
    </p>
    <div class="figure">
     <p class="noindent">
      <img alt="PIC" height="" src="figures/Introduction-to-Artificial-Neural-Networks/raster/neuron.png" width="150%"/>
      <a id="x8-87001r3">
      </a>
     </p>
     <figcaption class="caption">
      <span class="id">
       Figure 6.3:
      </span>
      <span class="content">
       A neuron or nerve cell is an excitable cell that fires electric signals called action potentials across a neural
         network in the nervous system. Neurons communicate with other cells via synapses, which are specialized
         connections that commonly use minute amounts of chemical neurotransmitters to pass the electric signal
         from the presynaptic neuron to the target cell through the synaptic gap <a href="#X0-sebe2023stepwise">[30]</a>.
      </span>
     </figcaption>
    </div>
    <p class="noindent">
     It’s composed of a cell body containing the nucleus and most of the cell’s complex components, many
branching extensions called dendrites, plus one very long extension called the axon. The axon’s length
may be just a few times longer than the cell body, or up to tens of thousands of times
longer.
    </p>
    <p class="noindent">
     Near its extremity the axon splits off into many branches called
     <italic>
      telodendria
     </italic>
     , and at the tip of these
branches are minuscule structures called synaptic terminals (or simply synapses), which are connected
to the dendrites or cell bodies of other neurons. Biological neurons produce short electrical impulses
called action potentials (APs, or just signals), which travel along the axons and make the synapses
release chemical signals called neurotransmitters. When a neuron receives a sufficient amount
of these neurotransmitters within a few milliseconds, it fires its own electrical impulses
(actually, it depends on the neurotransmitters, as some of them inhibit the neuron from
firing).
    </p>
    <p class="noindent">
     Therefore, individual biological neurons seem to behave in a simple way, but they’re organized
in a vast network of billions, with each neuron typically connected to thousands of other
neurons. Highly complex computations can be performed by a network of fairly simple
neurons, much like a complex anthill can emerge from the combined efforts of simple ants. The
architecture of biological neural networks (BNNs) is the subject of active research, but
some parts of the brain have been mapped <a id="x8-87003"></a><a href="#X0-barrett2019analyzing">[31]</a>. These efforts show that neurons are often
organized in consecutive layers, especially in the cerebral cortex (the outer layer of the
brain).
    </p>
    <div class="figure">
     <p class="noindent">
      <img alt="PIC" height="" src="figures/Introduction-to-Artificial-Neural-Networks/raster/neuron-layers.jpg" width="150%"/>
      <a id="x8-87004r4">
      </a>
     </p>
     <figcaption class="caption">
      <span class="id">
       Figure 6.4:
      </span>
      <span class="content">
       A cortical column is a group of neurons forming a cylindrical structure through the cerebral cortex of
         the brain perpendicular to the cortical surface. The structure was first identified by Vernon Benjamin
         Mountcastle in 1957. He later identified minicolumns as the basic units of the neocortex which were
         arranged  into  columns.  Each  contains  the  same  types  of  neurons,  connectivity,  and  firing  properties.
         Columns are also called hypercolumn, macrocolumn, functional column or sometimes cortical module
      </span>
     </figcaption>
    </div>
    <h3 class="subsectionHead">
     <a aria-label="Link to this section in PDF version" class="pdf-link" href="DataScienceIILectureBook.pdf#subsection.6.2.2" style="color: red;  position: absolute;  left: -30px;" target="_blank">
      <svg class="bi bi-file-pdf" fill="currentColor" height="20" viewbox="0 0 16 16" width="20" xmlns="http://www.w3.org/2000/svg">
       <path d="M4 0a2 2 0 0 0-2 2v12a2 2 0 0 0 2 2h8a2 2 0 0 0 2-2V2a2 2 0 0 0-2-2H4zm0 1h8a1 1 0 0 1 1 1v12a1 1 0 0 1-1 1H4a1 1 0 0 1-1-1V2a1 1 0 0 1 1-1z">
       </path>
       <path d="M4.603 12.087a.81.81 0 0 1-.438-.42c-.195-.388-.13-.776.08-1.102.198-.307.526-.568.897-.787a7.68 7.68 0 0 1 1.482-.645 19.701 19.701 0 0 0 1.062-2.227 7.269 7.269 0 0 1-.43-1.295c-.086-.4-.119-.796-.046-1.136.075-.354.274-.672.65-.823.192-.077.4-.12.602-.077a.7.7 0 0 1 .477.365c.088.164.12.356.127.538.007.187-.012.395-.047.614-.084.51-.27 1.134-.52 1.794a10.954 10.954 0 0 0 .98 1.686 5.753 5.753 0 0 1 1.334.05c.364.065.734.195.96.465.12.144.193.32.2.518.007.192-.047.382-.138.563a1.04 1.04 0 0 1-.354.416.856.856 0 0 1-.51.138c-.331-.014-.654-.196-.933-.417a5.716 5.716 0 0 1-.911-.95 11.642 11.642 0 0 0-1.997.406 11.311 11.311 0 0 1-1.021 1.51c-.29.35-.608.655-.926.787a.793.793 0 0 1-.58.029zm1.379-1.901c-.166.076-.32.156-.459.238-.328.194-.541.383-.647.547-.094.145-.096.25-.04.361.01.022.02.036.026.044a.27.27 0 0 0 .035-.012c.137-.056.355-.235.635-.572a8.18 8.18 0 0 0 .45-.606zm1.64-1.33a12.647 12.647 0 0 1 1.01-.193 11.666 11.666 0 0 1-.51-.858 20.741 20.741 0 0 1-.5 1.05zm2.446.45c.15.162.296.3.435.41.24.19.407.253.498.256a.107.107 0 0 0 .07-.015.307.307 0 0 0 .094-.125.436.436 0 0 0 .059-.2.095.095 0 0 0-.026-.063c-.052-.062-.2-.152-.518-.209a3.881 3.881 0 0 0-.612-.053zM8.078 5.8a6.7 6.7 0 0 0 .2-.828c.031-.188.043-.343.038-.465a.613.613 0 0 0-.032-.198.517.517 0 0 0-.145.04c-.087.035-.158.106-.196.283-.04.192-.03.469.046.822.024.111.054.227.09.346z">
       </path>
      </svg>
     </a>
     <small class="titlemark">
      6.2.2
     </small>
     <a id="x8-880006.2.2">
     </a>
     Logical Computations with Neurons
    </h3>
    <p class="noindent">
     McCulloch and Pitts proposed a very simple model of the biological neuron, which later became known
as an
     <span id="bold" style="font-weight:bold;">
      artificial neuron
     </span>
     : it has one or more binary (on/off) inputs and one binary output. The artificial
neuron activates its output when more than a certain number of its inputs are active. In their paper,
McCulloch and Pitts showed that even with such a simplified model it is possible to build a network of
artificial neurons that can compute any logical proposition you want. To see how such a network works,
let’s build a few
     <a class="glossary" href="DataScienceIILectureBookli2.html#glo:ann">
      ANN
     </a>
     s that perform various logical computationsm, assuming that a neuron is
activated when at least two of its input connections are active. Let’s see what these networks
do:
    </p>
    <ul class="itemize1">
     <li class="itemize">
      <p class="noindent">
       The first network on the left is the identity function: if neuron
       <span id="bold" style="font-weight:bold;">
        A
       </span>
       is activated, then neuron
       <span id="bold" style="font-weight:bold;">
        C
       </span>
       gets activated as well (since it receives two input signals from neuron
       <span id="bold" style="font-weight:bold;">
        A
       </span>
       ); but if neuron
       <span id="bold" style="font-weight:bold;">
        A
       </span>
       is off, then neuron
       <span id="bold" style="font-weight:bold;">
        C
       </span>
       is off as well.
      </p>
     </li>
     <li class="itemize">
      <p class="noindent">
       The second network performs a logical
       <span id="bold" style="font-weight:bold;">
        AND
       </span>
       : neuron
       <span id="bold" style="font-weight:bold;">
        C
       </span>
       is activated only when both neurons
       <span id="bold" style="font-weight:bold;">
        A
       </span>
       and
       <span id="bold" style="font-weight:bold;">
        B
       </span>
       are activated (a single input signal is not enough to activate neuron
       <span id="bold" style="font-weight:bold;">
        C
       </span>
       ).
      </p>
     </li>
     <li class="itemize">
      <p class="noindent">
       The third network performs a logical
       <span id="bold" style="font-weight:bold;">
        OR
       </span>
       : neuron
       <span id="bold" style="font-weight:bold;">
        C
       </span>
       gets activated if either neuron
       <span id="bold" style="font-weight:bold;">
        A
       </span>
       or
neuron
       <span id="bold" style="font-weight:bold;">
        B
       </span>
       is activated (or both).
      </p>
     </li>
     <li class="itemize">
      <p class="noindent">
       Finally, if we suppose that an input connection can inhibit the neuron’s activity (which
is the case with biological neurons), then the fourth network computes a slightly more
complex logical proposition: neuron C is activated only if neuron A is active and neuron
B is off. If neuron A is active all the time, then you get a logical NOT: neuron C is active
when neuron B is off, and vice versa.
      </p>
     </li>
    </ul>
    <p class="noindent">
     You can imagine how these networks can be combined to compute complex logical expressions.
    </p>
    <h3 class="subsectionHead">
     <a aria-label="Link to this section in PDF version" class="pdf-link" href="DataScienceIILectureBook.pdf#subsection.6.2.3" style="color: red;  position: absolute;  left: -30px;" target="_blank">
      <svg class="bi bi-file-pdf" fill="currentColor" height="20" viewbox="0 0 16 16" width="20" xmlns="http://www.w3.org/2000/svg">
       <path d="M4 0a2 2 0 0 0-2 2v12a2 2 0 0 0 2 2h8a2 2 0 0 0 2-2V2a2 2 0 0 0-2-2H4zm0 1h8a1 1 0 0 1 1 1v12a1 1 0 0 1-1 1H4a1 1 0 0 1-1-1V2a1 1 0 0 1 1-1z">
       </path>
       <path d="M4.603 12.087a.81.81 0 0 1-.438-.42c-.195-.388-.13-.776.08-1.102.198-.307.526-.568.897-.787a7.68 7.68 0 0 1 1.482-.645 19.701 19.701 0 0 0 1.062-2.227 7.269 7.269 0 0 1-.43-1.295c-.086-.4-.119-.796-.046-1.136.075-.354.274-.672.65-.823.192-.077.4-.12.602-.077a.7.7 0 0 1 .477.365c.088.164.12.356.127.538.007.187-.012.395-.047.614-.084.51-.27 1.134-.52 1.794a10.954 10.954 0 0 0 .98 1.686 5.753 5.753 0 0 1 1.334.05c.364.065.734.195.96.465.12.144.193.32.2.518.007.192-.047.382-.138.563a1.04 1.04 0 0 1-.354.416.856.856 0 0 1-.51.138c-.331-.014-.654-.196-.933-.417a5.716 5.716 0 0 1-.911-.95 11.642 11.642 0 0 0-1.997.406 11.311 11.311 0 0 1-1.021 1.51c-.29.35-.608.655-.926.787a.793.793 0 0 1-.58.029zm1.379-1.901c-.166.076-.32.156-.459.238-.328.194-.541.383-.647.547-.094.145-.096.25-.04.361.01.022.02.036.026.044a.27.27 0 0 0 .035-.012c.137-.056.355-.235.635-.572a8.18 8.18 0 0 0 .45-.606zm1.64-1.33a12.647 12.647 0 0 1 1.01-.193 11.666 11.666 0 0 1-.51-.858 20.741 20.741 0 0 1-.5 1.05zm2.446.45c.15.162.296.3.435.41.24.19.407.253.498.256a.107.107 0 0 0 .07-.015.307.307 0 0 0 .094-.125.436.436 0 0 0 .059-.2.095.095 0 0 0-.026-.063c-.052-.062-.2-.152-.518-.209a3.881 3.881 0 0 0-.612-.053zM8.078 5.8a6.7 6.7 0 0 0 .2-.828c.031-.188.043-.343.038-.465a.613.613 0 0 0-.032-.198.517.517 0 0 0-.145.04c-.087.035-.158.106-.196.283-.04.192-.03.469.046.822.024.111.054.227.09.346z">
       </path>
      </svg>
     </a>
     <small class="titlemark">
      6.2.3
     </small>
     <a id="x8-890006.2.3">
     </a>
     The Perceptron
    </h3>
    <p class="noindent">
     The perceptron is one of the simplest
     <a class="glossary" href="DataScienceIILectureBookli2.html#glo:ann">
      ANN
     </a>
     architectures, invented in 1957 by Frank Rosenblatt <a id="x8-89001"></a><a href="#X0-block1962perceptron">[32]</a>. It
is based on a slightly different artificial neuron called a
     <a class="glossary" href="DataScienceIILectureBookli2.html#glo:tlu">
      Threshold Logic Unit (TLU)
     </a>, or sometimes a
     <a class="glossary" href="DataScienceIILectureBookli2.html#glo:ltu">
      Linear Threshold Unit (LTU)
     </a>
     which can be seen in Fig.
     <a href="#x8-89002r5">
      6.5
     </a>. The inputs and output are numbers (this is
instead of binary on/off values), and each input connection is associated with a
     <alert style="color: #821131;">
      weight
     </alert>
     . The
     <a class="glossary" href="DataScienceIILectureBookli2.html#glo:tlu">
      TLU
     </a>
     first
computes a linear function of its inputs:
    </p>
    <table class="equation-star">
     <tr>
      <td>
       <math class="equation" display="block" xmlns="http://www.w3.org/1998/Math/MathML">
        <mi>
         z
        </mi>
        <mo class="MathClass-rel" stretchy="false">
         =
        </mo>
        <msubsup>
         <mrow>
          <mi>
           w
          </mi>
         </mrow>
         <mrow>
          <mn>
           1
          </mn>
         </mrow>
         <mrow>
         </mrow>
        </msubsup>
        <msubsup>
         <mrow>
          <mi>
           x
          </mi>
         </mrow>
         <mrow>
          <mn>
           1
          </mn>
         </mrow>
         <mrow>
         </mrow>
        </msubsup>
        <mo class="MathClass-bin" stretchy="false">
         +
        </mo>
        <msubsup>
         <mrow>
          <mi>
           w
          </mi>
         </mrow>
         <mrow>
          <mn>
           2
          </mn>
         </mrow>
         <mrow>
         </mrow>
        </msubsup>
        <msubsup>
         <mrow>
          <mi>
           x
          </mi>
         </mrow>
         <mrow>
          <mn>
           2
          </mn>
         </mrow>
         <mrow>
         </mrow>
        </msubsup>
        <mo class="MathClass-bin" stretchy="false">
         +
        </mo>
        <mo class="MathClass-rel" stretchy="false">
         ⋯
        </mo>
        <mo class="MathClass-bin" stretchy="false">
         +
        </mo>
        <msubsup>
         <mrow>
          <mi>
           w
          </mi>
         </mrow>
         <mrow>
          <mi>
           n
          </mi>
         </mrow>
         <mrow>
         </mrow>
        </msubsup>
        <msubsup>
         <mrow>
          <mi>
           x
          </mi>
         </mrow>
         <mrow>
          <mi>
           n
          </mi>
         </mrow>
         <mrow>
         </mrow>
        </msubsup>
        <mo class="MathClass-bin" stretchy="false">
         +
        </mo>
        <mi>
         b
        </mi>
        <mo class="MathClass-rel" stretchy="false">
         =
        </mo>
        <msubsup>
         <mrow>
          <mover accent="true">
           <mrow>
            <mi>
             x
            </mi>
           </mrow>
           <mo accent="true">
            →
           </mo>
          </mover>
         </mrow>
         <mrow>
         </mrow>
         <mrow>
          <mi>
           T
          </mi>
         </mrow>
        </msubsup>
        <mover accent="true">
         <mrow>
          <mi>
           w
          </mi>
         </mrow>
         <mo accent="true">
          →
         </mo>
        </mover>
        <mo class="MathClass-bin" stretchy="false">
         +
        </mo>
        <mi>
         b
        </mi>
       </math>
      </td>
     </tr>
    </table>
    <p class="noindent">
     Then it applies a
     <span id="bold" style="font-weight:bold;">
      step function
     </span>
     to the result:
    </p>
    <table class="equation-star">
     <tr>
      <td>
       <math class="equation" display="block" xmlns="http://www.w3.org/1998/Math/MathML">
        <mi>
         h
        </mi>
        <msup>
         <mrow>
          <mrow>
           <mo fence="true" form="prefix">
            (
           </mo>
           <mrow>
            <mi>
             x
            </mi>
           </mrow>
           <mo fence="true" form="postfix">
            )
           </mo>
          </mrow>
         </mrow>
         <mrow>
         </mrow>
        </msup>
        <mo class="MathClass-rel" stretchy="false">
         =
        </mo>
        <mstyle class="text">
         <mtext>
          step
         </mtext>
        </mstyle>
        <msup>
         <mrow>
          <mrow>
           <mo fence="true" form="prefix">
            (
           </mo>
           <mrow>
            <mi>
             z
            </mi>
           </mrow>
           <mo fence="true" form="postfix">
            )
           </mo>
          </mrow>
         </mrow>
         <mrow>
         </mrow>
        </msup>
        <mspace class="qquad" width="2em">
        </mspace>
        <mstyle class="text">
         <mtext>
          where
         </mtext>
        </mstyle>
        <mspace class="qquad" width="2em">
        </mspace>
        <mspace class="qquad" width="2em">
        </mspace>
        <mi>
         z
        </mi>
        <mo class="MathClass-rel" stretchy="false">
         =
        </mo>
        <msubsup>
         <mrow>
          <mover accent="true">
           <mrow>
            <mi>
             x
            </mi>
           </mrow>
           <mo accent="true">
            →
           </mo>
          </mover>
         </mrow>
         <mrow>
         </mrow>
         <mrow>
          <mi>
           T
          </mi>
         </mrow>
        </msubsup>
        <mover accent="true">
         <mrow>
          <mi>
           w
          </mi>
         </mrow>
         <mo accent="true">
          →
         </mo>
        </mover>
        <mo class="MathClass-punc" stretchy="false">
         .
        </mo>
       </math>
      </td>
     </tr>
    </table>
    <p class="noindent">
     It is similar to logistic regression, except it uses a step function instead of the logistic function. Just
like in logistic regression, the model parameters are the input weights w and the bias term
     <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
      <mi>
       b
      </mi>
     </math>
     .
    </p>
    <div class="figure">
     <p class="noindent">
      <img alt="PIC" height="" src="figures/Introduction-to-Artificial-Neural-Networks/figures-1.svg" width="150%"/>
      <a id="x8-89002r5">
      </a>
     </p>
     <figcaption class="caption">
      <span class="id">
       Figure 6.5:
      </span>
      <span class="content">
       Threshold logic unit: an artificial neuron which computes a weighted sum of its inputs then applies a
         certain activation function.
      </span>
     </figcaption>
    </div>
    <p class="noindent">
     The most common step function used in perceptrons is the
     <span id="bold" style="font-weight:bold;">
      Heaviside step
     </span>
     and sometimes the
     <span id="bold" style="font-weight:bold;">
      sign
function
     </span>
     is used instead <a id="x8-89003"></a><a href="#X0-sharma2017activation">[33]</a>. 5
    </p>
    <table class="equation-star">
     <tr>
      <td>
       <math class="equation" display="block" xmlns="http://www.w3.org/1998/Math/MathML">
        <mstyle class="text">
         <mtext>
          heaviside
         </mtext>
        </mstyle>
        <msup>
         <mrow>
          <mrow>
           <mo fence="true" form="prefix">
            (
           </mo>
           <mrow>
            <mi>
             z
            </mi>
           </mrow>
           <mo fence="true" form="postfix">
            )
           </mo>
          </mrow>
         </mrow>
         <mrow>
         </mrow>
        </msup>
        <mo class="MathClass-rel" stretchy="false">
         =
        </mo>
        <mrow class="cases">
         <mrow>
          <mo fence="true" form="prefix">
           {
          </mo>
          <mrow>
           <mtable align="axis" class="array" columnlines="none" displaystyle="true" equalcolumns="false" equalrows="false" style="">
            <mtr class="array-row">
             <mtd class="array-td" columnalign="left">
              <mn>
               0
              </mn>
              <mspace class="qquad" width="2em">
              </mspace>
              <mspace class="quad" width="1em">
              </mspace>
             </mtd>
             <mtd class="array-td" columnalign="left">
              <mstyle class="text">
               <mtext>
                if
               </mtext>
              </mstyle>
              <mspace class="quad" width="1em">
              </mspace>
              <mi>
               z
              </mi>
              <mo class="MathClass-rel" stretchy="false">
               &lt;
              </mo>
              <mn>
               0
              </mn>
              <mo class="MathClass-punc" stretchy="false">
               ,
              </mo>
             </mtd>
            </mtr>
            <mtr class="array-row">
             <mtd class="array-td" columnalign="left">
              <mn>
               1
              </mn>
              <mspace class="qquad" width="2em">
              </mspace>
              <mspace class="quad" width="1em">
              </mspace>
             </mtd>
             <mtd class="array-td" columnalign="left">
              <mstyle class="text">
               <mtext>
                if
               </mtext>
              </mstyle>
              <mspace class="quad" width="1em">
              </mspace>
              <mi>
               z
              </mi>
              <mo class="MathClass-rel" stretchy="false">
               ≥
              </mo>
              <mn>
               0
              </mn>
              <mo class="MathClass-punc" stretchy="false">
               .
              </mo>
             </mtd>
            </mtr>
           </mtable>
          </mrow>
          <mo fence="true" form="postfix">
          </mo>
         </mrow>
        </mrow>
        <mspace class="qquad" width="2em">
        </mspace>
        <mstyle class="text">
         <mtext>
          sgn
         </mtext>
        </mstyle>
        <msup>
         <mrow>
          <mrow>
           <mo fence="true" form="prefix">
            (
           </mo>
           <mrow>
            <mi>
             z
            </mi>
           </mrow>
           <mo fence="true" form="postfix">
            )
           </mo>
          </mrow>
         </mrow>
         <mrow>
         </mrow>
        </msup>
        <mo class="MathClass-rel" stretchy="false">
         =
        </mo>
        <mrow class="cases">
         <mrow>
          <mo fence="true" form="prefix">
           {
          </mo>
          <mrow>
           <mtable align="axis" class="array" columnlines="none" displaystyle="true" equalcolumns="false" equalrows="false" style="">
            <mtr class="array-row">
             <mtd class="array-td" columnalign="left">
              <mo class="MathClass-bin" stretchy="false">
               −
              </mo>
              <mn>
               1
              </mn>
              <mspace class="qquad" width="2em">
              </mspace>
              <mspace class="quad" width="1em">
              </mspace>
             </mtd>
             <mtd class="array-td" columnalign="left">
              <mstyle class="text">
               <mtext>
                if
               </mtext>
              </mstyle>
              <mspace class="quad" width="1em">
              </mspace>
              <mi>
               z
              </mi>
              <mo class="MathClass-rel" stretchy="false">
               &lt;
              </mo>
              <mn>
               0
              </mn>
              <mo class="MathClass-punc" stretchy="false">
               ,
              </mo>
             </mtd>
            </mtr>
            <mtr class="array-row">
             <mtd class="array-td" columnalign="left">
              <mn>
               0
              </mn>
              <mspace class="qquad" width="2em">
              </mspace>
              <mspace class="quad" width="1em">
              </mspace>
             </mtd>
             <mtd class="array-td" columnalign="left">
              <mstyle class="text">
               <mtext>
                if
               </mtext>
              </mstyle>
              <mspace class="quad" width="1em">
              </mspace>
              <mi>
               z
              </mi>
              <mo class="MathClass-rel" stretchy="false">
               =
              </mo>
              <mn>
               0
              </mn>
              <mo class="MathClass-punc" stretchy="false">
               ,
              </mo>
             </mtd>
            </mtr>
            <mtr class="array-row">
             <mtd class="array-td" columnalign="left">
              <mo class="MathClass-bin" stretchy="false">
               +
              </mo>
              <mn>
               1
              </mn>
              <mspace class="qquad" width="2em">
              </mspace>
              <mspace class="quad" width="1em">
              </mspace>
             </mtd>
             <mtd class="array-td" columnalign="left">
              <mstyle class="text">
               <mtext>
                if
               </mtext>
              </mstyle>
              <mspace class="quad" width="1em">
              </mspace>
              <mi>
               z
              </mi>
              <mo class="MathClass-rel" stretchy="false">
               &gt;
              </mo>
              <mn>
               0
              </mn>
              <mo class="MathClass-punc" stretchy="false">
               ,
              </mo>
             </mtd>
            </mtr>
           </mtable>
          </mrow>
          <mo fence="true" form="postfix">
          </mo>
         </mrow>
        </mrow>
       </math>
      </td>
     </tr>
    </table>
    <p class="noindent">
     A single
     <a class="glossary" href="DataScienceIILectureBookli2.html#glo:tlu">
      TLU
     </a>
     can be used for simple
     <span id="bold" style="font-weight:bold;">
      linear binary classification
     </span>
     :
    </p>
    <div class="quoteblock">
     <p class="noindent">
      <italic>
       It computes a linear function of its inputs, and if the result exceeds a threshold, it
outputs the positive class. Otherwise, it outputs the negative class. They exhibit a
similar behaviour to logistic regression or linear
       <a class="glossary" href="DataScienceIILectureBookli2.html#glo:svm">
        SVM
       </a>
       classification.
      </italic>
     </p>
    </div>
    <p class="noindent">
     It is possible, for example, use a single
     <a class="glossary" href="DataScienceIILectureBookli2.html#glo:tlu">
      TLU
     </a>
     to classify iris flowers <a id="x8-89004"></a><a href="#X0-fisher1936use">[34]</a> (a famous dataset used by statisticians
and
     <a class="glossary" href="DataScienceIILectureBookli2.html#glo:ml">
      ML
     </a>
     researchers) based on
     <alert style="color: #821131;">
      petal length
     </alert>
     and
     <alert style="color: #821131;">
      width
     </alert>
     . Training such a
     <a class="glossary" href="DataScienceIILectureBookli2.html#glo:tlu">
      TLU
     </a>
     would require finding the right
values for
     <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
      <msubsup>
       <mrow>
        <mi>
         w
        </mi>
       </mrow>
       <mrow>
        <mn>
         1
        </mn>
       </mrow>
       <mrow>
       </mrow>
      </msubsup>
      <mo class="MathClass-punc" stretchy="false">
       ,
      </mo>
      <msubsup>
       <mrow>
        <mi>
         w
        </mi>
       </mrow>
       <mrow>
        <mn>
         2
        </mn>
       </mrow>
       <mrow>
       </mrow>
      </msubsup>
      <mo class="MathClass-punc" stretchy="false">
       ,
      </mo>
      <mspace class="thinspace" width="0.17em">
      </mspace>
      <mi>
       b
      </mi>
     </math>
     .
    </p>
    <p class="noindent">
     A perceptron is composed of one or more
     <a class="glossary" href="DataScienceIILectureBookli2.html#glo:tlu">
      TLU
     </a>
     s organized in a single layer, where every
     <a class="glossary" href="DataScienceIILectureBookli2.html#glo:tlu">
      TLU
     </a>
     is
connected to every input. Such a layer is called a
     <span id="bold" style="font-weight:bold;">
      fully connected layer
     </span>
     , or a dense layer. The inputs
constitute the input layer and since the layer of
     <a class="glossary" href="DataScienceIILectureBookli2.html#glo:tlu">
      TLU
     </a>
     s produces the final outputs, it is called the output
layer.
    </p>
    <p class="noindent">
     This perceptron can classify instances simultaneously into three <alert style="color: #821131;">(3)</alert> different binary classes, which
makes it a
     <span id="bold" style="font-weight:bold;">
      multilabel classifier
     </span>
     . It may also be used for multiclass classification.
    </p>
    <p class="noindent">
     Using linear algebra, the following equation can be used to efficiently compute the outputs of a layer of
artificial neurons for several instances at once.
    </p>
    <table class="equation-star">
     <tr>
      <td>
       <math class="equation" display="block" xmlns="http://www.w3.org/1998/Math/MathML">
        <msubsup>
         <mrow>
          <mi>
           h
          </mi>
         </mrow>
         <mrow>
          <mover accent="true">
           <mrow>
            <mi>
             W
            </mi>
           </mrow>
           <mo accent="true">
            →
           </mo>
          </mover>
          <mo class="MathClass-punc" stretchy="false">
           ,
          </mo>
          <mspace class="thinspace" width="0.17em">
          </mspace>
          <mover accent="true">
           <mrow>
            <mi>
             b
            </mi>
           </mrow>
           <mo accent="true">
            →
           </mo>
          </mover>
         </mrow>
         <mrow>
         </mrow>
        </msubsup>
        <msup>
         <mrow>
          <mrow>
           <mo fence="true" form="prefix">
            (
           </mo>
           <mrow>
            <mover accent="true">
             <mrow>
              <mi>
               X
              </mi>
             </mrow>
             <mo accent="true">
              →
             </mo>
            </mover>
           </mrow>
           <mo fence="true" form="postfix">
            )
           </mo>
          </mrow>
         </mrow>
         <mrow>
         </mrow>
        </msup>
        <mo class="MathClass-rel" stretchy="false">
         =
        </mo>
        <mi>
         ϕ
        </mi>
        <msup>
         <mrow>
          <mrow>
           <mo fence="true" form="prefix">
            (
           </mo>
           <mrow>
            <mover accent="true">
             <mrow>
              <mi>
               X
              </mi>
             </mrow>
             <mo accent="true">
              →
             </mo>
            </mover>
            <mover accent="true">
             <mrow>
              <mi>
               W
              </mi>
             </mrow>
             <mo accent="true">
              →
             </mo>
            </mover>
            <mo class="MathClass-bin" stretchy="false">
             +
            </mo>
            <mover accent="true">
             <mrow>
              <mi>
               b
              </mi>
             </mrow>
             <mo accent="true">
              →
             </mo>
            </mover>
           </mrow>
           <mo fence="true" form="postfix">
            )
           </mo>
          </mrow>
         </mrow>
         <mrow>
         </mrow>
        </msup>
       </math>
      </td>
     </tr>
    </table>
    <p class="noindent">
     In this equation:
    </p>
    <ul class="itemize1">
     <li class="itemize">
      <p class="noindent">
       <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
        <mover accent="true">
         <mrow>
          <mi>
           X
          </mi>
         </mrow>
         <mo accent="true">
          →
         </mo>
        </mover>
       </math>
       represents the matrix of input features. It has one row per instance and one column per
feature.
      </p>
     </li>
     <li class="itemize">
      <p class="noindent">
       <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
        <mover accent="true">
         <mrow>
          <mi>
           W
          </mi>
         </mrow>
         <mo accent="true">
          →
         </mo>
        </mover>
       </math>
       is the weight matrix containing all the connection weights. It has one row per input and
one column per neuron.
      </p>
     </li>
     <li class="itemize">
      <p class="noindent">
       <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
        <mover accent="true">
         <mrow>
          <mi>
           b
          </mi>
         </mrow>
         <mo accent="true">
          →
         </mo>
        </mover>
       </math>
       is the bias term containing all the bias terms: one per neuron.
      </p>
     </li>
     <li class="itemize">
      <p class="noindent">
       <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
        <mi>
         ϕ
        </mi>
       </math>
       is the activation function is called the activation function: when the artificial neurons are
       <a class="glossary" href="DataScienceIILectureBookli2.html#glo:tlu">
        TLU
       </a>
       s, it is a step function.
      </p>
     </li>
    </ul>
    <p class="noindent">
     Now the question is:
    </p>
    <div class="quoteblock">
     <p class="noindent">
      <italic>
       How is this perceptron train?
      </italic>
     </p>
    </div>
    <p class="noindent">
     The original perceptron training algorithm proposed by Rosenblatt was largely inspired by Hebb’s rule <a id="x8-89005"></a><a href="#X0-sompolinsky2006theory">[35]</a>. In his 1949 book
     <italic>
      The Organization of Behaviour
     </italic>
     , Donald Hebb suggested that when a biological
neuron triggers another neuron often, the connection between these two neurons grows stronger <a id="x8-89006"></a><a href="#X0-hebb2005organization">[36]</a>.
    </p>
    <p class="noindent">
     Siegrid Löwel later summarized Hebb’s idea in the catchy phrase,
    </p>
    <div class="quoteblock">
     <p class="noindent">
      <italic>
       Cells that fire together, wire together
      </italic>
     </p>
    </div>
    <p class="noindent">
     This means the connection weight between two neurons
     <alert style="color: #821131;">
      tends to increase
     </alert>
     when they fire
simultaneously.
    </p>
    <div class="knowledge">
     <p class="noindent">
      This rule later became known as Hebb’s rule (or Hebbian learning <a id="x8-89007"></a><a href="#X0-gerstner2002mathematical">[37]</a>)
     </p>
    </div>
    <p class="noindent">
     Perceptrons are trained using a variant of this rule that takes into account the error made by the
network when it makes a prediction. The perceptron learning rule
     <span id="bold" style="font-weight:bold;">
      reinforces connections that help
reduce the error
     </span>
     .
    </p>
    <p class="noindent">
     More specifically, the perceptron is fed one training instance at a time, and for each instance it makes
its predictions. For every output neuron that produced a wrong prediction, it reinforces the
connection weights from the inputs that would have contributed to the correct prediction.
    </p>
    <table class="equation-star">
     <tr>
      <td>
       <math class="equation" display="block" xmlns="http://www.w3.org/1998/Math/MathML">
        <msubsup>
         <mrow>
          <mi>
           w
          </mi>
         </mrow>
         <mrow>
          <mi>
           i
          </mi>
          <mo class="MathClass-punc" stretchy="false">
           ,
          </mo>
          <mi>
           j
          </mi>
         </mrow>
         <mrow>
          <mo class="MathClass-open" stretchy="false">
           (
          </mo>
          <mi>
           n
          </mi>
          <mi>
           e
          </mi>
          <mi>
           x
          </mi>
          <mi>
           t
          </mi>
          <mi>
           s
          </mi>
          <mi>
           t
          </mi>
          <mi>
           e
          </mi>
          <mi>
           p
          </mi>
          <mo class="MathClass-close" stretchy="false">
           )
          </mo>
         </mrow>
        </msubsup>
        <mo class="MathClass-rel" stretchy="false">
         =
        </mo>
        <msubsup>
         <mrow>
          <mi>
           w
          </mi>
         </mrow>
         <mrow>
          <mi>
           i
          </mi>
          <mo class="MathClass-punc" stretchy="false">
           ,
          </mo>
          <mi>
           j
          </mi>
         </mrow>
         <mrow>
         </mrow>
        </msubsup>
        <mo class="MathClass-bin" stretchy="false">
         +
        </mo>
        <mi>
         η
        </mi>
        <msup>
         <mrow>
          <mrow>
           <mo fence="true" form="prefix">
            (
           </mo>
           <mrow>
            <msubsup>
             <mrow>
              <mi>
               y
              </mi>
             </mrow>
             <mrow>
              <mi>
               j
              </mi>
             </mrow>
             <mrow>
             </mrow>
            </msubsup>
            <mo class="MathClass-bin" stretchy="false">
             −
            </mo>
            <msubsup>
             <mrow>
              <mi>
               ŷ
              </mi>
             </mrow>
             <mrow>
              <mi>
               j
              </mi>
             </mrow>
             <mrow>
             </mrow>
            </msubsup>
           </mrow>
           <mo fence="true" form="postfix">
            )
           </mo>
          </mrow>
         </mrow>
         <mrow>
         </mrow>
        </msup>
        <msubsup>
         <mrow>
          <mi>
           x
          </mi>
         </mrow>
         <mrow>
          <mi>
           i
          </mi>
         </mrow>
         <mrow>
         </mrow>
        </msubsup>
        <mi>
         x
        </mi>
       </math>
      </td>
     </tr>
    </table>
    <p class="noindent">
     where:
    </p>
    <ul class="itemize1">
     <li class="itemize">
      <p class="noindent">
       <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
        <msubsup>
         <mrow>
          <mi>
           w
          </mi>
         </mrow>
         <mrow>
          <mi>
           i
          </mi>
          <mo class="MathClass-punc" stretchy="false">
           ,
          </mo>
          <mi>
           j
          </mi>
         </mrow>
         <mrow>
         </mrow>
        </msubsup>
       </math>
       is the connection weight between the
       <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
        <msubsup>
         <mrow>
          <mi>
           i
          </mi>
         </mrow>
         <mrow>
         </mrow>
         <mrow>
          <mi>
           t
          </mi>
          <mi>
           h
          </mi>
         </mrow>
        </msubsup>
       </math>
       input and the
       <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
        <msubsup>
         <mrow>
          <mi>
           j
          </mi>
         </mrow>
         <mrow>
         </mrow>
         <mrow>
          <mi>
           t
          </mi>
          <mi>
           h
          </mi>
         </mrow>
        </msubsup>
       </math>
       neuron.
      </p>
     </li>
     <li class="itemize">
      <p class="noindent">
       <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
        <msubsup>
         <mrow>
          <mi>
           x
          </mi>
         </mrow>
         <mrow>
          <mi>
           i
          </mi>
         </mrow>
         <mrow>
         </mrow>
        </msubsup>
       </math>
       is the
       <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
        <msubsup>
         <mrow>
          <mi>
           i
          </mi>
         </mrow>
         <mrow>
         </mrow>
         <mrow>
          <mi>
           t
          </mi>
          <mi>
           h
          </mi>
         </mrow>
        </msubsup>
       </math>
       input value of the current training instance.
      </p>
     </li>
     <li class="itemize">
      <p class="noindent">
       <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
        <msubsup>
         <mrow>
          <mi>
           ŷ
          </mi>
         </mrow>
         <mrow>
         </mrow>
         <mrow>
          <mi>
           j
          </mi>
         </mrow>
        </msubsup>
       </math>
       is the output of the
       <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
        <msubsup>
         <mrow>
          <mi>
           j
          </mi>
         </mrow>
         <mrow>
         </mrow>
         <mrow>
          <mi>
           t
          </mi>
          <mi>
           h
          </mi>
         </mrow>
        </msubsup>
       </math>
       output neuron for the current training instance.
      </p>
     </li>
     <li class="itemize">
      <p class="noindent">
       <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
        <msubsup>
         <mrow>
          <mi>
           y
          </mi>
         </mrow>
         <mrow>
         </mrow>
         <mrow>
          <mi>
           j
          </mi>
         </mrow>
        </msubsup>
       </math>
       is the target output of the
       <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
        <msubsup>
         <mrow>
          <mi>
           j
          </mi>
         </mrow>
         <mrow>
         </mrow>
         <mrow>
          <mi>
           t
          </mi>
          <mi>
           h
          </mi>
         </mrow>
        </msubsup>
       </math>
       output neuron for the current training instance.
      </p>
     </li>
     <li class="itemize">
      <p class="noindent">
       <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
        <mi>
         η
        </mi>
       </math>
       is the learning rate.
      </p>
     </li>
    </ul>
    <p class="noindent">
     The decision boundary of each output neuron is
     <span id="bold" style="font-weight:bold;">
      linear
     </span>
     , therefore perceptrons are incapable of learning
complex patterns. However, if the training instances are
     <alert style="color: #821131;">
      linearly separable
     </alert>
     , Rosenblatt
demonstrated that this algorithm would converge to a solution.
    </p>
    <div class="knowledge">
     <p class="noindent">
      This is called the perceptron convergence theorem.
     </p>
    </div>
    <pre><div id="fancyvrb136" style="padding:20px;border-radius: 3px;"><a id="x8-89009r27"></a><span style="color:#2B2BFF;">import</span><span style="color:#BABABA;"> </span>numpy<span style="color:#BABABA;"> </span><span style="color:#2B2BFF;">as</span><span style="color:#BABABA;"> </span>np 
<a id="x8-89011r28"></a><span style="color:#2B2BFF;">from</span><span style="color:#BABABA;"> </span>sklearn.datasets<span style="color:#BABABA;"> </span><span style="color:#2B2BFF;">import</span> load_iris 
<a id="x8-89013r29"></a><span style="color:#2B2BFF;">from</span><span style="color:#BABABA;"> </span>sklearn.linear_model<span style="color:#BABABA;"> </span><span style="color:#2B2BFF;">import</span> Perceptron 
<a id="x8-89015r30"></a>iris = load_iris(as_frame=<span style="color:#2B2BFF;"><span id="bold" style="font-weight:bold;">True</span></span>) 
<a id="x8-89017r31"></a>X = iris.data[[<span style="color:#800080;">"petal length (cm)"</span>, <span style="color:#800080;">"petal width (cm)"</span>]].values y = (iris.target == <span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">0</span></span>) <span style="color:#008700;"><italic># Iris setosa</italic></span> 
<a id="x8-89019r32"></a>per_clf = Perceptron(random_state=<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">42</span></span>) per_clf.fit(X, y) 
<a id="x8-89021r33"></a>X_new = [[<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">2</span></span>, <span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">0.5</span></span>], [<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">3</span></span>, <span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">1</span></span>]] 
<a id="x8-89023r34"></a>y_pred = per_clf.predict(X_new) <span style="color:#008700;"><italic># predicts True and False for these 2 flowers</italic></span></div></pre>
    <p class="noindent">
     For those of you who have taken a
     <span id="bold" style="font-weight:bold;">
      Data Science II
     </span>
     course, you may have noticed that the
perceptron learning algorithm strongly resembles
     <italic>
      stochastic gradient descent
     </italic>
     . In fact, <span style="color:#054C5C;"><code class="verb">sklearn</code></span>
     ’s <span style="color:#054C5C;"><code class="verb">Perceptron</code></span>
     class is equivalent to using an <span style="color:#054C5C;"><code class="verb">SGDClassifier</code></span>
     with the following hyperparameters:
    </p>
    <ul class="itemize1">
     <li class="itemize">
      <p class="noindent"> <span style="color:#054C5C;"><code class="verb">loss="perceptron"</code></span>,
      </p>
     </li>
     <li class="itemize">
      <p class="noindent"> <span style="color:#054C5C;"><code class="verb">learning_rate="constant"</code></span>,
      </p>
     </li>
     <li class="itemize">
      <p class="noindent"> <span style="color:#054C5C;"><code class="verb">eta0=1</code></span>
       (the learning rate),
      </p>
     </li>
     <li class="itemize">
      <p class="noindent"> <span style="color:#054C5C;"><code class="verb">penalty=None</code></span>
       (no regularization).
      </p>
     </li>
    </ul>
    <p class="noindent">
     In their 1969 monograph Perceptrons, Marvin Minsky and Seymour Papert highlighted a number of
     <alert style="color: #821131;">
      serious weaknesses
     </alert>
     of perceptrons: in particular, they are incapable of solving some trivial problems
(e.g., the exclusive OR (XOR) classification problem).
    </p>
    <div class="informationblock" id="tcolobox-67">
     <div class="title">
      <p class="noindent">
       <span id="bold" style="font-weight:bold;">
        Information
       </span>
       : XOR Problem
      </p>
     </div>
     <div class="box-content">
      <p class="noindent">
       A simple logic gate problem which is proven to be unsolvable using a single-layer perceptron.
      </p>
     </div>
    </div>
    <p class="noindent">
     This is true of any other linear classification model, but researchers had expected much more from
perceptrons, and some were so disappointed, they dropped neural networks altogether in favour of
higher-level problems such as logic, problem solving, and search. The lack of practical applications also
didn’t help.
    </p>
    <p class="noindent">
     It turns out that some of the limitations of perceptrons can be eliminated by stacking multiple
perceptrons. The resulting
     <a class="glossary" href="DataScienceIILectureBookli2.html#glo:ann">
      ANN
     </a>
     is called a
     <a class="glossary" href="DataScienceIILectureBookli2.html#glo:mlp">
      MLP
     </a>
     and a
     <a class="glossary" href="DataScienceIILectureBookli2.html#glo:mlp">
      MLP
     </a>
     can solve the XOR problem <a id="x8-89024"></a><a href="#X0-popescu2009multilayer">[38]</a>.
    </p>
    <div class="warning">
     <p class="noindent">
      Perceptrons
      <span id="bold" style="font-weight:bold;">
       DO NOT
      </span>
      output a class probability. This is one reason to prefer logistic regression over
perceptrons. Moreover, perceptrons do not use any regularization by default, and training stops as soon
as there are no more prediction errors on the training set, so the model typically does not generalize as
well as logistic regression or a linear SVM classifier. However, perceptrons may train a bit faster.
     </p>
    </div>
    <h3 class="subsectionHead">
     <a aria-label="Link to this section in PDF version" class="pdf-link" href="DataScienceIILectureBook.pdf#subsection.6.2.4" style="color: red;  position: absolute;  left: -30px;" target="_blank">
      <svg class="bi bi-file-pdf" fill="currentColor" height="20" viewbox="0 0 16 16" width="20" xmlns="http://www.w3.org/2000/svg">
       <path d="M4 0a2 2 0 0 0-2 2v12a2 2 0 0 0 2 2h8a2 2 0 0 0 2-2V2a2 2 0 0 0-2-2H4zm0 1h8a1 1 0 0 1 1 1v12a1 1 0 0 1-1 1H4a1 1 0 0 1-1-1V2a1 1 0 0 1 1-1z">
       </path>
       <path d="M4.603 12.087a.81.81 0 0 1-.438-.42c-.195-.388-.13-.776.08-1.102.198-.307.526-.568.897-.787a7.68 7.68 0 0 1 1.482-.645 19.701 19.701 0 0 0 1.062-2.227 7.269 7.269 0 0 1-.43-1.295c-.086-.4-.119-.796-.046-1.136.075-.354.274-.672.65-.823.192-.077.4-.12.602-.077a.7.7 0 0 1 .477.365c.088.164.12.356.127.538.007.187-.012.395-.047.614-.084.51-.27 1.134-.52 1.794a10.954 10.954 0 0 0 .98 1.686 5.753 5.753 0 0 1 1.334.05c.364.065.734.195.96.465.12.144.193.32.2.518.007.192-.047.382-.138.563a1.04 1.04 0 0 1-.354.416.856.856 0 0 1-.51.138c-.331-.014-.654-.196-.933-.417a5.716 5.716 0 0 1-.911-.95 11.642 11.642 0 0 0-1.997.406 11.311 11.311 0 0 1-1.021 1.51c-.29.35-.608.655-.926.787a.793.793 0 0 1-.58.029zm1.379-1.901c-.166.076-.32.156-.459.238-.328.194-.541.383-.647.547-.094.145-.096.25-.04.361.01.022.02.036.026.044a.27.27 0 0 0 .035-.012c.137-.056.355-.235.635-.572a8.18 8.18 0 0 0 .45-.606zm1.64-1.33a12.647 12.647 0 0 1 1.01-.193 11.666 11.666 0 0 1-.51-.858 20.741 20.741 0 0 1-.5 1.05zm2.446.45c.15.162.296.3.435.41.24.19.407.253.498.256a.107.107 0 0 0 .07-.015.307.307 0 0 0 .094-.125.436.436 0 0 0 .059-.2.095.095 0 0 0-.026-.063c-.052-.062-.2-.152-.518-.209a3.881 3.881 0 0 0-.612-.053zM8.078 5.8a6.7 6.7 0 0 0 .2-.828c.031-.188.043-.343.038-.465a.613.613 0 0 0-.032-.198.517.517 0 0 0-.145.04c-.087.035-.158.106-.196.283-.04.192-.03.469.046.822.024.111.054.227.09.346z">
       </path>
      </svg>
     </a>
     <small class="titlemark">
      6.2.4
     </small>
     <a id="x8-900006.2.4">
     </a>
     Multilayer Perceptron and Backpropagation
    </h3>
    <p class="noindent">
     An
     <a class="glossary" href="DataScienceIILectureBookli2.html#glo:mlp">
      MLP
     </a>
     is composed of one input layer, one or more layers of
     <a class="glossary" href="DataScienceIILectureBookli2.html#glo:tlu">
      TLU
     </a>
     s called
     <span id="bold" style="font-weight:bold;">
      hidden layers
     </span>
     ,
and one final layer of
     <a class="glossary" href="DataScienceIILectureBookli2.html#glo:tlu">
      TLU
     </a>
     s called the output layer. The layers close to the input layer are
usually called the lower layers, and the ones close to the outputs are usually called the upper
layers.
    </p>
    <div class="knowledge">
     <p class="noindent">
      The signal flows only in one direction (inputs to outputs), so this architecture is an example of a
      <a class="glossary" href="DataScienceIILectureBookli2.html#glo:fnn">
       Feedforward Neural Networks (FNN)
      </a> <a id="x8-90001"></a><a href="#X0-bebis1994feed">[39]</a>.
     </p>
    </div>
    <p class="noindent">
     When an
     <a class="glossary" href="DataScienceIILectureBookli2.html#glo:ann">
      ANN
     </a>
     contains a deep stack of hidden layers, it is called a
     <a class="glossary" href="DataScienceIILectureBookli2.html#glo:dnn">
      Deep Neural Networks (DNN)
     </a>. The
field of deep learning studies
     <a class="glossary" href="DataScienceIILectureBookli2.html#glo:dnn">
      DNN
     </a>
     s, and more generally it is interested in models containing deep
stacks of computations <a id="x8-90002"></a><a href="#X0-samek2021explaining">[40]</a>.
    </p>
    <div class="figure">
     <p class="noindent">
      <img alt="PIC" height="" src="figures/Introduction-to-Artificial-Neural-Networks/figures-2.svg" width="150%"/>
      <a id="x8-90003r6">
      </a>
     </p>
     <figcaption class="caption">
      <span class="id">
       Figure 6.6:
      </span>
      <span class="content">
       Architecture of a Multilayer Perceptron with five inputs, three hidden layer of four neurons, and three
         output neurons.
      </span>
     </figcaption>
    </div>
    <p class="noindent">
     For many years researchers struggled to find a way to train
     <a class="glossary" href="DataScienceIILectureBookli2.html#glo:mlp">
      MLP
     </a>
     s, without success. In the early 1960s
several researchers discussed of using
     <alert style="color: #821131;">
      gradient descent
     </alert>
     to train neural networks. This requires
computing the gradients of the model’s error with regard to the model parameters and at that time, it
wasn’t clear at the time how to do this efficiently with such a complex model containing so many
parameters.
    </p>
    <p class="noindent">
     Then, in 1970, a researcher named
     <italic>
      Seppo Linnainmaa
     </italic>
     introduced in his master’s thesis a technique to
compute all the gradients automatically and efficiently. This algorithm is now called
     <span id="bold" style="font-weight:bold;">
      reverse-mode
automatic differentiation
     </span> <a id="x8-90004"></a><a href="#X0-Seppo1970">[41]</a>. In just two passes through the network (one forward, one backward),
it is able to compute the gradients of the neural network’s error with regard to every single model
parameter.
    </p>
    <p class="noindent">
     In other words, it can find out how each connection weight and each bias should be tweaked in order to
reduce the neural network’s error. These gradients can then be used to perform a gradient descent
step. Repeating the process of computing the gradients automatically and taking a gradient
descent step, the neural network’s error will gradually drop until it eventually reaches a
minimum.
    </p>
    <p class="noindent">
     This combination of reverse-mode automatic differentiation and gradient descent is now called
     <span id="bold" style="font-weight:bold;">
      backpropagation
     </span> <a id="x8-90005"></a><a href="#X0-hecht1992theory">[42]</a>.
    </p>
    <div class="knowledge">
     <p class="noindent">
      There are various automatic differentiation techniques (i.e., forward and reverse), with each having its
own advantages and disadvantages. Reverse-mode autodiff is well suited when the function to
differentiate has many variables (e.g., connection weights and biases) and few outputs (e.g., one loss).
     </p>
    </div>
    <p class="noindent">
     Backpropagation can actually be applied to all sorts of computational graphs, not just neural networks:
Linnainmaa’s M.Sc thesis was not about neural nets, it was more general. It was several
more years before backprop started to be used to train neural networks, but it still wasn’t
mainstream.
    </p>
    <p class="noindent">
     Then, in 1986, David Rumelhart, Geoffrey Hinton, and Ronald Williams published a groundbreaking
paper analyzing how backpropagation allowed neural networks to learn useful internal representations <a id="x8-90006"></a><a href="#X0-rumelhart1986learning">[43]</a>. Their results were so impressive that backpropagation was quickly popularized in the field. Today,
it is by far the most popular training technique for neural networks.
    </p>
    <p class="noindent">
     Let’s run through how backpropagation works again in a bit more detail:
    </p>
    <dl class="enumerate-enumitem">
     <dt class="enumerate-enumitem">
      1.
     </dt>
     <dd class="enumerate-enumitem">
      It handles one mini-batch at a time, and goes through the full training set multiple times.
Each pass is called an
      <span id="bold" style="font-weight:bold;">
       epoch
      </span>
      .
     </dd>
     <dt class="enumerate-enumitem">
      2.
     </dt>
     <dd class="enumerate-enumitem">
      Each mini-batch enters the network through the input layer. The algorithm then computes
the output of all the neurons in the first hidden layer, for every instance in the mini-batch.
The result is passed on to the next layer, its output is computed and passed to the next
layer, and so on until we get the output of the last layer, the output layer.
     </dd>
     <dt class="enumerate-enumitem">
      3.
     </dt>
     <dd class="enumerate-enumitem">
      Next, the algorithm measures the network’s output error. This means, it uses a loss function
                                                                                
                                                                                
     
that compares the desired output and the actual output of the network, and returns some
measure of the error.
     </dd>
     <dt class="enumerate-enumitem">
      4.
     </dt>
     <dd class="enumerate-enumitem">
      It then computes how much each output bias and each connection to the output layer
contributed to the error. This is done analytically by applying the chain rule, which makes
this step fast and precise.
     </dd>
     <dt class="enumerate-enumitem">
      5.
     </dt>
     <dd class="enumerate-enumitem">
      The  algorithm  then  measures  how  much  of  these  error  contributions  came  from  each
connection in the layer below, again using the chain rule, working backward until it reaches
the input layer. As explained earlier, this reverse pass efficiently measures the error gradient
across all the connection weights and biases in the network by propagating the error gradient
backward through the network.
     </dd>
     <dt class="enumerate-enumitem">
      6.
     </dt>
     <dd class="enumerate-enumitem">
      Finally, the algorithm performs a gradient descent step to tweak all the connection weights
in the network, using the error gradients it just computed.
     </dd>
    </dl>
    <div class="warning">
     <p class="noindent">
      Initialize all the hidden layers’ connection weights randomly, or training will fail.
     </p>
    </div>
    <p class="noindent">
     For example, if you initialize all weights and biases to zero, then all neurons in a given layer will be
perfectly identical, and therefore backpropagation will affect them in exactly the same way, so they will
remain identical.
    </p>
    <p class="noindent">
     In other words, despite having hundreds of neurons per layer, your model will act as if it had
only one neuron per layer: it won’t be too smart. If instead you randomly initialize the
weights, you
     <span id="bold" style="font-weight:bold;">
      break the symmetry
     </span>
     and allow back-propagation to train a diverse team of
neurons.
    </p>
    <p class="noindent">
     In short, backpropagation makes predictions for a mini-batch (forward pass), measures the error, then
goes through each layer in reverse to measure the error contribution from each parameter (reverse
pass), and finally tweaks the connection weights and biases to reduce the error, which is the gradient
descent step.
    </p>
    <p class="noindent">
     For back-propagation to work properly, Rumelhart and his colleagues made a key change to the
     <a class="glossary" href="DataScienceIILectureBookli2.html#glo:mlp">
      MLP
     </a>
     ’s
architecture by replacing the step function with the logistic function:
    </p>
    <table class="equation-star">
     <tr>
      <td>
       <math class="equation" display="block" xmlns="http://www.w3.org/1998/Math/MathML">
        <mi>
         σ
        </mi>
        <msup>
         <mrow>
          <mrow>
           <mo fence="true" form="prefix">
            (
           </mo>
           <mrow>
            <mi>
             z
            </mi>
           </mrow>
           <mo fence="true" form="postfix">
            )
           </mo>
          </mrow>
         </mrow>
         <mrow>
         </mrow>
        </msup>
        <mo class="MathClass-rel" stretchy="false">
         =
        </mo>
        <mfrac>
         <mrow>
          <mn>
           1
          </mn>
         </mrow>
         <mrow>
          <mn>
           1
          </mn>
          <mo class="MathClass-bin" stretchy="false">
           +
          </mo>
          <msubsup>
           <mrow>
            <mi>
             e
            </mi>
           </mrow>
           <mrow>
           </mrow>
           <mrow>
            <mo class="MathClass-bin" stretchy="false">
             −
            </mo>
            <mi>
             z
            </mi>
           </mrow>
          </msubsup>
         </mrow>
        </mfrac>
       </math>
      </td>
     </tr>
    </table>
    <p class="noindent">
     Which is also called the
     <span id="bold" style="font-weight:bold;">
      sigmoid function
     </span>
     . This was an important improvement as step function
contains only flat segments, so there is no gradient to work with, while the sigmoid function has a
well-defined nonzero derivative everywhere. In fact, the backpropagation algorithm works well with
many other activation functions, not just the sigmoid function.
    </p>
    <p class="noindent">
     Here are two <alert style="color: #821131;">(2)</alert> other popular choices:
    </p>
    <div class="exerciseblock" id="tcolobox-68">
     <div class="title">
      <a id="x8-90015r2">
      </a>
      <a id="x8-90016">
      </a>
      <p class="noindent">
       <span id="bold" style="font-weight:bold;">
        Exercise
       </span>
       6.2: Hyperbolic tangent function
      </p>
     </div>
     <div class="box-content">
      <div class="columns-1">
       <table class="equation-star">
        <tr>
         <td>
          <math class="equation" display="block" xmlns="http://www.w3.org/1998/Math/MathML">
           <mi class="loglike">
            tanh
           </mi>
           <mo>
            ⁡
           </mo>
           <msup>
            <mrow>
             <mrow>
              <mo fence="true" form="prefix">
               (
              </mo>
              <mrow>
               <mi>
                z
               </mi>
              </mrow>
              <mo fence="true" form="postfix">
               )
              </mo>
             </mrow>
            </mrow>
            <mrow>
            </mrow>
           </msup>
           <mo class="MathClass-rel" stretchy="false">
            =
           </mo>
           <mn>
            2
           </mn>
           <mi>
            σ
           </mi>
           <msup>
            <mrow>
             <mrow>
              <mo fence="true" form="prefix">
               (
              </mo>
              <mrow>
               <mn>
                2
               </mn>
               <mi>
                z
               </mi>
              </mrow>
              <mo fence="true" form="postfix">
               )
              </mo>
             </mrow>
            </mrow>
            <mrow>
            </mrow>
           </msup>
           <mo class="MathClass-bin" stretchy="false">
            −
           </mo>
           <mn>
            1
           </mn>
          </math>
         </td>
        </tr>
       </table>
       <p class="noindent">
        Similar sigmoid function, this activation function is also S-shaped, continuous, and differentiable, but its output value
ranges from -1 to 1, instead of 0 to 1 like the sigmoid function.
       </p>
       <p class="noindent">
        This bigger range tends to make each layer’s output more or less centered around 0 at the beginning of training, which
often helps speed up convergence.
       </p>
      </div>
     </div>
    </div>
    <div class="exerciseblock" id="tcolobox-69">
     <div class="title">
      <a id="x8-90019r4">
      </a>
      <a id="x8-90020">
      </a>
      <p class="noindent">
       <span id="bold" style="font-weight:bold;">
        Exercise
       </span>
       6.4: The rectified linear unit function
      </p>
     </div>
     <div class="box-content">
      <div class="columns-1">
       <table class="equation-star">
        <tr>
         <td>
          <math class="equation" display="block" xmlns="http://www.w3.org/1998/Math/MathML">
           <mi>
            R
           </mi>
           <mi>
            e
           </mi>
           <mi>
            L
           </mi>
           <mi>
            U
           </mi>
           <mo class="MathClass-open" stretchy="false">
            (
           </mo>
           <mi>
            z
           </mi>
           <mo class="MathClass-close" stretchy="false">
            )
           </mo>
           <mo class="MathClass-rel" stretchy="false">
            =
           </mo>
           <mi>
            m
           </mi>
           <mi>
            a
           </mi>
           <mi>
            x
           </mi>
           <mo class="MathClass-open" stretchy="false">
            (
           </mo>
           <mn>
            0
           </mn>
           <mo class="MathClass-punc" stretchy="false">
            ,
           </mo>
           <mi>
            z
           </mi>
           <mo class="MathClass-close" stretchy="false">
            )
           </mo>
          </math>
         </td>
        </tr>
       </table>
       <p class="noindent">
        It is continuous but unfortunately not differentiable at
        <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
         <mi>
          z
         </mi>
         <mo class="MathClass-rel" stretchy="false">
          =
         </mo>
         <mn>
          0
         </mn>
        </math>
        as the slope changes abruptly, which can make gradient descent bounce around, and its derivative is 0 for z &lt;
0.
       </p>
       <p class="noindent">
        In practice, however, it works very well and has the advantage of being fast to compute, so it has become the
default.
       </p>
       <p class="noindent">
        Importantly, the fact that it does not have a maximum output value helps reduce some issues during gradient
descent.
       </p>
      </div>
     </div>
    </div>
    <p class="noindent">
     You might wonder what is the point of an activation function, let alone whether it is linear or not?
Chaining several linear transformations, gives you only linear transformation. For example:
    </p>
    <table class="equation-star">
     <tr>
      <td>
       <math class="equation" display="block" xmlns="http://www.w3.org/1998/Math/MathML">
        <mi>
         f
        </mi>
        <msup>
         <mrow>
          <mrow>
           <mo fence="true" form="prefix">
            (
           </mo>
           <mrow>
            <mi>
             x
            </mi>
           </mrow>
           <mo fence="true" form="postfix">
            )
           </mo>
          </mrow>
         </mrow>
         <mrow>
         </mrow>
        </msup>
        <mo class="MathClass-rel" stretchy="false">
         =
        </mo>
        <mn>
         2
        </mn>
        <mi>
         x
        </mi>
        <mo class="MathClass-bin" stretchy="false">
         +
        </mo>
        <mn>
         3
        </mn>
        <mspace class="quad" width="1em">
        </mspace>
        <mstyle class="text">
         <mtext>
          and
         </mtext>
        </mstyle>
        <mspace class="quad" width="1em">
        </mspace>
        <mi>
         g
        </mi>
        <msup>
         <mrow>
          <mrow>
           <mo fence="true" form="prefix">
            (
           </mo>
           <mrow>
            <mi>
             x
            </mi>
           </mrow>
           <mo fence="true" form="postfix">
            )
           </mo>
          </mrow>
         </mrow>
         <mrow>
         </mrow>
        </msup>
        <mo class="MathClass-rel" stretchy="false">
         =
        </mo>
        <mn>
         5
        </mn>
        <mi>
         x
        </mi>
        <mo class="MathClass-bin" stretchy="false">
         −
        </mo>
        <mn>
         1
        </mn>
        <mspace class="quad" width="1em">
        </mspace>
        <mo class="MathClass-rel" stretchy="false">
         →
        </mo>
        <mspace class="quad" width="1em">
        </mspace>
        <mi>
         f
        </mi>
        <msup>
         <mrow>
          <mrow>
           <mo fence="true" form="prefix">
            (
           </mo>
           <mrow>
            <mi>
             g
            </mi>
            <msup>
             <mrow>
              <mrow>
               <mo fence="true" form="prefix">
                (
               </mo>
               <mrow>
                <mi>
                 x
                </mi>
               </mrow>
               <mo fence="true" form="postfix">
                )
               </mo>
              </mrow>
             </mrow>
             <mrow>
             </mrow>
            </msup>
           </mrow>
           <mo fence="true" form="postfix">
            )
           </mo>
          </mrow>
         </mrow>
         <mrow>
         </mrow>
        </msup>
        <mo class="MathClass-rel" stretchy="false">
         =
        </mo>
        <mn>
         2
        </mn>
        <msup>
         <mrow>
          <mrow>
           <mo fence="true" form="prefix">
            (
           </mo>
           <mrow>
            <mn>
             5
            </mn>
            <mi>
             x
            </mi>
            <mo class="MathClass-bin" stretchy="false">
             −
            </mo>
            <mn>
             1
            </mn>
           </mrow>
           <mo fence="true" form="postfix">
            )
           </mo>
          </mrow>
         </mrow>
         <mrow>
         </mrow>
        </msup>
        <mo class="MathClass-bin" stretchy="false">
         +
        </mo>
        <mn>
         3
        </mn>
        <mo class="MathClass-rel" stretchy="false">
         =
        </mo>
        <mn>
         1
        </mn>
        <mn>
         0
        </mn>
        <mi>
         x
        </mi>
        <mo class="MathClass-bin" stretchy="false">
         +
        </mo>
        <mn>
         1
        </mn>
       </math>
      </td>
     </tr>
    </table>
    <p class="noindent">
     You don’t have some nonlinearity between layers, then even a deep stack of layers is equivalent to a
single layer, and you can’t solve very complex problems with that.
    </p>
    <div class="warning">
     <p class="noindent">
      A large enough
      <a class="glossary" href="DataScienceIILectureBookli2.html#glo:dnn">
       DNN
      </a>
      with nonlinear activations can theoretically approximate any continuous function.
     </p>
    </div>
    <div class="figure">
     <p class="noindent">
      <img alt="PIC" height="" src="codes/images/Introduction-to-Artificial-Neural-Networks/activation_functions_plot-.svg" width="150%"/>
      <a id="x8-90021r7">
      </a>
     </p>
     <figcaption class="caption">
      <span class="id">
       Figure 6.7:
      </span>
      <span class="content">
       The activation function of a node in an
       <a class="glossary" href="DataScienceIILectureBookli2.html#glo:ann">
        ANN
       </a>
       is a function which calculates the output of the node based
         on its individual inputs and their weights. Nontrivial problems can be solved using only a few nodes if the
         activation function is nonlinear <a href="#X0-knut2018neural">[44]</a>. Modern activation functions include the smooth version of the ReLU,
         the GELU, which was used in the 2018 BERT model <a href="#X0-hendrycks2016gaussian">[45]</a>, the logistic (sigmoid) function used in the 2012
         speech recognition model developed by Hinton et al <a href="#X0-hinton2012deep">[46]</a>, the ReLU used in the 2012 AlexNet computer
         vision model <a href="#X0-krizhevsky2012imagenet">[47]</a> and in the 2015 ResNet model.
      </span>
     </figcaption>
    </div>
    <h3 class="subsectionHead">
     <a aria-label="Link to this section in PDF version" class="pdf-link" href="DataScienceIILectureBook.pdf#subsection.6.2.5" style="color: red;  position: absolute;  left: -30px;" target="_blank">
      <svg class="bi bi-file-pdf" fill="currentColor" height="20" viewbox="0 0 16 16" width="20" xmlns="http://www.w3.org/2000/svg">
       <path d="M4 0a2 2 0 0 0-2 2v12a2 2 0 0 0 2 2h8a2 2 0 0 0 2-2V2a2 2 0 0 0-2-2H4zm0 1h8a1 1 0 0 1 1 1v12a1 1 0 0 1-1 1H4a1 1 0 0 1-1-1V2a1 1 0 0 1 1-1z">
       </path>
       <path d="M4.603 12.087a.81.81 0 0 1-.438-.42c-.195-.388-.13-.776.08-1.102.198-.307.526-.568.897-.787a7.68 7.68 0 0 1 1.482-.645 19.701 19.701 0 0 0 1.062-2.227 7.269 7.269 0 0 1-.43-1.295c-.086-.4-.119-.796-.046-1.136.075-.354.274-.672.65-.823.192-.077.4-.12.602-.077a.7.7 0 0 1 .477.365c.088.164.12.356.127.538.007.187-.012.395-.047.614-.084.51-.27 1.134-.52 1.794a10.954 10.954 0 0 0 .98 1.686 5.753 5.753 0 0 1 1.334.05c.364.065.734.195.96.465.12.144.193.32.2.518.007.192-.047.382-.138.563a1.04 1.04 0 0 1-.354.416.856.856 0 0 1-.51.138c-.331-.014-.654-.196-.933-.417a5.716 5.716 0 0 1-.911-.95 11.642 11.642 0 0 0-1.997.406 11.311 11.311 0 0 1-1.021 1.51c-.29.35-.608.655-.926.787a.793.793 0 0 1-.58.029zm1.379-1.901c-.166.076-.32.156-.459.238-.328.194-.541.383-.647.547-.094.145-.096.25-.04.361.01.022.02.036.026.044a.27.27 0 0 0 .035-.012c.137-.056.355-.235.635-.572a8.18 8.18 0 0 0 .45-.606zm1.64-1.33a12.647 12.647 0 0 1 1.01-.193 11.666 11.666 0 0 1-.51-.858 20.741 20.741 0 0 1-.5 1.05zm2.446.45c.15.162.296.3.435.41.24.19.407.253.498.256a.107.107 0 0 0 .07-.015.307.307 0 0 0 .094-.125.436.436 0 0 0 .059-.2.095.095 0 0 0-.026-.063c-.052-.062-.2-.152-.518-.209a3.881 3.881 0 0 0-.612-.053zM8.078 5.8a6.7 6.7 0 0 0 .2-.828c.031-.188.043-.343.038-.465a.613.613 0 0 0-.032-.198.517.517 0 0 0-.145.04c-.087.035-.158.106-.196.283-.04.192-.03.469.046.822.024.111.054.227.09.346z">
       </path>
      </svg>
     </a>
     <small class="titlemark">
      6.2.5
     </small>
     <a id="x8-910006.2.5">
     </a>
     Regression MLPs
    </h3>
    <p class="noindent">
     First,
     <a class="glossary" href="DataScienceIILectureBookli2.html#glo:mlp">
      MLP
     </a>
     s can be used for regression tasks. If you want to predict a single value (e.g., the price of a
house, given many of its features), you just need a single output neuron:
    </p>
    <blockquote class="quotation">
     <p class="indent">
      <italic>
       its output is the predicted value
      </italic>
     </p>
    </blockquote>
    <p class="noindent">
     For multivariate regression (i.e., to predict multiple values at once), you need one output neuron per
output dimension. As an example, to locate the center of an object in an image, you need to predict 2D
coordinates, so you need two <alert style="color: #821131;">(2)</alert> output neurons. If you also want to place a bounding
box around the object, then you need two more numbers: the width and the height of the
object.
    </p>
    <p class="noindent">
     So, in the end you end up with four <alert style="color: #821131;">(4)</alert> output neurons.
    </p>
    <p class="noindent"> <span style="color:#054C5C;"><code class="verb">sklearn</code></span>
     includes an <span style="color:#054C5C;"><code class="verb">MLPRegressor</code></span>
     class, so let’s use it to build an
     <a class="glossary" href="DataScienceIILectureBookli2.html#glo:mlp">
      MLP
     </a>
     with three hidden layers
composed of 50 neurons each, and train it on the California housing dataset.
    </p>
    <p class="noindent">
     For simplicity, we will use <span style="color:#054C5C;"><code class="verb">sklearn</code></span>
     ’s <span style="color:#054C5C;"><code class="verb">fetch_california_housing()</code></span>
     function to load the data instead
of downloading from a sketchy website.
    </p>
    <p class="noindent">
     The following code starts by fetching and splitting the dataset, then it creates a pipeline to standardise
the input features before sending them to the <span style="color:#054C5C;"><code class="verb">MLPRegressor</code></span>. This is very important for neural
networks as they are trained using gradient descent, and gradient descent does not converge very well
when the features have very different scales.
    </p>
    <p class="noindent">
     Finally, the code trains the model and evaluates its validation error. The model
uses the ReLU activation function in the hidden layers, and it uses a variant of
gradient descent called Adam to minimize the mean squared error, with a little bit of
     <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
      <msub>
       <mrow>
        <mi>
         ℓ
        </mi>
       </mrow>
       <mrow>
        <mn>
         2
        </mn>
       </mrow>
      </msub>
     </math>
     regularisation:
    </p>
    <pre><div id="fancyvrb137" style="padding:20px;border-radius: 3px;"><a id="x8-91002r85"></a><span style="color:#2B2BFF;">from</span><span style="color:#BABABA;"> </span>sklearn.datasets<span style="color:#BABABA;"> </span><span style="color:#2B2BFF;">import</span> fetch_california_housing 
<a id="x8-91004r86"></a><span style="color:#2B2BFF;">from</span><span style="color:#BABABA;"> </span>sklearn.metrics<span style="color:#BABABA;"> </span><span style="color:#2B2BFF;">import</span> mean_squared_error 
<a id="x8-91006r87"></a><span style="color:#2B2BFF;">from</span><span style="color:#BABABA;"> </span>sklearn.model_selection<span style="color:#BABABA;"> </span><span style="color:#2B2BFF;">import</span> train_test_split 
<a id="x8-91008r88"></a><span style="color:#2B2BFF;">from</span><span style="color:#BABABA;"> </span>sklearn.neural_network<span style="color:#BABABA;"> </span><span style="color:#2B2BFF;">import</span> MLPRegressor 
<a id="x8-91010r89"></a><span style="color:#2B2BFF;">from</span><span style="color:#BABABA;"> </span>sklearn.pipeline<span style="color:#BABABA;"> </span><span style="color:#2B2BFF;">import</span> make_pipeline 
<a id="x8-91012r90"></a><span style="color:#2B2BFF;">from</span><span style="color:#BABABA;"> </span>sklearn.preprocessing<span style="color:#BABABA;"> </span><span style="color:#2B2BFF;">import</span> StandardScaler 
<a id="x8-91014r91"></a> 
<a id="x8-91016r92"></a>housing = fetch_california_housing() 
<a id="x8-91018r93"></a>X_train_full, X_test, y_train_full, y_test = train_test_split( 
<a id="x8-91020r94"></a>    housing.data, housing.target, random_state=<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">42</span></span>) 
<a id="x8-91022r95"></a>X_train, X_valid, y_train, y_valid = train_test_split( 
<a id="x8-91024r96"></a>    X_train_full, y_train_full, random_state=<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">42</span></span>) 
<a id="x8-91026r97"></a> 
<a id="x8-91028r98"></a>mlp_reg = MLPRegressor(hidden_layer_sizes=[<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">50</span></span>, <span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">50</span></span>, <span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">50</span></span>], random_state=<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">42</span></span>) 
<a id="x8-91030r99"></a>pipeline = make_pipeline(StandardScaler(), mlp_reg) 
<a id="x8-91032r100"></a>pipeline.fit(X_train, y_train) 
<a id="x8-91034r101"></a>y_pred = pipeline.predict(X_valid) 
<a id="x8-91036r102"></a>rmse = mean_squared_error(y_valid, y_pred, squared=<span style="color:#2B2BFF;"><span id="bold" style="font-weight:bold;">False</span></span>)</div></pre>
    <p class="noindent">
     We get a validation RMSE of about 0.505, which is comparable to what you would get with a random
forest classifier.
    </p>
    <div class="warning">
     <p class="noindent">
      This
      <a class="glossary" href="DataScienceIILectureBookli2.html#glo:mlp">
       MLP
      </a>
      does not use any activation function for the output layer, so it’s free to output any value it
wants.
     </p>
    </div>
    <p class="noindent">
     This is generally fine, but if you want to guarantee that the output will always be positive, then you
should use the ReLU activation function in the output layer, or the softplus activation function, which
is a smooth variant of ReLU: softplus(z) = log(1 + exp(z)).
    </p>
    <p class="noindent">
     Softplus is close to 0 when z is negative, and close to z when z is positive. Finally, if you want to
guarantee that the predictions will always fall within a given range of values, then you should use the
sigmoid function or the hyperbolic tangent, and scale the targets to the appropriate range: 0 to 1 for
                                                                                
                                                                                
sigmoid and -1 to 1 for tanh.
    </p>
    <p class="noindent">
     Sadly, the <span style="color:#054C5C;"><code class="verb">MLPRegressor</code></span>
     class does not support activation functions in the output layer.
    </p>
    <div class="warning">
     <p class="noindent">
      Building and training a standard
      <a class="glossary" href="DataScienceIILectureBookli2.html#glo:mlp">
       MLP
      </a>
      with <span style="color:#054C5C;"><code class="verb">sklearn</code></span>
      is very convenient, but features are limited. This
is why we will switch to Keras in the second part of this chapter.
     </p>
    </div>
    <p class="noindent">
     The <span style="color:#054C5C;"><code class="verb">MLPRegressor</code></span>
     class uses the mean squared error, which is usually what you want for regression,
but if you have a lot of outliers in the training set, you may prefer to use the mean absolute error
instead. Alternatively, you may want to use the Huber loss, which is a combination of both. It is
quadratic when the error is smaller than a threshold  (typically 1) but linear when the error is larger
than . The linear part makes it less sensitive to outliers than the mean squared error, and the quadratic
part allows it to converge faster and be more precise than the mean absolute error. However, <span style="color:#054C5C;"><code class="verb">MLPRegressor</code></span>
     only supports the MSE.
    </p>
    <h3 class="subsectionHead">
     <a aria-label="Link to this section in PDF version" class="pdf-link" href="DataScienceIILectureBook.pdf#subsection.6.2.6" style="color: red;  position: absolute;  left: -30px;" target="_blank">
      <svg class="bi bi-file-pdf" fill="currentColor" height="20" viewbox="0 0 16 16" width="20" xmlns="http://www.w3.org/2000/svg">
       <path d="M4 0a2 2 0 0 0-2 2v12a2 2 0 0 0 2 2h8a2 2 0 0 0 2-2V2a2 2 0 0 0-2-2H4zm0 1h8a1 1 0 0 1 1 1v12a1 1 0 0 1-1 1H4a1 1 0 0 1-1-1V2a1 1 0 0 1 1-1z">
       </path>
       <path d="M4.603 12.087a.81.81 0 0 1-.438-.42c-.195-.388-.13-.776.08-1.102.198-.307.526-.568.897-.787a7.68 7.68 0 0 1 1.482-.645 19.701 19.701 0 0 0 1.062-2.227 7.269 7.269 0 0 1-.43-1.295c-.086-.4-.119-.796-.046-1.136.075-.354.274-.672.65-.823.192-.077.4-.12.602-.077a.7.7 0 0 1 .477.365c.088.164.12.356.127.538.007.187-.012.395-.047.614-.084.51-.27 1.134-.52 1.794a10.954 10.954 0 0 0 .98 1.686 5.753 5.753 0 0 1 1.334.05c.364.065.734.195.96.465.12.144.193.32.2.518.007.192-.047.382-.138.563a1.04 1.04 0 0 1-.354.416.856.856 0 0 1-.51.138c-.331-.014-.654-.196-.933-.417a5.716 5.716 0 0 1-.911-.95 11.642 11.642 0 0 0-1.997.406 11.311 11.311 0 0 1-1.021 1.51c-.29.35-.608.655-.926.787a.793.793 0 0 1-.58.029zm1.379-1.901c-.166.076-.32.156-.459.238-.328.194-.541.383-.647.547-.094.145-.096.25-.04.361.01.022.02.036.026.044a.27.27 0 0 0 .035-.012c.137-.056.355-.235.635-.572a8.18 8.18 0 0 0 .45-.606zm1.64-1.33a12.647 12.647 0 0 1 1.01-.193 11.666 11.666 0 0 1-.51-.858 20.741 20.741 0 0 1-.5 1.05zm2.446.45c.15.162.296.3.435.41.24.19.407.253.498.256a.107.107 0 0 0 .07-.015.307.307 0 0 0 .094-.125.436.436 0 0 0 .059-.2.095.095 0 0 0-.026-.063c-.052-.062-.2-.152-.518-.209a3.881 3.881 0 0 0-.612-.053zM8.078 5.8a6.7 6.7 0 0 0 .2-.828c.031-.188.043-.343.038-.465a.613.613 0 0 0-.032-.198.517.517 0 0 0-.145.04c-.087.035-.158.106-.196.283-.04.192-.03.469.046.822.024.111.054.227.09.346z">
       </path>
      </svg>
     </a>
     <small class="titlemark">
      6.2.6
     </small>
     <a id="x8-920006.2.6">
     </a>
     Classification MLPs
    </h3>
    <p class="noindent">
     <a class="glossary" href="DataScienceIILectureBookli2.html#glo:mlp">
      MLP
     </a>
     s can also be used for
     <span id="bold" style="font-weight:bold;">
      classification
     </span>
     tasks. For a binary classification problem, you just
need a single output neuron using the sigmoid activation function: the output will be a
number between 0 and 1, which you can interpret as the estimated probability of the positive
class.
    </p>
    <div class="knowledge">
     <p class="noindent">
      The estimated probability of the negative class is equal to one minus that number.
     </p>
    </div>
    <p class="noindent">
     <a class="glossary" href="DataScienceIILectureBookli2.html#glo:mlp">
      MLP
     </a>
     s can also easily handle multilabel binary classification tasks. For example, you could have an
email classification system that predicts whether each incoming email is ham or spam, and
simultaneously predicts whether it is an urgent or nonurgent email.
    </p>
    <p class="noindent">
     In this case, you would need two output neurons, both using the sigmoid activation function: the
first would output the probability that the email is spam, and the second would output
the probability that it is urgent. More generally, you would dedicate one output neuron
for each positive class. Note that the output probabilities do not necessarily add up to 1.
This lets the model output any combination of labels: you can have nonurgent ham, urgent
ham, nonurgent spam, and perhaps even urgent spam (although that would probably be an
error).
    </p>
    <p class="noindent">
     If each instance can belong only to a single class, out of three or more possible classes (e.g., classes 0
through 9 for digit image classification), then you need to have one output neuron per class, and you
should use the softmax activation function for the whole output layer (see Figure 10-9). The softmax
function (introduced in Chapter 4) will ensure that all the estimated probabilities are between 0 and 1
and that they add up to 1, since the classes are exclusive. As you saw in Chapter 3, this is called
multiclass classification.
    </p>
    <p class="noindent">
     Regarding the loss function, since we are predicting probability distributions, the cross-entropy loss (or
x-entropy or log loss for short, see Chapter 4) is generally a good choice.
    </p>
    <p class="noindent"> <span style="color:#054C5C;"><code class="verb">sklearn</code></span>
     has an <span style="color:#054C5C;"><code class="verb">MLPClassifier</code></span>
     class in the <span style="color:#054C5C;"><code class="verb">sklearn.neural_network</code></span>
     package. It is almost identical
to the <span style="color:#054C5C;"><code class="verb">MLPRegressor</code></span>
     class, except that it minimizes the cross entropy rather than the MSE. Give it a
                                                                                
                                                                                
try now, for example on the iris dataset. It’s almost a linear task, so a single layer with 5 to 10 neurons
should suffice (make sure to scale the features).
    </p>
    <p class="noindent">
    </p>
    <h2 class="sectionHead">
     <a aria-label="Link to this section in PDF version" class="pdf-link" href="DataScienceIILectureBook.pdf#section.6.3" style="color: red;  position: absolute;  left: -30px;" target="_blank">
      <svg class="bi bi-file-pdf" fill="currentColor" height="20" viewbox="0 0 16 16" width="20" xmlns="http://www.w3.org/2000/svg">
       <path d="M4 0a2 2 0 0 0-2 2v12a2 2 0 0 0 2 2h8a2 2 0 0 0 2-2V2a2 2 0 0 0-2-2H4zm0 1h8a1 1 0 0 1 1 1v12a1 1 0 0 1-1 1H4a1 1 0 0 1-1-1V2a1 1 0 0 1 1-1z">
       </path>
       <path d="M4.603 12.087a.81.81 0 0 1-.438-.42c-.195-.388-.13-.776.08-1.102.198-.307.526-.568.897-.787a7.68 7.68 0 0 1 1.482-.645 19.701 19.701 0 0 0 1.062-2.227 7.269 7.269 0 0 1-.43-1.295c-.086-.4-.119-.796-.046-1.136.075-.354.274-.672.65-.823.192-.077.4-.12.602-.077a.7.7 0 0 1 .477.365c.088.164.12.356.127.538.007.187-.012.395-.047.614-.084.51-.27 1.134-.52 1.794a10.954 10.954 0 0 0 .98 1.686 5.753 5.753 0 0 1 1.334.05c.364.065.734.195.96.465.12.144.193.32.2.518.007.192-.047.382-.138.563a1.04 1.04 0 0 1-.354.416.856.856 0 0 1-.51.138c-.331-.014-.654-.196-.933-.417a5.716 5.716 0 0 1-.911-.95 11.642 11.642 0 0 0-1.997.406 11.311 11.311 0 0 1-1.021 1.51c-.29.35-.608.655-.926.787a.793.793 0 0 1-.58.029zm1.379-1.901c-.166.076-.32.156-.459.238-.328.194-.541.383-.647.547-.094.145-.096.25-.04.361.01.022.02.036.026.044a.27.27 0 0 0 .035-.012c.137-.056.355-.235.635-.572a8.18 8.18 0 0 0 .45-.606zm1.64-1.33a12.647 12.647 0 0 1 1.01-.193 11.666 11.666 0 0 1-.51-.858 20.741 20.741 0 0 1-.5 1.05zm2.446.45c.15.162.296.3.435.41.24.19.407.253.498.256a.107.107 0 0 0 .07-.015.307.307 0 0 0 .094-.125.436.436 0 0 0 .059-.2.095.095 0 0 0-.026-.063c-.052-.062-.2-.152-.518-.209a3.881 3.881 0 0 0-.612-.053zM8.078 5.8a6.7 6.7 0 0 0 .2-.828c.031-.188.043-.343.038-.465a.613.613 0 0 0-.032-.198.517.517 0 0 0-.145.04c-.087.035-.158.106-.196.283-.04.192-.03.469.046.822.024.111.054.227.09.346z">
       </path>
      </svg>
     </a>
     <small class="titlemark">
      6.3
     </small>
     <a id="x8-930006.3">
     </a>
     Implementing
     <a class="glossary" href="DataScienceIILectureBookli2.html#glo:mlp">
      MLP
     </a>
     s with Keras
    </h2>
    <p class="noindent">
     Keras is TensorFlow’s high-level deep learning API: it allows you to build, train, evaluate, and
execute all sorts of neural networks. The original Keras 12 library was developed by Francois
Chollet as part of a research project and was released as a standalone open source project in
March 2015. It quickly gained popularity, owing to its ease of use, flexibility, and beautiful
design.
    </p>
    <div class="informationblock" id="tcolobox-70">
     <div class="title">
      <p class="noindent">
       <span id="bold" style="font-weight:bold;">
        Information
       </span>
       : Application
      </p>
     </div>
     <div class="box-content">
      <p class="noindent">
       Keras used to support multiple backends, including TensorFlow, PlaidML, Theano, and Microsoft Cognitive
Toolkit (CNTK) (the last two are sadly deprecated), but since version 2.4, Keras is TensorFlow-only. Similarly,
TensorFlow used to include multiple high-level APIs, but Keras was officially chosen as its preferred high-level
API when TensorFlow 2 came out. Installing TensorFlow will automatically install Keras as well, and Keras
will not work without TensorFlow installed. In short, Keras and TensorFlow fell in love and got married. Other
popular deep learning libraries include PyTorch by Facebook and JAX by Google.13
      </p>
     </div>
    </div>
    <h3 class="subsectionHead">
     <a aria-label="Link to this section in PDF version" class="pdf-link" href="DataScienceIILectureBook.pdf#subsection.6.3.1" style="color: red;  position: absolute;  left: -30px;" target="_blank">
      <svg class="bi bi-file-pdf" fill="currentColor" height="20" viewbox="0 0 16 16" width="20" xmlns="http://www.w3.org/2000/svg">
       <path d="M4 0a2 2 0 0 0-2 2v12a2 2 0 0 0 2 2h8a2 2 0 0 0 2-2V2a2 2 0 0 0-2-2H4zm0 1h8a1 1 0 0 1 1 1v12a1 1 0 0 1-1 1H4a1 1 0 0 1-1-1V2a1 1 0 0 1 1-1z">
       </path>
       <path d="M4.603 12.087a.81.81 0 0 1-.438-.42c-.195-.388-.13-.776.08-1.102.198-.307.526-.568.897-.787a7.68 7.68 0 0 1 1.482-.645 19.701 19.701 0 0 0 1.062-2.227 7.269 7.269 0 0 1-.43-1.295c-.086-.4-.119-.796-.046-1.136.075-.354.274-.672.65-.823.192-.077.4-.12.602-.077a.7.7 0 0 1 .477.365c.088.164.12.356.127.538.007.187-.012.395-.047.614-.084.51-.27 1.134-.52 1.794a10.954 10.954 0 0 0 .98 1.686 5.753 5.753 0 0 1 1.334.05c.364.065.734.195.96.465.12.144.193.32.2.518.007.192-.047.382-.138.563a1.04 1.04 0 0 1-.354.416.856.856 0 0 1-.51.138c-.331-.014-.654-.196-.933-.417a5.716 5.716 0 0 1-.911-.95 11.642 11.642 0 0 0-1.997.406 11.311 11.311 0 0 1-1.021 1.51c-.29.35-.608.655-.926.787a.793.793 0 0 1-.58.029zm1.379-1.901c-.166.076-.32.156-.459.238-.328.194-.541.383-.647.547-.094.145-.096.25-.04.361.01.022.02.036.026.044a.27.27 0 0 0 .035-.012c.137-.056.355-.235.635-.572a8.18 8.18 0 0 0 .45-.606zm1.64-1.33a12.647 12.647 0 0 1 1.01-.193 11.666 11.666 0 0 1-.51-.858 20.741 20.741 0 0 1-.5 1.05zm2.446.45c.15.162.296.3.435.41.24.19.407.253.498.256a.107.107 0 0 0 .07-.015.307.307 0 0 0 .094-.125.436.436 0 0 0 .059-.2.095.095 0 0 0-.026-.063c-.052-.062-.2-.152-.518-.209a3.881 3.881 0 0 0-.612-.053zM8.078 5.8a6.7 6.7 0 0 0 .2-.828c.031-.188.043-.343.038-.465a.613.613 0 0 0-.032-.198.517.517 0 0 0-.145.04c-.087.035-.158.106-.196.283-.04.192-.03.469.046.822.024.111.054.227.09.346z">
       </path>
      </svg>
     </a>
     <small class="titlemark">
      6.3.1
     </small>
     <a id="x8-940006.3.1">
     </a>
     Building an Image Classifier Using Sequential API
    </h3>
    <p class="noindent">
     Before we do anything, we need to load a dataset. We will use Fashion MNIST. There are 70,000
grayscale images of 28 Œ 28 pixels each, with 10 classes where images represent fashion items rather
than handwritten digits, so each class is more diverse, and the problem turns out to be significantly
challenging.
     <a id="subsubsection*.17">
     </a>
    </p>
    <h5 class="subsubsectionHead">
     <a id="x8-95000">
     </a>
     Using Keras to load the dataset
    </h5>
    <p class="noindent"> <span style="color:#054C5C;"><code class="verb">keras</code></span>
     provides utility functions to fetch and load common datasets, including MNIST, Fashion
MNIST, and a few more.
    </p>
    <p class="noindent">
     Let’s load Fashion MNIST. It’s already shuffled and split into a training set (60,000 images) and a
test set (10,000 images), but we’ll hold out the last 5,000 images from the training set for
validation:
    </p>
    <pre><div id="fancyvrb138" style="padding:20px;border-radius: 3px;"><a id="x8-95002r113"></a><span style="color:#2B2BFF;">import</span><span style="color:#BABABA;"> </span>tensorflow<span style="color:#BABABA;"> </span><span style="color:#2B2BFF;">as</span><span style="color:#BABABA;"> </span>tf 
<a id="x8-95004r114"></a> 
<a id="x8-95006r115"></a>fashion_mnist = tf.keras.datasets.fashion_mnist.load_data() 
<a id="x8-95008r116"></a>(X_train_full, y_train_full), (X_test, y_test) = fashion_mnist 
<a id="x8-95010r117"></a>X_train, y_train = X_train_full[:-<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">5000</span></span>], y_train_full[:-<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">5000</span></span>] 
<a id="x8-95012r118"></a>X_valid, y_valid = X_train_full[-<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">5000</span></span>:], y_train_full[-<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">5000</span></span>:]</div></pre>
    <div class="knowledge">
     <p class="noindent">
      TensorFlow is usually imported as <span style="color:#054C5C;"><code class="verb">tf</code></span>, and the Keras API is available via <span style="color:#054C5C;"><code class="verb">tf.keras</code></span>.
     </p>
    </div>
    <div class="warning">
     <p class="noindent">
      When loading MNIST or Fashion MNIST using <span style="color:#054C5C;"><code class="verb">tf.keras</code></span>
      rather than <span style="color:#054C5C;"><code class="verb">sklearn</code></span>, an important difference
is that every image is represented as a 28-by-28 array rather than a 1D array of size 784 with
intensities are represented as integers (from 0 to 255) rather than floats (from 0.0 to 255.0).
     </p>
    </div>
    <p class="noindent">
     Let’s take a look at the shape and data type of the training set:
    </p>
    <p class="noindent">
    </p>
    <pre><div id="fancyvrb139" style="padding:20px;border-radius: 3px;"><a id="x8-95014r125"></a><span style="color:#2B2BFF;">print</span>(<span style="color:#800080;">"The size of the training dataset: "</span>, X_train.shape) 
<a id="x8-95016r126"></a><span style="color:#2B2BFF;">print</span>(<span style="color:#800080;">"The type of the training dataset: "</span>, X_train.dtype)</div></pre>
    <div class="tcolorbox tcolorbox" id="tcolobox-71">
     <div class="tcolorbox-content">
      <pre><div id="fancyvrb140" style="padding:20px;border-radius: 3px;"><a id="x8-95018r1"></a>The size of the training dataset:  (55000, 28, 28) 
<a id="x8-95020r2"></a>The type of the training dataset:  uint8</div></pre>
     </div>
    </div>
    <p class="noindent">
     To make it simple, we’ll scale the pixel intensities down to the 0-1 range by dividing them by
255.0
    </p>
    <div class="knowledge">
     <p class="noindent">
      This operation also converts the integer values to floats.
     </p>
    </div>
    <p class="noindent">
    </p>
    <pre><div id="fancyvrb141" style="padding:20px;border-radius: 3px;"><a id="x8-95022r137"></a>X_train, X_valid, X_test = X_train / <span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">255.</span></span>, X_valid / <span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">255.</span></span>, X_test / <span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">255.</span></span></div></pre>
    <p class="noindent">
     Using Fashion MNIST, we need the list of class names to know what we are dealing with:
    </p>
    <pre><div id="fancyvrb142" style="padding:20px;border-radius: 3px;"><a id="x8-95024r144"></a>class_names = [<span style="color:#800080;">"T-shirt/top"</span>, <span style="color:#800080;">"Trouser"</span>, <span style="color:#800080;">"Pullover"</span>, <span style="color:#800080;">"Dress"</span>, <span style="color:#800080;">"Coat"</span>, 
<a id="x8-95026r145"></a>               <span style="color:#800080;">"Sandal"</span>, <span style="color:#800080;">"Shirt"</span>, <span style="color:#800080;">"Sneaker"</span>, <span style="color:#800080;">"Bag"</span>, <span style="color:#800080;">"Ankle boot"</span>]</div></pre>
    <p class="noindent">
     For example, the first image in the training set represents an ankle boot:
    </p>
    <div class="figure">
     <img alt="PIC" height="" src="codes/images/Introduction-to-Artificial-Neural-Networks/ankle-boot-.svg" width="150%"/>
     <a id="x8-95027r8">
     </a>
     <figcaption class="caption">
      <span class="id">
       Figure 6.8:
      </span>
      <span class="content">
       An example of a data within the Fashion MNIST.
      </span>
     </figcaption>
    </div>
    <p class="noindent">
     and below we can see some examples of the Fashion MNIST dataset.
    </p>
    <div class="figure">
     <p class="noindent">
      <img alt="PIC" height="" src="codes/images/Introduction-to-Artificial-Neural-Networks/fashion_mnist_plot-.svg" width="150%"/>
      <a id="x8-95028r9">
      </a>
     </p>
     <figcaption class="caption">
      <span class="id">
       Figure 6.9:
      </span>
      <span class="content">
       A random collection of dataset, making the Fashion MNIST.
      </span>
     </figcaption>
    </div>
    <h3 class="subsectionHead">
     <a aria-label="Link to this section in PDF version" class="pdf-link" href="DataScienceIILectureBook.pdf#subsection.6.3.2" style="color: red;  position: absolute;  left: -30px;" target="_blank">
      <svg class="bi bi-file-pdf" fill="currentColor" height="20" viewbox="0 0 16 16" width="20" xmlns="http://www.w3.org/2000/svg">
       <path d="M4 0a2 2 0 0 0-2 2v12a2 2 0 0 0 2 2h8a2 2 0 0 0 2-2V2a2 2 0 0 0-2-2H4zm0 1h8a1 1 0 0 1 1 1v12a1 1 0 0 1-1 1H4a1 1 0 0 1-1-1V2a1 1 0 0 1 1-1z">
       </path>
       <path d="M4.603 12.087a.81.81 0 0 1-.438-.42c-.195-.388-.13-.776.08-1.102.198-.307.526-.568.897-.787a7.68 7.68 0 0 1 1.482-.645 19.701 19.701 0 0 0 1.062-2.227 7.269 7.269 0 0 1-.43-1.295c-.086-.4-.119-.796-.046-1.136.075-.354.274-.672.65-.823.192-.077.4-.12.602-.077a.7.7 0 0 1 .477.365c.088.164.12.356.127.538.007.187-.012.395-.047.614-.084.51-.27 1.134-.52 1.794a10.954 10.954 0 0 0 .98 1.686 5.753 5.753 0 0 1 1.334.05c.364.065.734.195.96.465.12.144.193.32.2.518.007.192-.047.382-.138.563a1.04 1.04 0 0 1-.354.416.856.856 0 0 1-.51.138c-.331-.014-.654-.196-.933-.417a5.716 5.716 0 0 1-.911-.95 11.642 11.642 0 0 0-1.997.406 11.311 11.311 0 0 1-1.021 1.51c-.29.35-.608.655-.926.787a.793.793 0 0 1-.58.029zm1.379-1.901c-.166.076-.32.156-.459.238-.328.194-.541.383-.647.547-.094.145-.096.25-.04.361.01.022.02.036.026.044a.27.27 0 0 0 .035-.012c.137-.056.355-.235.635-.572a8.18 8.18 0 0 0 .45-.606zm1.64-1.33a12.647 12.647 0 0 1 1.01-.193 11.666 11.666 0 0 1-.51-.858 20.741 20.741 0 0 1-.5 1.05zm2.446.45c.15.162.296.3.435.41.24.19.407.253.498.256a.107.107 0 0 0 .07-.015.307.307 0 0 0 .094-.125.436.436 0 0 0 .059-.2.095.095 0 0 0-.026-.063c-.052-.062-.2-.152-.518-.209a3.881 3.881 0 0 0-.612-.053zM8.078 5.8a6.7 6.7 0 0 0 .2-.828c.031-.188.043-.343.038-.465a.613.613 0 0 0-.032-.198.517.517 0 0 0-.145.04c-.087.035-.158.106-.196.283-.04.192-.03.469.046.822.024.111.054.227.09.346z">
       </path>
      </svg>
     </a>
     <small class="titlemark">
      6.3.2
     </small>
     <a id="x8-960006.3.2">
     </a>
     Creating the model using the sequential API
    </h3>
    <p class="noindent">
     It is time to build the neural network. Here is a classification
     <a class="glossary" href="DataScienceIILectureBookli2.html#glo:mlp">
      MLP
     </a>
     with two <alert style="color: #821131;">(2)</alert> hidden
layers:
    </p>
    <pre><div id="fancyvrb143" style="padding:20px;border-radius: 3px;"><a id="x8-96002r188"></a>tf.random.set_seed(<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">42</span></span>) 
<a id="x8-96004r189"></a>model = tf.keras.Sequential() 
<a id="x8-96006r190"></a>model.add(tf.keras.layers.InputLayer(shape=[<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">28</span></span>, <span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">28</span></span>])) 
<a id="x8-96008r191"></a>model.add(tf.keras.layers.Flatten()) 
<a id="x8-96010r192"></a>model.add(tf.keras.layers.Dense(<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">300</span></span>, activation=<span style="color:#800080;">"relu"</span>)) 
<a id="x8-96012r193"></a>model.add(tf.keras.layers.Dense(<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">100</span></span>, activation=<span style="color:#800080;">"relu"</span>)) 
<a id="x8-96014r194"></a>model.add(tf.keras.layers.Dense(<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">10</span></span>, activation=<span style="color:#800080;">"softmax"</span>))</div></pre>
    <p class="noindent">
     Let’s try to understand the code:
    </p>
    <dl class="enumerate-enumitem">
     <dt class="enumerate-enumitem">
      1.
     </dt>
     <dd class="enumerate-enumitem">
      Set <span style="color:#054C5C;"><code class="verb">tf</code></span>
      random  seed  to  make  the  results  reproducible:  the  random  weights  of  the
hidden  layers  and  the  output  layer  will  be  the  same  every  time  you  run  your  code.
You could also choose to use the <span style="color:#054C5C;"><code class="verb">tf.keras.utils.set_random_seed()</code></span>
      function, which
conveniently sets the random seeds for TensorFlow, Python <span style="color:#054C5C;">(<code class="verb">random.seed()</code>)</span>, and NumPy <span style="color:#054C5C;">(<code class="verb">np.random.seed()</code>)</span>.
     </dd>
     <dt class="enumerate-enumitem">
      2.
     </dt>
     <dd class="enumerate-enumitem">
      Next line creates a
      <italic>
       Sequential model
      </italic>
      . This is the simplest kind of Keras model for neural
networks, composed of a single stack of layers connected sequentially. This is called the
sequential API.
     </dd>
     <dt class="enumerate-enumitem">
      3.
     </dt>
     <dd class="enumerate-enumitem">
      We build the first layer (an Input layer) and add it to the model. We specify the input
shape, which doesn’t include the batch size, only the shape of the instances. Keras needs
to know the shape of the inputs so it can determine the shape of the connection weight
matrix of the first hidden layer.
     </dd>
     <dt class="enumerate-enumitem">
      4.
     </dt>
     <dd class="enumerate-enumitem">
      We add a Flatten layer. Its role is to convert each input image into a 1D array: for example,
if it receives a batch of shape [32, 28, 28], it will reshape it to [32, 784]. In other words,
if it receives input data X, it computes <span style="color:#054C5C;"><code class="verb">X.reshape(-1, 784)</code></span>. This layer doesn’t have any
parameters; it’s just there to do some simple pre-processing.
     </dd>
     <dt class="enumerate-enumitem">
      5.
     </dt>
     <dd class="enumerate-enumitem">
      We add a Dense hidden layer with 300 neurons. It will use the ReLU activation function.
Each Dense layer manages its own weight matrix, containing all the connection weights
between the neurons and their inputs. It also manages a vector of bias terms (one per
neuron).
     </dd>
     <dt class="enumerate-enumitem">
      6.
     </dt>
     <dd class="enumerate-enumitem">
      We add a second Dense hidden layer with 100 neurons, also using the ReLU activation
function.
     </dd>
     <dt class="enumerate-enumitem">
      7.
     </dt>
     <dd class="enumerate-enumitem">
      We add a Dense output layer with 10 neurons (one per class), using the softmax activation
function because the classes are exclusive.
     </dd>
    </dl>
    <div class="knowledge">
     <p class="noindent">
      Writing the argument <span style="color:#054C5C;"><code class="verb">activation="relu"</code></span>
      is equivalent to specifying <span style="color:#054C5C;"><code class="verb">activation=tf.keras.activations.relu</code></span>.
Other activation functions are available in the <span style="color:#054C5C;"><code class="verb">tf.keras.activations</code></span>
      package.
     </p>
    </div>
    <p class="noindent">
     Instead of adding the layers one by one as we just did, it’s often more convenient to pass a list of layers
when creating the Sequential model. You can also drop the Input layer and instead specify the <span style="color:#054C5C;"><code class="verb">input_shape</code></span>
     in the first layer:
    </p>
    <pre><div id="fancyvrb144" style="padding:20px;border-radius: 3px;"><a id="x8-96023r199"></a>tf.keras.backend.clear_session() 
<a id="x8-96025r200"></a>tf.random.set_seed(<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">42</span></span>) 
<a id="x8-96027r201"></a> 
<a id="x8-96029r202"></a>model = tf.keras.Sequential([ 
<a id="x8-96031r203"></a>    tf.keras.layers.Flatten(input_shape=[<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">28</span></span>, <span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">28</span></span>]), 
<a id="x8-96033r204"></a>    tf.keras.layers.Dense(<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">300</span></span>, activation=<span style="color:#800080;">"relu"</span>), 
<a id="x8-96035r205"></a>    tf.keras.layers.Dense(<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">100</span></span>, activation=<span style="color:#800080;">"relu"</span>), 
<a id="x8-96037r206"></a>    tf.keras.layers.Dense(<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">10</span></span>, activation=<span style="color:#800080;">"softmax"</span>) 
<a id="x8-96039r207"></a>])</div></pre>
    <p class="noindent">
     The model’s <span style="color:#054C5C;"><code class="verb">summary()</code></span>
     method displays all the model’s layers, including each layer’s name, which is
automatically generated, its output shape, and its number of parameters.
    </p>
    <p class="noindent">
     The summary ends with the total number of parameters, including
     <alert style="color: #821131;">
      trainable
     </alert>
     and
     <alert style="color: #821131;">
      non-trainable
     </alert>
     parameters. Here we only have trainable parameters:
    </p>
    <pre><div id="fancyvrb145" style="padding:20px;border-radius: 3px;"><a id="x8-96041r232"></a>tf.keras.utils.plot_model(model, imagePath+<span style="color:#800080;">"mnist-_model.pdf"</span>, show_shapes=<span style="color:#2B2BFF;"><span id="bold" style="font-weight:bold;">True</span></span>)</div></pre>
    <div class="figure">
     <p class="noindent">
      <img alt="PIC" height="" src="codes/images/Introduction-to-Artificial-Neural-Networks/mnist-model-.svg" width="150%"/>
      <a id="x8-96042r10">
      </a>
     </p>
     <figcaption class="caption">
      <span class="id">
       Figure 6.10:
      </span>
      <span class="content">
       The plot of the neural network, showcasing its layers.
      </span>
     </figcaption>
    </div>
    <p class="noindent">
     Dense layers often have a lot of parameters. For example, the first hidden layer has 784-by-300
connection weights, with 300 bias terms, which adds up to 235,500 parameters.
    </p>
    <p class="noindent">
     This gives the model quite a lot of flexibility to fit the training data, but it also means that
the model runs the risk of
     <span id="bold" style="font-weight:bold;">
      over-fitting
     </span>
     , especially when you do not have a lot of training
data.
    </p>
    <p class="noindent">
     Each layer in a model must have a unique name (e.g., <span style="color:#054C5C;"><code class="verb">dense_2</code></span>
     ). You can set the layer names explicitly
using the constructor’s name argument, but generally it’s simpler to let Keras name the layers
automatically, as we just did. Keras takes the layer’s class name and converts it to snake case (i.e., a
layer from the <span style="color:#054C5C;"><code class="verb">MyCoolLayer</code></span>
     class is named <span style="color:#054C5C;"><code class="verb">my_cool_layer</code></span>
     by default). Keras also ensures that
the name is
     <span id="bold" style="font-weight:bold;">
      globally unique
     </span>
     , even across models, by appending an index if needed, as in <span style="color:#054C5C;"><code class="verb">dense_2</code></span>.
    </p>
    <p class="noindent">
     This naming scheme makes it possible to merge models easily without getting name conflicts.
    </p>
    <div class="knowledge">
     <p class="noindent">
      All global state managed by Keras is stored in a Keras session, which you can clear using <span style="color:#054C5C;"><code class="verb">tf.keras.backend.clear_session()</code></span>.
     </p>
    </div>
    <p class="noindent">
     You can easily get a model’s list of layers using the layers attribute, or use the <span style="color:#054C5C;"><code class="verb">get_layer()</code></span>
     method to
access a layer by name:
    </p>
    <pre><div id="fancyvrb146" style="padding:20px;border-radius: 3px;"><a id="x8-96044r239"></a><span style="color:#2B2BFF;">print</span>(model.layers)</div></pre>
    <pre><div id="fancyvrb147" style="padding:20px;border-radius: 3px;"><a id="x8-96046r244"></a>[&lt;Flatten name=flatten, built=True&gt;, 
<a id="x8-96048r245"></a>&lt;Dense name=dense, built=True&gt;, 
<a id="x8-96050r246"></a>&lt;Dense name=dense_1, built=True&gt;, 
<a id="x8-96052r247"></a>&lt;Dense name=dense_2, built=True&gt;]</div></pre>
    <p class="noindent">
     All the parameters of a layer can be accessed using its <span style="color:#054C5C;"><code class="verb">get_weights()</code></span>
     and <span style="color:#054C5C;"><code class="verb">set_weights()</code></span>
     methods.
    </p>
    <p class="noindent">
     For a Dense layer, this includes both the connection weights and the bias terms:
    </p>
    <pre><div id="fancyvrb148" style="padding:20px;border-radius: 3px;"><a id="x8-96054r252"></a>hidden1 = model.layers[<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">1</span></span>] 
<a id="x8-96056r253"></a>weights, biases = hidden1.get_weights() 
<a id="x8-96058r254"></a><span style="color:#2B2BFF;">print</span>(weights)</div></pre>
    <pre><div id="fancyvrb149" style="padding:20px;border-radius: 3px;"><a id="x8-96060r259"></a>[[-0.05415904  0.00010975 -0.00299759 ...  0.05136904  0.0740822 
<a id="x8-96062r260"></a>   0.06472497] 
<a id="x8-96064r261"></a> [ 0.05510217 -0.01353022 -0.00363479 ...  0.07100512 -0.04926914 
<a id="x8-96066r262"></a>  -0.02905609] 
<a id="x8-96068r263"></a> [-0.07024231  0.02524897 -0.04784295 ... -0.0521326   0.05084455 
<a id="x8-96070r264"></a>  -0.06636713] 
<a id="x8-96072r265"></a> ... 
<a id="x8-96074r266"></a> [ 0.0067075  -0.00256791 -0.064556   ...  0.05266081  0.03520959 
<a id="x8-96076r267"></a>  -0.02309504] 
<a id="x8-96078r268"></a> [ 0.05826265 -0.0361187  -0.04228947 ...  0.05612285 -0.03179397 
<a id="x8-96080r269"></a>   0.06843598] 
<a id="x8-96082r270"></a> [ 0.06636336 -0.00123435 -0.00247347 ...  0.01809192  0.03434542 
<a id="x8-96084r271"></a>   0.00700523]]</div></pre>
    <p class="noindent">
     Notice that the Dense layer initialized the connection weights randomly.
    </p>
    <div class="warning">
     <p class="noindent">
      This is needed to break symmetry.
     </p>
    </div>
    <p class="noindent">
     The biases were initialized to zeros, which is fine.
    </p>
    <pre><div id="fancyvrb150" style="padding:20px;border-radius: 3px;"><a id="x8-96086r276"></a><span style="color:#2B2BFF;">print</span>(biases)</div></pre>
    <pre><div id="fancyvrb151" style="padding:20px;border-radius: 3px;"><a id="x8-96088r281"></a>[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 
<a id="x8-96090r282"></a> 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 
<a id="x8-96092r283"></a> 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 
<a id="x8-96094r284"></a> 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 
<a id="x8-96096r285"></a> 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 
<a id="x8-96098r286"></a> 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 
<a id="x8-96100r287"></a> 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 
<a id="x8-96102r288"></a> 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 
<a id="x8-96104r289"></a> 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 
<a id="x8-96106r290"></a> 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 
<a id="x8-96108r291"></a> 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 
<a id="x8-96110r292"></a> 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 
<a id="x8-96112r293"></a> 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]</div></pre>
    <p class="noindent">
     If you want to use a different initialization method, you can set <span style="color:#054C5C;"><code class="verb">kernel_initializer</code></span>
     or <span style="color:#054C5C;"><code class="verb">bias_initializer</code></span>
     when creating the layer.
    </p>
    <div class="informationblock" id="tcolobox-72">
     <div class="title">
      <p class="noindent">
       <span id="bold" style="font-weight:bold;">
        Information
       </span>
       : Weight Matrix Shape
      </p>
     </div>
     <div class="box-content">
      <p class="noindent">
       The shape of the weight matrix depends on the number of inputs, which is why we specified the <span style="color:#054C5C;"><code class="verb">input_shape</code></span>
       when creating the model. If you do not specify the input shape, it’s OK: Keras will simply wait until it knows
the input shape before it actually builds the model parameters. This will happen either when you feed it some
data (e.g., during training), or when you call its build() method. Until the model parameters are built, you will
not be able to do certain things, such as display the model summary or save the model. So, if you know the
input shape when creating the model, it is best to specify it.
      </p>
     </div>
    </div>
    <a id="subsubsection*.18">
    </a>
    <h5 class="subsubsectionHead">
     <a id="x8-97000">
     </a>
     Model Compiling
    </h5>
    <p class="noindent">
     After a model is created, we need to call its <span style="color:#054C5C;"><code class="verb">compile()</code></span>
     method to specify the loss function and the
optimizer to use, or you can specify a list of extra metrics to compute during training and
evaluation:
    </p>
    <pre><div id="fancyvrb152" style="padding:20px;border-radius: 3px;"><a id="x8-97002r298"></a>model.compile(loss=<span style="color:#800080;">"sparse_categorical_crossentropy"</span>, 
<a id="x8-97004r299"></a>              optimizer=<span style="color:#800080;">"sgd"</span>, 
<a id="x8-97006r300"></a>              metrics=[<span style="color:#800080;">"accuracy"</span>])</div></pre>
    <p class="noindent">
     Before continuing, we need to explain what is going on here.
    </p>
    <p class="noindent">
     We use the <span style="color:#054C5C;"><code class="verb">sparse_categorical_crossentropy</code></span>
     loss because we have sparse labels (i.e., for each
instance, there is just a target class index, from 0 to 9 in this case), and the classes are
     <span id="bold" style="font-weight:bold;">
      exclusive
     </span>
     .
    </p>
    <ul class="itemize1">
     <li class="itemize">
      <p class="noindent">
       If we had one target probability per class for each instance (such as one-hot vectors, e.g.,
[0., 0., 0., 1., 0., 0., 0., 0., 0., 0.] to represent class 3), then we would need to use the <span style="color:#054C5C;"><code class="verb">categorical_crossentropy</code></span>
       loss instead.
      </p>
     </li>
     <li class="itemize">
      <p class="noindent">
       If we were doing binary classification or multilabel binary classification, then we would use
the
       <alert style="color: #821131;">
        sigmoid activation
       </alert>
       function in the output layer instead of the softmax activation
function, and we would use the <span style="color:#054C5C;"><code class="verb">binary_crossentropy</code></span>
       loss.
      </p>
     </li>
    </ul>
    <p class="noindent">
     Regarding the optimizer, <span style="color:#054C5C;"><code class="verb">sgd</code></span>
     means that we will train the model using stochastic gradient descent.
Keras will perform the backpropagation algorithm described earlier (i.e., reverse-mode autodiff plus
gradient descent).
    </p>
    <p class="noindent">
     Finally, as this is a classifier, it’s useful to measure its accuracy during training and evaluation, which is
why we set <span style="color:#054C5C;"><code class="verb">metrics=["accuracy"]</code></span>.
     <a id="subsubsection*.19">
     </a>
    </p>
    <h5 class="subsubsectionHead">
     <a id="x8-98000">
     </a>
     Training and Evaluating Models
    </h5>
    <p class="noindent">
     Now the model is ready to be trained. For this we simply need to call its <span style="color:#054C5C;"><code class="verb">fit()</code></span>
     method:
    </p>
    <pre><div id="fancyvrb153" style="padding:20px;border-radius: 3px;"><a id="x8-98002r305"></a>history = model.fit(X_train, y_train, epochs=<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">30</span></span>, 
<a id="x8-98004r306"></a>                    validation_data=(X_valid, y_valid))</div></pre>
    <p class="noindent">
     We pass it the input features <span style="color:#054C5C;">(<code class="verb">X_train</code>)</span> and the target classes <span style="color:#054C5C;">(<code class="verb">y_train</code>)</span>, as well as the number of
epochs to train (or else it would default to just 1, which would definitely not be enough to converge to a
good solution).
    </p>
    <p class="noindent">
     We also pass a validation set which is optional. Keras will measure the loss and the extra metrics on
this set at the end of each epoch, which is very useful to see how well the model really
performs.
    </p>
    <div class="warning">
     <p class="noindent">
      If the performance on the training set is much better than on the validation set, the model is probably
overfitting the training set, or there is a bug, such as a data mismatch between the training set and the
validation set.
     </p>
    </div>
    <p class="noindent">
     And that’s it! The neural network is trained. At each epoch during training, Keras displays the number
of mini-batches processed so far on the left side of the progress bar.
    </p>
    <p class="noindent">
     The batch size is 32 by default, and since the training set has 55,000 images, the model goes through
1,719 batches per epoch: 1,718 of size 32, and 1 of size 24.
    </p>
    <p class="noindent">
     After the progress bar, you can see the mean training time per sample, and the loss and accuracy (or
any other extra metrics you asked for) on both the training set and the validation set and notice that
the training loss went down, which is a good sign, and the validation accuracy reached 88.94% after 30
epochs.
    </p>
    <p class="noindent">
     That’s slightly below the training accuracy, so there is a little bit of overfitting going on, but not a huge
amount.
    </p>
    <p class="noindent">
     If the training set was very skewed, with some classes being overrepresented and others
underrepresented, it would be useful to set the <span style="color:#054C5C;"><code class="verb">class_weight</code></span>
     argument when calling the <span style="color:#054C5C;"><code class="verb">fit()</code></span>
     method, to give a larger weight to underrepresented classes and a lower weight to overrepresented
classes.
    </p>
    <p class="noindent">
     These weights would be used by Keras when computing the loss. If you need per-instance weights, set
the <span style="color:#054C5C;"><code class="verb">sample_weight</code></span>
     argument. If both <span style="color:#054C5C;"><code class="verb">class_weight</code></span>
     and <span style="color:#054C5C;"><code class="verb">sample_weight</code></span>
     are provided, then Keras
multiplies them. Per-instance weights could be useful, for example, if some instances were labeled by
experts while others were labeled using a crowdsourcing platform: you might want to give more weight
to the former.
    </p>
    <p class="noindent">
     You can also provide sample weights (but not class weights) for the validation set by adding them as
a third item in the <span style="color:#054C5C;"><code class="verb">validation_data</code></span>
     tuple. The <span style="color:#054C5C;"><code class="verb">fit()</code></span>
     method returns a History object
containing the training parameters (history.params), the list of epochs it went through
(history.epoch), and most importantly a dictionary (history.history) containing the loss and extra
metrics it measured at the end of each epoch on the training set and on the validation set (if
any).
    </p>
    <pre><div id="fancyvrb154" style="padding:20px;border-radius: 3px;"><a id="x8-98006r312"></a><span style="color:#2B2BFF;">print</span>(history.params) 
<a id="x8-98008r313"></a><span style="color:#2B2BFF;">print</span>(history.epoch)</div></pre>
    <p class="noindent">
    </p>
    <pre><div id="fancyvrb155" style="padding:20px;border-radius: 3px;"><a id="x8-98010r318"></a>{'verbose': 'auto', 'epochs': 30, 'steps': 1719} 
<a id="x8-98012r319"></a>[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]</div></pre>
    <p class="noindent">
     If you use this dictionary to create a Pandas DataFrame and call its <span style="color:#054C5C;"><code class="verb">plot()</code></span>
     method, you get the
learning curves shown in Fig.
     <a href="#x8-98013r11">
      6.11
     </a>.
    </p>
    <div class="figure">
     <p class="noindent">
      <img alt="PIC" height="" src="codes/images/Introduction-to-Artificial-Neural-Networks/keras-learning-curves-plot-.svg" width="150%"/>
      <a id="x8-98013r11">
      </a>
     </p>
     <figcaption class="caption">
      <span class="id">
       Figure 6.11:
      </span>
      <span class="content">
       Learning curves: the mean training loss and accuracy measured over each epoch, and the mean validation
          loss and accuracy measured at the end of each epoch
      </span>
     </figcaption>
    </div>
    <p class="noindent">
     You can see that both the training accuracy and the validation accuracy steadily increase during
training, while the training loss and the validation loss decrease.
    </p>
    <p class="noindent">
     This is good.
    </p>
    <p class="noindent">
     The validation curves are relatively close to each other at first, but they get further apart over time,
which shows that there’s a little bit of overfitting. In this particular case, the model looks like it
performed better on the validation set than on the training set at the beginning of training, but that’s
not actually the case.
    </p>
    <p class="noindent">
     The validation error is computed at the end of each epoch, while the training error is computed using a
     <alert style="color: #821131;">
      running mean
     </alert>
     during each epoch, so the training curve should be shifted by half an epoch to the
left.
    </p>
    <p class="noindent">
     If you do that, you will see that the training and validation curves overlap almost perfectly at the
beginning of training. The training set performance ends up beating the validation performance, as is
generally the case when you train for long enough.
    </p>
    <p class="noindent">
     You can tell that the model has not quite converged yet, as the validation loss is still going
down, so it would be better to continue training. This is as simple as calling the <span style="color:#054C5C;"><code class="verb">fit()</code></span>
     method again, as Keras just continues training where it left off: you should be able to reach
about 89.8% validation accuracy, while the training accuracy will continue to rise up to
100%.
    </p>
    <div class="warning">
     <p class="noindent">
      This is not always the case.
     </p>
    </div>
    <p class="noindent">
     If you are not satisfied with the performance of your model, it is a good idea to back and tune the
hyperparameters.
    </p>
    <dl class="enumerate-enumitem">
     <dt class="enumerate-enumitem">
      1.
     </dt>
     <dd class="enumerate-enumitem">
      First check the learning rate (
      <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
       <mi>
        η
       </mi>
      </math>
      ).
     </dd>
     <dt class="enumerate-enumitem">
      2.
     </dt>
     <dd class="enumerate-enumitem">
      If  that  doesn’t  help,  try  another  optimizer,  and  always  retune  the  learning  rate  after
changing any hyperparameter,
     </dd>
     <dt class="enumerate-enumitem">
      3.
     </dt>
     <dd class="enumerate-enumitem">
      If the performance is still not great, try tuning model hyperparameters such as the number
of layers, the number of neurons per layer, and the types of activation functions to use for
each hidden layer.
     </dd>
    </dl>
    <p class="noindent">
     You can also try tuning other hyperparameters, such as the batch size (it can be set in the <span style="color:#054C5C;"><code class="verb">fit()</code></span>
     method using the <span style="color:#054C5C;"><code class="verb">batch_size</code></span>
     argument, which defaults to 32).
    </p>
    <p class="noindent">
     Once you are satisfied with your model’s validation accuracy, you should evaluate it on the test set to
estimate the generalization error before you deploy the model to production. You can easily do this
using the <span style="color:#054C5C;"><code class="verb">evaluate()</code></span>
     method.
    </p>
    <div class="knowledge">
     <p class="noindent">
      It also supports several other arguments, such as <span style="color:#054C5C;"><code class="verb">batch_size</code></span>
      and <span style="color:#054C5C;"><code class="verb">sample_weight</code></span>.
     </p>
    </div>
    <p class="noindent">
     It is common to get slightly lower performance on the test set than on the validation set, as
hyperparameters are
     <span id="bold" style="font-weight:bold;">
      tuned on the validation set
     </span>
     , not the test set. However, in this example, we did
                                                                                
                                                                                
not do any hyperparameter tuning, so the lower accuracy is just bad luck.
    </p>
    <div class="warning">
     <p class="noindent">
      Resist the temptation to tweak the hyperparameters on the test set, or else your estimate of the
generalization error will be too optimistic.
     </p>
    </div>
    <a id="subsubsection*.20">
    </a>
    <h5 class="subsubsectionHead">
     <a id="x8-99000">
     </a>
     Using Model to Make Predictions
    </h5>
    <p class="noindent">
     It is time to use the model’s <span style="color:#054C5C;"><code class="verb">predict()</code></span>
     method to make predictions on new instances. As
we don’t have actual new instances, we’ll just use the first three <alert style="color: #821131;">(3)</alert> instances of the test
set:
    </p>
    <pre><div id="fancyvrb156" style="padding:20px;border-radius: 3px;"><a id="x8-99002r340"></a>X_new = X_test[:<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">3</span></span>] 
<a id="x8-99004r341"></a>y_proba = model.predict(X_new) 
<a id="x8-99006r342"></a><span style="color:#2B2BFF;">print</span>(y_proba.round(<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">2</span></span>))</div></pre>
    <p class="noindent">
    </p>
    <pre><div id="fancyvrb157" style="padding:20px;border-radius: 3px;"><a id="x8-99008r347"></a>[[0.   0.   0.   0.   0.   0.12 0.   0.01 0.   0.87] 
<a id="x8-99010r348"></a> [0.   0.   1.   0.   0.   0.   0.   0.   0.   0.  ] 
<a id="x8-99012r349"></a> [0.   1.   0.   0.   0.   0.   0.   0.   0.   0.  ]]</div></pre>
    <p class="noindent">
     For each instance the model estimates one probability per class, from class 0 to class 9. This is similar
to the output of the <span style="color:#054C5C;"><code class="verb">predict_proba()</code></span>
     method in <span style="color:#054C5C;"><code class="verb">sklearn</code></span>
     classifiers.
    </p>
    <p class="noindent">
     For example, for the first image it estimates that the probability of class 9 (ankle boot) is 87%, the
probability of class 7 (sneaker) is 1%, the probability of class 5 (sandal) is 12%, and the probabilities of
the other classes are negligible.
    </p>
    <p class="noindent">
     In other words, it is highly confident that the first image is footwear, most likely ankle boots but
possibly sneakers or sandals. If you only care about the class with the highest estimated probability
(even if that probability is quite low), then you can use the <span style="color:#054C5C;"><code class="verb">argmax()</code></span>
     method to get the highest
probability class index for each instance:
    </p>
    <pre><div id="fancyvrb158" style="padding:20px;border-radius: 3px;"><a id="x8-99014r355"></a>y_pred = y_proba.argmax(axis=-<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">1</span></span>) 
<a id="x8-99016r356"></a><span style="color:#2B2BFF;">print</span>(y_pred)</div></pre>
    <p class="noindent">
    </p>
    <pre><div id="fancyvrb159" style="padding:20px;border-radius: 3px;"><a id="x8-99018r361"></a>[9 2 1]</div></pre>
    <p class="noindent">
     Here, the classifier actually classified all three images correctly, where these images are shown
in Fig.
     <a href="#x8-99019r12">
      6.12
     </a>.
    </p>
    <div class="figure">
     <img alt="PIC" height="" src="codes/images/Introduction-to-Artificial-Neural-Networks/fashion-mnist-images-plot-.svg" width="150%"/>
     <a id="x8-99019r12">
     </a>
     <figcaption class="caption">
      <span class="id">
       Figure 6.12:
      </span>
      <span class="content">
       Correctly classified Fashion MNIST images.
      </span>
     </figcaption>
    </div>
    <h3 class="subsectionHead">
     <a aria-label="Link to this section in PDF version" class="pdf-link" href="DataScienceIILectureBook.pdf#subsection.6.3.3" style="color: red;  position: absolute;  left: -30px;" target="_blank">
      <svg class="bi bi-file-pdf" fill="currentColor" height="20" viewbox="0 0 16 16" width="20" xmlns="http://www.w3.org/2000/svg">
       <path d="M4 0a2 2 0 0 0-2 2v12a2 2 0 0 0 2 2h8a2 2 0 0 0 2-2V2a2 2 0 0 0-2-2H4zm0 1h8a1 1 0 0 1 1 1v12a1 1 0 0 1-1 1H4a1 1 0 0 1-1-1V2a1 1 0 0 1 1-1z">
       </path>
       <path d="M4.603 12.087a.81.81 0 0 1-.438-.42c-.195-.388-.13-.776.08-1.102.198-.307.526-.568.897-.787a7.68 7.68 0 0 1 1.482-.645 19.701 19.701 0 0 0 1.062-2.227 7.269 7.269 0 0 1-.43-1.295c-.086-.4-.119-.796-.046-1.136.075-.354.274-.672.65-.823.192-.077.4-.12.602-.077a.7.7 0 0 1 .477.365c.088.164.12.356.127.538.007.187-.012.395-.047.614-.084.51-.27 1.134-.52 1.794a10.954 10.954 0 0 0 .98 1.686 5.753 5.753 0 0 1 1.334.05c.364.065.734.195.96.465.12.144.193.32.2.518.007.192-.047.382-.138.563a1.04 1.04 0 0 1-.354.416.856.856 0 0 1-.51.138c-.331-.014-.654-.196-.933-.417a5.716 5.716 0 0 1-.911-.95 11.642 11.642 0 0 0-1.997.406 11.311 11.311 0 0 1-1.021 1.51c-.29.35-.608.655-.926.787a.793.793 0 0 1-.58.029zm1.379-1.901c-.166.076-.32.156-.459.238-.328.194-.541.383-.647.547-.094.145-.096.25-.04.361.01.022.02.036.026.044a.27.27 0 0 0 .035-.012c.137-.056.355-.235.635-.572a8.18 8.18 0 0 0 .45-.606zm1.64-1.33a12.647 12.647 0 0 1 1.01-.193 11.666 11.666 0 0 1-.51-.858 20.741 20.741 0 0 1-.5 1.05zm2.446.45c.15.162.296.3.435.41.24.19.407.253.498.256a.107.107 0 0 0 .07-.015.307.307 0 0 0 .094-.125.436.436 0 0 0 .059-.2.095.095 0 0 0-.026-.063c-.052-.062-.2-.152-.518-.209a3.881 3.881 0 0 0-.612-.053zM8.078 5.8a6.7 6.7 0 0 0 .2-.828c.031-.188.043-.343.038-.465a.613.613 0 0 0-.032-.198.517.517 0 0 0-.145.04c-.087.035-.158.106-.196.283-.04.192-.03.469.046.822.024.111.054.227.09.346z">
       </path>
      </svg>
     </a>
     <small class="titlemark">
      6.3.3
     </small>
     <a id="x8-1000006.3.3">
     </a>
     Building a Regression MLP Using the Sequential API
    </h3>
    <p class="noindent">
     Instead of classifying categories, lets try to estimate a
     <alert style="color: #821131;">
      value
     </alert>
     . For this application, we need a different
dataset. Let’s switch back to the California housing problem and tackle it using the same
     <a class="glossary" href="DataScienceIILectureBookli2.html#glo:mlp">
      MLP
     </a>
     as
earlier, with 3 hidden layers composed of 50 neurons each, but this time building it with <span style="color:#054C5C;"><code class="verb">tf.keras</code></span>.
    </p>
    <p class="noindent">
     Using the sequential API to build, train, evaluate, and use a regression MLP is quite similar to what we
did for classification. The main differences in the following code example are the fact that the output
layer has a
     <alert style="color: #821131;">
      single neuron
     </alert>
     (since we only want to predict a single value) and it uses no activation
function, the loss function is the mean squared error, the metric is the RMSE, and we’re using an
Adam optimizer like <span style="color:#054C5C;"><code class="verb">sklearn</code></span>
     s <span style="color:#054C5C;"><code class="verb">MLPRegressor</code></span>
     did.
    </p>
    <p class="noindent">
     In addition, in this example we don’t need a Flatten layer, and instead we’re using a Normalization
layer as the first layer: it does the same thing as <span style="color:#054C5C;"><code class="verb">sklearn</code></span>
     s <span style="color:#054C5C;"><code class="verb">StandardScaler</code></span>, but it must be
fitted to the training data using its <span style="color:#054C5C;"><code class="verb">adapt()</code></span>
     method before you call the model’s <span style="color:#054C5C;"><code class="verb">fit()</code></span>
     method.
    </p>
    <p class="noindent">
     Let’s look at the code:
    </p>
    <pre><div id="fancyvrb160" style="padding:20px;border-radius: 3px;"><a id="x8-100002r381"></a>housing = fetch_california_housing() 
<a id="x8-100004r382"></a>X_train_full, X_test, y_train_full, y_test = train_test_split( 
<a id="x8-100006r383"></a>    housing.data, housing.target, random_state=<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">42</span></span>) 
<a id="x8-100008r384"></a>X_train, X_valid, y_train, y_valid = train_test_split( 
<a id="x8-100010r385"></a>    X_train_full, y_train_full, random_state=<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">42</span></span>)</div></pre>
    <p class="noindent">
    </p>
    <pre><div id="fancyvrb161" style="padding:20px;border-radius: 3px;"><a id="x8-100012r392"></a>tf.random.set_seed(<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">42</span></span>) 
<a id="x8-100014r393"></a>norm_layer = tf.keras.layers.Normalization(input_shape=X_train.shape[<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">1</span></span>:]) 
<a id="x8-100016r394"></a>model = tf.keras.Sequential([ 
<a id="x8-100018r395"></a>    norm_layer, 
<a id="x8-100020r396"></a>    tf.keras.layers.Dense(<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">50</span></span>, activation=<span style="color:#800080;">"relu"</span>), 
<a id="x8-100022r397"></a>    tf.keras.layers.Dense(<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">50</span></span>, activation=<span style="color:#800080;">"relu"</span>), 
<a id="x8-100024r398"></a>    tf.keras.layers.Dense(<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">50</span></span>, activation=<span style="color:#800080;">"relu"</span>), 
<a id="x8-100026r399"></a>    tf.keras.layers.Dense(<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">1</span></span>) 
<a id="x8-100028r400"></a>]) 
<a id="x8-100030r401"></a>optimizer = tf.keras.optimizers.Adam(learning_rate=<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">1e-3</span></span>) 
<a id="x8-100032r402"></a>model.compile(loss=<span style="color:#800080;">"mse"</span>, optimizer=optimizer, metrics=[<span style="color:#800080;">"RootMeanSquaredError"</span>]) 
<a id="x8-100034r403"></a>norm_layer.adapt(X_train) 
<a id="x8-100036r404"></a>history = model.fit(X_train, y_train, epochs=<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">20</span></span>, 
<a id="x8-100038r405"></a>                    validation_data=(X_valid, y_valid)) 
<a id="x8-100040r406"></a>mse_test, rmse_test = model.evaluate(X_test, y_test) 
<a id="x8-100042r407"></a>X_new = X_test[:<span style="color:#2B8554;"><span id="bold" style="font-weight:bold;">3</span></span>] 
<a id="x8-100044r408"></a>y_pred = model.predict(X_new)</div></pre>
    <p class="noindent">
     As you can see, the sequential API is quite clean and straightforward. However, although Sequential
models are extremely common, it is sometimes useful to build neural networks with more complex
topologies, or with multiple inputs or outputs. For this purpose, Keras offers the functional
API.
    </p>
   </article>
   <footer>
    <div class="next">
     <p class="noindent">
      <a href="DataScienceIILectureBookch7.html" style="float: right;">
       Next Chapter →
      </a>
      <a href="DataScienceIILectureBookch5.html" style="float: left;">
       ← Previous Chapter
      </a>
      <a href="index.html">
       TOP
      </a>
     </p>
    </div>
   </footer>
  </div>
  <p class="noindent">
   <a id="tailDataScienceIILectureBookch6.html">
   </a>
  </p>
 </body>
</html>
