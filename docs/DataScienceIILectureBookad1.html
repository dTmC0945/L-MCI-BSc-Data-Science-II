<!DOCTYPE html>
<html lang="en-US" xml:lang="en-US">
 <head>
  <title>
   7 Bibliography
  </title>
    <link rel='icon' type='image/x-icon' href='figures/logo.svg'></link>
  <meta charset="utf-8"/>
  <meta content="ParSnip" name="generator"/>
  <meta content="width=device-width,initial-scale=1" name="viewport"/>
  <link href="DataScienceIILectureBook.css" rel="stylesheet" type="text/css"/>
  <meta content="DataScienceIILectureBook.tex" name="src"/>
  <script async="async" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/mml-chtml.js" type="text/javascript">
  </script>
  <link href="flatweb.css" rel="stylesheet" type="text/css"/>
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.7.1/jquery.min.js">
  </script>
  <script src="flatjs.js">
  </script>
  <link href="http://fonts.googleapis.com/css?family=Roboto" rel="stylesheet" type="text/css"/>
  <script type="module">
   import pdfjsDist from 'https://cdn.jsdelivr.net/npm/pdfjs-dist@5.3.93/+esm'
  </script>
  <script>
   function moveTOC() { 
var htmlShow = document.getElementsByClassName("wide"); 
if (htmlShow[0].style.display === "none") { 
htmlShow[0].style.display = "block"; 
} else { 
htmlShow[0].style.display = "none"; 
} 
}
  </script>
  <link href="https://unpkg.com/nord-highlightjs@0.1.0/dist/nord.css" rel="stylesheet" type="text/css">
   <script src="https://unpkg.com/@highlightjs/cdn-assets@11.3.1/highlight.min.js">
   </script>
   <script src="https://unpkg.com/highlightjs-copy/dist/highlightjs-copy.min.js">
   </script>
   <link href="https://unpkg.com/highlightjs-copy/dist/highlightjs-copy.min.css" rel="stylesheet">
    <script src="flatjs.js">
    </script>
    <header>
    </header>
   </link>
  </link>
 </head>
 <body>
  <!--l. 94-->
  <nav class="wide">
   <div class="contents">
    <h3 style="font-weight:lighter;padding: 20px 0 20px 0;">
     WebBook
    </h3>
    <h2 style="border-bottom: solid 1px #3b4b5e;">
     Bibliography
    </h2>
    <div class="prev-next" style="text-align: center">
     <p class="noindent">
      <a href="DataScienceIILectureBookch7.html" style="float:left; font-size:10px">
       ← PREV CHAPTER
      </a>
     </p>
    </div>
    <div>
     <img src="figures/logo.svg" style="margin: 400px 0 0 0;"/>
    </div>
    <div id="author-info" style="position: absolute; bottom: 0; padding-bottom: 10px;">
     <div id="author-logo">
      <img src="figures/logo.svg" style="margin: 10px 0; width: 75%;"/>
      <div>
       <div>
        <h4 style="color:#7aa0b8; font-weight:lighter;">
         WebBook | D. T. McGuiness, P.hD
        </h4>
       </div>
      </div>
     </div>
    </div>
   </div>
  </nav>
  <div class="crosslinks">
   <p class="noindent">
    <a href="DataScienceIILectureBookli2.html">
     PREV
    </a>
    <a href="DataScienceIILectureBook.html# ">
     TOP
    </a>
   </p>
  </div>
  <div class="page">
   <article class="chapter">
    <h1 class="addchapHead">
     <a id="x11-121000">
     </a>
     Bibliography
    </h1>
    <dl class="thebibliography">
     <dt class="thebibliography" id="X0-millard2011wisdom">
      [1]
     </dt>
     <dd class="thebibliography" id="bib-1">
      <!--l. 94-->
      <p class="noindent">
       <a id="page.544">
       </a>
       <a id="X0-">
       </a>
       W. B. Millard, “The wisdom of crowds, the madness of crowds: Rethinking peer review in
the web era,” Annals of Emergency Medicine, vol. 57, no. 1, A13–A20, 2011.
      </p>
     </dd>
     <dt class="thebibliography" id="X0-heath1993k">
      [2]
     </dt>
     <dd class="thebibliography" id="bib-2">
      <!--l. 94-->
      <p class="noindent">
       D Heath, S Kasif, and S Salzberg, “K-dt: A multi-tree learning method,” in Proc. of the
Second Int. Workshop on Multistrategy Learning, 1993, pp. 138–149.
      </p>
     </dd>
     <dt class="thebibliography" id="X0-ho1995random">
      [3]
     </dt>
     <dd class="thebibliography" id="bib-3">
      <!--l. 94-->
      <p class="noindent">
       T. K. Ho, “Random decision forests,” in Proceedings of 3rd international conference on
document analysis and recognition, IEEE, vol. 1, 1995, pp. 278–282.
      </p>
     </dd>
     <dt class="thebibliography" id="X0-ho1998random">
      [4]
     </dt>
     <dd class="thebibliography" id="bib-4">
      <!--l. 94-->
      <p class="noindent">
       T. K.  Ho,  “The  random  subspace  method  for  constructing  decision  forests,”  IEEE
transactions on pattern analysis and machine intelligence, vol. 20, no. 8, pp. 832–844,
1998.
      </p>
     </dd>
     <dt class="thebibliography" id="X0-zhou2025ensemble">
      [5]
     </dt>
     <dd class="thebibliography" id="bib-5">
      <!--l. 94-->
      <p class="noindent">
       Z.-H. Zhou, Ensemble methods: foundations and algorithms. CRC press, 2025.
      </p>
     </dd>
     <dt class="thebibliography" id="X0-geurts2006extremely">
      [6]
     </dt>
     <dd class="thebibliography" id="bib-6">
      <!--l. 94-->
      <p class="noindent">
       P. Geurts, D. Ernst, and L. Wehenkel, “Extremely randomized trees,” Machine learning,
vol. 63, no. 1, pp. 3–42, 2006.
      </p>
     </dd>
     <dt class="thebibliography" id="X0-bellman1957dynamic">
      [7]
     </dt>
     <dd class="thebibliography" id="bib-7">
      <!--l. 94-->
      <p class="noindent">
       R. Bellman, “Dynamic programming princeton university press,” Princeton, NJ, pp. 4–9,
1957.
      </p>
     </dd>
     <dt class="thebibliography" id="X0-izenman2012introduction">
      [8]
     </dt>
     <dd class="thebibliography" id="bib-8">
      <!--l. 94-->
      <p class="noindent">
       A. J.  Izenman,  “Introduction  to  manifold  learning,”  Wiley  Interdisciplinary  Reviews:
Computational Statistics, vol. 4, no. 5, pp. 439–446, 2012.
      </p>
     </dd>
     <dt class="thebibliography" id="X0-nayak2021comprehensive">
      [9]
     </dt>
     <dd class="thebibliography" id="bib-9">
      <!--l. 94-->
      <p class="noindent">
       R. Nayak, U. C. Pati, and S. K. Das, “A comprehensive review on deep learning-based
methods for video anomaly detection,” Image and Vision Computing, vol. 106, p. 104 078,
2021.
      </p>
     </dd>
     <dt class="thebibliography" id="X0-matouvsek2008variants">
      [10]
     </dt>
     <dd class="thebibliography" id="bib-10">
      <!--l. 94-->
      <p class="noindent">
       J. Matou
       <!--l. 94-->
       <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
        <mi>
         v
        </mi>
       </math>
       sek,
“On  variants  of  the  johnson–lindenstrauss  lemma,”  Random  Structures  &amp;  Algorithms,
vol. 33, no. 2, pp. 142–156, 2008.
      </p>
     </dd>
     <dt class="thebibliography" id="X0-zhiqiang2017review">
      [11]
     </dt>
     <dd class="thebibliography" id="bib-11">
      <!--l. 94-->
      <p class="noindent">
       W. Zhiqiang and L. Jun, “A review of object detection based on convolutional neural
network,” in 2017 36th Chinese control conference (CCC), IEEE, 2017, pp. 11 104–11 109.
      </p>
     </dd>
     <dt class="thebibliography" id="X0-kansal2018customer">
      [12]
     </dt>
     <dd class="thebibliography" id="bib-12">
      <!--l. 94-->
      <p class="noindent">
       T. Kansal, S. Bahuguna, V. Singh, and T. Choudhury, “Customer segmentation using
k-means  clustering,”  in  2018  international  conference  on  computational  techniques,
electronics and mechanical systems (CTEMS), IEEE, 2018, pp. 135–139.
      </p>
     </dd>
     <dt class="thebibliography" id="X0-kumari2016anomaly">
      [13]
     </dt>
     <dd class="thebibliography" id="bib-13">
      <!--l. 94-->
      <p class="noindent">
       R. Kumari, M. Singh, R Jha, N. Singh, et al., “Anomaly detection in network traffic using
k-mean clustering,” in 2016 3rd international conference on recent advances in information
technology (RAIT), IEEE, 2016, pp. 387–393.
      </p>
     </dd>
     <dt class="thebibliography" id="X0-bair2013semi">
      [14]
     </dt>
     <dd class="thebibliography" id="bib-14">
      <!--l. 94-->
      <p class="noindent">
       E.   Bair,   “Semi-supervised   clustering   methods,”   Wiley   Interdisciplinary   Reviews:
Computational Statistics, vol. 5, no. 5, pp. 349–361, 2013.
      </p>
     </dd>
     <dt class="thebibliography" id="X0-zamir1999clustering">
      [15]
     </dt>
     <dd class="thebibliography" id="bib-15">
      <!--l. 94-->
      <p class="noindent">
       O. E. Zamir, Clustering web documents: a phrase-based method for grouping search engine
results. University of Washington, 1999.
      </p>
     </dd>
     <dt class="thebibliography" id="X0-burney2014k">
      [16]
     </dt>
     <dd class="thebibliography" id="bib-16">
      <!--l. 94-->
      <p class="noindent">
       S. A.  Burney  and  H.  Tariq,  “K-means  cluster  analysis  for  image  segmentation,”
International Journal of Computer Applications, vol. 96, no. 4, 2014.
      </p>
     </dd>
     <dt class="thebibliography" id="X0-lloyd1982least">
      [17]
     </dt>
     <dd class="thebibliography" id="bib-17">
      <!--l. 94-->
      <p class="noindent">
       S. Lloyd, “Least squares quantization in pcm,” IEEE transactions on information theory,
vol. 28, no. 2, pp. 129–137, 1982.
      </p>
     </dd>
     <dt class="thebibliography" id="X0-forgy1965cluster">
      [18]
     </dt>
     <dd class="thebibliography" id="bib-18">
      <!--l. 94-->
      <p class="noindent">
       E. W. Forgy, “Cluster analysis of multivariate data: Efficiency versus interpretability of
classifications,” biometrics, vol. 21, pp. 768–769, 1965.
      </p>
     </dd>
     <dt class="thebibliography" id="X0-arthur2006k">
      [19]
     </dt>
     <dd class="thebibliography" id="bib-19">
      <!--l. 94-->
      <p class="noindent">
       D. Arthur and S. Vassilvitskii, “K-means++: The advantages of careful seeding,” Stanford,
Tech. Rep., 2006.
      </p>
     </dd>
     <dt class="thebibliography" id="X0-elkan2003using">
      [20]
     </dt>
     <dd class="thebibliography" id="bib-20">
      <!--l. 94-->
      <p class="noindent">
       C. Elkan, “Using the triangle inequality to accelerate k-means,” in Proceedings of the 20th
international conference on Machine Learning (ICML-03), 2003, pp. 147–153.
      </p>
     </dd>
     <dt class="thebibliography" id="X0-sculley2010web">
      [21]
     </dt>
     <dd class="thebibliography" id="bib-21">
      <!--l. 94-->
      <p class="noindent">
       D.  Sculley,  “Web-scale  k-means  clustering,”  in  Proceedings  of  the  19th  international
conference on World wide web, 2010, pp. 1177–1178.
      </p>
     </dd>
     <dt class="thebibliography" id="X0-liu2008isolation">
      [22]
     </dt>
     <dd class="thebibliography" id="bib-22">
      <!--l. 94-->
      <p class="noindent">
       F. T. Liu, K. M. Ting, and Z.-H. Zhou, “Isolation forest,” in 2008 eighth ieee international
conference on data mining, IEEE, 2008, pp. 413–422.
      </p>
     </dd>
     <dt class="thebibliography" id="X0-BBC2019">
      [23]
     </dt>
     <dd class="thebibliography" id="bib-23">
      <!--l. 94-->
      <p class="noindent">
       BBC,  How  a  kingfisher  helped  reshape  japan’s  bullet  train,  2019.  [Online].  Available:
       <a class="url" href="https://www.bbc.com/news/av/science-environment-47673287">
        https://www.bbc.com/news/av/science-environment-47673287
       </a>.
      </p>
     </dd>
     <dt class="thebibliography" id="X0-vincent2006biomimetics">
      [24]
     </dt>
     <dd class="thebibliography" id="bib-24">
      <!--l. 94-->
      <p class="noindent">
       J. F.  Vincent,  O. A.  Bogatyreva,  N. R.  Bogatyrev,  A.  Bowyer,  and  A.-K.  Pahl,
“Biomimetics: Its practice and theory,” Journal of the Royal Society Interface, vol. 3, no. 9,
pp. 471–482, 2006.
      </p>
     </dd>
     <dt class="thebibliography" id="X0-agatonovic2000basic">
      [25]
     </dt>
     <dd class="thebibliography" id="bib-25">
      <!--l. 94-->
      <p class="noindent">
       S Agatonovic-Kustrin and R. Beresford, “Basic concepts of artificial neural network (ann)
modeling and its application in pharmaceutical research,” Journal of pharmaceutical and
biomedical analysis, vol. 22, no. 5, pp. 717–727, 2000.
      </p>
     </dd>
     <dt class="thebibliography" id="X0-holcomb2018overview">
      [26]
     </dt>
     <dd class="thebibliography" id="bib-26">
      <!--l. 94-->
      <p class="noindent">
       S. D. Holcomb, W. K. Porter, S. V. Ault, G. Mao, and J. Wang, “Overview on deepmind
and its alphago zero ai,” in Proceedings of the 2018 international conference on big data
and education, 2018, pp. 67–71.
      </p>
     </dd>
     <dt class="thebibliography" id="X0-mcculloch1943logical">
      [27]
     </dt>
     <dd class="thebibliography" id="bib-27">
      <!--l. 94-->
      <p class="noindent">
       W. S. McCulloch and W. Pitts, “A logical calculus of the ideas immanent in nervous
activity,” The bulletin of mathematical biophysics, vol. 5, pp. 115–133, 1943.
      </p>
     </dd>
     <dt class="thebibliography" id="X0-howe2007artificial">
      [28]
     </dt>
     <dd class="thebibliography" id="bib-28">
      <!--l. 94-->
      <p class="noindent">
       J. Howe, “Artificial intelligence at edinburgh university: A perspective,” Archived from the
original on, vol. 17, 2007.
      </p>
     </dd>
     <dt class="thebibliography" id="X0-goodfellow2014qualitatively">
      [29]
     </dt>
     <dd class="thebibliography" id="bib-29">
      <!--l. 94-->
      <p class="noindent">
       I. J. Goodfellow, O. Vinyals, and A. M. Saxe, “Qualitatively characterizing neural network
optimization problems,” arXiv preprint arXiv:1412.6544, 2014.
      </p>
     </dd>
     <dt class="thebibliography" id="X0-sebe2023stepwise">
      [30]
     </dt>
     <dd class="thebibliography" id="bib-30">
      <!--l. 94-->
      <p class="noindent">
       A. Sebé-Pedrós, “Stepwise emergence of the neuronal gene expression program in early
animal evolution,” 2023.
      </p>
     </dd>
     <dt class="thebibliography" id="X0-barrett2019analyzing">
      [31]
     </dt>
     <dd class="thebibliography" id="bib-31">
      <!--l. 94-->
      <p class="noindent">
       D. G. Barrett, A. S. Morcos, and J. H. Macke, “Analyzing biological and artificial neural
networks: Challenges with opportunities for synergy?” Current opinion in neurobiology,
vol. 55, pp. 55–64, 2019.
      </p>
     </dd>
     <dt class="thebibliography" id="X0-block1962perceptron">
      [32]
     </dt>
     <dd class="thebibliography" id="bib-32">
      <!--l. 94-->
      <p class="noindent">
       H.-D.  Block,  “The  perceptron:  A  model  for  brain  functioning.  i,”  Reviews of Modern
Physics, vol. 34, no. 1, p. 123, 1962.
      </p>
     </dd>
     <dt class="thebibliography" id="X0-sharma2017activation">
      [33]
     </dt>
     <dd class="thebibliography" id="bib-33">
      <!--l. 94-->
      <p class="noindent">
       S. Sharma, S. Sharma, and A. Athaiya, “Activation functions in neural networks,” Towards
Data Sci, vol. 6, no. 12, pp. 310–316, 2017.
      </p>
     </dd>
     <dt class="thebibliography" id="X0-fisher1936use">
      [34]
     </dt>
     <dd class="thebibliography" id="bib-34">
      <!--l. 94-->
      <p class="noindent">
       R. A.  Fisher,  “The  use  of  multiple  measurements  in  taxonomic  problems,”  Annals of
eugenics, vol. 7, no. 2, pp. 179–188, 1936.
      </p>
     </dd>
     <dt class="thebibliography" id="X0-sompolinsky2006theory">
      [35]
     </dt>
     <dd class="thebibliography" id="bib-35">
      <!--l. 94-->
      <p class="noindent">
       H.  Sompolinsky,  “The  theory  of  neural  networks:  The  hebb  rule  and  beyond,”  in
Heidelberg Colloquium on Glassy Dynamics: Proceedings of a Colloquium on Spin Glasses,
Optimization and Neural Networks Held at the University of Heidelberg June 9–13, 1986,
Springer, 2006, pp. 485–527.
      </p>
     </dd>
     <dt class="thebibliography" id="X0-hebb2005organization">
      [36]
     </dt>
     <dd class="thebibliography" id="bib-36">
      <!--l. 94-->
      <p class="noindent">
       D. O. Hebb, The organization of behavior: A neuropsychological theory. Psychology press,
2005.
      </p>
     </dd>
     <dt class="thebibliography" id="X0-gerstner2002mathematical">
      [37]
     </dt>
     <dd class="thebibliography" id="bib-37">
      <!--l. 94-->
      <p class="noindent">
       W.  Gerstner  and  W. M.  Kistler,  “Mathematical  formulations  of  hebbian  learning,”
Biological cybernetics, vol. 87, no. 5, pp. 404–415, 2002.
      </p>
     </dd>
     <dt class="thebibliography" id="X0-popescu2009multilayer">
      [38]
     </dt>
     <dd class="thebibliography" id="bib-38">
      <!--l. 94-->
      <p class="noindent">
       M.-C.  Popescu,  V. E.  Balas,  L.  Perescu-Popescu,  and  N.  Mastorakis,  “Multilayer
perceptron and neural networks,” WSEAS Transactions on Circuits and Systems, vol. 8,
no. 7, pp. 579–588, 2009.
      </p>
     </dd>
     <dt class="thebibliography" id="X0-bebis1994feed">
      [39]
     </dt>
     <dd class="thebibliography" id="bib-39">
      <!--l. 94-->
      <p class="noindent">
       G. Bebis and M. Georgiopoulos, “Feed-forward neural networks,” Ieee Potentials, vol. 13,
no. 4, pp. 27–31, 1994.
      </p>
     </dd>
     <dt class="thebibliography" id="X0-samek2021explaining">
      [40]
     </dt>
     <dd class="thebibliography" id="bib-40">
      <!--l. 94-->
      <p class="noindent">
       W. Samek, G. Montavon, S. Lapuschkin, C. J. Anders, and K.-R. Müller, “Explaining
deep neural networks and beyond: A review of methods and applications,” Proceedings of
the IEEE, vol. 109, no. 3, pp. 247–278, 2021.
      </p>
     </dd>
     <dt class="thebibliography" id="X0-Seppo1970">
      [41]
     </dt>
     <dd class="thebibliography" id="bib-41">
      <!--l. 94-->
      <p class="noindent">
       S.                     Linnainmaa,                     “Algoritmin                     kumulatiivinen
pyo¨ristysvirhe  yksitta¨isten  pyo¨ristysvirheiden  taylor-kehitelma¨na¨,”  Available  in
Finnish at
       <a class="url" href="https://people.idsia.ch/~juergen/linnainmaa1970thesis.pdf">
        https://people.idsia.ch/~juergen/linnainmaa1970thesis.pdf
       </a>, Master’s
thesis, University of Helsinki, 1970.
      </p>
     </dd>
     <dt class="thebibliography" id="X0-hecht1992theory">
      [42]
     </dt>
     <dd class="thebibliography" id="bib-42">
      <!--l. 94-->
      <p class="noindent">
       R. Hecht-Nielsen, “Theory of the backpropagation neural network,” in Neural networks for
perception, Elsevier, 1992, pp. 65–93.
      </p>
     </dd>
     <dt class="thebibliography" id="X0-rumelhart1986learning">
      [43]
     </dt>
     <dd class="thebibliography" id="bib-43">
      <!--l. 94-->
      <p class="noindent">
       D. E.  Rumelhart,  G. E.  Hinton,  and  R. J.  Williams,  “Learning  representations  by
back-propagating errors,” nature, vol. 323, no. 6088, pp. 533–536, 1986.
      </p>
     </dd>
     <dt class="thebibliography" id="X0-knut2018neural">
      [44]
     </dt>
     <dd class="thebibliography" id="bib-44">
      <!--l. 94-->
      <p class="noindent">
       H. Knut, “Neural networks p. 7,” University of Applied Sciences Northwestern Switzerland,
2018.
      </p>
     </dd>
     <dt class="thebibliography" id="X0-hendrycks2016gaussian">
      [45]
     </dt>
     <dd class="thebibliography" id="bib-45">
      <!--l. 94-->
      <p class="noindent">
       D.  Hendrycks  and  K.  Gimpel,  “Gaussian  error  linear  units  (gelus),”  arXiv  preprint
arXiv:1606.08415, 2016.
      </p>
     </dd>
     <dt class="thebibliography" id="X0-hinton2012deep">
      [46]
     </dt>
     <dd class="thebibliography" id="bib-46">
      <!--l. 94-->
      <p class="noindent">
       G. Hinton et al., “Deep neural networks for acoustic modeling in speech recognition: The
shared views of four research groups,” IEEE Signal processing magazine, vol. 29, no. 6,
pp. 82–97, 2012.
      </p>
     </dd>
     <dt class="thebibliography" id="X0-krizhevsky2012imagenet">
      [47]
     </dt>
     <dd class="thebibliography" id="bib-47">
      <!--l. 94-->
      <p class="noindent">
       A.  Krizhevsky,  I.  Sutskever,  and  G. E.  Hinton,  “Imagenet  classification  with  deep
convolutional  neural  networks,”  Advances  in  neural  information  processing  systems,
vol. 25, 2012.
      </p>
     </dd>
     <dt class="thebibliography" id="X0-zarandy2015overview">
      [48]
     </dt>
     <dd class="thebibliography" id="bib-48">
      <!--l. 94-->
      <p class="noindent">
       Á. Zarándy, C. Rekeczky, P. Szolgay, and L. O. Chua, “Overview of cnn research: 25
years history and the current trends,” in 2015 IEEE International Symposium on Circuits
and Systems (ISCAS), IEEE, 2015, pp. 401–404.
      </p>
     </dd>
     <dt class="thebibliography" id="X0-huang2020combination">
      [49]
     </dt>
     <dd class="thebibliography" id="bib-49">
      <!--l. 94-->
      <p class="noindent">
       Z.  Huang  and  W.  Zhao,  “Combination  of  elmo  representation  and  cnn  approaches  to
enhance service discovery,” IEEE Access, vol. 8, pp. 130 782–130 796, 2020.
      </p>
     </dd>
     <dt class="thebibliography" id="X0-li2021hybrid">
      [50]
     </dt>
     <dd class="thebibliography" id="bib-50">
      <!--l. 94-->
      <p class="noindent">
       Q. Li, X. Li, B. Lee, and J. Kim, “A hybrid cnn-based review helpfulness filtering model
for  improving  e-commerce  recommendation  service,”  Applied Sciences,  vol. 11,  no. 18,
p. 8613, 2021.
      </p>
     </dd>
     <dt class="thebibliography" id="X0-ouyang2019deep">
      [51]
     </dt>
     <dd class="thebibliography" id="bib-51">
      <!--l. 94-->
      <p class="noindent">
       Z.  Ouyang,  J.  Niu,  Y.  Liu,  and  M.  Guizani,  “Deep  cnn-based  real-time  traffic  light
detector for self-driving vehicles,” IEEE transactions on Mobile Computing, vol. 19, no. 2,
pp. 300–313, 2019.
      </p>
     </dd>
     <dt class="thebibliography" id="X0-nugraha2017towards">
      [52]
     </dt>
     <dd class="thebibliography" id="bib-52">
      <!--l. 94-->
      <p class="noindent">
       B. T.  Nugraha,  S.-F.  Su,  et  al.,  “Towards  self-driving  car  using  convolutional  neural
network and road lane detector,” in 2017 2nd international conference on automation,
cognitive  science,  optics,  micro  electro-mechanical  system,  and  information  technology
(ICACOMIT), IEEE, 2017, pp. 65–69.
      </p>
     </dd>
     <dt class="thebibliography" id="X0-ye2015evaluating">
      [53]
     </dt>
     <dd class="thebibliography" id="bib-53">
      <!--l. 94-->
      <p class="noindent">
       H. Ye, Z. Wu, R.-W. Zhao, X. Wang, Y.-G. Jiang, and X. Xue, “Evaluating two-stream
cnn for video classification,” in Proceedings of the 5th ACM on International Conference
on Multimedia Retrieval, 2015, pp. 435–442.
      </p>
     </dd>
     <dt class="thebibliography" id="X0-hubel1962receptive">
      [54]
     </dt>
     <dd class="thebibliography" id="bib-54">
      <!--l. 94-->
      <p class="noindent">
       D. H. Hubel and T. N. Wiesel, “Receptive fields, binocular interaction and functional
architecture in the cat’s visual cortex,” The Journal of physiology, vol. 160, no. 1, p. 106,
1962.
      </p>
     </dd>
     <dt class="thebibliography" id="X0-fukushima1980neocognitron">
      [55]
     </dt>
     <dd class="thebibliography" id="bib-55">
      <!--l. 94-->
      <p class="noindent">
       K. Fukushima, “Neocognitron: A self-organizing neural network model for a mechanism of
pattern recognition unaffected by shift in position,” Biological cybernetics, vol. 36, no. 4,
pp. 193–202, 1980.
      </p>
     </dd>
     <dt class="thebibliography" id="X0-lecun1998gradient">
      [56]
     </dt>
     <dd class="thebibliography" id="bib-56">
      <!--l. 94-->
      <p class="noindent">
       Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner, “Gradient-based learning applied to
document recognition,” Proceedings of the IEEE, vol. 86, no. 11, pp. 2278–2324, 1998.
      </p>
     </dd>
     <dt class="thebibliography" id="X0-Alto2020">
      [57]
     </dt>
     <dd class="thebibliography" id="bib-57">
      <!--l. 94-->
      <p class="noindent">
       V. Alto, Data augmentation in deep learning, 2020. [Online]. Available:
       <a class="url" href="https://medium.com/analytics-vidhya/data-augmentation-in-deep-learning-3d7a539f7a28">
        https://medium.com/analytics-vidhya/data-augmentation-in-deep-learning-3d7a539f7a28
       </a>.
      </p>
     </dd>
     <dt class="thebibliography" id="X0-deco2005neurodynamics">
      [58]
     </dt>
     <dd class="thebibliography" id="bib-58">
      <!--l. 94-->
      <p class="noindent">
       G.  Deco  and  E. T.  Rolls,  “Neurodynamics  of  biased  competition  and  cooperation  for
attention:  A  model  with  spiking  neurons,”  Journal of neurophysiology,  vol. 94,  no. 1,
pp. 295–313, 2005.
      </p>
     </dd>
     <dt class="thebibliography" id="X0-zeiler2014visualizing">
      [59]
     </dt>
     <dd class="thebibliography" id="bib-59">
      <!--l. 94-->
      <p class="noindent">
       M. Zeiler, “Visualizing and understanding convolutional networks,” in European conference
on computer vision/arXiv, vol. 1311, 2014.
      </p>
     </dd>
     <dt class="thebibliography" id="X0-szegedy2015going">
      [60]
     </dt>
     <dd class="thebibliography" id="bib-60">
      <!--l. 94-->
      <p class="noindent">
       C. Szegedy et al., “Going deeper with convolutions,” in Proceedings of the IEEE conference
on computer vision and pattern recognition, 2015, pp. 1–9.
      </p>
     </dd>
     <dt class="thebibliography" id="X0-xia2017inception">
      [61]
     </dt>
     <dd class="thebibliography" id="bib-61">
      <!--l. 94-->
      <p class="noindent">
       X.   Xia,   C.   Xu,   and   B.   Nan,   “Inception-v3   for   flower   classification,”   in   2017
2nd  international  conference  on  image,  vision  and  computing  (ICIVC),  IEEE,  2017,
pp. 783–787.
      </p>
     </dd>
     <dt class="thebibliography" id="X0-szegedy2017inception">
      [62]
     </dt>
     <dd class="thebibliography" id="bib-62">
      <!--l. 94-->
      <p class="noindent">
       C. Szegedy, S. Ioffe, V. Vanhoucke, and A. Alemi, “Inception-v4, inception-resnet and the
impact of residual connections on learning,” in Proceedings of the AAAI conference on
artificial intelligence, vol. 31, 2017.
      </p>
     </dd>
     <dt class="thebibliography" id="X0-karpathy2014large">
      [63]
     </dt>
     <dd class="thebibliography" id="bib-63">
      <!--l. 94-->
      <p class="noindent">
       A. Karpathy, G. Toderici, S. Shetty, T. Leung, R. Sukthankar, and L. Fei-Fei, “Large-scale
video  classification  with  convolutional  neural  networks,”  in  Proceedings  of  the  IEEE
conference on Computer Vision and Pattern Recognition, 2014, pp. 1725–1732.
      </p>
     </dd>
     <dt class="thebibliography" id="X0-he2016deep">
      [64]
     </dt>
     <dd class="thebibliography" id="bib-64">
      <!--l. 94-->
      <p class="noindent">
       K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image recognition,”
in Proceedings of the IEEE conference on computer vision and pattern recognition, 2016,
pp. 770–778.
      </p>
     </dd>
     <dt class="thebibliography" id="X0-liu2021rethinking">
      [65]
     </dt>
     <dd class="thebibliography" id="bib-65">
      <!--l. 94-->
      <p class="noindent">
       F. Liu, X. Ren, Z. Zhang, X. Sun, and Y. Zou, “Rethinking skip connection with layer
normalization in transformers and resnets,” arXiv preprint arXiv:2105.07205, 2021.
      </p>
     </dd>
     <dt class="thebibliography" id="X0-han2021pre">
      [66]
     </dt>
     <dd class="thebibliography" id="bib-66">
      <!--l. 94-->
      <p class="noindent">
       X. Han et al., “Pre-trained models: Past, present and future,” AI Open, vol. 2, pp. 225–250,
2021.
      </p>
     </dd>
    </dl>
   </article>
  </div>
  <!--l. 94-->
  <div class="crosslinks">
   <p class="noindent">
    <a href="DataScienceIILectureBookli2.html">
     PREV
    </a>
    <a href="DataScienceIILectureBook.html# ">
     TOP
    </a>
   </p>
  </div>
  <!--l. 94-->
  <p class="noindent">
   <a id="tailDataScienceIILectureBookad1.html">
   </a>
  </p>
 </body>
</html>
